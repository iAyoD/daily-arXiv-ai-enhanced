{"id": "2512.09065", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09065", "abs": "https://arxiv.org/abs/2512.09065", "authors": ["Shivendra Agrawal", "Jake Brawer", "Ashutosh Naik", "Alessandro Roncone", "Bradley Hayes"], "title": "ShelfAware: Real-Time Visual-Inertial Semantic Localization in Quasi-Static Environments with Low-Cost Sensors", "comment": "8 pages", "summary": "Many indoor workspaces are quasi-static: global layout is stable but local semantics change continually, producing repetitive geometry, dynamic clutter, and perceptual noise that defeat vision-based localization. We present ShelfAware, a semantic particle filter for robust global localization that treats scene semantics as statistical evidence over object categories rather than fixed landmarks. ShelfAware fuses a depth likelihood with a category-centric semantic similarity and uses a precomputed bank of semantic viewpoints to perform inverse semantic proposals inside MCL, yielding fast, targeted hypothesis generation on low-cost, vision-only hardware. Across 100 global-localization trials spanning four conditions (cart-mounted, wearable, dynamic obstacles, and sparse semantics) in a semantically dense, retail environment, ShelfAware achieves a 96% success rate (vs. 22% MCL and 10% AMCL) with a mean time-to-convergence of 1.91s, attains the lowest translational RMSE in all conditions, and maintains stable tracking in 80% of tested sequences, all while running in real time on a consumer laptop-class platform. By modeling semantics distributionally at the category level and leveraging inverse proposals, ShelfAware resolves geometric aliasing and semantic drift common to quasi-static domains. Because the method requires only vision sensors and VIO, it integrates as an infrastructure-free building block for mobile robots in warehouses, labs, and retail settings; as a representative application, it also supports the creation of assistive devices providing start-anytime, shared-control assistive navigation for people with visual impairments.", "AI": {"tldr": "ShelfAware\u662f\u4e00\u4e2a\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7\u5c06\u573a\u666f\u8bed\u4e49\u4f5c\u4e3a\u7c7b\u522b\u7ea7\u7edf\u8ba1\u8bc1\u636e\u800c\u975e\u56fa\u5b9a\u5730\u6807\uff0c\u89e3\u51b3\u4e86\u51c6\u9759\u6001\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u5b9a\u4f4d\u95ee\u9898\uff0c\u5728\u96f6\u552e\u73af\u5883\u4e2d\u5b9e\u73b0\u4e8696%\u7684\u6210\u529f\u7387\u548c1.91\u79d2\u7684\u5e73\u5747\u6536\u655b\u65f6\u95f4\u3002", "motivation": "\u8bb8\u591a\u5ba4\u5185\u5de5\u4f5c\u7a7a\u95f4\u662f\u51c6\u9759\u6001\u7684\uff1a\u5168\u5c40\u5e03\u5c40\u7a33\u5b9a\u4f46\u5c40\u90e8\u8bed\u4e49\u4e0d\u65ad\u53d8\u5316\uff0c\u4ea7\u751f\u91cd\u590d\u51e0\u4f55\u3001\u52a8\u6001\u6742\u4e71\u548c\u611f\u77e5\u566a\u58f0\uff0c\u8fd9\u5bfc\u81f4\u57fa\u4e8e\u89c6\u89c9\u7684\u5b9a\u4f4d\u65b9\u6cd5\u5931\u6548\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u8bed\u4e49\u53d8\u5316\u548c\u51e0\u4f55\u6df7\u6dc6\u7684\u9c81\u68d2\u5b9a\u4f4d\u65b9\u6cd5\u3002", "method": "ShelfAware\u662f\u4e00\u4e2a\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u5c06\u573a\u666f\u8bed\u4e49\u89c6\u4e3a\u5bf9\u8c61\u7c7b\u522b\u7684\u7edf\u8ba1\u8bc1\u636e\u800c\u975e\u56fa\u5b9a\u5730\u6807\u3002\u5b83\u878d\u5408\u6df1\u5ea6\u4f3c\u7136\u548c\u7c7b\u522b\u4e2d\u5fc3\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u4f7f\u7528\u9884\u8ba1\u7b97\u7684\u8bed\u4e49\u89c6\u70b9\u5e93\u5728MCL\u5185\u6267\u884c\u9006\u8bed\u4e49\u63d0\u8bae\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u6709\u9488\u5bf9\u6027\u7684\u5047\u8bbe\u751f\u6210\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u89c6\u89c9\u4f20\u611f\u5668\u548cVIO\u3002", "result": "\u5728\u5305\u542b\u56db\u79cd\u6761\u4ef6\uff08\u63a8\u8f66\u5b89\u88c5\u3001\u53ef\u7a7f\u6234\u3001\u52a8\u6001\u969c\u788d\u548c\u7a00\u758f\u8bed\u4e49\uff09\u7684100\u6b21\u5168\u5c40\u5b9a\u4f4d\u8bd5\u9a8c\u4e2d\uff0cShelfAware\u5b9e\u73b0\u4e8696%\u7684\u6210\u529f\u7387\uff08\u5bf9\u6bd4MCL\u768422%\u548cAMCL\u768410%\uff09\uff0c\u5e73\u5747\u6536\u655b\u65f6\u95f4\u4e3a1.91\u79d2\uff0c\u5728\u6240\u6709\u6761\u4ef6\u4e0b\u83b7\u5f97\u6700\u4f4e\u7684\u5e73\u79fbRMSE\uff0c\u5e76\u572880%\u7684\u6d4b\u8bd5\u5e8f\u5217\u4e2d\u4fdd\u6301\u7a33\u5b9a\u8ddf\u8e2a\uff0c\u540c\u65f6\u5728\u6d88\u8d39\u7ea7\u7b14\u8bb0\u672c\u7535\u8111\u5e73\u53f0\u4e0a\u5b9e\u65f6\u8fd0\u884c\u3002", "conclusion": "\u901a\u8fc7\u5728\u7c7b\u522b\u7ea7\u522b\u5bf9\u8bed\u4e49\u8fdb\u884c\u5206\u5e03\u5efa\u6a21\u5e76\u5229\u7528\u9006\u63d0\u8bae\uff0cShelfAware\u89e3\u51b3\u4e86\u51c6\u9759\u6001\u9886\u57df\u4e2d\u5e38\u89c1\u7684\u51e0\u4f55\u6df7\u6dc6\u548c\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u4ec5\u9700\u89c6\u89c9\u4f20\u611f\u5668\u548cVIO\uff0c\u53ef\u4f5c\u4e3a\u65e0\u57fa\u7840\u8bbe\u65bd\u7684\u79fb\u52a8\u673a\u5668\u4eba\u6784\u5efa\u6a21\u5757\uff0c\u5e94\u7528\u4e8e\u4ed3\u5e93\u3001\u5b9e\u9a8c\u5ba4\u548c\u96f6\u552e\u73af\u5883\uff0c\u4e5f\u53ef\u652f\u6301\u4e3a\u89c6\u969c\u4eba\u58eb\u521b\u5efa\u968f\u65f6\u542f\u52a8\u3001\u5171\u4eab\u63a7\u5236\u7684\u8f85\u52a9\u5bfc\u822a\u8bbe\u5907\u3002"}}
{"id": "2512.09086", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09086", "abs": "https://arxiv.org/abs/2512.09086", "authors": ["Xinyu Qi", "Zeyu Deng", "Shaun Alexander Macdonald", "Liying Li", "Chen Wang", "Muhammad Ali Imran", "Philip G. Zhao"], "title": "Inferring Operator Emotions from a Motion-Controlled Robotic Arm", "comment": null, "summary": "A remote robot operator's affective state can significantly impact the resulting robot's motions leading to unexpected consequences, even when the user follows protocol and performs permitted tasks. The recognition of a user operator's affective states in remote robot control scenarios is, however, underexplored. Current emotion recognition methods rely on reading the user's vital signs or body language, but the devices and user participation these measures require would add limitations to remote robot control. We demonstrate that the functional movements of a remote-controlled robotic avatar, which was not designed for emotional expression, can be used to infer the emotional state of the human operator via a machine-learning system. Specifically, our system achieved 83.3$\\%$ accuracy in recognizing the user's emotional state expressed by robot movements, as a result of their hand motions. We discuss the implications of this system on prominent current and future remote robot operation and affective robotic contexts.", "AI": {"tldr": "\u901a\u8fc7\u8fdc\u7a0b\u63a7\u5236\u673a\u5668\u4eba\u7684\u529f\u80fd\u8fd0\u52a8\uff08\u975e\u60c5\u611f\u8868\u8fbe\u8bbe\u8ba1\uff09\u6765\u63a8\u65ad\u64cd\u4f5c\u8005\u60c5\u611f\u72b6\u6001\uff0c\u51c6\u786e\u7387\u8fbe83.3%", "motivation": "\u8fdc\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u8005\u7684\u60c5\u611f\u72b6\u6001\u4f1a\u663e\u8457\u5f71\u54cd\u673a\u5668\u4eba\u8fd0\u52a8\u5e76\u5bfc\u81f4\u610f\u5916\u540e\u679c\uff0c\u4f46\u5f53\u524d\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u751f\u7406\u4fe1\u53f7\u6216\u8eab\u4f53\u8bed\u8a00\uff0c\u8fd9\u4e9b\u5728\u8fdc\u7a0b\u63a7\u5236\u573a\u666f\u4e2d\u4f1a\u5e26\u6765\u9650\u5236", "method": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5206\u6790\u8fdc\u7a0b\u63a7\u5236\u673a\u5668\u4eba\uff08\u975e\u60c5\u611f\u8868\u8fbe\u8bbe\u8ba1\uff09\u7684\u529f\u80fd\u6027\u8fd0\u52a8\uff0c\u901a\u8fc7\u64cd\u4f5c\u8005\u7684\u624b\u90e8\u52a8\u4f5c\u6765\u63a8\u65ad\u5176\u60c5\u611f\u72b6\u6001", "result": "\u7cfb\u7edf\u5728\u8bc6\u522b\u7528\u6237\u901a\u8fc7\u673a\u5668\u4eba\u8fd0\u52a8\u8868\u8fbe\u7684\u60c5\u611f\u72b6\u6001\u65b9\u9762\u8fbe\u523083.3%\u7684\u51c6\u786e\u7387", "conclusion": "\u5373\u4f7f\u662f\u529f\u80fd\u6027\u673a\u5668\u4eba\u8fd0\u52a8\u4e5f\u80fd\u53cd\u6620\u64cd\u4f5c\u8005\u60c5\u611f\u72b6\u6001\uff0c\u8fd9\u5bf9\u5f53\u524d\u548c\u672a\u6765\u7684\u8fdc\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u53ca\u60c5\u611f\u673a\u5668\u4eba\u5e94\u7528\u6709\u91cd\u8981\u542f\u793a"}}
{"id": "2512.09101", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09101", "abs": "https://arxiv.org/abs/2512.09101", "authors": ["Lipeng Zhuang", "Shiyu Fan", "Florent P. Audonnet", "Yingdong Ru", "Gerardo Aragon Camarasa", "Paul Henderson"], "title": "Masked Generative Policy for Robotic Control", "comment": null, "summary": "We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail.", "AI": {"tldr": "MGP\u662f\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u8fd0\u52a8\u6a21\u4eff\u5b66\u4e60\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u6563\u52a8\u4f5c\u4ee4\u724c\u8868\u793a\u548c\u6761\u4ef6\u63a9\u7801\u53d8\u6362\u5668\u5b9e\u73b0\u5e76\u884c\u751f\u6210\u4e0e\u5feb\u901f\u7ec6\u5316\uff0c\u5728150\u4e2a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8fd0\u52a8\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5b9e\u73b0\u5168\u5c40\u4e00\u81f4\u6027\u9884\u6d4b\u53c8\u5177\u6709\u9c81\u68d2\u81ea\u9002\u5e94\u6267\u884c\u80fd\u529b\u7684\u6846\u67b6\u3002", "method": "\u5c06\u52a8\u4f5c\u8868\u793a\u4e3a\u79bb\u6563\u4ee4\u724c\uff0c\u8bad\u7ec3\u6761\u4ef6\u63a9\u7801\u53d8\u6362\u5668\u5e76\u884c\u751f\u6210\u4ee4\u724c\u5e76\u5feb\u901f\u7ec6\u5316\u4f4e\u7f6e\u4fe1\u5ea6\u4ee4\u724c\u3002\u63d0\u51fa\u4e24\u79cd\u91c7\u6837\u8303\u5f0f\uff1aMGP-Short\u7528\u4e8e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\uff0cMGP-Long\u7528\u4e8e\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\uff0c\u540e\u8005\u80fd\u9884\u6d4b\u5b8c\u6574\u8f68\u8ff9\u5e76\u57fa\u4e8e\u65b0\u89c2\u6d4b\u52a8\u6001\u7ec6\u5316\u52a8\u4f5c\u4ee4\u724c\u3002", "result": "\u5728Meta-World\u548cLIBERO\u57fa\u51c6\u7684150\u4e2a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\uff0cMGP\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u6269\u6563\u548c\u81ea\u56de\u5f52\u7b56\u7565\uff0c\u5e73\u5747\u6210\u529f\u7387\u63d0\u9ad89%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe35\u500d\u3002\u5728\u52a8\u6001\u548c\u7f3a\u5931\u89c2\u6d4b\u73af\u5883\u4e2d\u5e73\u5747\u6210\u529f\u7387\u63d0\u9ad860%\uff0c\u5e76\u89e3\u51b3\u4e86\u5176\u4ed6\u65b9\u6cd5\u5931\u8d25\u7684\u4e24\u4e2a\u975e\u9a6c\u5c14\u53ef\u592b\u573a\u666f\u3002", "conclusion": "MGP\u6846\u67b6\u901a\u8fc7\u79bb\u6563\u4ee4\u724c\u8868\u793a\u548c\u6761\u4ef6\u63a9\u7801\u53d8\u6362\u5668\uff0c\u7ed3\u5408\u5e76\u884c\u751f\u6210\u4e0e\u81ea\u9002\u5e94\u7ec6\u5316\uff0c\u5728\u89c6\u89c9\u8fd0\u52a8\u6a21\u4eff\u5b66\u4e60\u4e2d\u5b9e\u73b0\u4e86\u5feb\u901f\u63a8\u7406\u548c\u5353\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u5728\u5904\u7406\u590d\u6742\u3001\u975e\u9a6c\u5c14\u53ef\u592b\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.09105", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.09105", "abs": "https://arxiv.org/abs/2512.09105", "authors": ["Adi Manor", "Dan Cohen", "Ziv Keidar", "Avi Parush", "Hadas Erel"], "title": "Cognitive Trust in HRI: \"Pay Attention to Me and I'll Trust You Even if You are Wrong\"", "comment": "Confrence paper", "summary": "Cognitive trust and the belief that a robot is capable of accurately performing tasks, are recognized as central factors in fostering high-quality human-robot interactions. It is well established that performance factors such as the robot's competence and its reliability shape cognitive trust. Recent studies suggest that affective factors, such as robotic attentiveness, also play a role in building cognitive trust. This work explores the interplay between these two factors that shape cognitive trust. Specifically, we evaluated whether different combinations of robotic competence and attentiveness introduce a compensatory mechanism, where one factor compensates for the lack of the other. In the experiment, participants performed a search task with a robotic dog in a 2x2 experimental design that included two factors: competence (high or low) and attentiveness (high or low). The results revealed that high attentiveness can compensate for low competence. Participants who collaborated with a highly attentive robot that performed poorly reported trust levels comparable to those working with a highly competent robot. When the robot did not demonstrate attentiveness, low competence resulted in a substantial decrease in cognitive trust. The findings indicate that building cognitive trust in human-robot interaction may be more complex than previously believed, involving emotional processes that are typically overlooked. We highlight an affective compensatory mechanism that adds a layer to consider alongside traditional competence-based models of cognitive trust.", "AI": {"tldr": "\u9ad8\u5ea6\u4e13\u6ce8\u7684\u673a\u5668\u4eba\u53ef\u4ee5\u5f25\u8865\u4f4e\u80fd\u529b\u5bf9\u8ba4\u77e5\u4fe1\u4efb\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u60c5\u611f\u8865\u507f\u673a\u5236\u5728\u5efa\u7acb\u4eba\u673a\u4fe1\u4efb\u4e2d\u7684\u91cd\u8981\u4f5c\u7528", "motivation": "\u63a2\u7d22\u673a\u5668\u4eba\u80fd\u529b\u548c\u4e13\u6ce8\u5ea6\u8fd9\u4e24\u4e2a\u56e0\u7d20\u5982\u4f55\u76f8\u4e92\u4f5c\u7528\u5f71\u54cd\u8ba4\u77e5\u4fe1\u4efb\uff0c\u7279\u522b\u662f\u662f\u5426\u5b58\u5728\u8865\u507f\u673a\u5236\uff0c\u5373\u4e00\u4e2a\u56e0\u7d20\u80fd\u5426\u5f25\u8865\u53e6\u4e00\u4e2a\u56e0\u7d20\u7684\u4e0d\u8db3", "method": "\u91c7\u75282x2\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5305\u542b\u4e24\u4e2a\u56e0\u7d20\uff1a\u80fd\u529b\uff08\u9ad8/\u4f4e\uff09\u548c\u4e13\u6ce8\u5ea6\uff08\u9ad8/\u4f4e\uff09\u3002\u53c2\u4e0e\u8005\u4e0e\u673a\u5668\u72d7\u5408\u4f5c\u5b8c\u6210\u641c\u7d22\u4efb\u52a1\uff0c\u6d4b\u91cf\u4e0d\u540c\u7ec4\u5408\u4e0b\u7684\u8ba4\u77e5\u4fe1\u4efb\u6c34\u5e73", "result": "\u9ad8\u4e13\u6ce8\u5ea6\u53ef\u4ee5\u8865\u507f\u4f4e\u80fd\u529b\uff1a\u4e0e\u9ad8\u4e13\u6ce8\u5ea6\u4f46\u8868\u73b0\u5dee\u7684\u673a\u5668\u4eba\u5408\u4f5c\u7684\u53c2\u4e0e\u8005\u62a5\u544a\u7684\u4fe1\u4efb\u6c34\u5e73\u4e0e\u9ad8\u80fd\u529b\u673a\u5668\u4eba\u76f8\u5f53\u3002\u5f53\u673a\u5668\u4eba\u4e0d\u5c55\u793a\u4e13\u6ce8\u5ea6\u65f6\uff0c\u4f4e\u80fd\u529b\u4f1a\u5bfc\u81f4\u8ba4\u77e5\u4fe1\u4efb\u5927\u5e45\u4e0b\u964d", "conclusion": "\u5efa\u7acb\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u8ba4\u77e5\u4fe1\u4efb\u6bd4\u4e4b\u524d\u8ba4\u4e3a\u7684\u66f4\u590d\u6742\uff0c\u6d89\u53ca\u901a\u5e38\u88ab\u5ffd\u89c6\u7684\u60c5\u611f\u8fc7\u7a0b\u3002\u7814\u7a76\u63ed\u793a\u4e86\u60c5\u611f\u8865\u507f\u673a\u5236\uff0c\u4e3a\u4f20\u7edf\u7684\u57fa\u4e8e\u80fd\u529b\u7684\u8ba4\u77e5\u4fe1\u4efb\u6a21\u578b\u589e\u52a0\u4e86\u65b0\u7684\u8003\u91cf\u7ef4\u5ea6"}}
{"id": "2512.08943", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08943", "abs": "https://arxiv.org/abs/2512.08943", "authors": ["Singon Kim"], "title": "Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models", "comment": "Master's thesis, Korea University, 2025", "summary": "Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.", "AI": {"tldr": "ACoRN\u901a\u8fc7\u7ec6\u7c92\u5ea6\u6587\u6863\u5206\u7c7b\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u63d0\u5347\u62bd\u8c61\u538b\u7f29\u5728\u566a\u58f0\u68c0\u7d22\u6587\u6863\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u6539\u5584RAG\u6027\u80fd", "motivation": "\u4f20\u7edf\u62bd\u8c61\u538b\u7f29\u6a21\u578b\u5728RAG\u4e2d\u9762\u4e34\u4e24\u4e2a\u95ee\u9898\uff1a1) \u68c0\u7d22\u5230\u7684\u6587\u6863\u53ef\u80fd\u5305\u542b\u4e0e\u67e5\u8be2\u65e0\u5173\u6216\u4e8b\u5b9e\u9519\u8bef\u7684\u5185\u5bb9\uff0c\u5c3d\u7ba1\u76f8\u5173\u6027\u8bc4\u5206\u9ad8\uff1b2) \u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u6ce8\u610f\u529b\u5206\u6563\uff0c\u5bb9\u6613\u9057\u6f0f\u5bf9\u6b63\u786e\u7b54\u6848\u81f3\u5173\u91cd\u8981\u7684\u4fe1\u606f", "method": "\u63d0\u51faACoRN\u65b9\u6cd5\uff1a1) \u5bf9\u68c0\u7d22\u6587\u6863\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff1b2) \u5f15\u5165\u4e24\u4e2a\u65b0\u9896\u8bad\u7ec3\u6b65\u9aa4\uff1a\u79bb\u7ebf\u6570\u636e\u589e\u5f3a\u4ee5\u589e\u5f3a\u5bf9\u4e24\u79cd\u68c0\u7d22\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u5fae\u8c03\u4ee5\u751f\u6210\u56f4\u7ed5\u652f\u6301\u6b63\u786e\u7b54\u6848\u7684\u5173\u952e\u4fe1\u606f\u7684\u6458\u8981", "result": "\u4f7f\u7528ACoRN\u8bad\u7ec3\u7684T5-large\u4f5c\u4e3a\u538b\u7f29\u5668\uff0c\u5728\u4fdd\u7559\u7b54\u6848\u5b57\u7b26\u4e32\u7684\u540c\u65f6\u63d0\u9ad8\u4e86EM\u548cF1\u5206\u6570\uff0c\u5728\u5305\u542b\u5927\u91cf\u964d\u4f4e\u51c6\u786e\u6027\u7684\u6587\u6863\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02", "conclusion": "ACoRN\u901a\u8fc7\u589e\u5f3a\u62bd\u8c61\u538b\u7f29\u5668\u5bf9\u566a\u58f0\u6587\u6863\u7684\u9c81\u68d2\u6027\u548c\u805a\u7126\u5173\u952e\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86RAG\u7cfb\u7edf\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027"}}
{"id": "2512.09111", "categories": ["cs.RO", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.09111", "abs": "https://arxiv.org/abs/2512.09111", "authors": ["Yuji Takubo", "Arpit Dwivedi", "Sukeerth Ramkumar", "Luis A. Pabon", "Daniele Gammelli", "Marco Pavone", "Simone D'Amico"], "title": "Semantic Trajectory Generation for Goal-Oriented Spacecraft Rendezvous", "comment": "28 pages, 12 figures. Submitted to AIAA SCITECH 2026", "summary": "Reliable real-time trajectory generation is essential for future autonomous spacecraft. While recent progress in nonconvex guidance and control is paving the way for onboard autonomous trajectory optimization, these methods still rely on extensive expert input (e.g., waypoints, constraints, mission timelines, etc.), which limits the operational scalability in real rendezvous missions.This paper introduces SAGES (Semantic Autonomous Guidance Engine for Space), a trajectory-generation framework that translates natural-language commands into spacecraft trajectories that reflect high-level intent while respecting nonconvex constraints. Experiments in two settings -- fault-tolerant proximity operations with continuous-time constraint enforcement and a free-flying robotic platform -- demonstrate that SAGES reliably produces trajectories aligned with human commands, achieving over 90\\% semantic-behavioral consistency across diverse behavior modes. Ultimately, this work marks an initial step toward language-conditioned, constraint-aware spacecraft trajectory generation, enabling operators to interactively guide both safety and behavior through intuitive natural-language commands with reduced expert burden.", "AI": {"tldr": "SAGES\uff1a\u4e00\u4e2a\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3a\u6ee1\u8db3\u975e\u51f8\u7ea6\u675f\u7684\u822a\u5929\u5668\u8f68\u8ff9\u751f\u6210\u6846\u67b6\uff0c\u5b9e\u73b0\u4e8690%\u4ee5\u4e0a\u7684\u8bed\u4e49\u884c\u4e3a\u4e00\u81f4\u6027\uff0c\u51cf\u5c11\u4e13\u5bb6\u8d1f\u62c5\u3002", "motivation": "\u73b0\u6709\u975e\u51f8\u5236\u5bfc\u63a7\u5236\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u4e13\u5bb6\u8f93\u5165\uff08\u5982\u822a\u70b9\u3001\u7ea6\u675f\u3001\u4efb\u52a1\u65f6\u95f4\u7ebf\u7b49\uff09\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u4ea4\u4f1a\u4efb\u52a1\u4e2d\u7684\u64cd\u4f5c\u53ef\u6269\u5c55\u6027\uff0c\u9700\u8981\u66f4\u76f4\u89c2\u7684\u4ea4\u4e92\u65b9\u5f0f\u3002", "method": "\u63d0\u51faSAGES\uff08\u8bed\u4e49\u81ea\u4e3b\u5236\u5bfc\u5f15\u64ce\uff09\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u8f6c\u6362\u4e3a\u822a\u5929\u5668\u8f68\u8ff9\uff0c\u540c\u65f6\u5c0a\u91cd\u975e\u51f8\u7ea6\u675f\u3002\u5728\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u5b9e\u9a8c\uff1a\u5177\u6709\u8fde\u7eed\u65f6\u95f4\u7ea6\u675f\u6267\u884c\u7684\u5bb9\u9519\u63a5\u8fd1\u64cd\u4f5c\u548c\u81ea\u7531\u98de\u884c\u673a\u5668\u4eba\u5e73\u53f0\u3002", "result": "SAGES\u53ef\u9760\u5730\u751f\u6210\u4e0e\u4eba\u7c7b\u6307\u4ee4\u4e00\u81f4\u7684\u8f68\u8ff9\uff0c\u5728\u591a\u79cd\u884c\u4e3a\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u8d85\u8fc790%\u7684\u8bed\u4e49\u884c\u4e3a\u4e00\u81f4\u6027\u3002", "conclusion": "\u8fd9\u662f\u8fc8\u5411\u8bed\u8a00\u6761\u4ef6\u3001\u7ea6\u675f\u611f\u77e5\u7684\u822a\u5929\u5668\u8f68\u8ff9\u751f\u6210\u7684\u521d\u6b65\u6b65\u9aa4\uff0c\u4f7f\u64cd\u4f5c\u5458\u80fd\u591f\u901a\u8fc7\u76f4\u89c2\u7684\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u4ea4\u4e92\u5f0f\u6307\u5bfc\u5b89\u5168\u6027\u548c\u884c\u4e3a\uff0c\u51cf\u5c11\u4e13\u5bb6\u8d1f\u62c5\u3002"}}
{"id": "2512.08944", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.08944", "abs": "https://arxiv.org/abs/2512.08944", "authors": ["Yudong Wang", "Zhe Yang", "Wenhan Ma", "Zhifang Sui", "Liang Zhao"], "title": "Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning", "comment": null, "summary": "While reinforcement learning has unlocked unprecedented complex reasoning in large language models, it has also amplified their propensity for hallucination, creating a critical trade-off between capability and reliability. This work confronts this challenge by introducing a targeted RL framework designed to mitigate both intrinsic and extrinsic hallucinations across short and long-form question answering. We address extrinsic hallucinations (flawed internal knowledge) by creating a novel training set from open-ended conversions of TriviaQA. Concurrently, we tackle intrinsic hallucinations (unfaithfulness to context) by leveraging long-form texts from FineWeb in a fact-grounding reward scheme. To further bolster reliability, our framework explicitly rewards the model for refusing to answer unanswerable questions, thereby cultivating crucial cautiousness. Extensive experiments demonstrate that our methodology yields significant performance gains across a diverse suite of benchmarks, substantially reducing both hallucination types. Ultimately, this research contributes a practical framework for resolving the critical tension between advanced reasoning and factual trustworthiness, paving the way for more capable and reliable large language models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9488\u5bf9\u6027\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u8bad\u7ec3\u6570\u636e\u548c\u5956\u52b1\u673a\u5236\uff0c\u540c\u65f6\u51cf\u5c11LLM\u7684\u5185\u5728\u548c\u5916\u5728\u5e7b\u89c9\uff0c\u5e76\u5728\u65e0\u6cd5\u56de\u7b54\u95ee\u9898\u65f6\u9f13\u52b1\u62d2\u7edd\u56de\u7b54\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4e5f\u52a0\u5267\u4e86\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u80fd\u529b\u4e0e\u53ef\u9760\u6027\u4e4b\u95f4\u5f62\u6210\u4e86\u5173\u952e\u6743\u8861\u3002\u9700\u8981\u89e3\u51b3\u5185\u5728\u5e7b\u89c9\uff08\u4e0d\u5fe0\u5b9e\u4e8e\u4e0a\u4e0b\u6587\uff09\u548c\u5916\u5728\u5e7b\u89c9\uff08\u5185\u90e8\u77e5\u8bc6\u7f3a\u9677\uff09\u7684\u53cc\u91cd\u6311\u6218\u3002", "method": "1) \u9488\u5bf9\u5916\u5728\u5e7b\u89c9\uff1a\u4eceTriviaQA\u7684\u5f00\u653e\u5f0f\u5bf9\u8bdd\u521b\u5efa\u65b0\u9896\u8bad\u7ec3\u96c6\uff1b2) \u9488\u5bf9\u5185\u5728\u5e7b\u89c9\uff1a\u5229\u7528FineWeb\u7684\u957f\u6587\u672c\u8fdb\u884c\u4e8b\u5b9e\u951a\u5b9a\u5956\u52b1\u65b9\u6848\uff1b3) \u589e\u5f3a\u53ef\u9760\u6027\uff1a\u660e\u786e\u5956\u52b1\u6a21\u578b\u62d2\u7edd\u56de\u7b54\u65e0\u6cd5\u56de\u7b54\u7684\u95ee\u9898\uff0c\u57f9\u517b\u8c28\u614e\u6027\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u4e86\u4e24\u79cd\u7c7b\u578b\u7684\u5e7b\u89c9\u3002\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ea7\u63a8\u7406\u4e0e\u4e8b\u5b9e\u53ef\u4fe1\u5ea6\u4e4b\u95f4\u7684\u5173\u952e\u77db\u76fe\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b9e\u7528\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5148\u8fdb\u63a8\u7406\u4e0e\u4e8b\u5b9e\u53ef\u4fe1\u5ea6\u4e4b\u95f4\u7684\u5173\u952e\u7d27\u5f20\u5173\u7cfb\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u3001\u66f4\u53ef\u9760\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2512.09283", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09283", "abs": "https://arxiv.org/abs/2512.09283", "authors": ["Fan Wu", "Chenguang Yang", "Haibin Yang", "Shuo Wang", "Yanrui Xu", "Xing Zhou", "Meng Gao", "Yaoqi Xian", "Zhihong Zhu", "Shifeng Huang"], "title": "UPETrack: Unidirectional Position Estimation for Tracking Occluded Deformable Linear Objects", "comment": null, "summary": "Real-time state tracking of Deformable Linear Objects (DLOs) is critical for enabling robotic manipulation of DLOs in industrial assembly, medical procedures, and daily-life applications. However, the high-dimensional configuration space, nonlinear dynamics, and frequent partial occlusions present fundamental barriers to robust real-time DLO tracking. To address these limitations, this study introduces UPETrack, a geometry-driven framework based on Unidirectional Position Estimation (UPE), which facilitates tracking without the requirement for physical modeling, virtual simulation, or visual markers. The framework operates in two phases: (1) visible segment tracking is based on a Gaussian Mixture Model (GMM) fitted via the Expectation Maximization (EM) algorithm, and (2) occlusion region prediction employing UPE algorithm we proposed. UPE leverages the geometric continuity inherent in DLO shapes and their temporal evolution patterns to derive a closed-form positional estimator through three principal mechanisms: (i) local linear combination displacement term, (ii) proximal linear constraint term, and (iii) historical curvature term. This analytical formulation allows efficient and stable estimation of occluded nodes through explicit linear combinations of geometric components, eliminating the need for additional iterative optimization. Experimental results demonstrate that UPETrack surpasses two state-of-the-art tracking algorithms, including TrackDLO and CDCPD2, in both positioning accuracy and computational efficiency.", "AI": {"tldr": "UPETrack\uff1a\u4e00\u79cd\u57fa\u4e8e\u5355\u5411\u4f4d\u7f6e\u4f30\u8ba1\u7684\u51e0\u4f55\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u8ddf\u8e2a\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\uff0c\u65e0\u9700\u7269\u7406\u5efa\u6a21\u3001\u865a\u62df\u4eff\u771f\u6216\u89c6\u89c9\u6807\u8bb0\uff0c\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\uff08DLOs\uff09\u5728\u5de5\u4e1a\u88c5\u914d\u3001\u533b\u7597\u7a0b\u5e8f\u548c\u65e5\u5e38\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u5176\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u3001\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u9891\u7e41\u90e8\u5206\u906e\u6321\u7ed9\u5b9e\u65f6\u8ddf\u8e2a\u5e26\u6765\u4e86\u6839\u672c\u6027\u969c\u788d\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u7269\u7406\u5efa\u6a21\u6216\u89c6\u89c9\u6807\u8bb0\u7684\u9c81\u68d2\u8ddf\u8e2a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faUPETrack\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1\uff09\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u548c\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u7684\u53ef\u89c1\u6bb5\u8ddf\u8e2a\uff1b2\uff09\u4f7f\u7528\u63d0\u51fa\u7684\u5355\u5411\u4f4d\u7f6e\u4f30\u8ba1\u7b97\u6cd5\u9884\u6d4b\u906e\u6321\u533a\u57df\u3002UPE\u7b97\u6cd5\u5229\u7528DLO\u5f62\u72b6\u7684\u51e0\u4f55\u8fde\u7eed\u6027\u548c\u65f6\u95f4\u6f14\u5316\u6a21\u5f0f\uff0c\u901a\u8fc7\u5c40\u90e8\u7ebf\u6027\u7ec4\u5408\u4f4d\u79fb\u9879\u3001\u8fd1\u7aef\u7ebf\u6027\u7ea6\u675f\u9879\u548c\u5386\u53f2\u66f2\u7387\u9879\u4e09\u4e2a\u673a\u5236\u63a8\u5bfc\u51fa\u95ed\u5f0f\u4f4d\u7f6e\u4f30\u8ba1\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUPETrack\u5728\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u90fd\u8d85\u8d8a\u4e86\u4e24\u79cd\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u7b97\u6cd5\uff08TrackDLO\u548cCDCPD2\uff09\u3002", "conclusion": "UPETrack\u901a\u8fc7\u51e0\u4f55\u9a71\u52a8\u7684\u5355\u5411\u4f4d\u7f6e\u4f30\u8ba1\u6846\u67b6\uff0c\u4e3aDLO\u5b9e\u65f6\u8ddf\u8e2a\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u7269\u7406\u5efa\u6a21\u6216\u89c6\u89c9\u6807\u8bb0\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.08945", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.08945", "abs": "https://arxiv.org/abs/2512.08945", "authors": ["Stefano Epifani", "Giuliano Castigliego", "Laura Kecskemeti", "Giuliano Razzicchia", "Elisabeth Seiwald-Sonderegger"], "title": "The Linguistic Architecture of Reflective Thought: Evaluation of a Large Language Model as a Tool to Isolate the Formal Structure of Mentalization", "comment": "18 pages, 1 table, Project coordinator: Stefano Epifani", "summary": "Background: Mentalization integrates cognitive, affective, and intersubjective components. Large Language Models (LLMs) display an increasing ability to generate reflective texts, raising questions regarding the relationship between linguistic form and mental representation. This study assesses the extent to which a single LLM can reproduce the linguistic structure of mentalization according to the parameters of Mentalization-Based Treatment (MBT).\n  Methods: Fifty dialogues were generated between human participants and an LLM configured in standard mode. Five psychiatrists trained in MBT, working under blinded conditions, evaluated the mentalization profiles produced by the model along the four MBT axes, assigning Likert-scale scores for evaluative coherence, argumentative coherence, and global quality. Inter-rater agreement was estimated using ICC(3,1).\n  Results: Mean scores (3.63-3.98) and moderate standard deviations indicate a high level of structural coherence in the generated profiles. ICC values (0.60-0.84) show substantial-to-high agreement among raters. The model proved more stable in the Implicit-Explicit and Self-Other dimensions, while presenting limitations in the integration of internal states and external contexts. The profiles were coherent and clinically interpretable yet characterized by affective neutrality.", "AI": {"tldr": "LLM\u80fd\u591f\u751f\u6210\u7b26\u5408MBT\u6846\u67b6\u7684\u53cd\u601d\u6027\u6587\u672c\uff0c\u5728\u8bed\u8a00\u7ed3\u6784\u4e0a\u8868\u73b0\u51fa\u9ad8\u4e00\u81f4\u6027\uff0c\u4f46\u5728\u60c5\u611f\u8868\u8fbe\u548c\u5185\u5916\u6574\u5408\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "motivation": "\u968f\u7740LLM\u751f\u6210\u53cd\u601d\u6027\u6587\u672c\u80fd\u529b\u589e\u5f3a\uff0c\u9700\u8981\u8bc4\u4f30\u5176\u8bed\u8a00\u5f62\u5f0f\u4e0e\u5fc3\u7406\u8868\u5f81\u7684\u5173\u7cfb\uff0c\u7279\u522b\u662f\u80fd\u5426\u590d\u73b0\u5fc3\u7406\u5316\u6cbb\u7597\u6846\u67b6\u4e0b\u7684\u8bed\u8a00\u7ed3\u6784\u3002", "method": "\u751f\u621050\u4e2a\u4eba\u673a\u5bf9\u8bdd\uff0c\u75315\u540dMBT\u8bad\u7ec3\u7684\u7cbe\u795e\u79d1\u533b\u751f\u76f2\u8bc4\uff0c\u6309MBT\u56db\u8f74\u8bc4\u4f30\u5fc3\u7406\u5316\u7279\u5f81\uff0c\u4f7f\u7528Likert\u91cf\u8868\u8bc4\u5206\uff0c\u8ba1\u7b97\u8bc4\u5206\u8005\u95f4\u4fe1\u5ea6\u3002", "result": "LLM\u751f\u6210\u7684\u5fc3\u7406\u5316\u7279\u5f81\u5177\u6709\u9ad8\u7ed3\u6784\u4e00\u81f4\u6027\uff08\u5e73\u5747\u52063.63-3.98\uff09\uff0c\u8bc4\u5206\u8005\u95f4\u4fe1\u5ea6\u826f\u597d\uff08ICC 0.60-0.84\uff09\uff0c\u5728\u9690\u663e\u7ef4\u5ea6\u548c\u81ea\u6211-\u4ed6\u4eba\u7ef4\u5ea6\u66f4\u7a33\u5b9a\uff0c\u4f46\u60c5\u611f\u8868\u8fbe\u4e2d\u6027\u4e14\u5185\u5916\u6574\u5408\u6709\u9650\u3002", "conclusion": "LLM\u80fd\u591f\u751f\u6210\u7ed3\u6784\u4e0a\u7b26\u5408MBT\u6846\u67b6\u7684\u5fc3\u7406\u5316\u6587\u672c\uff0c\u5177\u6709\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u5728\u60c5\u611f\u8868\u8fbe\u548c\u5185\u5916\u72b6\u6001\u6574\u5408\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u63d0\u793a\u8bed\u8a00\u5f62\u5f0f\u4e0e\u5fc3\u7406\u8868\u5f81\u7684\u590d\u6742\u5173\u7cfb\u3002"}}
{"id": "2512.09297", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09297", "abs": "https://arxiv.org/abs/2512.09297", "authors": ["Huayi Zhou", "Kui Jia"], "title": "One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation", "comment": "under review", "summary": "Learning dexterous bimanual manipulation policies critically depends on large-scale, high-quality demonstrations, yet current paradigms face inherent trade-offs: teleoperation provides physically grounded data but is prohibitively labor-intensive, while simulation-based synthesis scales efficiently but suffers from sim-to-real gaps. We present BiDemoSyn, a framework that synthesizes contact-rich, physically feasible bimanual demonstrations from a single real-world example. The key idea is to decompose tasks into invariant coordination blocks and variable, object-dependent adjustments, then adapt them through vision-guided alignment and lightweight trajectory optimization. This enables the generation of thousands of diverse and feasible demonstrations within several hour, without repeated teleoperation or reliance on imperfect simulation. Across six dual-arm tasks, we show that policies trained on BiDemoSyn data generalize robustly to novel object poses and shapes, significantly outperforming recent baselines. By bridging the gap between efficiency and real-world fidelity, BiDemoSyn provides a scalable path toward practical imitation learning for complex bimanual manipulation without compromising physical grounding.", "AI": {"tldr": "BiDemoSyn\u6846\u67b6\u901a\u8fc7\u5355\u6b21\u771f\u5b9e\u6f14\u793a\u5408\u6210\u6570\u5343\u4e2a\u63a5\u89e6\u4e30\u5bcc\u7684\u53cc\u624b\u64cd\u4f5c\u6f14\u793a\uff0c\u5206\u89e3\u4efb\u52a1\u4e3a\u4e0d\u53d8\u534f\u8c03\u5757\u548c\u53ef\u53d8\u8c03\u6574\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u7269\u7406\u53ef\u884c\u7684\u6f14\u793a\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u53cc\u624b\u64cd\u4f5c\u7b56\u7565\u5b66\u4e60\u9762\u4e34\u4e24\u96be\uff1a\u9065\u64cd\u4f5c\u63d0\u4f9b\u7269\u7406\u771f\u5b9e\u6570\u636e\u4f46\u52b3\u52a8\u5bc6\u96c6\uff0c\u4eff\u771f\u5408\u6210\u53ef\u6269\u5c55\u4f46\u5b58\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u5dee\u8ddd\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u9ad8\u6548\u6269\u5c55\u53c8\u4fdd\u6301\u7269\u7406\u771f\u5b9e\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u4e0d\u53d8\u534f\u8c03\u5757\u548c\u53ef\u53d8\u5bf9\u8c61\u76f8\u5173\u8c03\u6574\uff0c\u901a\u8fc7\u89c6\u89c9\u5f15\u5bfc\u5bf9\u9f50\u548c\u8f7b\u91cf\u7ea7\u8f68\u8ff9\u4f18\u5316\uff0c\u4ece\u5355\u4e2a\u771f\u5b9e\u4e16\u754c\u793a\u4f8b\u5408\u6210\u6570\u5343\u4e2a\u63a5\u89e6\u4e30\u5bcc\u7684\u7269\u7406\u53ef\u884c\u6f14\u793a\u3002", "result": "\u5728\u516d\u4e2a\u53cc\u81c2\u4efb\u52a1\u4e2d\uff0c\u4f7f\u7528BiDemoSyn\u6570\u636e\u8bad\u7ec3\u7684\u7b56\u7565\u5bf9\u65b0\u7269\u4f53\u59ff\u6001\u548c\u5f62\u72b6\u5177\u6709\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "BiDemoSyn\u5728\u6548\u7387\u548c\u771f\u5b9e\u4e16\u754c\u4fdd\u771f\u5ea6\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\uff0c\u4e3a\u590d\u6742\u53cc\u624b\u64cd\u4f5c\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u6a21\u4eff\u5b66\u4e60\u8def\u5f84\uff0c\u65e0\u9700\u59a5\u534f\u7269\u7406\u57fa\u7840\u3002"}}
{"id": "2512.09015", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09015", "abs": "https://arxiv.org/abs/2512.09015", "authors": ["DatologyAI", ":", "Luke Merrick", "Alex Fang", "Aldo Carranza", "Alvin Deng", "Amro Abbas", "Brett Larsen", "Cody Blakeney", "Darren Teh", "David Schwab", "Fan Pan", "Haakon Mongstad", "Haoli Yin", "Jack Urbanek", "Jason Lee", "Jason Telanoff", "Josh Wills", "Kaleigh Mentzer", "Paul Burstein", "Parth Doshi", "Paul Burnstein", "Pratyush Maini", "Ricardo Monti", "Rishabh Adiga", "Scott Loftin", "Siddharth Joshi", "Spandan Das", "Tony Jiang", "Vineeth Dorma", "Zhengping Wang", "Bogdan Gaza", "Ari Morcos", "Matthew Leavitt"], "title": "Luxical: High-Speed Lexical-Dense Text Embeddings", "comment": "9 pages, 6 figures", "summary": "Frontier language model quality increasingly hinges on our ability to organize web-scale text corpora for training. Today's dominant tools trade off speed and flexibility: lexical classifiers (e.g., FastText) are fast but limited to producing classification output scores, while the vector-valued outputs of transformer text embedding models flexibly support numerous workflows (e.g., clustering, classification, and retrieval) but are computationally expensive to produce. We introduce Luxical, a library for high-speed \"lexical-dense\" text embeddings that aims to recover the best properties of both approaches for web-scale text organization. Luxical combines sparse TF--IDF features, a small ReLU network, and a knowledge distillation training regimen to approximate large transformer embedding models at a fraction of their operational cost. In this technical report, we describe the Luxical architecture and training objective and evaluate a concrete Luxical model in two disparate applications: a targeted webcrawl document retrieval test and an end-to-end language model data curation task grounded in text classification. In these tasks we demonstrate speedups ranging from 3x to 100x over varying-sized neural baselines, and comparable to FastText model inference during the data curation task. On these evaluations, the tested Luxical model illustrates favorable compute/quality trade-offs for large-scale text organization, matching the quality of neural baselines. Luxical is available as open-source software at https://github.com/datologyai/luxical.", "AI": {"tldr": "Luxical\u662f\u4e00\u4e2a\u9ad8\u901f\"\u8bcd\u6cd5-\u7a20\u5bc6\"\u6587\u672c\u5d4c\u5165\u5e93\uff0c\u7ed3\u5408\u7a00\u758fTF-IDF\u7279\u5f81\u548c\u5c0f\u578bReLU\u7f51\u7edc\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u8fd1\u4f3c\u5927\u578bTransformer\u5d4c\u5165\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b03-100\u500d\u52a0\u901f\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u7ec4\u7ec7\u5de5\u5177\u5b58\u5728\u901f\u5ea6\u4e0e\u7075\u6d3b\u6027\u7684\u6743\u8861\uff1a\u8bcd\u6cd5\u5206\u7c7b\u5668\uff08\u5982FastText\uff09\u901f\u5ea6\u5feb\u4f46\u529f\u80fd\u6709\u9650\uff0c\u800cTransformer\u6587\u672c\u5d4c\u5165\u6a21\u578b\u7075\u6d3b\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u8d28\u91cf\u53c8\u80fd\u5927\u5e45\u52a0\u901f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u7a00\u758fTF-IDF\u7279\u5f81\u3001\u5c0f\u578bReLU\u7f51\u7edc\u548c\u77e5\u8bc6\u84b8\u998f\u8bad\u7ec3\u65b9\u6848\uff0c\u8fd1\u4f3c\u5927\u578bTransformer\u5d4c\u5165\u6a21\u578b\u3002\u901a\u8fc7\u8fd9\u79cd\"\u8bcd\u6cd5-\u7a20\u5bc6\"\u6df7\u5408\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u7f51\u9875\u6293\u53d6\u6587\u6863\u68c0\u7d22\u548c\u8bed\u8a00\u6a21\u578b\u6570\u636e\u6574\u7406\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u4e0d\u540c\u89c4\u6a21\u7684\u795e\u7ecf\u57fa\u7ebf\u6a21\u578b\u5b9e\u73b0\u4e863-100\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u5728\u6570\u636e\u6574\u7406\u4efb\u52a1\u4e2d\u63a8\u7406\u901f\u5ea6\u4e0eFastText\u76f8\u5f53\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u795e\u7ecf\u57fa\u7ebf\u76f8\u5f53\u7684\u8d28\u91cf\u3002", "conclusion": "Luxical\u4e3a\u5927\u89c4\u6a21\u6587\u672c\u7ec4\u7ec7\u63d0\u4f9b\u4e86\u6709\u5229\u7684\u8ba1\u7b97/\u8d28\u91cf\u6743\u8861\uff0c\u7ed3\u5408\u4e86\u8bcd\u6cd5\u548c\u795e\u7ecf\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u53ef\u4f5c\u4e3a\u5f00\u6e90\u8f6f\u4ef6\u4f7f\u7528\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u6548\u6587\u672c\u5904\u7406\u7684web\u89c4\u6a21\u5e94\u7528\u3002"}}
{"id": "2512.09310", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09310", "abs": "https://arxiv.org/abs/2512.09310", "authors": ["Kwang Bin Lee", "Jiho Kang", "Sung-Hee Lee"], "title": "Scene-agnostic Hierarchical Bimanual Task Planning via Visual Affordance Reasoning", "comment": "8 pages, 4 figures", "summary": "Embodied agents operating in open environments must translate high-level instructions into grounded, executable behaviors, often requiring coordinated use of both hands. While recent foundation models offer strong semantic reasoning, existing robotic task planners remain predominantly unimanual and fail to address the spatial, geometric, and coordination challenges inherent to bimanual manipulation in scene-agnostic settings. We present a unified framework for scene-agnostic bimanual task planning that bridges high-level reasoning with 3D-grounded two-handed execution. Our approach integrates three key modules. Visual Point Grounding (VPG) analyzes a single scene image to detect relevant objects and generate world-aligned interaction points. Bimanual Subgoal Planner (BSP) reasons over spatial adjacency and cross-object accessibility to produce compact, motion-neutralized subgoals that exploit opportunities for coordinated two-handed actions. Interaction-Point-Driven Bimanual Prompting (IPBP) binds these subgoals to a structured skill library, instantiating synchronized unimanual or bimanual action sequences that satisfy hand-state and affordance constraints. Together, these modules enable agents to plan semantically meaningful, physically feasible, and parallelizable two-handed behaviors in cluttered, previously unseen scenes. Experiments show that it produces coherent, feasible, and compact two-handed plans, and generalizes to cluttered scenes without retraining, demonstrating robust scene-agnostic affordance reasoning for bimanual tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u573a\u666f\u65e0\u5173\u7684\u53cc\u624b\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u70b9\u5b9a\u4f4d\u3001\u53cc\u624b\u673a\u5668\u4eba\u5b50\u76ee\u6807\u89c4\u5212\u548c\u4ea4\u4e92\u70b9\u9a71\u52a8\u7684\u53cc\u624b\u673a\u5668\u4eba\u63d0\u793a\uff0c\u5b9e\u73b0\u8bed\u4e49\u7406\u89e3\u3001\u7269\u7406\u53ef\u884c\u4e14\u53ef\u5e76\u884c\u6267\u884c\u7684\u53cc\u624b\u64cd\u4f5c\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u5668\u5927\u591a\u662f\u5355\u624b\u7684\uff0c\u65e0\u6cd5\u89e3\u51b3\u573a\u666f\u65e0\u5173\u8bbe\u7f6e\u4e2d\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7a7a\u95f4\u3001\u51e0\u4f55\u548c\u534f\u8c03\u6311\u6218\u3002\u867d\u7136\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5c06\u9ad8\u7ea7\u6307\u4ee4\u8f6c\u5316\u4e3a\u5730\u9762\u5316\u3001\u53ef\u6267\u884c\u7684\u53cc\u624b\u884c\u4e3a\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a1) \u89c6\u89c9\u70b9\u5b9a\u4f4d(VPG)\uff1a\u5206\u6790\u5355\u5f20\u573a\u666f\u56fe\u50cf\u68c0\u6d4b\u76f8\u5173\u5bf9\u8c61\u5e76\u751f\u6210\u4e16\u754c\u5bf9\u9f50\u7684\u4ea4\u4e92\u70b9\uff1b2) \u53cc\u624b\u673a\u5668\u4eba\u5b50\u76ee\u6807\u89c4\u5212\u5668(BSP)\uff1a\u57fa\u4e8e\u7a7a\u95f4\u90bb\u63a5\u548c\u8de8\u5bf9\u8c61\u53ef\u8fbe\u6027\u63a8\u7406\uff0c\u751f\u6210\u7d27\u51d1\u3001\u8fd0\u52a8\u4e2d\u7acb\u5316\u7684\u5b50\u76ee\u6807\uff1b3) \u4ea4\u4e92\u70b9\u9a71\u52a8\u7684\u53cc\u624b\u673a\u5668\u4eba\u63d0\u793a(IPBP)\uff1a\u5c06\u5b50\u76ee\u6807\u7ed1\u5b9a\u5230\u7ed3\u6784\u5316\u6280\u80fd\u5e93\uff0c\u5b9e\u4f8b\u5316\u6ee1\u8db3\u624b\u90e8\u72b6\u6001\u548c\u53ef\u4f9b\u6027\u7ea6\u675f\u7684\u540c\u6b65\u5355\u624b\u6216\u53cc\u624b\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u8fde\u8d2f\u3001\u53ef\u884c\u4e14\u7d27\u51d1\u7684\u53cc\u624b\u89c4\u5212\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u6cdb\u5316\u5230\u6742\u4e71\u573a\u666f\uff0c\u5c55\u793a\u4e86\u53cc\u624b\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9c81\u68d2\u7684\u573a\u666f\u65e0\u5173\u53ef\u4f9b\u6027\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u4f7f\u667a\u80fd\u4f53\u5728\u6742\u4e71\u3001\u672a\u89c1\u8fc7\u7684\u573a\u666f\u4e2d\u89c4\u5212\u8bed\u4e49\u4e0a\u6709\u610f\u4e49\u3001\u7269\u7406\u4e0a\u53ef\u884c\u4e14\u53ef\u5e76\u884c\u6267\u884c\u7684\u53cc\u624b\u884c\u4e3a\uff0c\u89e3\u51b3\u4e86\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u7a7a\u95f4\u534f\u8c03\u6311\u6218\u3002"}}
{"id": "2512.09127", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09127", "abs": "https://arxiv.org/abs/2512.09127", "authors": ["Zihan Han", "Junyan Ge", "Caifeng Li"], "title": "Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation", "comment": null, "summary": "Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u5f15\u5bfc\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08KG-LLM\uff09\uff0c\u7528\u4e8e\u513f\u79d1\u7259\u79d1\u4e34\u5e8a\u8bb0\u5f55\u7684\u51c6\u786e\u89e3\u8bfb\u548c\u6297\u751f\u7d20\u5b89\u5168\u5904\u65b9\u63a8\u8350\u3002", "motivation": "\u513f\u79d1\u7259\u79d1\u4e34\u5e8a\u8bb0\u5f55\u89e3\u8bfb\u548c\u6297\u751f\u7d20\u5b89\u5168\u5904\u65b9\u662f\u7259\u79d1\u4fe1\u606f\u5b66\u4e2d\u7684\u6301\u7eed\u6311\u6218\u3002\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u975e\u7ed3\u6784\u5316\u7259\u79d1\u53d9\u8ff0\u3001\u4e0d\u5b8c\u6574\u7684\u653e\u5c04\u5b66\u63cf\u8ff0\u548c\u590d\u6742\u7684\u5b89\u5168\u7ea6\u675f\u3002", "method": "\u63d0\u51faKG-LLM\u6846\u67b6\uff0c\u6574\u5408\u513f\u79d1\u7259\u79d1\u77e5\u8bc6\u56fe\u8c31\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u591a\u9636\u6bb5\u5b89\u5168\u9a8c\u8bc1\u6d41\u7a0b\u3002\u9996\u5148\u4f7f\u7528\u4e34\u5e8aNER/RE\u6a21\u5757\u4ece\u7259\u79d1\u8bb0\u5f55\u548c\u653e\u5c04\u62a5\u544a\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u5b9e\u4f53\u548c\u5173\u7cfb\uff0c\u7136\u540e\u4ece\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u76f8\u5173\u6307\u5357\u3001\u836f\u7269\u5b89\u5168\u89c4\u5219\u548c\u5386\u53f2\u6848\u4f8b\uff0c\u4f9bLLM\u8fdb\u884c\u8bca\u65ad\u603b\u7ed3\u548c\u5242\u91cf-\u836f\u7269-\u6301\u7eed\u65f6\u95f4\u9884\u6d4b\u3002\u901a\u8fc7\u786e\u5b9a\u6027\u89c4\u5219\u68c0\u67e5\u548c\u673a\u5668\u5b66\u4e60\u5206\u7c7b\u5668\u7684\u53cc\u5c42\u9a8c\u8bc1\u673a\u5236\u786e\u4fdd\u5b89\u5168\u3002", "result": "\u572832,000\u4efd\u53bb\u6807\u8bc6\u5316\u513f\u79d1\u7259\u79d1\u5c31\u8bca\u8bb0\u5f55\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u9886\u57df\u9002\u5e94\u7684Llama-2\u57fa\u7ebf\uff0cKG-LLM\u63d0\u9ad8\u4e86\u8bb0\u5f55\u7406\u89e3\u6027\u80fd\uff08F1: 0.914 vs. 0.867\uff09\u3001\u836f\u7269\u5242\u91cf\u6301\u7eed\u65f6\u95f4\u51c6\u786e\u6027\uff08Top-1: 0.782 vs. 0.716\uff09\uff0c\u5e76\u5c06\u4e0d\u5b89\u5168\u6297\u751f\u7d20\u5efa\u8bae\u51cf\u5c11\u4e8650%\u3002", "conclusion": "KG-LLM\u6846\u67b6\u901a\u8fc7\u6574\u5408\u77e5\u8bc6\u56fe\u8c31\u3001RAG\u548c\u5b89\u5168\u6a21\u5757\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u513f\u79d1\u7259\u79d1\u6297\u751f\u7d20\u63a8\u8350\u7684\u4e34\u5e8a\u53ef\u9760\u6027\u3001\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\uff0c\u4e3a\u7259\u79d1\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09343", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09343", "abs": "https://arxiv.org/abs/2512.09343", "authors": ["Ashik E Rasul", "Humaira Tasnim", "Ji Yu Kim", "Young Hyun Lim", "Scott Schmitz", "Bruce W. Jo", "Hyung-Jin Yoon"], "title": "Development and Testing for Perception Based Autonomous Landing of a Long-Range QuadPlane", "comment": null, "summary": "QuadPlanes combine the range efficiency of fixed-wing aircraft with the maneuverability of multi-rotor platforms for long-range autonomous missions. In GPS-denied or cluttered urban environments, perception-based landing is vital for reliable operation. Unlike structured landing zones, real-world sites are unstructured and highly variable, requiring strong generalization capabilities from the perception system. Deep neural networks (DNNs) provide a scalable solution for learning landing site features across diverse visual and environmental conditions. While perception-driven landing has been shown in simulation, real-world deployment introduces significant challenges. Payload and volume constraints limit high-performance edge AI devices like the NVIDIA Jetson Orin Nano, which are crucial for real-time detection and control. Accurate pose estimation during descent is necessary, especially in the absence of GPS, and relies on dependable visual-inertial odometry. Achieving this with limited edge AI resources requires careful optimization of the entire deployment framework. The flight characteristics of large QuadPlanes further complicate the problem. These aircraft exhibit high inertia, reduced thrust vectoring, and slow response times further complicate stable landing maneuvers. This work presents a lightweight QuadPlane system for efficient vision-based autonomous landing and visual-inertial odometry, specifically developed for long-range QuadPlane operations such as aerial monitoring. It describes the hardware platform, sensor configuration, and embedded computing architecture designed to meet demanding real-time, physical constraints. This establishes a foundation for deploying autonomous landing in dynamic, unstructured, GPS-denied environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7QuadPlane\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728GPS\u62d2\u6b62\u73af\u5883\u4e0b\u5b9e\u73b0\u57fa\u4e8e\u89c6\u89c9\u7684\u81ea\u4e3b\u7740\u9646\u548c\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff0c\u7279\u522b\u9488\u5bf9\u957f\u8ddd\u79bbQuadPlane\u64cd\u4f5c\u5982\u7a7a\u4e2d\u76d1\u6d4b\u3002", "motivation": "QuadPlane\u7ed3\u5408\u4e86\u56fa\u5b9a\u7ffc\u98de\u673a\u7684\u822a\u7a0b\u6548\u7387\u548c\u591a\u65cb\u7ffc\u5e73\u53f0\u7684\u673a\u52a8\u6027\uff0c\u4f46\u5728GPS\u62d2\u6b62\u6216\u6742\u4e71\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u57fa\u4e8e\u611f\u77e5\u7684\u7740\u9646\u81f3\u5173\u91cd\u8981\u3002\u73b0\u5b9e\u4e16\u754c\u7684\u7740\u9646\u70b9\u662f\u975e\u7ed3\u6784\u5316\u548c\u9ad8\u5ea6\u53ef\u53d8\u7684\uff0c\u9700\u8981\u611f\u77e5\u7cfb\u7edf\u5177\u5907\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7QuadPlane\u7cfb\u7edf\uff0c\u5305\u62ec\u786c\u4ef6\u5e73\u53f0\u3001\u4f20\u611f\u5668\u914d\u7f6e\u548c\u5d4c\u5165\u5f0f\u8ba1\u7b97\u67b6\u6784\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u6ee1\u8db3\u5b9e\u65f6\u7269\u7406\u7ea6\u675f\u3002\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7740\u9646\u70b9\u7279\u5f81\uff0c\u5e76\u4f18\u5316\u6574\u4e2a\u90e8\u7f72\u6846\u67b6\u4ee5\u5728\u6709\u9650\u7684\u8fb9\u7f18AI\u8d44\u6e90\u4e0a\u8fd0\u884c\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u80fd\u591f\u5728\u52a8\u6001\u3001\u975e\u7ed3\u6784\u5316\u3001GPS\u62d2\u6b62\u73af\u5883\u4e2d\u90e8\u7f72\u81ea\u4e3b\u7740\u9646\u7684\u57fa\u7840\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u9488\u5bf9\u5927\u578bQuadPlane\u7684\u9ad8\u60ef\u6027\u3001\u6709\u9650\u63a8\u529b\u77e2\u91cf\u548c\u6162\u54cd\u5e94\u65f6\u95f4\u7b49\u98de\u884c\u7279\u6027\u8fdb\u884c\u4e86\u4f18\u5316\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5728\u957f\u8ddd\u79bbQuadPlane\u64cd\u4f5c\u4e2d\u5b9e\u73b0\u9ad8\u6548\u89c6\u89c9\u81ea\u4e3b\u7740\u9646\u548c\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u5b9e\u90e8\u7f72\u4e2d\u7684\u786c\u4ef6\u7ea6\u675f\u548c\u98de\u884c\u7279\u6027\u6311\u6218\uff0c\u4e3a\u52a8\u6001GPS\u62d2\u6b62\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u7740\u9646\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.09148", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09148", "abs": "https://arxiv.org/abs/2512.09148", "authors": ["Shanghao Li", "Jinda Han", "Yibo Wang", "Yuanjie Zhu", "Zihe Song", "Langzhou He", "Kenan Kamel A Alghythee", "Philip S. Yu"], "title": "Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment", "comment": null, "summary": "Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u8f7b\u91cf\u7ea7\u53ef\u89e3\u91ca\u6027\u6307\u6807\uff08PRD\u548cSAS\uff09\u6765\u5206\u6790LLMs\u5728GraphRAG\u4e2d\u5904\u7406\u7ed3\u6784\u5316\u77e5\u8bc6\u65f6\u7684\u673a\u5236\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u8fd9\u4e9b\u6307\u6807\u7684\u5e7b\u89c9\u68c0\u6d4b\u5668GGA\uff0c\u4ee5\u63d0\u5347GraphRAG\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709GraphRAG\u65b9\u6cd5\u4e2d\uff0cLLMs\u96be\u4ee5\u6709\u6548\u7406\u89e3\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u5173\u7cfb\u548c\u62d3\u6251\u4fe1\u606f\uff0c\u5bfc\u81f4\u751f\u6210\u5185\u5bb9\u4e0e\u68c0\u7d22\u77e5\u8bc6\u4e0d\u4e00\u81f4\u7684\u5e7b\u89c9\u95ee\u9898\u3002\u9700\u8981\u6df1\u5165\u5206\u6790LLMs\u5728\u5904\u7406\u7ed3\u6784\u5316\u77e5\u8bc6\u65f6\u7684\u5185\u90e8\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u53ef\u89e3\u91ca\u6027\u6307\u6807\uff1a1) \u8def\u5f84\u4f9d\u8d56\u5ea6(PRD)\u8861\u91cf\u6a21\u578b\u5bf9\u6700\u77ed\u8def\u5f84\u4e09\u5143\u7ec4\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff1b2) \u8bed\u4e49\u5bf9\u9f50\u5206\u6570(SAS)\u8bc4\u4f30\u6a21\u578b\u5185\u90e8\u8868\u793a\u4e0e\u68c0\u7d22\u77e5\u8bc6\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002\u57fa\u4e8e\u8fd9\u4e9b\u6307\u6807\u5f00\u53d1\u4e86\u540e\u5904\u7406\u5e7b\u89c9\u68c0\u6d4b\u5668GGA\u3002", "result": "\u5728\u77e5\u8bc6\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u5206\u6790\u53d1\u73b0\uff0c\u9ad8PRD\u548c\u4f4eSAS\u5206\u6570\u4e0e\u5e7b\u89c9\u6a21\u5f0f\u76f8\u5173\u3002GGA\u68c0\u6d4b\u5668\u5728AUC\u548cF1\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u4e8e\u8bed\u4e49\u548c\u7f6e\u4fe1\u5ea6\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u673a\u5236\u53ef\u89e3\u91ca\u6027\u5206\u6790LLMs\u7684\u7ed3\u6784\u5c40\u9650\u6027\u5982\u4f55\u5bfc\u81f4\u5e7b\u89c9\uff0c\u4e3a\u672a\u6765\u8bbe\u8ba1\u66f4\u53ef\u9760\u7684GraphRAG\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6d1e\u89c1\uff0c\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u6307\u6807\u548c\u68c0\u6d4b\u5668\u6709\u52a9\u4e8e\u63d0\u5347\u77e5\u8bc6\u589e\u5f3a\u751f\u6210\u7684\u8d28\u91cf\u3002"}}
{"id": "2512.09349", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09349", "abs": "https://arxiv.org/abs/2512.09349", "authors": ["Lin Li", "Yuxin Cai", "Jianwu Fang", "Jianru Xue", "Chen Lv"], "title": "COVLM-RL: Critical Object-Oriented Reasoning for Autonomous Driving Using VLM-Guided Reinforcement Learning", "comment": "8 pages, 5 figures", "summary": "End-to-end autonomous driving frameworks face persistent challenges in generalization, training efficiency, and interpretability. While recent methods leverage Vision-Language Models (VLMs) through supervised learning on large-scale datasets to improve reasoning, they often lack robustness in novel scenarios. Conversely, reinforcement learning (RL)-based approaches enhance adaptability but remain data-inefficient and lack transparent decision-making. % contribution To address these limitations, we propose COVLM-RL, a novel end-to-end driving framework that integrates Critical Object-oriented (CO) reasoning with VLM-guided RL. Specifically, we design a Chain-of-Thought (CoT) prompting strategy that enables the VLM to reason over critical traffic elements and generate high-level semantic decisions, effectively transforming multi-view visual inputs into structured semantic decision priors. These priors reduce the input dimensionality and inject task-relevant knowledge into the RL loop, accelerating training and improving policy interpretability. However, bridging high-level semantic guidance with continuous low-level control remains non-trivial. To this end, we introduce a consistency loss that encourages alignment between the VLM's semantic plans and the RL agent's control outputs, enhancing interpretability and training stability. Experiments conducted in the CARLA simulator demonstrate that COVLM-RL significantly improves the success rate by 30\\% in trained driving environments and by 50\\% in previously unseen environments, highlighting its strong generalization capability.", "AI": {"tldr": "COVLM-RL\uff1a\u7ed3\u5408\u5173\u952e\u5bf9\u8c61\u5bfc\u5411\u63a8\u7406\u4e0eVLM\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u51b3\u7b56\u5148\u9a8c\u548c\u4e00\u81f4\u6027\u635f\u5931\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3001\u8bad\u7ec3\u6548\u7387\u4f4e\u548c\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\u3002\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u7f3a\u4e4f\u65b0\u573a\u666f\u9c81\u68d2\u6027\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6570\u636e\u6548\u7387\u4f4e\u4e14\u51b3\u7b56\u4e0d\u900f\u660e\u3002", "method": "1. \u8bbe\u8ba1\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u7b56\u7565\uff0c\u8ba9VLM\u5bf9\u5173\u952e\u4ea4\u901a\u5143\u7d20\u8fdb\u884c\u63a8\u7406\u5e76\u751f\u6210\u9ad8\u7ea7\u8bed\u4e49\u51b3\u7b56\uff0c\u5c06\u591a\u89c6\u89d2\u89c6\u89c9\u8f93\u5165\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u8bed\u4e49\u51b3\u7b56\u5148\u9a8c\uff1b2. \u5f15\u5165\u4e00\u81f4\u6027\u635f\u5931\uff0c\u786e\u4fddVLM\u7684\u8bed\u4e49\u89c4\u5212\u4e0eRL\u667a\u80fd\u4f53\u7684\u63a7\u5236\u8f93\u51fa\u5bf9\u9f50\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e2d\uff0cCOVLM-RL\u5728\u5df2\u8bad\u7ec3\u9a7e\u9a76\u73af\u5883\u4e2d\u6210\u529f\u7387\u63d0\u534730%\uff0c\u5728\u672a\u89c1\u73af\u5883\u4e2d\u6210\u529f\u7387\u63d0\u534750%\uff0c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "COVLM-RL\u901a\u8fc7\u7ed3\u5408VLM\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u548cRL\u7684\u9002\u5e94\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6cdb\u5316\u3001\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u4e3a\u7aef\u5230\u7aef\u9a7e\u9a76\u6846\u67b6\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.09149", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09149", "abs": "https://arxiv.org/abs/2512.09149", "authors": ["Anton Vasiliuk", "Irina Abdullaeva", "Polina Druzhinina", "Anton Razzhigaev", "Andrey Kuznetsov"], "title": "MindShift: Analyzing Language Models' Reactions to Psychological Prompts", "comment": null, "summary": "Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86MindShift\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6a21\u62df\u4eba\u7c7b\u4eba\u683c\u7279\u8d28\u65b9\u9762\u7684\u5fc3\u7406\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u6539\u7f16MMPI\u5fc3\u7406\u6d4b\u8bd5\u548c\u521b\u5efa\u4eba\u683c\u5bfc\u5411\u63d0\u793a\u6765\u6d4b\u91cfLLMs\u7684\u89d2\u8272\u626e\u6f14\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5438\u6536\u548c\u53cd\u6620\u7528\u6237\u6307\u5b9a\u4eba\u683c\u7279\u8d28\u548c\u6001\u5ea6\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u5fc3\u7406\u9002\u5e94\u6027\u548c\u89d2\u8272\u626e\u6f14\u80fd\u529b\uff0c\u4ee5\u4e86\u89e3\u5b83\u4eec\u6a21\u62df\u4eba\u7c7b\u4eba\u683c\u7279\u8d28\u7684\u6548\u679c\u3002", "method": "\u6539\u7f16\u5fc3\u7406\u5b66\u4e2d\u6700\u5e38\u7528\u7684\u660e\u5c3c\u82cf\u8fbe\u591a\u9879\u4eba\u683c\u91cf\u8868\uff08MMPI\uff09\uff0c\u521b\u5efa\u4eba\u683c\u5bfc\u5411\u63d0\u793a\uff0c\u8bbe\u8ba1\u4e0d\u540c\u7279\u8d28\u5f3a\u5ea6\u7684\u4eba\u683c\u89d2\u8272\uff0c\u6784\u5efaMindShift\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30LLMs\u7684\u5fc3\u7406\u9002\u5e94\u6027\u3002", "result": "LLMs\u5728\u89d2\u8272\u611f\u77e5\u65b9\u9762\u6709\u6301\u7eed\u6539\u8fdb\uff0c\u5f52\u56e0\u4e8e\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u5bf9\u9f50\u6280\u672f\u7684\u8fdb\u6b65\uff1b\u4e0d\u540c\u6a21\u578b\u7c7b\u578b\u548c\u5bb6\u65cf\u5728\u5fc3\u7406\u8bc4\u4f30\u54cd\u5e94\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660e\u5b83\u4eec\u5728\u6a21\u62df\u4eba\u7c7b\u4eba\u683c\u7279\u8d28\u80fd\u529b\u4e0a\u7684\u53ef\u53d8\u6027\u3002", "conclusion": "MindShift\u57fa\u51c6\u6d4b\u8bd5\u6709\u6548\u8bc4\u4f30\u4e86LLMs\u7684\u5fc3\u7406\u9002\u5e94\u6027\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u4eba\u683c\u7279\u8d28\u6a21\u62df\u65b9\u9762\u7684\u8fdb\u6b65\u548c\u5dee\u5f02\uff0c\u4e3a\u672a\u6765LLMs\u7684\u5fc3\u7406\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5de5\u5177\u548c\u65b9\u6cd5\u3002"}}
{"id": "2512.09377", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09377", "abs": "https://arxiv.org/abs/2512.09377", "authors": ["Lidan Xu", "Dadong Fan", "Junhong Wang", "Wenshuo Li", "Hao Lu", "Jianzhong Qiao"], "title": "Observability Analysis and Composite Disturbance Filtering for a Bar Tethered to Dual UAVs Subject to Multi-source Disturbances", "comment": null, "summary": "Cooperative suspended aerial transportation is highly susceptible to multi-source disturbances such as aerodynamic effects and thrust uncertainties. To achieve precise load manipulation, existing methods often rely on extra sensors to measure cable directions or the payload's pose, which increases the system cost and complexity. A fundamental question remains: is the payload's pose observable under multi-source disturbances using only the drones' odometry information? To answer this question, this work focuses on the two-drone-bar system and proves that the whole system is observable when only two or fewer types of lumped disturbances exist by using the observability rank criterion. To the best of our knowledge, we are the first to present such a conclusion and this result paves the way for more cost-effective and robust systems by minimizing their sensor suites. Next, to validate this analysis, we consider the situation where the disturbances are only exerted on the drones, and develop a composite disturbance filtering scheme. A disturbance observer-based error-state extended Kalman filter is designed for both state and disturbance estimation, which renders improved estimation performance for the whole system evolving on the manifold $(\\mathbb{R}^3)^2\\times(TS^2)^3$. Our simulation and experimental tests have validated that it is possible to fully estimate the state and disturbance of the system with only odometry information of the drones.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u4ec5\u4f7f\u7528\u65e0\u4eba\u673a\u91cc\u7a0b\u8ba1\u4fe1\u606f\u5373\u53ef\u89c2\u6d4b\u53cc\u65e0\u4eba\u673a-\u6746\u7cfb\u7edf\u7684\u72b6\u6001\u548c\u6270\u52a8\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8e\u6270\u52a8\u89c2\u6d4b\u5668\u7684\u8bef\u5dee\u72b6\u6001\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u989d\u5916\u4f20\u611f\u5668\u6d4b\u91cf\u7f06\u7ef3\u65b9\u5411\u6216\u8d1f\u8f7d\u59ff\u6001\uff0c\u589e\u52a0\u4e86\u7cfb\u7edf\u6210\u672c\u548c\u590d\u6742\u6027\u3002\u6838\u5fc3\u95ee\u9898\u662f\uff1a\u5728\u591a\u6e90\u6270\u52a8\u4e0b\uff0c\u4ec5\u4f7f\u7528\u65e0\u4eba\u673a\u91cc\u7a0b\u8ba1\u4fe1\u606f\u662f\u5426\u53ef\u89c2\u6d4b\u8d1f\u8f7d\u59ff\u6001\uff1f", "method": "\u9488\u5bf9\u53cc\u65e0\u4eba\u673a-\u6746\u7cfb\u7edf\uff0c\u4f7f\u7528\u53ef\u89c2\u6d4b\u6027\u79e9\u5224\u636e\u8bc1\u660e\u5f53\u53ea\u6709\u4e24\u79cd\u6216\u66f4\u5c11\u7c7b\u578b\u7684\u96c6\u603b\u6270\u52a8\u65f6\u7cfb\u7edf\u5b8c\u5168\u53ef\u89c2\u6d4b\u3002\u5f00\u53d1\u4e86\u57fa\u4e8e\u6270\u52a8\u89c2\u6d4b\u5668\u7684\u8bef\u5dee\u72b6\u6001\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u7528\u4e8e\u5728\u6d41\u5f62 $(\\mathbb{R}^3)^2\\times(TS^2)^3$ \u4e0a\u8fdb\u884c\u72b6\u6001\u548c\u6270\u52a8\u4f30\u8ba1\u3002", "result": "\u8bc1\u660e\u4e86\u4ec5\u4f7f\u7528\u65e0\u4eba\u673a\u91cc\u7a0b\u8ba1\u4fe1\u606f\u5373\u53ef\u5b8c\u5168\u4f30\u8ba1\u7cfb\u7edf\u72b6\u6001\u548c\u6270\u52a8\uff0c\u8fd9\u662f\u9996\u6b21\u63d0\u51fa\u8fd9\u6837\u7684\u7ed3\u8bba\u3002\u4eff\u771f\u548c\u5b9e\u9a8c\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u6700\u5c0f\u5316\u4f20\u611f\u5668\u5957\u4ef6\uff0c\u4e3a\u66f4\u7ecf\u6d4e\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002\u9a8c\u8bc1\u4e86\u4ec5\u4f7f\u7528\u65e0\u4eba\u673a\u91cc\u7a0b\u8ba1\u4fe1\u606f\u5373\u53ef\u5b8c\u5168\u4f30\u8ba1\u7cfb\u7edf\u72b6\u6001\u548c\u6270\u52a8\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.09212", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09212", "abs": "https://arxiv.org/abs/2512.09212", "authors": ["Zixuan Liu", "Siavash H. Khajavi", "Guangkai Jiang", "Xinru Liu"], "title": "Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based LLM Alignment", "comment": null, "summary": "Reward-model-based fine-tuning is a central paradigm in aligning Large Language Models with human preferences. However, such approaches critically rely on the assumption that proxy reward models accurately reflect intended supervision, a condition often violated due to annotation noise, bias, or limited coverage. This misalignment can lead to undesirable behaviors, where models optimize for flawed signals rather than true human values. In this paper, we investigate a novel framework to identify and mitigate such misalignment by treating the fine-tuning process as a form of knowledge integration. We focus on detecting instances of proxy-policy conflicts, cases where the base model strongly disagrees with the proxy. We argue that such conflicts often signify areas of shared ignorance, where neither the policy nor the reward model possesses sufficient knowledge, making them especially susceptible to misalignment. To this end, we propose two complementary metrics for identifying these conflicts: a localized Proxy-Policy Alignment Conflict Score (PACS) and a global Kendall-Tau Distance measure. Building on this insight, we design an algorithm named Selective Human-in-the-loop Feedback via Conflict-Aware Sampling (SHF-CAS) that targets high-conflict QA pairs for additional feedback, refining both the reward model and policy efficiently. Experiments on two alignment tasks demonstrate that our approach enhances general alignment performance, even when trained with a biased proxy reward. Our work provides a new lens for interpreting alignment failures and offers a principled pathway for targeted refinement in LLM training.", "AI": {"tldr": "\u63d0\u51faSHF-CAS\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u4ee3\u7406\u5956\u52b1\u6a21\u578b\u4e0e\u57fa\u7840\u6a21\u578b\u4e4b\u95f4\u7684\u51b2\u7a81\u6765\u8bc6\u522b\u5bf9\u9f50\u5931\u8d25\u533a\u57df\uff0c\u5e76\u9009\u62e9\u6027\u6536\u96c6\u4eba\u7c7b\u53cd\u9988\u8fdb\u884c\u9ad8\u6548\u4fee\u6b63\u3002", "motivation": "\u57fa\u4e8e\u5956\u52b1\u6a21\u578b\u7684\u5fae\u8c03\u65b9\u6cd5\u5047\u8bbe\u4ee3\u7406\u5956\u52b1\u6a21\u578b\u80fd\u51c6\u786e\u53cd\u6620\u4eba\u7c7b\u504f\u597d\uff0c\u4f46\u5b9e\u9645\u4e2d\u5e38\u56e0\u6807\u6ce8\u566a\u58f0\u3001\u504f\u89c1\u6216\u8986\u76d6\u4e0d\u8db3\u800c\u5931\u6548\uff0c\u5bfc\u81f4\u6a21\u578b\u4f18\u5316\u9519\u8bef\u4fe1\u53f7\u800c\u975e\u771f\u5b9e\u4eba\u7c7b\u4ef7\u503c\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u51b2\u7a81\u68c0\u6d4b\u6307\u6807\uff1a\u5c40\u90e8\u4ee3\u7406-\u7b56\u7565\u5bf9\u9f50\u51b2\u7a81\u5206\u6570(PACS)\u548c\u5168\u5c40Kendall-Tau\u8ddd\u79bb\u3002\u57fa\u4e8e\u6b64\u8bbe\u8ba1SHF-CAS\u7b97\u6cd5\uff0c\u9009\u62e9\u6027\u6536\u96c6\u9ad8\u51b2\u7a81QA\u5bf9\u7684\u4eba\u7c7b\u53cd\u9988\uff0c\u540c\u65f6\u4f18\u5316\u5956\u52b1\u6a21\u578b\u548c\u7b56\u7565\u3002", "result": "\u5728\u4e24\u4e2a\u5bf9\u9f50\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5373\u4f7f\u5728\u6709\u504f\u4ee3\u7406\u5956\u52b1\u4e0b\u8bad\u7ec3\uff0c\u4e5f\u80fd\u63d0\u5347\u6574\u4f53\u5bf9\u9f50\u6027\u80fd\u3002", "conclusion": "\u4e3a\u89e3\u91ca\u5bf9\u9f50\u5931\u8d25\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u4e3aLLM\u8bad\u7ec3\u4e2d\u7684\u9488\u5bf9\u6027\u4fee\u6b63\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8def\u5f84\u3002"}}
{"id": "2512.09406", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09406", "abs": "https://arxiv.org/abs/2512.09406", "authors": ["Hai Ci", "Xiaokang Liu", "Pei Yang", "Yiren Song", "Mike Zheng Shou"], "title": "H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos", "comment": "13 pages, 6 figures", "summary": "Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/", "AI": {"tldr": "\u63d0\u51fa\u89c6\u9891\u5230\u89c6\u9891\u7ffb\u8bd1\u6846\u67b6\uff0c\u5c06\u666e\u901a\u4eba\u673a\u4ea4\u4e92\u89c6\u9891\u8f6c\u6362\u4e3a\u8fd0\u52a8\u4e00\u81f4\u3001\u7269\u7406\u57fa\u7840\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\uff0c\u65e0\u9700\u914d\u5bf9\u7684\u4eba-\u673a\u5668\u4eba\u89c6\u9891\u8bad\u7ec3\u6570\u636e", "motivation": "\u8ba9\u673a\u5668\u4eba\u4ece\u65e5\u5e38\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u64cd\u4f5c\u6280\u80fd\uff0c\u907f\u514d\u7e41\u7410\u7684\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\uff0c\u6269\u5927\u673a\u5668\u4eba\u5b66\u4e60\u80fd\u529b", "method": "\u91c7\u7528\u53ef\u8fc1\u79fb\u8868\u793a\u6865\u63a5\u5b9e\u4f53\u5dee\u8ddd\uff1a\u901a\u8fc7\u4fee\u590d\u8bad\u7ec3\u89c6\u9891\u4e2d\u7684\u673a\u5668\u4eba\u624b\u81c2\u83b7\u5f97\u5e72\u51c0\u80cc\u666f\uff0c\u53e0\u52a0\u89c6\u89c9\u63d0\u793a\uff08\u6807\u8bb0\u548c\u7bad\u5934\u8868\u793a\u5939\u722a\u4f4d\u7f6e\u548c\u65b9\u5411\uff09\uff0c\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u91cd\u65b0\u63d2\u5165\u673a\u5668\u4eba\u624b\u81c2\u3002\u6d4b\u8bd5\u65f6\u5bf9\u4eba\u7269\u89c6\u9891\u5e94\u7528\u76f8\u540c\u5904\u7406\uff0c\u751f\u6210\u6a21\u4eff\u4eba\u7c7b\u52a8\u4f5c\u7684\u9ad8\u8d28\u91cf\u673a\u5668\u4eba\u89c6\u9891", "result": "\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u771f\u5b9e\u3001\u66f4\u7269\u7406\u57fa\u7840\u7684\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u4e3a\u4ece\u65e0\u6807\u7b7e\u4eba\u7c7b\u89c6\u9891\u6269\u5c55\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411", "conclusion": "\u63d0\u51fa\u7684\u89c6\u9891\u5230\u89c6\u9891\u7ffb\u8bd1\u6846\u67b6\u6210\u529f\u5c06\u4eba\u7c7b\u89c6\u9891\u8f6c\u6362\u4e3a\u7269\u7406\u57fa\u7840\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u89c6\u9891\uff0c\u65e0\u9700\u914d\u5bf9\u8bad\u7ec3\u6570\u636e\uff0c\u4e3a\u5927\u89c4\u6a21\u673a\u5668\u4eba\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84"}}
{"id": "2512.09222", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09222", "abs": "https://arxiv.org/abs/2512.09222", "authors": ["Vishwas Hegde", "Vindhya Shigehalli"], "title": "CORE: A Conceptual Reasoning Layer for Large Language Models", "comment": "Independent system-level architectural proposal with accompanying proof-of-concept", "summary": "Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.", "AI": {"tldr": "CORE\u63d0\u51fa\u6982\u5ff5\u4f18\u5148\u7684\u4ea4\u4e92\u5c42\uff0c\u901a\u8fc7\u6301\u4e45\u5316\u672c\u5730\u6982\u5ff5\u72b6\u6001\u548c\u8ba4\u77e5\u64cd\u4f5c\u7b26\uff0c\u51cf\u5c11\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u5386\u53f2\u91cd\u653e\uff0c\u63d0\u9ad8\u7a33\u5b9a\u6027\u5e76\u964d\u4f4etoken\u6d88\u8017\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5355\u8f6e\u751f\u6210\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u9700\u8981\u4ece\u4e0d\u65ad\u589e\u957f\u7684token\u5386\u53f2\u4e2d\u91cd\u5efa\u7528\u6237\u610f\u56fe\u548c\u4efb\u52a1\u72b6\u6001\uff0c\u5bfc\u81f4\u6f02\u79fb\u3001\u63a8\u7406\u6a21\u5f0f\u4e0d\u4e00\u81f4\u548c\u63d0\u793a\u589e\u957f\u7b49\u95ee\u9898\u3002", "method": "CORE\u7ed3\u5408\u5c0f\u578b\u901a\u7528\u8ba4\u77e5\u64cd\u4f5c\u7b26\u5e93\u548c\u6301\u4e45\u5316\u672c\u5730\u6982\u5ff5\u72b6\u6001\uff0c\u6bcf\u4e2a\u6a21\u578b\u8c03\u7528\u53ea\u63a5\u6536\u6982\u5ff5\u72b6\u6001\u3001\u6700\u65b0\u6307\u4ee4\u548c\u9009\u5b9a\u64cd\u4f5c\u7b26\uff0c\u65e0\u9700\u91cd\u653e\u5b8c\u6574\u5386\u53f2\u3002", "result": "\u521d\u6b65\u539f\u578b\u6a21\u62df\u663e\u793a\u7d2f\u8ba1\u63d0\u793atoken\u51cf\u5c11\u7ea642%\uff0c\u4f46\u8be5\u6570\u5b57\u53cd\u6620\u539f\u578b\u6761\u4ef6\uff0c\u4e0d\u5e94\u89c6\u4e3a\u5b9e\u9645\u6027\u80fd\u4f30\u8ba1\u3002", "conclusion": "CORE\u63d0\u4f9b\u6a21\u578b\u65e0\u5173\u7684\u673a\u5236\uff0c\u5c06\u6982\u5ff5\u63a8\u7406\u4e0e\u8bed\u8a00\u751f\u6210\u5206\u79bb\uff0c\u4e3a\u66f4\u7a33\u5b9a\u7684\u591a\u8f6e\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u65b9\u5411\u3002"}}
{"id": "2512.09410", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.09410", "abs": "https://arxiv.org/abs/2512.09410", "authors": ["Jialin Ying", "Zhihao Li", "Zicheng Dong", "Guohua Wu", "Yihuan Liao"], "title": "Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation", "comment": "7 pages, 7 figures", "summary": "Collaborative pursuit-evasion in cluttered environments presents significant challenges due to sparse rewards and constrained Fields of View (FOV). Standard Multi-Agent Reinforcement Learning (MARL) often suffers from inefficient exploration and fails to scale to large scenarios. We propose PGF-MAPPO (Path-Guided Frontier MAPPO), a hierarchical framework bridging topological planning with reactive control. To resolve local minima and sparse rewards, we integrate an A*-based potential field for dense reward shaping. Furthermore, we introduce Directional Frontier Allocation, combining Farthest Point Sampling (FPS) with geometric angle suppression to enforce spatial dispersion and accelerate coverage. The architecture employs a parameter-shared decentralized critic, maintaining O(1) model complexity suitable for robotic swarms. Experiments demonstrate that PGF-MAPPO achieves superior capture efficiency against faster evaders. Policies trained on 10x10 maps exhibit robust zero-shot generalization to unseen 20x20 environments, significantly outperforming rule-based and learning-based baselines.", "AI": {"tldr": "PGF-MAPPO\uff1a\u4e00\u79cd\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8def\u5f84\u5f15\u5bfc\u7684\u62d3\u6251\u89c4\u5212\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8ffd\u6355\u4e2d\u7684\u7a00\u758f\u5956\u52b1\u548c\u89c6\u91ce\u53d7\u9650\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u548c\u96f6\u6837\u672c\u6cdb\u5316\u3002", "motivation": "\u5728\u6742\u4e71\u73af\u5883\u4e2d\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u8ffd\u6355\u9762\u4e34\u7a00\u758f\u5956\u52b1\u548c\u89c6\u91ce\u53d7\u9650\u7684\u6311\u6218\uff0c\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u63a2\u7d22\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u573a\u666f\u3002", "method": "\u63d0\u51faPGF-MAPPO\u5206\u5c42\u6846\u67b6\uff0c\u7ed3\u5408A*\u52bf\u573a\u8fdb\u884c\u5bc6\u96c6\u5956\u52b1\u5851\u9020\uff0c\u91c7\u7528\u65b9\u5411\u6027\u524d\u6cbf\u5206\u914d\uff08\u7ed3\u5408\u6700\u8fdc\u70b9\u91c7\u6837\u548c\u51e0\u4f55\u89d2\u5ea6\u6291\u5236\uff09\u5b9e\u73b0\u7a7a\u95f4\u5206\u6563\uff0c\u4f7f\u7528\u53c2\u6570\u5171\u4eab\u7684\u5206\u6563\u5f0f\u8bc4\u8bba\u5bb6\u4fdd\u6301O(1)\u6a21\u578b\u590d\u6742\u5ea6\u3002", "result": "PGF-MAPPO\u5728\u5bf9\u6297\u66f4\u5feb\u9003\u907f\u8005\u65f6\u5b9e\u73b0\u66f4\u4f18\u6355\u83b7\u6548\u7387\uff0c\u572810x10\u5730\u56fe\u4e0a\u8bad\u7ec3\u7684\u7b56\u7565\u80fd\u96f6\u6837\u672c\u6cdb\u5316\u523020x20\u672a\u89c1\u73af\u5883\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u548c\u5b66\u4e60\u7684\u57fa\u7840\u65b9\u6cd5\u3002", "conclusion": "PGF-MAPPO\u6210\u529f\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u8ffd\u6355\u4e2d\u7684\u63a2\u7d22\u548c\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u534f\u540c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09238", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09238", "abs": "https://arxiv.org/abs/2512.09238", "authors": ["Zeng You", "Yaofo Chen", "Shuhai Zhang", "Zhijie Qiu", "Tingyu Wu", "Yingjian Li", "Yaowei Wang", "Mingkui Tan"], "title": "Training-free Context-adaptive Attention for Efficient Long Context Modeling", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.", "AI": {"tldr": "TCA-Attention\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u7684\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5173\u6ce8\u4fe1\u606f\u4e30\u5bcc\u7684token\u6765\u5b9e\u73b0\u9ad8\u6548\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u5728128K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u5b9e\u73b02.8\u500d\u52a0\u901f\u548c61%\u7684KV\u7f13\u5b58\u51cf\u5c11\u3002", "motivation": "\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u5e26\u6765\u663e\u8457\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6311\u6218\u3002\u73b0\u6709\u7684\u7a00\u758f\u6ce8\u610f\u529b\u548cKV\u7f13\u5b58\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u4f9d\u8d56\u56fa\u5b9a\u6a21\u5f0f\u3001\u65e0\u6cd5\u540c\u65f6\u5904\u7406\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u3001\u6216\u9700\u8981\u989d\u5916\u8bad\u7ec3\u7b49\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u6ce8\u610f\u529b(TCA-Attention)\uff0c\u5305\u542b\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u9636\u6bb5\uff1a1)\u79bb\u7ebf\u6821\u51c6\u9636\u6bb5\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u786e\u5b9a\u5934\u7279\u5b9a\u7684\u7a00\u758f\u9884\u7b97\uff1b2)\u5728\u7ebftoken\u9009\u62e9\u9636\u6bb5\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5197\u4f59\u5ea6\u91cf\u81ea\u9002\u5e94\u4fdd\u7559\u6838\u5fc3\u4e0a\u4e0b\u6587token\u3002", "result": "\u5728128K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u5b9e\u73b02.8\u500d\u52a0\u901f\uff0c\u51cf\u5c1161%\u7684KV\u7f13\u5b58\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5b8c\u6574\u6ce8\u610f\u529b\u76f8\u5f53\u7684\u6027\u80fd\u3002\u7406\u8bba\u5206\u6790\u663e\u793a\u8be5\u65b9\u6cd5\u4fdd\u6301\u6709\u754c\u8fd1\u4f3c\u8bef\u5dee\u3002", "conclusion": "TCA-Attention\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u5373\u63d2\u5373\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u6216\u67b6\u6784\u66f4\u6539\u5373\u53ef\u52a0\u901f\u9884\u586b\u5145\u548c\u89e3\u7801\uff0c\u540c\u65f6\u51cf\u5c11KV\u7f13\u5b58\u5185\u5b58\u5360\u7528\uff0c\u4e3a\u9ad8\u6548\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\u3002"}}
{"id": "2512.09411", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09411", "abs": "https://arxiv.org/abs/2512.09411", "authors": ["Siting Zhu", "Yuxiang Huang", "Wenhua Wu", "Chaokang Jiang", "Yongbo Chen", "I-Ming Chen", "Hesheng Wang"], "title": "D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM", "comment": null, "summary": "Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments. However, dense SLAM in dynamic environments remains challenging. Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects. In this paper, we present D$^2$GSLAM, a novel dynamic SLAM system utilizing Gaussian representation, which simultaneously performs accurate dynamic reconstruction and robust tracking within dynamic environments. Our system is composed of four key components: (i) We propose a geometric-prompt dynamic separation method to distinguish between static and dynamic elements of the scene. This approach leverages the geometric consistency of Gaussian representation and scene geometry to obtain coarse dynamic regions. The regions then serve as prompts to guide the refinement of the coarse mask for achieving accurate motion mask. (ii) To facilitate accurate and efficient mapping of the dynamic scene, we introduce dynamic-static composite representation that integrates static 3D Gaussians with dynamic 4D Gaussians. This representation allows for modeling the transitions between static and dynamic states of objects in the scene for composite mapping and optimization. (iii) We employ a progressive pose refinement strategy that leverages both the multi-view consistency of static scene geometry and motion information from dynamic objects to achieve accurate camera tracking. (iv) We introduce a motion consistency loss, which leverages the temporal continuity in object motions for accurate dynamic modeling. Our D$^2$GSLAM demonstrates superior performance on dynamic scenes in terms of mapping and tracking accuracy, while also showing capability in accurate dynamic modeling.", "AI": {"tldr": "D\u00b2GSLAM\uff1a\u57fa\u4e8e\u9ad8\u65af\u8868\u793a\u7684\u52a8\u6001SLAM\u7cfb\u7edf\uff0c\u80fd\u5728\u52a8\u6001\u73af\u5883\u4e2d\u540c\u65f6\u8fdb\u884c\u51c6\u786e\u7684\u52a8\u6001\u91cd\u5efa\u548c\u9c81\u68d2\u8ddf\u8e2a", "motivation": "\u73b0\u6709\u5bc6\u96c6SLAM\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u4e3b\u8981\u79fb\u9664\u52a8\u6001\u7269\u4f53\uff0c\u4e13\u6ce8\u4e8e\u9759\u6001\u573a\u666f\u91cd\u5efa\uff0c\u5ffd\u7565\u4e86\u52a8\u6001\u7269\u4f53\u5305\u542b\u7684\u8fd0\u52a8\u4fe1\u606f\u3002\u9700\u8981\u5f00\u53d1\u80fd\u540c\u65f6\u5904\u7406\u52a8\u6001\u548c\u9759\u6001\u5143\u7d20\u7684SLAM\u7cfb\u7edf\u3002", "method": "1) \u51e0\u4f55\u63d0\u793a\u52a8\u6001\u5206\u79bb\u65b9\u6cd5\uff1a\u5229\u7528\u9ad8\u65af\u8868\u793a\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u533a\u5206\u9759\u6001\u548c\u52a8\u6001\u5143\u7d20\uff1b2) \u52a8\u6001-\u9759\u6001\u590d\u5408\u8868\u793a\uff1a\u7ed3\u5408\u9759\u60013D\u9ad8\u65af\u548c\u52a8\u60014D\u9ad8\u65af\uff1b3) \u6e10\u8fdb\u4f4d\u59ff\u7ec6\u5316\u7b56\u7565\uff1a\u5229\u7528\u9759\u6001\u573a\u666f\u51e0\u4f55\u7684\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u52a8\u6001\u7269\u4f53\u8fd0\u52a8\u4fe1\u606f\uff1b4) \u8fd0\u52a8\u4e00\u81f4\u6027\u635f\u5931\uff1a\u5229\u7528\u7269\u4f53\u8fd0\u52a8\u7684\u65f6\u95f4\u8fde\u7eed\u6027", "result": "\u5728\u52a8\u6001\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u5efa\u56fe\u548c\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u540c\u65f6\u5177\u5907\u51c6\u786e\u7684\u52a8\u6001\u5efa\u6a21\u80fd\u529b", "conclusion": "D\u00b2GSLAM\u901a\u8fc7\u521b\u65b0\u7684\u52a8\u6001\u5206\u79bb\u3001\u590d\u5408\u8868\u793a\u548c\u4f18\u5316\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001SLAM\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u540c\u65f6\u7684\u9759\u6001\u548c\u52a8\u6001\u573a\u666f\u91cd\u5efa\u4e0e\u8ddf\u8e2a"}}
{"id": "2512.09292", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09292", "abs": "https://arxiv.org/abs/2512.09292", "authors": ["Kevin Stowe", "Svetlana Afanaseva", "Rodolfo Raimundo", "Yitao Sun", "Kailash Patil"], "title": "Identifying Bias in Machine-generated Text Detection", "comment": "13 pages, 2 figures, 7 tables", "summary": "The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b016\u4e2a\u82f1\u6587\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u7cfb\u7edf\u5b58\u5728\u591a\u79cd\u504f\u89c1\uff0c\u7279\u522b\u662f\u5c06\u5f31\u52bf\u7fa4\u4f53\uff08\u5982\u82f1\u8bed\u5b66\u4e60\u8005\u3001\u975e\u767d\u4eba\uff09\u7684\u6587\u672c\u8bef\u5224\u4e3a\u673a\u5668\u751f\u6210\uff0c\u800c\u4eba\u7c7b\u6807\u6ce8\u8005\u867d\u68c0\u6d4b\u80fd\u529b\u5dee\u4f46\u65e0\u663e\u8457\u504f\u89c1\u3002", "motivation": "\u968f\u7740\u6587\u672c\u751f\u6210\u80fd\u529b\u7684\u98de\u901f\u53d1\u5c55\uff0c\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u6280\u672f\u4e5f\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\u3002\u867d\u7136\u68c0\u6d4b\u6a21\u578b\u8868\u73b0\u51fa\u5f3a\u5927\u6027\u80fd\uff0c\u4f46\u5b83\u4eec\u53ef\u80fd\u5e26\u6765\u663e\u8457\u7684\u8d1f\u9762\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u82f1\u6587\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u7cfb\u7edf\u4e2d\u6f5c\u5728\u7684\u504f\u89c1\u95ee\u9898\u3002", "method": "\u7814\u7a76\u6536\u96c6\u4e86\u5b66\u751f\u8bba\u6587\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e8616\u4e2a\u4e0d\u540c\u7684\u68c0\u6d4b\u7cfb\u7edf\u5728\u56db\u4e2a\u5c5e\u6027\u4e0a\u7684\u504f\u89c1\uff1a\u6027\u522b\u3001\u79cd\u65cf/\u6c11\u65cf\u3001\u82f1\u8bed\u5b66\u4e60\u8005\u72b6\u6001\u548c\u7ecf\u6d4e\u72b6\u51b5\u3002\u4f7f\u7528\u56de\u5f52\u6a21\u578b\u8bc4\u4f30\u504f\u89c1\u7684\u663e\u8457\u6027\u548c\u6548\u5e94\u5927\u5c0f\uff0c\u5e76\u8fdb\u884c\u4e9a\u7ec4\u5206\u6790\u3002\u540c\u65f6\u8fdb\u884c\u4e86\u4eba\u7c7b\u6807\u6ce8\u5b9e\u9a8c\u4f5c\u4e3a\u5bf9\u6bd4\u3002", "result": "\u53d1\u73b0\u504f\u89c1\u5728\u4e0d\u540c\u7cfb\u7edf\u4e2d\u4e0d\u4e00\u81f4\uff0c\u4f46\u5b58\u5728\u51e0\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u591a\u4e2a\u6a21\u578b\u503e\u5411\u4e8e\u5c06\u5f31\u52bf\u7fa4\u4f53\u7684\u6587\u672c\u5206\u7c7b\u4e3a\u673a\u5668\u751f\u6210\uff1b\u82f1\u8bed\u5b66\u4e60\u8005\u7684\u8bba\u6587\u66f4\u53ef\u80fd\u88ab\u8bef\u5224\u4e3a\u673a\u5668\u751f\u6210\uff1b\u7ecf\u6d4e\u56f0\u96be\u5b66\u751f\u7684\u8bba\u6587\u8f83\u5c11\u88ab\u8bef\u5224\uff1b\u975e\u767d\u4eba\u82f1\u8bed\u5b66\u4e60\u8005\u7684\u8bba\u6587\u76f8\u5bf9\u4e8e\u767d\u4eba\u82f1\u8bed\u5b66\u4e60\u8005\u88ab\u8fc7\u5ea6\u5206\u7c7b\u4e3a\u673a\u5668\u751f\u6210\u3002\u4eba\u7c7b\u6807\u6ce8\u8005\u6574\u4f53\u68c0\u6d4b\u8868\u73b0\u5dee\uff0c\u4f46\u5728\u7814\u7a76\u7684\u5c5e\u6027\u4e0a\u6ca1\u6709\u663e\u8457\u504f\u89c1\u3002", "conclusion": "\u673a\u5668\u751f\u6210\u6587\u672c\u68c0\u6d4b\u7cfb\u7edf\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u7279\u522b\u662f\u5bf9\u5f31\u52bf\u7fa4\u4f53\uff08\u5982\u82f1\u8bed\u5b66\u4e60\u8005\u3001\u975e\u767d\u4eba\uff09\u7684\u6587\u672c\u5b58\u5728\u8bef\u5224\u98ce\u9669\u3002\u8fd9\u4e9b\u504f\u89c1\u53ef\u80fd\u52a0\u5267\u793e\u4f1a\u4e0d\u5e73\u7b49\uff0c\u800c\u4eba\u7c7b\u6807\u6ce8\u8005\u867d\u7136\u68c0\u6d4b\u80fd\u529b\u6709\u9650\u4f46\u76f8\u5bf9\u516c\u5e73\u3002\u9700\u8981\u5f00\u53d1\u66f4\u516c\u5e73\u7684\u68c0\u6d4b\u7cfb\u7edf\u3002"}}
{"id": "2512.09431", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09431", "abs": "https://arxiv.org/abs/2512.09431", "authors": ["Quanyou Wang", "Mingzhang Zhu", "Ruochen Hou", "Kay Gillespie", "Alvin Zhu", "Shiqi Wang", "Yicheng Wang", "Gaberiel I. Fernandez", "Yeting Liu", "Colin Togashi", "Hyunwoo Nam", "Aditya Navghare", "Alex Xu", "Taoyuanmin Zhu", "Min Sung Ahn", "Arturo Flores Alvarez", "Justin Quan", "Ethan Hong", "Dennis W. Hong"], "title": "A Hierarchical, Model-Based System for High-Performance Humanoid Soccer", "comment": null, "summary": "The development of athletic humanoid robots has gained significant attention as advances in actuation, sensing, and control enable increasingly dynamic, real-world capabilities. RoboCup, an international competition of fully autonomous humanoid robots, provides a uniquely challenging benchmark for such systems, culminating in the long-term goal of competing against human soccer players by 2050. This paper presents the hardware and software innovations underlying our team's victory in the RoboCup 2024 Adult-Sized Humanoid Soccer Competition. On the hardware side, we introduce an adult-sized humanoid platform built with lightweight structural components, high-torque quasi-direct-drive actuators, and a specialized foot design that enables powerful in-gait kicks while preserving locomotion robustness. On the software side, we develop an integrated perception and localization framework that combines stereo vision, object detection, and landmark-based fusion to provide reliable estimates of the ball, goals, teammates, and opponents. A mid-level navigation stack then generates collision-aware, dynamically feasible trajectories, while a centralized behavior manager coordinates high-level decision making, role selection, and kick execution based on the evolving game state. The seamless integration of these subsystems results in fast, precise, and tactically effective gameplay, enabling robust performance under the dynamic and adversarial conditions of real matches. This paper presents the design principles, system architecture, and experimental results that contributed to ARTEMIS's success as the 2024 Adult-Sized Humanoid Soccer champion.", "AI": {"tldr": "ARTEMIS\u56e2\u961f\u8d62\u5f972024\u5e74RoboCup\u6210\u4eba\u5c3a\u5bf8\u4eba\u5f62\u673a\u5668\u4eba\u8db3\u7403\u8d5b\u51a0\u519b\uff0c\u901a\u8fc7\u8f7b\u91cf\u5316\u786c\u4ef6\u8bbe\u8ba1\u3001\u9ad8\u626d\u77e9\u51c6\u76f4\u9a71\u6267\u884c\u5668\u3001\u4e13\u7528\u8db3\u90e8\u8bbe\u8ba1\uff0c\u4ee5\u53ca\u96c6\u6210\u7684\u611f\u77e5\u5b9a\u4f4d\u3001\u5bfc\u822a\u89c4\u5212\u548c\u884c\u4e3a\u51b3\u7b56\u8f6f\u4ef6\u7cfb\u7edf\u3002", "motivation": "RoboCup\u4f5c\u4e3a\u5168\u81ea\u4e3b\u4eba\u5f62\u673a\u5668\u4eba\u7684\u56fd\u9645\u7ade\u8d5b\uff0c\u4e3a\u52a8\u6001\u73b0\u5b9e\u4e16\u754c\u80fd\u529b\u63d0\u4f9b\u4e86\u72ec\u7279\u6311\u6218\u6027\u57fa\u51c6\uff0c\u76ee\u6807\u662f205\u5e74\u524d\u4e0e\u4eba\u7c7b\u8db3\u7403\u8fd0\u52a8\u5458\u6bd4\u8d5b\u3002\u672c\u6587\u65e8\u5728\u5c55\u793a\u56e2\u961f\u8d62\u5f972024\u5e74\u6210\u4eba\u5c3a\u5bf8\u4eba\u5f62\u8db3\u7403\u8d5b\u51a0\u519b\u7684\u6280\u672f\u521b\u65b0\u3002", "method": "\u786c\u4ef6\u65b9\u9762\uff1a\u91c7\u7528\u8f7b\u91cf\u5316\u7ed3\u6784\u7ec4\u4ef6\u3001\u9ad8\u626d\u77e9\u51c6\u76f4\u9a71\u6267\u884c\u5668\u3001\u4e13\u7528\u8db3\u90e8\u8bbe\u8ba1\u5b9e\u73b0\u5f3a\u529b\u8e22\u7403\u540c\u65f6\u4fdd\u6301\u8fd0\u52a8\u7a33\u5b9a\u6027\u3002\u8f6f\u4ef6\u65b9\u9762\uff1a\u5f00\u53d1\u96c6\u6210\u611f\u77e5\u5b9a\u4f4d\u6846\u67b6\uff0c\u7ed3\u5408\u7acb\u4f53\u89c6\u89c9\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u5730\u6807\u878d\u5408\uff1b\u4e2d\u5c42\u5bfc\u822a\u6808\u751f\u6210\u78b0\u649e\u611f\u77e5\u7684\u52a8\u6001\u53ef\u884c\u8f68\u8ff9\uff1b\u96c6\u4e2d\u884c\u4e3a\u7ba1\u7406\u5668\u57fa\u4e8e\u6e38\u620f\u72b6\u6001\u534f\u8c03\u9ad8\u7ea7\u51b3\u7b56\u3001\u89d2\u8272\u9009\u62e9\u548c\u8e22\u7403\u6267\u884c\u3002", "result": "\u7cfb\u7edf\u96c6\u6210\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u7cbe\u786e\u3001\u6218\u672f\u6709\u6548\u7684\u6e38\u620f\u73a9\u6cd5\uff0c\u5728\u771f\u5b9e\u6bd4\u8d5b\u7684\u52a8\u6001\u5bf9\u6297\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u80fd\uff0c\u4f7fARTEMIS\u6210\u4e3a2024\u5e74\u6210\u4eba\u5c3a\u5bf8\u4eba\u5f62\u8db3\u7403\u8d5b\u51a0\u519b\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86ARTEMIS\u8d62\u5f97RoboCup 2024\u51a0\u519b\u7684\u8bbe\u8ba1\u539f\u5219\u3001\u7cfb\u7edf\u67b6\u6784\u548c\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u52a8\u6001\u80fd\u529b\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u63a8\u52a8\u4e86205\u5e74\u4e0e\u4eba\u7c7b\u6bd4\u8d5b\u76ee\u6807\u7684\u5b9e\u73b0\u3002"}}
{"id": "2512.09386", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09386", "abs": "https://arxiv.org/abs/2512.09386", "authors": ["Peter Baile Chen", "Weiyue Li", "Dan Roth", "Michael Cafarella", "Samuel Madden", "Jacob Andreas"], "title": "CONCUR: A Framework for Continual Constrained and Unconstrained Routing", "comment": null, "summary": "AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.", "AI": {"tldr": "CONCUR\u662f\u4e00\u4e2a\u6301\u7eed\u8def\u7531\u6846\u67b6\uff0c\u652f\u6301\u6709\u7ea6\u675f\u548c\u65e0\u7ea6\u675f\u8def\u7531\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u591a\u8868\u5f81\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u7684\u4efb\u52a1\u5230\u8ba1\u7b97\u7b56\u7565\u7684\u6620\u5c04\u3002", "motivation": "AI\u4efb\u52a1\u590d\u6742\u5ea6\u4e0d\u540c\uff0c\u9700\u8981\u4e0d\u540c\u7684\u8ba1\u7b97\u7b56\u7565\uff08\u6a21\u578b+\u89e3\u7801\u65b9\u6cd5\uff09\u3002\u73b0\u6709\u8def\u7531\u7cfb\u7edf\u901a\u5e38\u8bad\u7ec3\u5355\u4e00\u6a21\u578b\u8986\u76d6\u6240\u6709\u7b56\u7565\uff0c\u5bfc\u81f4\u6dfb\u52a0\u65b0\u7b56\u7565\u65f6\u9700\u8981\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\u3002\u540c\u65f6\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u5355\u4e00\u8f93\u5165\u8868\u5f81\uff0c\u96be\u4ee5\u6355\u6349\u8def\u7531\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u6b21\u4f18\u8def\u7531\u51b3\u7b56\u3002", "method": "\u63d0\u51faCONCUR\u6846\u67b6\uff1a1\uff09\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u4e3a\u6bcf\u4e2a\u7b56\u7565\u8bad\u7ec3\u72ec\u7acb\u7684\u9884\u6d4b\u5668\u6a21\u578b\uff0c\u652f\u6301\u65b0\u7b56\u7565\u7684\u65e0\u7f1d\u52a0\u5165\u4e14\u8bad\u7ec3\u6210\u672c\u4f4e\uff1b2\uff09\u5229\u7528\u4efb\u52a1\u548c\u8ba1\u7b97\u7b56\u7565\u7684\u591a\u91cd\u8868\u5f81\u6765\u66f4\u597d\u5730\u6355\u6349\u95ee\u9898\u590d\u6742\u6027\uff1b3\uff09\u652f\u6301\u6709\u7ea6\u675f\u548c\u65e0\u7ea6\u675f\u4e24\u79cd\u8def\u7531\u6a21\u5f0f\u3002", "result": "\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u3001\u77e5\u8bc6\u548c\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCONCUR\u4f18\u4e8e\u6700\u4f73\u5355\u4e00\u7b56\u7565\u548c\u73b0\u6709\u8def\u7531\u6280\u672f\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u7aef\u5230\u7aef\u51c6\u786e\u7387\u548c\u66f4\u4f4e\u7684\u63a8\u7406\u6210\u672c\u3002\u5728\u6301\u7eed\u8bbe\u7f6e\u4e2d\u8fd8\u80fd\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u3002", "conclusion": "CONCUR\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u591a\u8868\u5f81\u5b66\u4e60\u89e3\u51b3\u4e86\u6301\u7eed\u8def\u7531\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u4efb\u52a1\u8def\u7531\uff0c\u5728\u51c6\u786e\u6027\u548c\u6210\u672c\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2512.09447", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09447", "abs": "https://arxiv.org/abs/2512.09447", "authors": ["Jaehyun Kim", "Seungwon Choi", "Tae-Wan Kim"], "title": "Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments", "comment": "8 pages, 4 figures", "summary": "We propose a descriptor-agnostic, multi-frame loop closure verification method that formulates LiDAR loop closure as a truncated Sequential Probability Ratio Test (SPRT). Instead of deciding from a single descriptor comparison or using fixed thresholds with late-stage Iterative Closest Point (ICP) vetting, the verifier accumulates a short temporal stream of descriptor similarities between a query and each candidate. It then issues an accept/reject decision adaptively once sufficient multi-frame evidence has been observed, according to user-specified Type-I/II error design targets. This precision-first policy is designed to suppress false positives in structurally repetitive indoor environments. We evaluate the verifier on a five-sequence library dataset, using a fixed retrieval front-end with several representative LiDAR global descriptors. Performance is assessed via segment-level K-hit precision-recall and absolute trajectory error (ATE) and relative pose error (RPE) after pose graph optimization. Across descriptors, the sequential verifier consistently improves precision and reduces the impact of aliased loops compared with single-frame and heuristic multi-frame baselines. Our implementation and dataset will be released at: https://github.com/wanderingcar/snu_library_dataset.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u63cf\u8ff0\u7b26\u65e0\u5173\u7684\u591a\u5e27\u95ed\u73af\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u5c06LiDAR\u95ed\u73af\u5efa\u6a21\u4e3a\u622a\u65ad\u5e8f\u8d2f\u6982\u7387\u6bd4\u68c0\u9a8c(SPRT)\uff0c\u901a\u8fc7\u7d2f\u79ef\u77ed\u65f6\u63cf\u8ff0\u7b26\u76f8\u4f3c\u6027\u6d41\u81ea\u9002\u5e94\u51b3\u7b56\uff0c\u5728\u91cd\u590d\u5ba4\u5185\u73af\u5883\u4e2d\u6291\u5236\u8bef\u62a5\u3002", "motivation": "\u4f20\u7edfLiDAR\u95ed\u73af\u9a8c\u8bc1\u901a\u5e38\u57fa\u4e8e\u5355\u5e27\u63cf\u8ff0\u7b26\u6bd4\u8f83\u6216\u56fa\u5b9a\u9608\u503c\u914d\u5408\u540e\u671fICP\u9a8c\u8bc1\uff0c\u5728\u7ed3\u6784\u91cd\u590d\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u5bb9\u6613\u4ea7\u751f\u8bef\u62a5\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u9a8c\u8bc1\u65b9\u6cd5\u6765\u6291\u5236\u8bef\u62a5\uff0c\u63d0\u9ad8\u95ed\u73af\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u622a\u65ad\u5e8f\u8d2f\u6982\u7387\u6bd4\u68c0\u9a8c(SPRT)\u6846\u67b6\uff0c\u7d2f\u79ef\u67e5\u8be2\u5e27\u4e0e\u5019\u9009\u5e27\u4e4b\u95f4\u7684\u591a\u5e27\u63cf\u8ff0\u7b26\u76f8\u4f3c\u6027\u6d41\uff0c\u6839\u636e\u9884\u8bbe\u7684I/II\u7c7b\u9519\u8bef\u76ee\u6807\u81ea\u9002\u5e94\u51b3\u7b56\u3002\u91c7\u7528\u7cbe\u5ea6\u4f18\u5148\u7b56\u7565\uff0c\u5728\u89c2\u5bdf\u5230\u8db3\u591f\u591a\u5e27\u8bc1\u636e\u540e\u624d\u505a\u51fa\u63a5\u53d7/\u62d2\u7edd\u51b3\u5b9a\u3002", "result": "\u5728\u4e94\u4e2a\u5e8f\u5217\u7684\u56fe\u4e66\u9986\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528\u56fa\u5b9a\u68c0\u7d22\u524d\u7aef\u548c\u591a\u79cd\u4ee3\u8868\u6027LiDAR\u5168\u5c40\u63cf\u8ff0\u7b26\u3002\u76f8\u6bd4\u5355\u5e27\u548c\u542f\u53d1\u5f0f\u591a\u5e27\u57fa\u7ebf\uff0c\u5e8f\u8d2f\u9a8c\u8bc1\u5668\u5728\u6240\u6709\u63cf\u8ff0\u7b26\u4e0a\u90fd\u4e00\u81f4\u63d0\u9ad8\u4e86\u7cbe\u5ea6\uff0c\u51cf\u5c11\u4e86\u6df7\u53e0\u95ed\u73af\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u5e27\u5e8f\u8d2f\u9a8c\u8bc1\u65b9\u6cd5\u80fd\u6709\u6548\u6291\u5236\u5ba4\u5185\u91cd\u590d\u73af\u5883\u4e2d\u7684\u8bef\u62a5\uff0c\u63d0\u9ad8\u95ed\u73af\u7cbe\u5ea6\uff0c\u4e14\u4e0e\u63cf\u8ff0\u7b26\u65e0\u5173\uff0c\u5177\u6709\u901a\u7528\u6027\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u5f00\u6e90\u3002"}}
{"id": "2512.09394", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09394", "abs": "https://arxiv.org/abs/2512.09394", "authors": ["Julie Kallini", "Christopher Potts"], "title": "Language models as tools for investigating the distinction between possible and impossible natural languages", "comment": null, "summary": "We argue that language models (LMs) have strong potential as investigative tools for probing the distinction between possible and impossible natural languages and thus uncovering the inductive biases that support human language learning. We outline a phased research program in which LM architectures are iteratively refined to better discriminate between possible and impossible languages, supporting linking hypotheses to human cognition.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u53ef\u4f5c\u4e3a\u7814\u7a76\u5de5\u5177\uff0c\u63a2\u7d22\u81ea\u7136\u8bed\u8a00\u53ef\u80fd/\u4e0d\u53ef\u80fd\u7684\u754c\u9650\uff0c\u63ed\u793a\u4eba\u7c7b\u8bed\u8a00\u5b66\u4e60\u7684\u5f52\u7eb3\u504f\u7f6e", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u7814\u7a76\u5de5\u5177\uff0c\u63ed\u793a\u4eba\u7c7b\u8bed\u8a00\u5b66\u4e60\u7684\u8ba4\u77e5\u673a\u5236\u548c\u5f52\u7eb3\u504f\u7f6e", "method": "\u63d0\u51fa\u5206\u9636\u6bb5\u7814\u7a76\u8ba1\u5212\uff0c\u8fed\u4ee3\u6539\u8fdb\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u4ee5\u66f4\u597d\u533a\u5206\u53ef\u80fd/\u4e0d\u53ef\u80fd\u8bed\u8a00", "result": "\u8bed\u8a00\u6a21\u578b\u5177\u6709\u4f5c\u4e3a\u7814\u7a76\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u53ef\u5e2e\u52a9\u7406\u89e3\u4eba\u7c7b\u8bed\u8a00\u5b66\u4e60\u7684\u8ba4\u77e5\u57fa\u7840", "conclusion": "\u8bed\u8a00\u6a21\u578b\u53ef\u4f5c\u4e3a\u8fde\u63a5\u4eba\u7c7b\u8ba4\u77e5\u7684\u6865\u6881\uff0c\u901a\u8fc7\u8fed\u4ee3\u6539\u8fdb\u63ed\u793a\u8bed\u8a00\u5b66\u4e60\u7684\u5f52\u7eb3\u504f\u7f6e"}}
{"id": "2512.09462", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09462", "abs": "https://arxiv.org/abs/2512.09462", "authors": ["Jayant Unde", "Takumi Inden", "Yuki Wakayama", "Jacinto Colan", "Yaonan Zhu", "Tadayoshi Aoyama", "Yasuhisa Hasegawa"], "title": "Development of a Compliant Gripper for Safe Robot-Assisted Trouser Dressing-Undressing", "comment": null, "summary": "In recent years, many countries, including Japan, have rapidly aging populations, making the preservation of seniors' quality of life a significant concern. For elderly people with impaired physical abilities, support for toileting is one of the most important issues. This paper details the design, development, experimental assessment, and potential application of the gripper system, with a focus on the unique requirements and obstacles involved in aiding elderly or hemiplegic individuals in dressing and undressing trousers. The gripper we propose seeks to find the right balance between compliance and grasping forces, ensuring precise manipulation while maintaining a safe and compliant interaction with the users. The gripper's integration into a custom--built robotic manipulator system provides a comprehensive solution for assisting hemiplegic individuals in their dressing and undressing tasks. Experimental evaluations and comparisons with existing studies demonstrate the gripper's ability to successfully assist in both dressing and dressing of trousers in confined spaces with a high success rate. This research contributes to the advancement of assistive robotics, empowering elderly, and physically impaired individuals to maintain their independence and improve their quality of life.", "AI": {"tldr": "\u5f00\u53d1\u7528\u4e8e\u8f85\u52a9\u8001\u5e74\u4eba\u6216\u504f\u762b\u60a3\u8005\u7a7f\u8131\u88e4\u5b50\u7684\u5939\u6301\u5668\u7cfb\u7edf\uff0c\u5728\u53d7\u9650\u7a7a\u95f4\u5185\u5b9e\u73b0\u9ad8\u6210\u529f\u7387\u64cd\u4f5c", "motivation": "\u5e94\u5bf9\u4eba\u53e3\u8001\u9f84\u5316\u80cc\u666f\u4e0b\u8001\u5e74\u4eba\u751f\u6d3b\u8d28\u91cf\u4fdd\u969c\u95ee\u9898\uff0c\u7279\u522b\u662f\u8eab\u4f53\u80fd\u529b\u53d7\u635f\u8001\u5e74\u4eba\u7684\u5982\u5395\u8f85\u52a9\u9700\u6c42\uff0c\u5e2e\u52a9\u504f\u762b\u60a3\u8005\u5b8c\u6210\u7a7f\u8131\u88e4\u5b50\u7b49\u65e5\u5e38\u4efb\u52a1", "method": "\u8bbe\u8ba1\u5f00\u53d1\u5939\u6301\u5668\u7cfb\u7edf\uff0c\u5e73\u8861\u67d4\u987a\u6027\u548c\u6293\u53d6\u529b\uff0c\u5b9e\u73b0\u7cbe\u786e\u64cd\u4f5c\u548c\u7528\u6237\u5b89\u5168\u4ea4\u4e92\uff1b\u96c6\u6210\u5230\u5b9a\u5236\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u4e2d\uff0c\u5f62\u6210\u5b8c\u6574\u7684\u8f85\u52a9\u89e3\u51b3\u65b9\u6848", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\u5939\u6301\u5668\u5728\u53d7\u9650\u7a7a\u95f4\u5185\u6210\u529f\u8f85\u52a9\u7a7f\u8131\u88e4\u5b50\uff0c\u5177\u6709\u9ad8\u6210\u529f\u7387\uff1b\u4e0e\u73b0\u6709\u7814\u7a76\u76f8\u6bd4\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd", "conclusion": "\u8be5\u7814\u7a76\u63a8\u8fdb\u4e86\u8f85\u52a9\u673a\u5668\u4eba\u6280\u672f\u53d1\u5c55\uff0c\u5e2e\u52a9\u8001\u5e74\u4eba\u548c\u8eab\u4f53\u53d7\u635f\u8005\u4fdd\u6301\u72ec\u7acb\u6027\uff0c\u63d0\u9ad8\u751f\u6d3b\u8d28\u91cf"}}
{"id": "2512.09434", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09434", "abs": "https://arxiv.org/abs/2512.09434", "authors": ["Sebastian Nagl", "Mohamed Elganayni", "Melanie Pospisil", "Matthias Grabmair"], "title": "CourtPressGER: A German Court Decision to Press Release Summarization Dataset", "comment": "Preprint - This contribution was accepted at JURIX AI4A2J Workshop 2025", "summary": "Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.", "AI": {"tldr": "\u63d0\u51faCourtPressGER\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30LLM\u751f\u6210\u5fb7\u56fd\u6cd5\u9662\u5224\u51b3\u7684\u65b0\u95fb\u7a3f\uff0c\u5305\u542b6.4k\u4e2a\u5224\u51b3-\u65b0\u95fb\u7a3f-\u63d0\u793a\u4e09\u5143\u7ec4\uff0c\u901a\u8fc7\u591a\u79cd\u8bc4\u4f30\u65b9\u6cd5\u53d1\u73b0\u5927\u578bLLM\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u8349\u7a3f\uff0c\u5c0f\u578b\u6a21\u578b\u9700\u8981\u5206\u5c42\u5904\u7406\u957f\u6587\u672c\u3002", "motivation": "\u73b0\u6709NLP\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6280\u672f\u6027\u6458\u8981\uff0c\u5ffd\u89c6\u4e86\u9762\u5411\u516c\u4f17\u7684\u53f8\u6cd5\u6c9f\u901a\u9700\u6c42\u3002\u5fb7\u56fd\u6700\u9ad8\u6cd5\u9662\u7684\u5b98\u65b9\u65b0\u95fb\u7a3f\u5411\u516c\u4f17\u548c\u4e13\u5bb6\u89e3\u91ca\u5224\u51b3\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u751f\u6210\u51c6\u786e\u3001\u53ef\u8bfb\u6027\u5f3a\u7684\u53f8\u6cd5\u6587\u672c\u6458\u8981\u7684\u6a21\u578b\u3002", "method": "\u6784\u5efaCourtPressGER\u6570\u636e\u96c6\uff086.4k\u4e2a\u4e09\u5143\u7ec4\uff1a\u5224\u51b3\u3001\u4eba\u5de5\u64b0\u5199\u7684\u65b0\u95fb\u7a3f\u3001LLM\u5408\u6210\u63d0\u793a\uff09\uff0c\u4f7f\u7528\u57fa\u4e8e\u53c2\u8003\u7684\u6307\u6807\u3001\u4e8b\u5b9e\u4e00\u81f4\u6027\u68c0\u67e5\u3001LLM-as-judge\u548c\u4e13\u5bb6\u6392\u540d\u7b49\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e0d\u540c\u89c4\u6a21LLM\u5728\u751f\u6210\u65b0\u95fb\u7a3f\u65b9\u9762\u7684\u6027\u80fd\u3002", "result": "\u5927\u578bLLM\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u8349\u7a3f\uff0c\u6027\u80fd\u635f\u5931\u6700\u5c0f\uff1b\u5c0f\u578b\u6a21\u578b\u9700\u8981\u5206\u5c42\u8bbe\u7f6e\u6765\u5904\u7406\u957f\u5224\u51b3\u3002\u521d\u6b65\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u6a21\u578b\u6027\u80fd\u5404\u5f02\uff0c\u4eba\u5de5\u64b0\u5199\u7684\u65b0\u95fb\u7a3f\u6392\u540d\u6700\u9ad8\u3002", "conclusion": "CourtPressGER\u4e3a\u8bad\u7ec3\u548c\u8bc4\u4f30LLM\u751f\u6210\u53f8\u6cd5\u65b0\u95fb\u7a3f\u63d0\u4f9b\u4e86\u57fa\u51c6\uff0c\u5927\u578bLLM\u5728\u751f\u6210\u51c6\u786e\u3001\u53ef\u8bfb\u7684\u53f8\u6cd5\u6458\u8981\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u4eba\u5de5\u64b0\u5199\u4ecd\u662f\u6700\u4f73\u6807\u51c6\uff0c\u5c0f\u578b\u6a21\u578b\u9700\u8981\u7279\u5b9a\u67b6\u6784\u6765\u5904\u7406\u957f\u6587\u672c\u3002"}}
{"id": "2512.09495", "categories": ["cs.RO", "cs.CG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.09495", "abs": "https://arxiv.org/abs/2512.09495", "authors": ["Edwin Meriaux", "Shuo Wen", "Louis-Roy Langevin", "Doina Precup", "Antonio Lor\u00eda", "Gregory Dudek"], "title": "On Mobile Ad Hoc Networks for Coverage of Partially Observable Worlds", "comment": null, "summary": "This paper addresses the movement and placement of mobile agents to establish a communication network in initially unknown environments. We cast the problem in a computational-geometric framework by relating the coverage problem and line-of-sight constraints to the Cooperative Guard Art Gallery Problem, and introduce its partially observable variant, the Partially Observable Cooperative Guard Art Gallery Problem (POCGAGP). We then present two algorithms that solve POCGAGP: CADENCE, a centralized planner that incrementally selects 270 degree corners at which to deploy agents, and DADENCE, a decentralized scheme that coordinates agents using local information and lightweight messaging. Both approaches operate under partial observability and target simultaneous coverage and connectivity. We evaluate the methods in simulation across 1,500 test cases of varied size and structure, demonstrating consistent success in forming connected networks while covering and exploring unknown space. These results highlight the value of geometric abstractions for communication-driven exploration and show that decentralized policies are competitive with centralized performance while retaining scalability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5\uff08\u96c6\u4e2d\u5f0fCADENCE\u548c\u5206\u5e03\u5f0fDADENCE\uff09\u89e3\u51b3\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u79fb\u52a8\u4ee3\u7406\u90e8\u7f72\u95ee\u9898\uff0c\u7528\u4e8e\u5efa\u7acb\u901a\u4fe1\u7f51\u7edc\u5e76\u540c\u65f6\u6ee1\u8db3\u8986\u76d6\u548c\u8fde\u63a5\u9700\u6c42\u3002", "motivation": "\u5728\u521d\u59cb\u672a\u77e5\u73af\u5883\u4e2d\uff0c\u79fb\u52a8\u4ee3\u7406\u9700\u8981\u5efa\u7acb\u901a\u4fe1\u7f51\u7edc\uff0c\u8fd9\u6d89\u53ca\u5230\u8986\u76d6\u95ee\u9898\u548c\u89c6\u7ebf\u7ea6\u675f\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u540c\u65f6\u5b9e\u73b0\u7a7a\u95f4\u8986\u76d6\u548c\u7f51\u7edc\u8fde\u63a5\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u534f\u4f5c\u5b88\u536b\u7f8e\u672f\u9986\u95ee\u9898\uff08POCGAGP\uff09\uff0c\u63d0\u51fa\u4e24\u79cd\u7b97\u6cd5\uff1a\u96c6\u4e2d\u5f0fCADENCE\uff08\u589e\u91cf\u9009\u62e9270\u5ea6\u89d2\u843d\u90e8\u7f72\u4ee3\u7406\uff09\u548c\u5206\u5e03\u5f0fDADENCE\uff08\u4f7f\u7528\u672c\u5730\u4fe1\u606f\u548c\u8f7b\u91cf\u7ea7\u6d88\u606f\u534f\u8c03\u4ee3\u7406\uff09\u3002", "result": "\u57281500\u4e2a\u4e0d\u540c\u89c4\u6a21\u548c\u7ed3\u6784\u7684\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u6210\u529f\u5f62\u6210\u8fde\u63a5\u7f51\u7edc\uff0c\u540c\u65f6\u8986\u76d6\u548c\u63a2\u7d22\u672a\u77e5\u7a7a\u95f4\u3002\u5206\u5e03\u5f0f\u7b56\u7565\u4e0e\u96c6\u4e2d\u5f0f\u6027\u80fd\u76f8\u5f53\u4e14\u66f4\u5177\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u51e0\u4f55\u62bd\u8c61\u5bf9\u901a\u4fe1\u9a71\u52a8\u7684\u63a2\u7d22\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u5206\u5e03\u5f0f\u7b56\u7565\u5728\u4fdd\u6301\u53ef\u6269\u5c55\u6027\u7684\u540c\u65f6\u80fd\u4e0e\u96c6\u4e2d\u5f0f\u6027\u80fd\u7ade\u4e89\uff0c\u4e3a\u89e3\u51b3\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u7f51\u7edc\u90e8\u7f72\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2512.09440", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09440", "abs": "https://arxiv.org/abs/2512.09440", "authors": ["Qingyuan Zhang", "Yuxi Wang", "Cancan Hua", "Yulin Huang", "Ning Lyu"], "title": "Knowledge-Augmented Large Language Model Agents for Explainable Financial Decision-Making", "comment": null, "summary": "This study investigates an explainable reasoning method for financial decision-making based on knowledge-enhanced large language model agents. To address the limitations of traditional financial decision methods that rely on parameterized knowledge, lack factual consistency, and miss reasoning chains, an integrated framework is proposed that combines external knowledge retrieval, semantic representation, and reasoning generation. The method first encodes financial texts and structured data to obtain semantic representations, and then retrieves task-related information from external knowledge bases using similarity computation. Internal representations and external knowledge are combined through weighted fusion, which ensures fluency while improving factual accuracy and completeness of generated content. In the reasoning stage, a multi-head attention mechanism is introduced to construct logical chains, allowing the model to present transparent causal relationships and traceability during generation. Finally, the model jointly optimizes task objectives and explanation consistency objectives, which enhances predictive performance and reasoning interpretability. Experiments on financial text processing and decision tasks show that the method outperforms baseline approaches in accuracy, text generation quality, and factual support, verifying the effectiveness of knowledge enhancement and explainable reasoning. Overall, the proposed approach overcomes the limitations of traditional models in semantic coverage and reasoning transparency, and demonstrates strong practical value in complex financial scenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u53ef\u89e3\u91ca\u91d1\u878d\u51b3\u7b56\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u5916\u90e8\u77e5\u8bc6\u68c0\u7d22\u3001\u8bed\u4e49\u8868\u793a\u548c\u63a8\u7406\u751f\u6210\u96c6\u6210\u6846\u67b6\uff0c\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u63a8\u7406\u900f\u660e\u5ea6", "motivation": "\u4f20\u7edf\u91d1\u878d\u51b3\u7b56\u65b9\u6cd5\u4f9d\u8d56\u53c2\u6570\u5316\u77e5\u8bc6\u3001\u7f3a\u4e4f\u4e8b\u5b9e\u4e00\u81f4\u6027\u3001\u7f3a\u5c11\u63a8\u7406\u94fe\uff0c\u9700\u8981\u89e3\u51b3\u8bed\u4e49\u8986\u76d6\u548c\u63a8\u7406\u900f\u660e\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898", "method": "1) \u7f16\u7801\u91d1\u878d\u6587\u672c\u548c\u7ed3\u6784\u5316\u6570\u636e\u83b7\u53d6\u8bed\u4e49\u8868\u793a\uff1b2) \u901a\u8fc7\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u4ece\u5916\u90e8\u77e5\u8bc6\u5e93\u68c0\u7d22\u4efb\u52a1\u76f8\u5173\u4fe1\u606f\uff1b3) \u52a0\u6743\u878d\u5408\u5185\u90e8\u8868\u793a\u548c\u5916\u90e8\u77e5\u8bc6\uff1b4) \u5f15\u5165\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u6784\u5efa\u903b\u8f91\u94fe\uff1b5) \u8054\u5408\u4f18\u5316\u4efb\u52a1\u76ee\u6807\u548c\u89e3\u91ca\u4e00\u81f4\u6027\u76ee\u6807", "result": "\u5728\u91d1\u878d\u6587\u672c\u5904\u7406\u548c\u51b3\u7b56\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0c\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u6587\u672c\u751f\u6210\u8d28\u91cf\u548c\u4e8b\u5b9e\u652f\u6301\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u77e5\u8bc6\u589e\u5f3a\u548c\u53ef\u89e3\u91ca\u63a8\u7406\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u514b\u670d\u4e86\u4f20\u7edf\u6a21\u578b\u5728\u8bed\u4e49\u8986\u76d6\u548c\u63a8\u7406\u900f\u660e\u5ea6\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5728\u590d\u6742\u91d1\u878d\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2512.09510", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09510", "abs": "https://arxiv.org/abs/2512.09510", "authors": ["Donato Caramia", "Florian T. Pokorny", "Giuseppe Triggiani", "Denis Ruffino", "David Naso", "Paolo Roberto Massenio"], "title": "ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics", "comment": null, "summary": "Occlusions in robotic bin picking compromise accurate and reliable grasp planning. We present ViTA-Seg, a class-agnostic Vision Transformer framework for real-time amodal segmentation that leverages global attention to recover complete object masks, including hidden regions. We proposte two architectures: a) Single-Head for amodal mask prediction; b) Dual-Head for amodal and occluded mask prediction. We also introduce ViTA-SimData, a photo-realistic synthetic dataset tailored to industrial bin-picking scenario. Extensive experiments on two amodal benchmarks, COOCA and KINS, demonstrate that ViTA-Seg Dual Head achieves strong amodal and occlusion segmentation accuracy with computational efficiency, enabling robust, real-time robotic manipulation.", "AI": {"tldr": "ViTA-Seg\u662f\u4e00\u4e2a\u57fa\u4e8eVision Transformer\u7684\u7c7b\u65e0\u5173\u5b9e\u65f6amodal\u5206\u5272\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u7bb1\u4f53\u62e3\u9009\u4e2d\u7684\u906e\u6321\u5904\u7406\uff0c\u901a\u8fc7\u5168\u5c40\u6ce8\u610f\u529b\u6062\u590d\u5b8c\u6574\u7269\u4f53\u63a9\u7801\uff0c\u5305\u542b\u5355\u5934\u548c\u53cc\u5934\u67b6\u6784\uff0c\u5e76\u63d0\u51fa\u4e86\u5de5\u4e1a\u573a\u666f\u5408\u6210\u6570\u636e\u96c6ViTA-SimData\u3002", "motivation": "\u673a\u5668\u4eba\u7bb1\u4f53\u62e3\u9009\u4e2d\u7684\u906e\u6321\u95ee\u9898\u4f1a\u5f71\u54cd\u6293\u53d6\u89c4\u5212\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u9700\u8981\u80fd\u591f\u6062\u590d\u5b8c\u6574\u7269\u4f53\u5f62\u72b6\uff08\u5305\u62ec\u88ab\u906e\u6321\u533a\u57df\uff09\u7684amodal\u5206\u5272\u65b9\u6cd5\u3002", "method": "\u63d0\u51faViTA-Seg\u6846\u67b6\uff1a1\uff09\u5355\u5934\u67b6\u6784\u7528\u4e8eamodal\u63a9\u7801\u9884\u6d4b\uff1b2\uff09\u53cc\u5934\u67b6\u6784\u540c\u65f6\u9884\u6d4bamodal\u548c\u906e\u6321\u63a9\u7801\uff1b3\uff09\u5f15\u5165ViTA-SimData\u5408\u6210\u6570\u636e\u96c6\u6a21\u62df\u5de5\u4e1a\u62e3\u9009\u573a\u666f\uff1b4\uff09\u5229\u7528Vision Transformer\u7684\u5168\u5c40\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728COOCA\u548cKINS\u4e24\u4e2aamodal\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cViTA-Seg\u53cc\u5934\u67b6\u6784\u5728amodal\u548c\u906e\u6321\u5206\u5272\u7cbe\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff0c\u652f\u6301\u5b9e\u65f6\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "conclusion": "ViTA-Seg\u901a\u8fc7Vision Transformer\u7684\u5168\u5c40\u6ce8\u610f\u529b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684amodal\u5206\u5272\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u7bb1\u4f53\u62e3\u9009\u4e2d\u7684\u906e\u6321\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u53ef\u9760\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09444", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09444", "abs": "https://arxiv.org/abs/2512.09444", "authors": ["Ning Lyu", "Yuxi Wang", "Feng Chen", "Qingyuan Zhang"], "title": "Advancing Text Classification with Large Language Models and Neural Attention Mechanisms", "comment": null, "summary": "This study proposes a text classification algorithm based on large language models, aiming to address the limitations of traditional methods in capturing long-range dependencies, understanding contextual semantics, and handling class imbalance. The framework includes text encoding, contextual representation modeling, attention-based enhancement, feature aggregation, and classification prediction. In the representation stage, deep semantic embeddings are obtained through large-scale pretrained language models, and attention mechanisms are applied to enhance the selective representation of key features. In the aggregation stage, global and weighted strategies are combined to generate robust text-level vectors. In the classification stage, a fully connected layer and Softmax output are used to predict class distributions, and cross-entropy loss is employed to optimize model parameters. Comparative experiments introduce multiple baseline models, including recurrent neural networks, graph neural networks, and Transformers, and evaluate them on Precision, Recall, F1-Score, and AUC. Results show that the proposed method outperforms existing models on all metrics, with especially strong improvements in Recall and AUC. In addition, sensitivity experiments are conducted on hyperparameters and data conditions, covering the impact of hidden dimensions on AUC and the impact of class imbalance ratios on Recall. The findings demonstrate that proper model configuration has a significant effect on performance and reveal the adaptability and stability of the model under different conditions. Overall, the proposed text classification method not only achieves effective performance improvement but also verifies its robustness and applicability in complex data environments through systematic analysis.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u5206\u7c7b\u7b97\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u589e\u5f3a\u548c\u7279\u5f81\u805a\u5408\u7b56\u7565\uff0c\u5728\u957f\u8ddd\u79bb\u4f9d\u8d56\u3001\u4e0a\u4e0b\u6587\u8bed\u4e49\u7406\u89e3\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u5728\u6355\u6349\u957f\u8ddd\u79bb\u4f9d\u8d56\u3001\u7406\u89e3\u4e0a\u4e0b\u6587\u8bed\u4e49\u548c\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u5305\u62ec\u6587\u672c\u7f16\u7801\u3001\u4e0a\u4e0b\u6587\u8868\u793a\u5efa\u6a21\u3001\u6ce8\u610f\u529b\u589e\u5f3a\u3001\u7279\u5f81\u805a\u5408\u548c\u5206\u7c7b\u9884\u6d4b\u3002\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u5173\u952e\u7279\u5f81\u8868\u793a\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u52a0\u6743\u7b56\u7565\u751f\u6210\u6587\u672c\u7ea7\u5411\u91cf\uff0c\u901a\u8fc7\u5168\u8fde\u63a5\u5c42\u548cSoftmax\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\uff08\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u3001AUC\uff09\u4e0a\u5747\u4f18\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u3001\u56fe\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u7b49\u57fa\u7ebf\u6a21\u578b\uff0c\u5c24\u5176\u5728\u53ec\u56de\u7387\u548cAUC\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8fd8\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u6570\u636e\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u9002\u5e94\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2512.09537", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09537", "abs": "https://arxiv.org/abs/2512.09537", "authors": ["Qihao Yuan", "Ziyu Cao", "Ming Cao", "Kailai Li"], "title": "REASAN: Learning Reactive Safe Navigation for Legged Robots", "comment": "8 pages", "summary": "We present a novel modularized end-to-end framework for legged reactive navigation in complex dynamic environments using a single light detection and ranging (LiDAR) sensor. The system comprises four simulation-trained modules: three reinforcement-learning (RL) policies for locomotion, safety shielding, and navigation, and a transformer-based exteroceptive estimator that processes raw point-cloud inputs. This modular decomposition of complex legged motor-control tasks enables lightweight neural networks with simple architectures, trained using standard RL practices with targeted reward shaping and curriculum design, without reliance on heuristics or sophisticated policy-switching mechanisms. We conduct comprehensive ablations to validate our design choices and demonstrate improved robustness compared to existing approaches in challenging navigation tasks. The resulting reactive safe navigation (REASAN) system achieves fully onboard and real-time reactive navigation across both single- and multi-robot settings in complex environments. We release our training and deployment code at https://github.com/ASIG-X/REASAN.", "AI": {"tldr": "\u63d0\u51faREASAN\u7cfb\u7edf\uff1a\u6a21\u5757\u5316\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u4f7f\u7528\u5355LiDAR\u4f20\u611f\u5668\u5b9e\u73b0\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u53cd\u5e94\u5f0f\u5bfc\u822a\uff0c\u5305\u542b\u56db\u4e2a\u4eff\u771f\u8bad\u7ec3\u6a21\u5757\uff08\u4e09\u4e2aRL\u7b56\u7565+Transformer\u4f30\u8ba1\u5668\uff09\uff0c\u5b9e\u73b0\u5b8c\u5168\u677f\u8f7d\u5b9e\u65f6\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u8fdb\u884c\u53cd\u5e94\u5f0f\u5bfc\u822a\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u542f\u53d1\u5f0f\u89c4\u5219\u6216\u590d\u6742\u7684\u7b56\u7565\u5207\u6362\u673a\u5236\uff0c\u96be\u4ee5\u5b9e\u73b0\u8f7b\u91cf\u3001\u9c81\u68d2\u7684\u5b9e\u65f6\u5bfc\u822a\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u5206\u89e3\u65b9\u6cd5\uff1a1) \u4e09\u4e2a\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff08\u8fd0\u52a8\u63a7\u5236\u3001\u5b89\u5168\u5c4f\u853d\u3001\u5bfc\u822a\uff09\uff1b2) Transformer\u5916\u611f\u53d7\u4f30\u8ba1\u5668\u5904\u7406\u539f\u59cb\u70b9\u4e91\u8f93\u5165\uff1b\u4f7f\u7528\u6807\u51c6RL\u8bad\u7ec3\u914d\u5408\u9488\u5bf9\u6027\u5956\u52b1\u5851\u9020\u548c\u8bfe\u7a0b\u8bbe\u8ba1\uff0c\u65e0\u9700\u542f\u53d1\u5f0f\u89c4\u5219\u3002", "result": "REASAN\u7cfb\u7edf\u5728\u590d\u6742\u5bfc\u822a\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u5728\u5355\u673a\u5668\u4eba\u548c\u591a\u673a\u5668\u4eba\u573a\u666f\u4e2d\u5b9e\u73b0\u5b8c\u5168\u677f\u8f7d\u5b9e\u65f6\u53cd\u5e94\u5f0f\u5bfc\u822a\uff0c\u5e76\u901a\u8fc7\u5168\u9762\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u9009\u62e9\u3002", "conclusion": "\u6a21\u5757\u5316\u5206\u89e3\u65b9\u6cd5\u4f7f\u590d\u6742\u817f\u5f0f\u8fd0\u52a8\u63a7\u5236\u4efb\u52a1\u80fd\u591f\u4f7f\u7528\u8f7b\u91cf\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\uff0c\u901a\u8fc7\u6807\u51c6RL\u8bad\u7ec3\u5373\u53ef\u83b7\u5f97\u9c81\u68d2\u6027\u80fd\uff0c\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09483", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.09483", "abs": "https://arxiv.org/abs/2512.09483", "authors": ["Peixian Zhang", "Qiming Ye", "Zifan Peng", "Kiran Garimella", "Gareth Tyson"], "title": "Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines", "comment": null, "summary": "LLM-based Search Engines (LLM-SEs) introduces a new paradigm for information seeking. Unlike Traditional Search Engines (TSEs) (e.g., Google), these systems summarize results, often providing limited citation transparency. The implications of this shift remain largely unexplored, yet raises key questions regarding trust and transparency. In this paper, we present a large-scale empirical study of LLM-SEs, analyzing 55,936 queries and the corresponding search results across six LLM-SEs and two TSEs. We confirm that LLM-SEs cites domain resources with greater diversity than TSEs. Indeed, 37% of domains are unique to LLM-SEs. However, certain risks still persist: LLM-SEs do not outperform TSEs in credibility, political neutrality and safety metrics. Finally, to understand the selection criteria of LLM-SEs, we perform a feature-based analysis to identify key factors influencing source choice. Our findings provide actionable insights for end users, website owners, and developers.", "AI": {"tldr": "LLM\u641c\u7d22\u5f15\u64ce\u4e0e\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u5bf9\u6bd4\u7814\u7a76\uff1aLLM-SEs\u5f15\u7528\u66f4\u591a\u6837\u5316\u4f46\u53ef\u4fe1\u5ea6\u3001\u4e2d\u7acb\u6027\u3001\u5b89\u5168\u6027\u672a\u8d85\u8d8a\u4f20\u7edf\u5f15\u64ce", "motivation": "LLM\u641c\u7d22\u5f15\u64ce\u4f5c\u4e3a\u4fe1\u606f\u68c0\u7d22\u65b0\u8303\u5f0f\uff0c\u901a\u5e38\u603b\u7ed3\u7ed3\u679c\u4e14\u5f15\u7528\u900f\u660e\u5ea6\u6709\u9650\uff0c\u8fd9\u79cd\u8f6c\u53d8\u5bf9\u4fe1\u4efb\u548c\u900f\u660e\u5ea6\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22", "method": "\u5bf96\u4e2aLLM\u641c\u7d22\u5f15\u64ce\u548c2\u4e2a\u4f20\u7edf\u641c\u7d22\u5f15\u64ce\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u679055,936\u4e2a\u67e5\u8be2\u53ca\u5bf9\u5e94\u641c\u7d22\u7ed3\u679c\uff0c\u5e76\u8fdb\u884c\u7279\u5f81\u5206\u6790\u4ee5\u4e86\u89e3LLM-SEs\u7684\u9009\u62e9\u6807\u51c6", "result": "LLM-SEs\u5f15\u7528\u9886\u57df\u8d44\u6e90\u6bd4TSEs\u66f4\u591a\u6837\u5316\uff0837%\u7684\u9886\u57df\u4e3aLLM-SEs\u72ec\u6709\uff09\uff0c\u4f46\u5728\u53ef\u4fe1\u5ea6\u3001\u653f\u6cbb\u4e2d\u7acb\u6027\u548c\u5b89\u5168\u6027\u6307\u6807\u4e0a\u672a\u8d85\u8d8aTSEs", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7ec8\u7aef\u7528\u6237\u3001\u7f51\u7ad9\u6240\u6709\u8005\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\uff0c\u63ed\u793a\u4e86LLM\u641c\u7d22\u5f15\u64ce\u7684\u4f18\u52bf\u548c\u98ce\u9669"}}
{"id": "2512.09571", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09571", "abs": "https://arxiv.org/abs/2512.09571", "authors": ["Feng Yu", "Yu Hu", "Yang Su", "Yang Deng", "Linzuo Zhang", "Danping Zou"], "title": "Mastering Diverse, Unknown, and Cluttered Tracks for Robust Vision-Based Drone Racing", "comment": "8 pages, 9 figures, Robotics and Automation Letters accept", "summary": "Most reinforcement learning(RL)-based methods for drone racing target fixed, obstacle-free tracks, leaving the generalization to unknown, cluttered environments largely unaddressed. This challenge stems from the need to balance racing speed and collision avoidance, limited feasible space causing policy exploration trapped in local optima during training, and perceptual ambiguity between gates and obstacles in depth maps-especially when gate positions are only coarsely specified. To overcome these issues, we propose a two-phase learning framework: an initial soft-collision training phase that preserves policy exploration for high-speed flight, followed by a hard-collision refinement phase that enforces robust obstacle avoidance. An adaptive, noise-augmented curriculum with an asymmetric actor-critic architecture gradually shifts the policy's reliance from privileged gate-state information to depth-based visual input. We further impose Lipschitz constraints and integrate a track-primitive generator to enhance motion stability and cross-environment generalization. We evaluate our framework through extensive simulation and ablation studies, and validate it in real-world experiments on a computationally constrained quadrotor. The system achieves agile flight while remaining robust to gate-position errors, developing a generalizable drone racing framework with the capability to operate in diverse, partially unknown and cluttered environments. https://yufengsjtu.github.io/MasterRacing.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\u7528\u4e8e\u65e0\u4eba\u673a\u7ade\u901f\uff0c\u901a\u8fc7\u8f6f\u78b0\u649e\u8bad\u7ec3\u548c\u786c\u78b0\u649e\u7cbe\u70bc\u89e3\u51b3\u672a\u77e5\u590d\u6742\u73af\u5883\u4e2d\u7684\u901f\u5ea6\u4e0e\u907f\u969c\u5e73\u8861\u95ee\u9898\uff0c\u5b9e\u73b0\u8de8\u73af\u5883\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u4eba\u673a\u7ade\u901f\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u56fa\u5b9a\u3001\u65e0\u969c\u788d\u7684\u8d5b\u9053\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u672a\u77e5\u3001\u590d\u6742\u7684\u6742\u4e71\u73af\u5883\u3002\u4e3b\u8981\u6311\u6218\u5305\u62ec\uff1a\u5e73\u8861\u7ade\u901f\u901f\u5ea6\u4e0e\u78b0\u649e\u907f\u514d\u3001\u53ef\u884c\u7a7a\u95f4\u6709\u9650\u5bfc\u81f4\u7b56\u7565\u63a2\u7d22\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3001\u6df1\u5ea6\u56fe\u4e2d\u95e8\u4e0e\u969c\u788d\u7269\u7684\u611f\u77e5\u6a21\u7cca\u6027\uff08\u7279\u522b\u662f\u5f53\u95e8\u4f4d\u7f6e\u4ec5\u7c97\u7565\u6307\u5b9a\u65f6\uff09\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u521d\u59cb\u8f6f\u78b0\u649e\u8bad\u7ec3\u9636\u6bb5\uff0c\u4fdd\u7559\u7b56\u7565\u63a2\u7d22\u4ee5\u5b9e\u73b0\u9ad8\u901f\u98de\u884c\uff1b2\uff09\u786c\u78b0\u649e\u7cbe\u70bc\u9636\u6bb5\uff0c\u5f3a\u5236\u9c81\u68d2\u7684\u969c\u788d\u7269\u907f\u514d\u3002\u91c7\u7528\u81ea\u9002\u5e94\u566a\u58f0\u589e\u5f3a\u8bfe\u7a0b\u5b66\u4e60\u4e0e\u4e0d\u5bf9\u79f0\u6f14\u5458-\u8bc4\u8bba\u5bb6\u67b6\u6784\uff0c\u9010\u6b65\u5c06\u7b56\u7565\u4ece\u7279\u6743\u95e8\u72b6\u6001\u4fe1\u606f\u8f6c\u5411\u57fa\u4e8e\u6df1\u5ea6\u7684\u89c6\u89c9\u8f93\u5165\u3002\u65bd\u52a0Lipschitz\u7ea6\u675f\u5e76\u96c6\u6210\u8d5b\u9053\u57fa\u5143\u751f\u6210\u5668\u4ee5\u589e\u5f3a\u8fd0\u52a8\u7a33\u5b9a\u6027\u548c\u8de8\u73af\u5883\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u4eff\u771f\u548c\u6d88\u878d\u7814\u7a76\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5728\u8ba1\u7b97\u53d7\u9650\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u8fdb\u884c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u3002\u7cfb\u7edf\u5b9e\u73b0\u4e86\u654f\u6377\u98de\u884c\uff0c\u540c\u65f6\u5bf9\u95e8\u4f4d\u7f6e\u8bef\u5dee\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5f00\u53d1\u51fa\u80fd\u591f\u5728\u591a\u6837\u5316\u3001\u90e8\u5206\u672a\u77e5\u548c\u6742\u4e71\u73af\u5883\u4e2d\u8fd0\u884c\u7684\u901a\u7528\u65e0\u4eba\u673a\u7ade\u901f\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u7ade\u901f\u5728\u672a\u77e5\u590d\u6742\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u548c\u81ea\u9002\u5e94\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u901f\u5ea6\u4e0e\u5b89\u5168\u6027\u7684\u5e73\u8861\uff0c\u4e3a\u65e0\u4eba\u673a\u5728\u771f\u5b9e\u4e16\u754c\u6742\u4e71\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u7ade\u901f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09487", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.09487", "abs": "https://arxiv.org/abs/2512.09487", "authors": ["Yucan Guo", "Miao Su", "Saiping Guan", "Zihao Sun", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "title": "RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \\model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \\model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \\model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u8f6e\u81ea\u9002\u5e94\u56fe-\u6587\u672c\u6df7\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\u548c\u590d\u6742\u63a8\u7406", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u6216\u6df7\u5408\u7684\u68c0\u7d22\u7cfb\u7edf\u901a\u5e38\u4f9d\u8d56\u56fa\u5b9a\u6216\u624b\u5de5\u5236\u4f5c\u7684\u68c0\u7d22\u6d41\u7a0b\uff0c\u65e0\u6cd5\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6574\u5408\u8865\u5145\u8bc1\u636e\uff0c\u4e14\u56fe\u8bc1\u636e\u68c0\u7d22\u6210\u672c\u9ad8\u6602\uff0c\u9700\u8981\u81ea\u9002\u5e94\u9ad8\u6548\u7684\u6df7\u5408\u68c0\u7d22\u65b9\u6cd5", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u6574\u4e2a\u751f\u6210\u8fc7\u7a0b\uff0c\u8ba9\u6a21\u578b\u5b66\u4e60\u4f55\u65f6\u63a8\u7406\u3001\u4ece\u6587\u672c\u6216\u56fe\u4e2d\u68c0\u7d22\u4ec0\u4e48\u3001\u4f55\u65f6\u751f\u6210\u6700\u7ec8\u7b54\u6848\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u540c\u65f6\u8003\u8651\u4efb\u52a1\u7ed3\u679c\u548c\u68c0\u7d22\u6548\u7387", "result": "\u5728\u4e94\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684RAG\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u5728\u652f\u6301\u81ea\u9002\u5e94\u9ad8\u6548\u68c0\u7d22\u8fdb\u884c\u590d\u6742\u63a8\u7406\u65b9\u9762\u7684\u4f18\u52bf", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u7684\u591a\u8f6e\u81ea\u9002\u5e94\u56fe-\u6587\u672c\u6df7\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5229\u7528\u6df7\u5408\u8bc1\u636e\u540c\u65f6\u907f\u514d\u4e0d\u5fc5\u8981\u7684\u68c0\u7d22\u5f00\u9500\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.09607", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09607", "abs": "https://arxiv.org/abs/2512.09607", "authors": ["Yanghong Mei", "Yirong Yang", "Longteng Guo", "Qunbo Wang", "Ming-Ming Yu", "Xingjian He", "Wenjun Wu", "Jing Liu"], "title": "UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories", "comment": "9 pages, 5 figures, accepted to AAAI 2026", "summary": "Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.", "AI": {"tldr": "UrbanNav\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u89c4\u6a21\u7f51\u7edc\u89c6\u9891\u6570\u636e\u8bad\u7ec3\u5177\u8eab\u667a\u80fd\u4f53\u9075\u5faa\u81ea\u7531\u5f62\u5f0f\u8bed\u8a00\u6307\u4ee4\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u6846\u67b6\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u5bfc\u822a\u65b9\u6cd5\u901a\u5e38\u5c40\u9650\u4e8e\u6a21\u62df\u73af\u5883\u6216\u7279\u5b9a\u8857\u9053\u73af\u5883\uff0c\u4f9d\u8d56\u7cbe\u786e\u7684\u76ee\u6807\u683c\u5f0f\uff08\u5982\u5750\u6807\u6216\u56fe\u50cf\uff09\uff0c\u9650\u5236\u4e86\u81ea\u4e3b\u667a\u80fd\u4f53\uff08\u5982\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u673a\u5668\u4eba\uff09\u5728\u964c\u751f\u57ce\u5e02\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5bfc\u822a\u9762\u4e34\u566a\u58f0\u6307\u4ee4\u3001\u6a21\u7cca\u7a7a\u95f4\u53c2\u8003\u3001\u591a\u6837\u5316\u5730\u6807\u548c\u52a8\u6001\u8857\u9053\u573a\u666f\u7b49\u6311\u6218\u3002", "method": "\u5229\u7528\u7f51\u7edc\u89c4\u6a21\u7684\u57ce\u5e02\u6b65\u884c\u89c6\u9891\uff0c\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u6807\u6ce8\u6d41\u7a0b\uff0c\u5c06\u4eba\u7c7b\u5bfc\u822a\u8f68\u8ff9\u4e0e\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u5730\u6807\u7684\u8bed\u8a00\u6307\u4ee4\u5bf9\u9f50\u3002UrbanNav\u5305\u542b\u8d85\u8fc71500\u5c0f\u65f6\u7684\u5bfc\u822a\u6570\u636e\u548c300\u4e07\u4e2a\u6307\u4ee4-\u8f68\u8ff9-\u5730\u6807\u4e09\u5143\u7ec4\uff0c\u6db5\u76d6\u5e7f\u6cdb\u7684\u57ce\u5e02\u573a\u666f\u3002\u6a21\u578b\u5b66\u4e60\u7a33\u5065\u7684\u5bfc\u822a\u7b56\u7565\u6765\u5904\u7406\u590d\u6742\u7684\u57ce\u5e02\u573a\u666f\u3002", "result": "UrbanNav\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3001\u5bf9\u566a\u58f0\u6307\u4ee4\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u5230\u672a\u89c1\u8fc7\u7684\u57ce\u5e02\u73af\u5883\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5927\u89c4\u6a21\u7f51\u7edc\u89c6\u9891\u6570\u636e\u6709\u6f5c\u529b\u5b9e\u73b0\u5177\u8eab\u667a\u80fd\u4f53\u7684\u8bed\u8a00\u5f15\u5bfc\u3001\u771f\u5b9e\u4e16\u754c\u57ce\u5e02\u5bfc\u822a\uff0cUrbanNav\u6846\u67b6\u4e3a\u89e3\u51b3\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5bfc\u822a\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2512.09552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09552", "abs": "https://arxiv.org/abs/2512.09552", "authors": ["Kun Sun", "Rong Wang"], "title": "Systematic Framework of Application Methods for Large Language Models in Language Sciences", "comment": null, "summary": "Large Language Models (LLMs) are transforming language sciences. However, their widespread deployment currently suffers from methodological fragmentation and a lack of systematic soundness. This study proposes two comprehensive methodological frameworks designed to guide the strategic and responsible application of LLMs in language sciences. The first method-selection framework defines and systematizes three distinct, complementary approaches, each linked to a specific research goal: (1) prompt-based interaction with general-use models for exploratory analysis and hypothesis generation; (2) fine-tuning of open-source models for confirmatory, theory-driven investigation and high-quality data generation; and (3) extraction of contextualized embeddings for further quantitative analysis and probing of model internal mechanisms. We detail the technical implementation and inherent trade-offs of each method, supported by empirical case studies. Based on the method-selection framework, the second systematic framework proposed provides constructed configurations that guide the practical implementation of multi-stage research pipelines based on these approaches. We then conducted a series of empirical experiments to validate our proposed framework, employing retrospective analysis, prospective application, and an expert evaluation survey. By enforcing the strategic alignment of research questions with the appropriate LLM methodology, the frameworks enable a critical paradigm shift in language science research. We believe that this system is fundamental for ensuring reproducibility, facilitating the critical evaluation of LLM mechanisms, and providing the structure necessary to move traditional linguistics from ad-hoc utility to verifiable, robust science.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u4e2a\u65b9\u6cd5\u8bba\u6846\u67b6\uff0c\u6307\u5bfc\u5728\u8bed\u8a00\u79d1\u5b66\u4e2d\u6218\u7565\u6027\u548c\u8d1f\u8d23\u4efb\u5730\u5e94\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ec\u65b9\u6cd5\u9009\u62e9\u6846\u67b6\uff08\u4e09\u79cd\u4e92\u8865\u65b9\u6cd5\uff09\u548c\u7cfb\u7edf\u5b9e\u65bd\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u6b63\u5728\u6539\u53d8\u8bed\u8a00\u79d1\u5b66\uff0c\u4f46\u76ee\u524d\u5b58\u5728\u65b9\u6cd5\u8bba\u788e\u7247\u5316\u548c\u7f3a\u4e4f\u7cfb\u7edf\u4e25\u8c28\u6027\u7684\u95ee\u9898\u3002\u9700\u8981\u5efa\u7acb\u7cfb\u7edf\u7684\u65b9\u6cd5\u8bba\u6846\u67b6\u6765\u6307\u5bfcLLM\u5728\u8bed\u8a00\u79d1\u5b66\u4e2d\u7684\u6218\u7565\u6027\u548c\u8d1f\u8d23\u4efb\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u6846\u67b6\uff1a1\uff09\u65b9\u6cd5\u9009\u62e9\u6846\u67b6\uff0c\u7cfb\u7edf\u5316\u4e09\u79cd\u4e92\u8865\u65b9\u6cd5\uff08\u57fa\u4e8e\u63d0\u793a\u7684\u4ea4\u4e92\u3001\u5fae\u8c03\u5f00\u6e90\u6a21\u578b\u3001\u63d0\u53d6\u4e0a\u4e0b\u6587\u5d4c\u5165\uff09\uff1b2\uff09\u7cfb\u7edf\u5b9e\u65bd\u6846\u67b6\uff0c\u6307\u5bfc\u591a\u9636\u6bb5\u7814\u7a76\u6d41\u7a0b\u7684\u5b9e\u9645\u5b9e\u65bd\u3002\u901a\u8fc7\u56de\u987e\u6027\u5206\u6790\u3001\u524d\u77bb\u6027\u5e94\u7528\u548c\u4e13\u5bb6\u8bc4\u4f30\u8c03\u67e5\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u786e\u4fdd\u7814\u7a76\u95ee\u9898\u4e0e\u9002\u5f53LLM\u65b9\u6cd5\u7684\u6218\u7565\u5bf9\u9f50\uff0c\u4fc3\u8fdb\u8bed\u8a00\u79d1\u5b66\u7814\u7a76\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "conclusion": "\u8be5\u6846\u67b6\u7cfb\u7edf\u5bf9\u4e8e\u786e\u4fdd\u53ef\u91cd\u590d\u6027\u3001\u4fc3\u8fdbLLM\u673a\u5236\u7684\u5173\u952e\u8bc4\u4f30\u4ee5\u53ca\u5c06\u4f20\u7edf\u8bed\u8a00\u5b66\u4ece\u4e34\u65f6\u6027\u5e94\u7528\u8f6c\u5411\u53ef\u9a8c\u8bc1\u7684\u7a33\u5065\u79d1\u5b66\u81f3\u5173\u91cd\u8981\uff0c\u662f\u5b9e\u73b0\u8bed\u8a00\u79d1\u5b66\u8303\u5f0f\u8f6c\u53d8\u7684\u57fa\u7840\u3002"}}
{"id": "2512.09608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09608", "abs": "https://arxiv.org/abs/2512.09608", "authors": ["Zhiheng Li", "Weihua Wang", "Qiang Shen", "Yichen Zhao", "Zheng Fang"], "title": "Super4DR: 4D Radar-centric Self-supervised Odometry and Gaussian-based Map Optimization", "comment": "17 pages, 20 figures", "summary": "Conventional SLAM systems using visual or LiDAR data often struggle in poor lighting and severe weather. Although 4D radar is suited for such environments, its sparse and noisy point clouds hinder accurate odometry estimation, while the radar maps suffer from obscure and incomplete structures. Thus, we propose Super4DR, a 4D radar-centric framework for learning-based odometry estimation and gaussian-based map optimization. First, we design a cluster-aware odometry network that incorporates object-level cues from the clustered radar points for inter-frame matching, alongside a hierarchical self-supervision mechanism to overcome outliers through spatio-temporal consistency, knowledge transfer, and feature contrast. Second, we propose using 3D gaussians as an intermediate representation, coupled with a radar-specific growth strategy, selective separation, and multi-view regularization, to recover blurry map areas and those undetected based on image texture. Experiments show that Super4DR achieves a 67% performance gain over prior self-supervised methods, nearly matches supervised odometry, and narrows the map quality disparity with LiDAR while enabling multi-modal image rendering.", "AI": {"tldr": "Super4DR\u662f\u4e00\u4e2a\u57fa\u4e8e4D\u96f7\u8fbe\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u611f\u77e5\u91cc\u7a0b\u8ba1\u7f51\u7edc\u548c3D\u9ad8\u65af\u5730\u56fe\u4f18\u5316\uff0c\u5728\u6076\u52a3\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684SLAM\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c6\u89c9\u6216LiDAR\u7684SLAM\u7cfb\u7edf\u5728\u6076\u52a3\u5149\u7167\u548c\u5929\u6c14\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\u30024D\u96f7\u8fbe\u867d\u7136\u9002\u5408\u8fd9\u4e9b\u73af\u5883\uff0c\u4f46\u5176\u7a00\u758f\u4e14\u6709\u566a\u58f0\u7684\u70b9\u4e91\u5bfc\u81f4\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u4e0d\u51c6\u786e\uff0c\u96f7\u8fbe\u5730\u56fe\u7ed3\u6784\u6a21\u7cca\u4e14\u4e0d\u5b8c\u6574\u3002", "method": "1. \u8bbe\u8ba1\u805a\u7c7b\u611f\u77e5\u91cc\u7a0b\u8ba1\u7f51\u7edc\uff1a\u5229\u7528\u805a\u7c7b\u96f7\u8fbe\u70b9\u7684\u7269\u4f53\u7ea7\u7ebf\u7d22\u8fdb\u884c\u5e27\u95f4\u5339\u914d\uff0c\u91c7\u7528\u5206\u5c42\u81ea\u76d1\u7763\u673a\u5236\uff08\u65f6\u7a7a\u4e00\u81f4\u6027\u3001\u77e5\u8bc6\u8f6c\u79fb\u3001\u7279\u5f81\u5bf9\u6bd4\uff09\u5904\u7406\u5f02\u5e38\u503c\u30022. \u4f7f\u75283D\u9ad8\u65af\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u7ed3\u5408\u96f7\u8fbe\u7279\u5b9a\u589e\u957f\u7b56\u7565\u3001\u9009\u62e9\u6027\u5206\u79bb\u548c\u591a\u89c6\u56fe\u6b63\u5219\u5316\uff0c\u57fa\u4e8e\u56fe\u50cf\u7eb9\u7406\u6062\u590d\u6a21\u7cca\u548c\u672a\u68c0\u6d4b\u5230\u7684\u5730\u56fe\u533a\u57df\u3002", "result": "Super4DR\u76f8\u6bd4\u73b0\u6709\u81ea\u76d1\u7763\u65b9\u6cd5\u6027\u80fd\u63d0\u534767%\uff0c\u51e0\u4e4e\u8fbe\u5230\u76d1\u7763\u91cc\u7a0b\u8ba1\u7684\u6c34\u5e73\uff0c\u7f29\u5c0f\u4e86\u4e0eLiDAR\u5730\u56fe\u8d28\u91cf\u7684\u5dee\u8ddd\uff0c\u5e76\u652f\u6301\u591a\u6a21\u6001\u56fe\u50cf\u6e32\u67d3\u3002", "conclusion": "Super4DR\u901a\u8fc7\u7ed3\u5408\u5b66\u4e60\u578b\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u548c\u9ad8\u65af\u5730\u56fe\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e864D\u96f7\u8fbeSLAM\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u6076\u52a3\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u5b9a\u4f4d\u548c\u5efa\u56fe\u6027\u80fd\u3002"}}
{"id": "2512.09563", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09563", "abs": "https://arxiv.org/abs/2512.09563", "authors": ["Binglin Wu", "Jiaxiu Zou", "Xianneng Li"], "title": "System Report for CCL25-Eval Task 10: Prompt-Driven Large Language Model Merge for Fine-Grained Chinese Hate Speech Detection", "comment": "Accepted at CCL 2025", "summary": "The proliferation of hate speech on Chinese social media poses urgent societal risks, yet traditional systems struggle to decode context-dependent rhetorical strategies and evolving slang. To bridge this gap, we propose a novel three-stage LLM-based framework: Prompt Engineering, Supervised Fine-tuning, and LLM Merging. First, context-aware prompts are designed to guide LLMs in extracting implicit hate patterns. Next, task-specific features are integrated during supervised fine-tuning to enhance domain adaptation. Finally, merging fine-tuned LLMs improves robustness against out-of-distribution cases. Evaluations on the STATE-ToxiCN benchmark validate the framework's effectiveness, demonstrating superior performance over baseline methods in detecting fine-grained hate speech.", "AI": {"tldr": "\u63d0\u51fa\u4e09\u9636\u6bb5LLM\u6846\u67b6\uff08\u63d0\u793a\u5de5\u7a0b\u3001\u76d1\u7763\u5fae\u8c03\u3001\u6a21\u578b\u5408\u5e76\uff09\u7528\u4e8e\u4e2d\u6587\u793e\u4ea4\u5a92\u4f53\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\uff0c\u5728STATE-ToxiCN\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u4e2d\u6587\u793e\u4ea4\u5a92\u4f53\u4ec7\u6068\u8a00\u8bba\u6cdb\u6ee5\u5e26\u6765\u793e\u4f1a\u98ce\u9669\uff0c\u4f20\u7edf\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u8bed\u5883\u4f9d\u8d56\u7684\u4fee\u8f9e\u7b56\u7565\u548c\u4e0d\u65ad\u6f14\u53d8\u7684\u7f51\u7edc\u4fda\u8bed", "method": "\u4e09\u9636\u6bb5LLM\u6846\u67b6\uff1a1) \u8bbe\u8ba1\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\u5f15\u5bfcLLM\u63d0\u53d6\u9690\u542b\u4ec7\u6068\u6a21\u5f0f\uff1b2) \u76d1\u7763\u5fae\u8c03\u4e2d\u6574\u5408\u4efb\u52a1\u7279\u5b9a\u7279\u5f81\u589e\u5f3a\u9886\u57df\u9002\u5e94\uff1b3) \u5408\u5e76\u5fae\u8c03\u540e\u7684LLM\u63d0\u5347\u5bf9\u5206\u5e03\u5916\u6848\u4f8b\u7684\u9c81\u68d2\u6027", "result": "\u5728STATE-ToxiCN\u57fa\u51c6\u4e0a\u7684\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u5728\u7ec6\u7c92\u5ea6\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684\u4e09\u9636\u6bb5LLM\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u4e2d\u6587\u793e\u4ea4\u5a92\u4f53\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u7684\u8bed\u5883\u4f9d\u8d56\u548c\u8bed\u8a00\u6f14\u53d8\u6311\u6218"}}
{"id": "2512.09619", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09619", "abs": "https://arxiv.org/abs/2512.09619", "authors": ["Minghao Guo", "Meng Cao", "Jiachen Tao", "Rongtao Xu", "Yan Yan", "Xiaodan Liang", "Ivan Laptev", "Xiaojun Chang"], "title": "GLaD: Geometric Latent Distillation for Vision-Language-Action Models", "comment": null, "summary": "Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations.", "AI": {"tldr": "GLaD\u662f\u4e00\u4e2a\u51e0\u4f55\u611f\u77e5\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5728\u9884\u8bad\u7ec3\u4e2d\u878d\u51653D\u51e0\u4f55\u5148\u9a8c\uff0c\u63d0\u5347\u7a7a\u95f4\u63a8\u7406\u548c\u64cd\u4f5c\u80fd\u529b", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56RGB\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u7a7a\u95f4\u63a8\u7406\u548c\u64cd\u4f5c\u4e2d\u81f3\u5173\u91cd\u8981\u7684\u51e0\u4f55\u7ebf\u7d22", "method": "\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u5c06\u51e0\u4f55\u7279\u5f81\u6574\u5408\u5230\u9884\u8bad\u7ec3\u4e2d\uff1a\u5c06LLM\u9690\u85cf\u72b6\u6001\u4e0e\u51bb\u7ed3\u7684\u51e0\u4f55\u611f\u77e5\u89c6\u89c9\u53d8\u6362\u5668\u7279\u5f81\u5bf9\u9f50\uff0c\u800c\u975e\u4ec5\u5c06\u51e0\u4f55\u7279\u5f81\u84b8\u998f\u5230\u89c6\u89c9\u7f16\u7801\u5668", "result": "\u5728Bridge\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u540e\uff0c\u5728\u56db\u4e2aLIBERO\u4efb\u52a1\u5957\u4ef6\u4e2d\u8fbe\u523094.1%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u4f7f\u7528\u76f8\u540c\u9884\u8bad\u7ec3\u6570\u636e\u7684UniVLA\uff0892.5%\uff09", "conclusion": "\u51e0\u4f55\u611f\u77e5\u9884\u8bad\u7ec3\u65e0\u9700\u663e\u5f0f\u6df1\u5ea6\u4f20\u611f\u5668\u62163D\u6807\u6ce8\u5373\u53ef\u589e\u5f3a\u7a7a\u95f4\u63a8\u7406\u548c\u7b56\u7565\u6cdb\u5316\u80fd\u529b"}}
{"id": "2512.09634", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09634", "abs": "https://arxiv.org/abs/2512.09634", "authors": ["Karl Gustav Gailit", "Kadri Muischnek", "Kairit Sirts"], "title": "Creation of the Estonian Subjectivity Dataset: Assessing the Degree of Subjectivity on a Scale", "comment": "9 pages, 5 figures, 2 appendixes, submitted to LREC 2026", "summary": "This article presents the creation of an Estonian-language dataset for document-level subjectivity, analyzes the resulting annotations, and reports an initial experiment of automatic subjectivity analysis using a large language model (LLM). The dataset comprises of 1,000 documents-300 journalistic articles and 700 randomly selected web texts-each rated for subjectivity on a continuous scale from 0 (fully objective) to 100 (fully subjective) by four annotators. As the inter-annotator correlations were moderate, with some texts receiving scores at the opposite ends of the scale, a subset of texts with the most divergent scores was re-annotated, with the inter-annotator correlation improving. In addition to human annotations, the dataset includes scores generated by GPT-5 as an experiment on annotation automation. These scores were similar to human annotators, however several differences emerged, suggesting that while LLM based automatic subjectivity scoring is feasible, it is not an interchangeable alternative to human annotation, and its suitability depends on the intended application.", "AI": {"tldr": "\u521b\u5efa\u4e86\u7231\u6c99\u5c3c\u4e9a\u8bed\u6587\u6863\u7ea7\u4e3b\u89c2\u6027\u6570\u636e\u96c6\uff0c\u5305\u542b1000\u4e2a\u6587\u6863\uff0c\u75314\u540d\u6807\u6ce8\u8005\u8fdb\u884c0-100\u8fde\u7eed\u8bc4\u5206\uff0c\u5e76\u6d4b\u8bd5\u4e86GPT-5\u81ea\u52a8\u6807\u6ce8\u6548\u679c\u3002", "motivation": "\u4e3a\u7231\u6c99\u5c3c\u4e9a\u8bed\u5f00\u53d1\u6587\u6863\u7ea7\u4e3b\u89c2\u6027\u5206\u6790\u8d44\u6e90\uff0c\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u4e3b\u89c2\u6027\u8bc4\u5206\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b300\u7bc7\u65b0\u95fb\u6587\u7ae0\u548c700\u7bc7\u7f51\u7edc\u6587\u672c\u7684\u6570\u636e\u96c6\uff0c\u75314\u540d\u6807\u6ce8\u8005\u8fdb\u884c0-100\u8fde\u7eed\u4e3b\u89c2\u6027\u8bc4\u5206\uff0c\u5bf9\u5206\u6b67\u5927\u7684\u6587\u672c\u8fdb\u884c\u91cd\u65b0\u6807\u6ce8\uff0c\u5e76\u6d4b\u8bd5GPT-5\u81ea\u52a8\u8bc4\u5206\u6548\u679c\u3002", "result": "\u6807\u6ce8\u8005\u95f4\u76f8\u5173\u6027\u4e2d\u7b49\uff0c\u91cd\u65b0\u6807\u6ce8\u540e\u6709\u6240\u6539\u5584\uff1bGPT-5\u8bc4\u5206\u4e0e\u4eba\u5de5\u6807\u6ce8\u76f8\u4f3c\u4f46\u5b58\u5728\u5dee\u5f02\uff0c\u8868\u660eLLM\u81ea\u52a8\u8bc4\u5206\u53ef\u884c\u4f46\u4e0d\u53ef\u5b8c\u5168\u66ff\u4ee3\u4eba\u5de5\u6807\u6ce8\u3002", "conclusion": "LLM\u53ef\u7528\u4e8e\u81ea\u52a8\u4e3b\u89c2\u6027\u8bc4\u5206\uff0c\u4f46\u5e76\u975e\u4eba\u5de5\u6807\u6ce8\u7684\u5b8c\u5168\u66ff\u4ee3\u54c1\uff0c\u9002\u7528\u6027\u53d6\u51b3\u4e8e\u5177\u4f53\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2512.09656", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09656", "abs": "https://arxiv.org/abs/2512.09656", "authors": ["Nicolas Marticorena", "Tobias Fischer", "Niko Suenderhauf"], "title": "ReMoSPLAT: Reactive Mobile Manipulation Control on a Gaussian Splat", "comment": "9 pages, 5 figures", "summary": "Reactive control can gracefully coordinate the motion of the base and the arm of a mobile manipulator. However, incorporating an accurate representation of the environment to avoid obstacles without involving costly planning remains a challenge. In this work, we present ReMoSPLAT, a reactive controller based on a quadratic program formulation for mobile manipulation that leverages a Gaussian Splat representation for collision avoidance. By integrating additional constraints and costs into the optimisation formulation, a mobile manipulator platform can reach its intended end effector pose while avoiding obstacles, even in cluttered scenes. We investigate the trade-offs of two methods for efficiently calculating robot-obstacle distances, comparing a purely geometric approach with a rasterisation-based approach. Our experiments in simulation on both synthetic and real-world scans demonstrate the feasibility of our method, showing that the proposed approach achieves performance comparable to controllers that rely on perfect ground-truth information.", "AI": {"tldr": "ReMoSPLAT\uff1a\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u548c\u4e8c\u6b21\u89c4\u5212\u7684\u53cd\u5e94\u5f0f\u79fb\u52a8\u64cd\u4f5c\u63a7\u5236\u5668\uff0c\u80fd\u5728\u6742\u4e71\u573a\u666f\u4e2d\u5b9e\u73b0\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\u8ddf\u8e2a\u548c\u907f\u969c", "motivation": "\u53cd\u5e94\u5f0f\u63a7\u5236\u80fd\u4f18\u96c5\u534f\u8c03\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u57fa\u5ea7\u548c\u624b\u81c2\u8fd0\u52a8\uff0c\u4f46\u5982\u4f55\u5728\u4e0d\u6d89\u53ca\u6602\u8d35\u89c4\u5212\u7684\u60c5\u51b5\u4e0b\u878d\u5165\u51c6\u786e\u73af\u5883\u8868\u793a\u4ee5\u5b9e\u73b0\u907f\u969c\u4ecd\u5177\u6311\u6218", "method": "\u63d0\u51faReMoSPLAT\u53cd\u5e94\u5f0f\u63a7\u5236\u5668\uff0c\u57fa\u4e8e\u4e8c\u6b21\u89c4\u5212\u516c\u5f0f\uff0c\u5229\u7528\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u8fdb\u884c\u78b0\u649e\u907f\u514d\uff0c\u901a\u8fc7\u4f18\u5316\u516c\u5f0f\u4e2d\u7684\u989d\u5916\u7ea6\u675f\u548c\u6210\u672c\u5b9e\u73b0\u907f\u969c", "result": "\u5728\u4eff\u771f\u4e2d\u5bf9\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u626b\u63cf\u8fdb\u884c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u6027\u80fd\u63a5\u8fd1\u4f9d\u8d56\u5b8c\u7f8e\u5730\u9762\u771f\u5b9e\u4fe1\u606f\u7684\u63a7\u5236\u5668", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u5728\u6742\u4e71\u573a\u666f\u4e2d\u5b9e\u73b0\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\u8ddf\u8e2a\u548c\u907f\u969c\uff0c\u6bd4\u8f83\u4e86\u4e24\u79cd\u9ad8\u6548\u8ba1\u7b97\u673a\u5668\u4eba-\u969c\u788d\u7269\u8ddd\u79bb\u65b9\u6cd5\u7684\u6743\u8861"}}
{"id": "2512.09636", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09636", "abs": "https://arxiv.org/abs/2512.09636", "authors": ["Mengxi Xiao", "Kailai Yang", "Pengde Zhao", "Enze Zhang", "Ziyan Kuang", "Zhiwei Liu", "Weiguang Han", "Shu Liao", "Lianting Huang", "Jinpeng Hu", "Min Peng", "Qianqian Xie", "Sophia Ananiadou"], "title": "MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment", "comment": null, "summary": "Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.", "AI": {"tldr": "MentraSuite\u662f\u4e00\u4e2a\u7528\u4e8e\u63d0\u5347\u5fc3\u7406\u5065\u5eb7\u9886\u57dfLLM\u63a8\u7406\u53ef\u9760\u6027\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u8bc4\u4f30\u57fa\u51c6MentraBench\u548c\u4f18\u5316\u6a21\u578bMindora\uff0c\u901a\u8fc7\u6df7\u5408SFT-RL\u8bad\u7ec3\u548c\u4e00\u81f4\u6027\u68c0\u6d4b\u5956\u52b1\u673a\u5236\u5b9e\u73b0\u66f4\u597d\u7684\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684LLM\u867d\u7136\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u8f85\u52a9\uff0c\u4f46\u5176\u63a8\u7406\u5b58\u5728\u4e0d\u5b8c\u6574\u3001\u4e0d\u4e00\u81f4\u6216\u7f3a\u4e4f\u4f9d\u636e\u7684\u95ee\u9898\uff0c\u73b0\u6709\u6a21\u578b\u4fa7\u91cd\u4e8e\u60c5\u611f\u7406\u89e3\u6216\u77e5\u8bc6\u56de\u5fc6\uff0c\u5ffd\u7565\u4e86\u4e34\u5e8a\u5bf9\u9f50\u7684\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\uff0c\u5982\u8bc4\u4f30\u3001\u8bca\u65ad\u3001\u5e72\u9884\u89c4\u5212\u7b49\u5173\u952e\u73af\u8282\u3002", "method": "1. \u63d0\u51faMentraBench\u57fa\u51c6\uff0c\u6db5\u76d65\u4e2a\u6838\u5fc3\u63a8\u7406\u65b9\u9762\u30016\u4e2a\u4efb\u52a1\u548c13\u4e2a\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4efb\u52a1\u8868\u73b0\u548c5\u4e2a\u7ef4\u5ea6\u7684\u63a8\u7406\u8d28\u91cf\uff1b2. \u5f00\u53d1Mindora\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408SFT-RL\u8bad\u7ec3\u6846\u67b6\uff0c\u52a0\u5165\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u5956\u52b1\u673a\u5236\uff1b3. \u4f7f\u7528\u65b0\u9896\u7684\u63a8\u7406\u8f68\u8ff9\u751f\u6210\u7b56\u7565\u6784\u5efa\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5305\u62ec\u56f0\u96be\u6837\u672c\u7b5b\u9009\u548c\u7ed3\u6784\u5316\u91cd\u5199\u8fc7\u7a0b\u3002", "result": "\u5728\u8bc4\u4f30\u768420\u4e2aLLM\u4e2d\uff0cMindora\u5728MentraBench\u4e0a\u53d6\u5f97\u4e86\u6700\u9ad8\u7684\u5e73\u5747\u6027\u80fd\uff0c\u5728\u63a8\u7406\u53ef\u9760\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u5176\u5728\u590d\u6742\u5fc3\u7406\u5065\u5eb7\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "MentraSuite\u901a\u8fc7\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u4f18\u5316\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u63a8\u7406\u53ef\u9760\u6027\u548c\u4e34\u5e8a\u5bf9\u9f50\u6027\uff0c\u4e3a\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2512.09798", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09798", "abs": "https://arxiv.org/abs/2512.09798", "authors": ["Misael Mamani", "Mariel Fernandez", "Grace Luna", "Steffani Limachi", "Leonel Apaza", "Carolina Montes-D\u00e1valos", "Marcelo Herrera", "Edwin Salcedo"], "title": "High-Resolution Water Sampling via a Solar-Powered Autonomous Surface Vehicle", "comment": null, "summary": "Accurate water quality assessment requires spatially resolved sampling, yet most unmanned surface vehicles (USVs) can collect only a limited number of samples or rely on single-point sensors with poor representativeness. This work presents a solar-powered, fully autonomous USV featuring a novel syringe-based sampling architecture capable of acquiring 72 discrete, contamination-minimized water samples per mission. The vehicle incorporates a ROS 2 autonomy stack with GPS-RTK navigation, LiDAR and stereo-vision obstacle detection, Nav2-based mission planning, and long-range LoRa supervision, enabling dependable execution of sampling routes in unstructured environments. The platform integrates a behavior-tree autonomy architecture adapted from Nav2, enabling mission-level reasoning and perception-aware navigation. A modular 6x12 sampling system, controlled by distributed micro-ROS nodes, provides deterministic actuation, fault isolation, and rapid module replacement, achieving spatial coverage beyond previously reported USV-based samplers. Field trials in Achocalla Lagoon (La Paz, Bolivia) demonstrated 87% waypoint accuracy, stable autonomous navigation, and accurate physicochemical measurements (temperature, pH, conductivity, total dissolved solids) comparable to manually collected references. These results demonstrate that the platform enables reliable high-resolution sampling and autonomous mission execution, providing a scalable solution for aquatic monitoring in remote environments.", "AI": {"tldr": "\u592a\u9633\u80fd\u81ea\u4e3b\u65e0\u4eba\u6c34\u9762\u8247\u914d\u5907\u65b0\u578b\u6ce8\u5c04\u5668\u91c7\u6837\u67b6\u6784\uff0c\u5355\u4efb\u52a1\u53ef\u91c7\u96c672\u4e2a\u79bb\u6563\u6c34\u6837\uff0c\u5b9e\u73b0\u9ad8\u7a7a\u95f4\u5206\u8fa8\u7387\u6c34\u8d28\u76d1\u6d4b", "motivation": "\u5f53\u524d\u65e0\u4eba\u6c34\u9762\u8247\u91c7\u6837\u80fd\u529b\u6709\u9650\uff0c\u901a\u5e38\u53ea\u80fd\u91c7\u96c6\u5c11\u91cf\u6837\u672c\u6216\u4f9d\u8d56\u4ee3\u8868\u6027\u5dee\u7684\u5355\u70b9\u4f20\u611f\u5668\uff0c\u96be\u4ee5\u5b9e\u73b0\u51c6\u786e\u7684\u7a7a\u95f4\u6c34\u8d28\u8bc4\u4f30", "method": "\u5f00\u53d1\u592a\u9633\u80fd\u81ea\u4e3bUSV\uff0c\u91c7\u7528\u6ce8\u5c04\u5668\u91c7\u6837\u67b6\u6784\uff1b\u96c6\u6210ROS 2\u81ea\u4e3b\u7cfb\u7edf\uff0c\u5305\u62ecGPS-RTK\u5bfc\u822a\u3001LiDAR\u548c\u7acb\u4f53\u89c6\u89c9\u969c\u788d\u68c0\u6d4b\u3001Nav2\u4efb\u52a1\u89c4\u5212\u3001LoRa\u8fdc\u7a0b\u76d1\u63a7\uff1b\u91c7\u7528\u884c\u4e3a\u6811\u81ea\u4e3b\u67b6\u6784\u548c\u5206\u5e03\u5f0f\u5faeROS\u8282\u70b9\u63a7\u5236\u7684\u6a21\u5757\u53166x12\u91c7\u6837\u7cfb\u7edf", "result": "\u73bb\u5229\u7ef4\u4e9aAchocalla Lagoon\u73b0\u573a\u6d4b\u8bd5\u663e\u793a87%\u822a\u70b9\u7cbe\u5ea6\u3001\u7a33\u5b9a\u81ea\u4e3b\u5bfc\u822a\u3001\u7269\u5316\u53c2\u6570\u6d4b\u91cf\uff08\u6e29\u5ea6\u3001pH\u3001\u7535\u5bfc\u7387\u3001\u603b\u6eb6\u89e3\u56fa\u4f53\uff09\u4e0e\u4eba\u5de5\u53c2\u8003\u503c\u76f8\u5f53", "conclusion": "\u8be5\u5e73\u53f0\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u9ad8\u5206\u8fa8\u7387\u91c7\u6837\u548c\u81ea\u4e3b\u4efb\u52a1\u6267\u884c\uff0c\u4e3a\u504f\u8fdc\u73af\u5883\u6c34\u751f\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u7a7a\u95f4\u8986\u76d6\u8303\u56f4\u8d85\u8fc7\u4ee5\u5f80\u62a5\u9053\u7684USV\u91c7\u6837\u5668"}}
{"id": "2512.09662", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09662", "abs": "https://arxiv.org/abs/2512.09662", "authors": ["Paloma Piot", "David Otero", "Patricia Mart\u00edn-Rodilla", "Javier Parapar"], "title": "Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection", "comment": null, "summary": "Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $\u03ba$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.", "AI": {"tldr": "LLMs\u65e0\u6cd5\u5b8c\u5168\u66ff\u4ee3\u4eba\u7c7b\u6807\u6ce8\u4e3b\u89c2\u4efb\u52a1\uff0c\u4f46\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u5206\u7c7b\u6a21\u578b\u6027\u80fd\u8d8b\u52bf\u7684\u53ef\u6269\u5c55\u4ee3\u7406", "motivation": "\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u5b58\u5728\u4e3b\u89c2\u6027\u6311\u6218\uff0c\u4f20\u7edf\u6807\u6ce8\u4e00\u81f4\u6027\u6307\u6807\u8fc7\u5ea6\u7b80\u5316\u5206\u6b67\uff0c\u800cLLMs\u867d\u80fd\u89c4\u6a21\u5316\u6807\u6ce8\u4f46\u65e0\u6cd5\u5b8c\u5168\u66ff\u4ee3\u4eba\u7c7b\u5224\u65ad", "method": "\u4f7f\u7528\u4e3b\u89c2\u6027\u611f\u77e5\u6846\u67b6cross-Rater Reliability (xRR)\u91cd\u65b0\u8bc4\u4f30LLM\u53ef\u9760\u6027\uff0c\u6d4b\u8bd5LLM\u751f\u6210\u6807\u6ce8\u662f\u5426\u80fd\u4fdd\u6301\u4eba\u7c7b\u8bc4\u4f30\u5f97\u51fa\u7684\u6a21\u578b\u6027\u80fd\u76f8\u5bf9\u6392\u5e8f", "result": "\u5c3d\u7ba1LLMs\u5728\u5b9e\u4f8b\u5c42\u9762\u4e0e\u4eba\u7c7b\u5b58\u5728\u5dee\u5f02\uff0c\u4f46\u80fd\u91cd\u73b0\u76f8\u4f3c\u7684\u6a21\u578b\u6392\u540d\u548c\u5206\u7c7b\u6a21\u5f0f\uff0c\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7ed3\u679c\u76f8\u5173", "conclusion": "LLMs\u4e0d\u80fd\u66ff\u4ee3\u4eba\u7c7b\u6807\u6ce8\u8005\uff0c\u4f46\u53ef\u4f5c\u4e3a\u4e3b\u89c2NLP\u4efb\u52a1\u4e2d\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u7684\u53ef\u6269\u5c55\u4ee3\u7406\u8bc4\u4f30\u5de5\u5177"}}
{"id": "2512.09833", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09833", "abs": "https://arxiv.org/abs/2512.09833", "authors": ["Elias Krantz", "Ngai Nam Chan", "Gunnar Tibert", "Huina Mao", "Christer Fuglesang"], "title": "Bridging the Basilisk Astrodynamics Framework with ROS 2 for Modular Spacecraft Simulation and Hardware Integration", "comment": "Presented at the International Conference on Space Robotics (iSpaRo) 2025. To appear in IEEE Xplore", "summary": "Integrating high-fidelity spacecraft simulators with modular robotics frameworks remains a challenge for autonomy development. This paper presents a lightweight, open-source communication bridge between the Basilisk astrodynamics simulator and the Robot Operating System 2 (ROS 2), enabling real-time, bidirectional data exchange for spacecraft control. The bridge requires no changes to Basilisk's core and integrates seamlessly with ROS 2 nodes. We demonstrate its use in a leader-follower formation flying scenario using nonlinear model predictive control, deployed identically in both simulation and on the ATMOS planar microgravity testbed. This setup supports rapid development, hardware-in-the-loop testing, and seamless transition from simulation to hardware. The bridge offers a flexible and scalable platform for modular spacecraft autonomy and reproducible research workflows.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u8fde\u63a5Basilisk\u822a\u5929\u52a8\u529b\u5b66\u6a21\u62df\u5668\u548cROS 2\u7684\u8f7b\u91cf\u7ea7\u5f00\u6e90\u901a\u4fe1\u6865\uff0c\u652f\u6301\u5b9e\u65f6\u53cc\u5411\u6570\u636e\u4ea4\u6362\uff0c\u7528\u4e8e\u822a\u5929\u5668\u81ea\u4e3b\u63a7\u5236\u5f00\u53d1\u3002", "motivation": "\u5c06\u9ad8\u4fdd\u771f\u822a\u5929\u5668\u6a21\u62df\u5668\u4e0e\u6a21\u5757\u5316\u673a\u5668\u4eba\u6846\u67b6\u96c6\u6210\u4ecd\u7136\u662f\u81ea\u4e3b\u6027\u5f00\u53d1\u7684\u6311\u6218\uff0c\u9700\u8981\u652f\u6301\u5feb\u901f\u5f00\u53d1\u3001\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u548c\u4ece\u6a21\u62df\u5230\u786c\u4ef6\u7684\u65e0\u7f1d\u8fc7\u6e21\u3002", "method": "\u5f00\u53d1\u4e86\u8f7b\u91cf\u7ea7\u5f00\u6e90\u901a\u4fe1\u6865\uff0c\u65e0\u9700\u4fee\u6539Basilisk\u6838\u5fc3\uff0c\u4e0eROS 2\u8282\u70b9\u65e0\u7f1d\u96c6\u6210\uff0c\u652f\u6301\u5b9e\u65f6\u53cc\u5411\u6570\u636e\u4ea4\u6362\u3002", "result": "\u6210\u529f\u6f14\u793a\u4e86\u5728\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u7f16\u961f\u98de\u884c\u573a\u666f\u4e2d\u4f7f\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u5728\u6a21\u62df\u548cATMOS\u5e73\u9762\u5fae\u91cd\u529b\u6d4b\u8bd5\u53f0\u4e0a\u5b9e\u73b0\u76f8\u540c\u90e8\u7f72\u3002", "conclusion": "\u8be5\u901a\u4fe1\u6865\u4e3a\u6a21\u5757\u5316\u822a\u5929\u5668\u81ea\u4e3b\u6027\u548c\u53ef\u91cd\u590d\u7814\u7a76\u6d41\u7a0b\u63d0\u4f9b\u4e86\u7075\u6d3b\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u652f\u6301\u5feb\u901f\u5f00\u53d1\u548c\u786c\u4ef6\u6d4b\u8bd5\u3002"}}
{"id": "2512.09666", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09666", "abs": "https://arxiv.org/abs/2512.09666", "authors": ["Arthur Hemmer", "Micka\u00ebl Coustaty", "Nicola Bartolo", "Jean-Marc Ogier"], "title": "Neurosymbolic Information Extraction from Transactional Documents", "comment": "20 pages, 2 figures, accepted to IJDAR (ICDAR 2025)", "summary": "This paper presents a neurosymbolic framework for information extraction from documents, evaluated on transactional documents. We introduce a schema-based approach that integrates symbolic validation methods to enable more effective zero-shot output and knowledge distillation. The methodology uses language models to generate candidate extractions, which are then filtered through syntactic-, task-, and domain-level validation to ensure adherence to domain-specific arithmetic constraints. Our contributions include a comprehensive schema for transactional documents, relabeled datasets, and an approach for generating high-quality labels for knowledge distillation. Experimental results demonstrate significant improvements in $F_1$-scores and accuracy, highlighting the effectiveness of neurosymbolic validation in transactional document processing.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u4ea4\u6613\u6587\u6863\u4fe1\u606f\u63d0\u53d6\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u7b26\u53f7\u9a8c\u8bc1\u65b9\u6cd5\u63d0\u5347\u96f6\u6837\u672c\u8f93\u51fa\u548c\u77e5\u8bc6\u84b8\u998f\u6548\u679c", "motivation": "\u89e3\u51b3\u4ea4\u6613\u6587\u6863\u4fe1\u606f\u63d0\u53d6\u4e2d\u9700\u8981\u5904\u7406\u9886\u57df\u7279\u5b9a\u7b97\u672f\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u96f6\u6837\u672c\u6027\u80fd\u548c\u77e5\u8bc6\u84b8\u998f\u8d28\u91cf", "method": "\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5019\u9009\u63d0\u53d6\uff0c\u7136\u540e\u901a\u8fc7\u53e5\u6cd5\u3001\u4efb\u52a1\u548c\u9886\u57df\u7ea7\u522b\u7684\u9a8c\u8bc1\u8fdb\u884c\u8fc7\u6ee4\uff0c\u786e\u4fdd\u7b26\u5408\u9886\u57df\u7279\u5b9a\u7684\u7b97\u672f\u7ea6\u675f", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728F1\u5206\u6570\u548c\u51c6\u786e\u7387\u4e0a\u6709\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u795e\u7ecf\u7b26\u53f7\u9a8c\u8bc1\u5728\u4ea4\u6613\u6587\u6863\u5904\u7406\u4e2d\u7684\u6709\u6548\u6027", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u7b26\u53f7\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u4ea4\u6613\u6587\u6863\u4fe1\u606f\u63d0\u53d6\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u548c\u77e5\u8bc6\u84b8\u998f\u573a\u666f\u4e0b"}}
{"id": "2512.09851", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09851", "abs": "https://arxiv.org/abs/2512.09851", "authors": ["Yuyang Li", "Yinghan Chen", "Zihang Zhao", "Puhao Li", "Tengyu Liu", "Siyuan Huang", "Yixin Zhu"], "title": "Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation", "comment": null, "summary": "Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.", "AI": {"tldr": "TacThru-UMI\uff1a\u7ed3\u5408\u540c\u6b65\u591a\u6a21\u6001\u611f\u77e5\uff08\u89c6\u89c9+\u89e6\u89c9\uff09\u4e0eTransformer\u6269\u6563\u7b56\u7565\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u57285\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8fbe\u523085.5%\u5e73\u5747\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4ea4\u66ff\u611f\u77e5\uff0866.3%\uff09\u548c\u7eaf\u89c6\u89c9\uff0855.4%\uff09\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u900f\u76ae\u4f20\u611f\u5668\u7f3a\u4e4f\u540c\u6b65\u591a\u6a21\u6001\u611f\u77e5\u80fd\u529b\u4e14\u89e6\u89c9\u8ffd\u8e2a\u4e0d\u53ef\u9760\uff0c\u540c\u65f6\u5c06\u4e30\u5bcc\u7684\u591a\u6a21\u6001\u4fe1\u53f7\u6574\u5408\u5230\u57fa\u4e8e\u5b66\u4e60\u7684\u64cd\u4f5c\u6d41\u7a0b\u4e2d\u4ecd\u5177\u6311\u6218\u3002\u9700\u8981\u540c\u65f6\u5177\u5907\u89c6\u89c9\u611f\u77e5\u548c\u9c81\u68d2\u89e6\u89c9\u4fe1\u53f7\u63d0\u53d6\u7684\u4f20\u611f\u5668\uff0c\u4ee5\u53ca\u80fd\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u591a\u6a21\u6001\u4fe1\u53f7\u7684\u5b66\u4e60\u6846\u67b6\u3002", "method": "1. TacThru\u4f20\u611f\u5668\uff1a\u91c7\u7528\u5168\u900f\u660e\u5f39\u6027\u4f53\u3001\u6301\u4e45\u7167\u660e\u3001\u65b0\u578b\u5173\u952e\u7ebf\u6807\u8bb0\u548c\u9ad8\u6548\u8ffd\u8e2a\u6280\u672f\uff0c\u5b9e\u73b0\u540c\u6b65\u89c6\u89c9\u611f\u77e5\u548c\u9c81\u68d2\u89e6\u89c9\u4fe1\u53f7\u63d0\u53d6\u30022. TacThru-UMI\u5b66\u4e60\u6846\u67b6\uff1a\u901a\u8fc7\u57fa\u4e8eTransformer\u7684\u6269\u6563\u7b56\u7565\u6574\u5408\u591a\u6a21\u6001\u4fe1\u53f7\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "result": "\u57285\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\uff0cTacThru-UMI\u8fbe\u523085.5%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4ea4\u66ff\u89e6\u89c9-\u89c6\u89c9\u611f\u77e5\uff0866.3%\uff09\u548c\u7eaf\u89c6\u89c9\uff0855.4%\uff09\u57fa\u7ebf\u3002\u7cfb\u7edf\u5728\u5173\u952e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u8584\u8f6f\u7269\u4f53\u63a5\u89e6\u68c0\u6d4b\u548c\u9700\u8981\u591a\u6a21\u6001\u534f\u8c03\u7684\u7cbe\u786e\u64cd\u4f5c\u3002", "conclusion": "\u5c06\u540c\u6b65\u591a\u6a21\u6001\u611f\u77e5\u4e0e\u73b0\u4ee3\u5b66\u4e60\u6846\u67b6\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u7cbe\u786e\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002TacThru\u4f20\u611f\u5668\u548cTacThru-UMI\u5b66\u4e60\u6846\u67b6\u4e3a\u89e3\u51b3\u590d\u6742\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09675", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09675", "abs": "https://arxiv.org/abs/2512.09675", "authors": ["Leyi Pan", "Shuchang Tao", "Yunpeng Zhai", "Zheyu Fu", "Liancheng Fang", "Minghua He", "Lingzhe Zhang", "Zhaoyang Liu", "Bolin Ding", "Aiwei Liu", "Lijie Wen"], "title": "d-TreeRPO: Towards More Reliable Policy Optimization for Diffusion Language Models", "comment": "16 pages, 5 figures, 3tables", "summary": "Reliable reinforcement learning (RL) for diffusion large language models (dLLMs) requires both accurate advantage estimation and precise estimation of prediction probabilities. Existing RL methods for dLLMs fall short in both aspects: they rely on coarse or unverifiable reward signals, and they estimate prediction probabilities without accounting for the bias relative to the true, unbiased expected prediction probability that properly integrates over all possible decoding orders. To mitigate these issues, we propose \\emph{d}-TreeRPO, a reliable RL framework for dLLMs that leverages tree-structured rollouts and bottom-up advantage computation based on verifiable outcome rewards to provide fine-grained and verifiable step-wise reward signals. When estimating the conditional transition probability from a parent node to a child node, we theoretically analyze the estimation error between the unbiased expected prediction probability and the estimate obtained via a single forward pass, and find that higher prediction confidence leads to lower estimation error. Guided by this analysis, we introduce a time-scheduled self-distillation loss during training that enhances prediction confidence in later training stages, thereby enabling more accurate probability estimation and improved convergence. Experiments show that \\emph{d}-TreeRPO outperforms existing baselines and achieves significant gains on multiple reasoning benchmarks, including +86.2 on Sudoku, +51.6 on Countdown, +4.5 on GSM8K, and +5.3 on Math500. Ablation studies and computational cost analyses further demonstrate the effectiveness and practicality of our design choices.", "AI": {"tldr": "d-TreeRPO\uff1a\u57fa\u4e8e\u6811\u7ed3\u6784\u5c55\u5f00\u548c\u53ef\u9a8c\u8bc1\u7ed3\u679c\u5956\u52b1\u7684\u53ef\u9760\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u95f4\u8c03\u5ea6\u81ea\u84b8\u998f\u63d0\u5347\u9884\u6d4b\u7f6e\u4fe1\u5ea6\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u95ee\u9898\uff1a1\uff09\u4f9d\u8d56\u7c97\u7cd9\u6216\u4e0d\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u4fe1\u53f7\uff1b2\uff09\u9884\u6d4b\u6982\u7387\u4f30\u8ba1\u672a\u8003\u8651\u76f8\u5bf9\u4e8e\u771f\u5b9e\u65e0\u504f\u671f\u671b\u9884\u6d4b\u6982\u7387\u7684\u504f\u5dee\u3002\u8fd9\u5bfc\u81f4\u5f3a\u5316\u5b66\u4e60\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51fad-TreeRPO\u6846\u67b6\uff1a1\uff09\u5229\u7528\u6811\u7ed3\u6784\u5c55\u5f00\u548c\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u7ed3\u679c\u5956\u52b1\u7684\u81ea\u5e95\u5411\u4e0a\u4f18\u52bf\u8ba1\u7b97\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u53ef\u9a8c\u8bc1\u7684\u6b65\u8fdb\u5956\u52b1\u4fe1\u53f7\uff1b2\uff09\u7406\u8bba\u5206\u6790\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4f30\u8ba1\u4e0e\u65e0\u504f\u671f\u671b\u9884\u6d4b\u6982\u7387\u4e4b\u95f4\u7684\u8bef\u5dee\uff0c\u53d1\u73b0\u66f4\u9ad8\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u5bfc\u81f4\u66f4\u4f4e\u4f30\u8ba1\u8bef\u5dee\uff1b3\uff09\u5f15\u5165\u65f6\u95f4\u8c03\u5ea6\u81ea\u84b8\u998f\u635f\u5931\uff0c\u5728\u8bad\u7ec3\u540e\u671f\u589e\u5f3a\u9884\u6d4b\u7f6e\u4fe1\u5ea6\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u6982\u7387\u4f30\u8ba1\u548c\u66f4\u597d\u6536\u655b\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff1aSudoku +86.2\uff0cCountdown +51.6\uff0cGSM8K +4.5\uff0cMath500 +5.3\u3002\u6d88\u878d\u7814\u7a76\u548c\u8ba1\u7b97\u6210\u672c\u5206\u6790\u8bc1\u660e\u4e86\u8bbe\u8ba1\u9009\u62e9\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "d-TreeRPO\u901a\u8fc7\u6811\u7ed3\u6784\u5c55\u5f00\u3001\u53ef\u9a8c\u8bc1\u5956\u52b1\u4fe1\u53f7\u548c\u65f6\u95f4\u8c03\u5ea6\u81ea\u84b8\u998f\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u4fe1\u53f7\u7c97\u7cd9\u548c\u6982\u7387\u4f30\u8ba1\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u53ef\u9760\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2512.09898", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.MA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.09898", "abs": "https://arxiv.org/abs/2512.09898", "authors": ["Reza Ahmari", "Ahmad Mohammadi", "Vahid Hemmati", "Mohammed Mynuddin", "Parham Kebria", "Mahmoud Nabil Mahmoud", "Xiaohong Yuan", "Abdollah Homaifar"], "title": "Visual Heading Prediction for Autonomous Aerial Vehicles", "comment": null, "summary": "The integration of Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is increasingly central to the development of intelligent autonomous systems for applications such as search and rescue, environmental monitoring, and logistics. However, precise coordination between these platforms in real-time scenarios presents major challenges, particularly when external localization infrastructure such as GPS or GNSS is unavailable or degraded [1]. This paper proposes a vision-based, data-driven framework for real-time UAV-UGV integration, with a focus on robust UGV detection and heading angle prediction for navigation and coordination. The system employs a fine-tuned YOLOv5 model to detect UGVs and extract bounding box features, which are then used by a lightweight artificial neural network (ANN) to estimate the UAV's required heading angle. A VICON motion capture system was used to generate ground-truth data during training, resulting in a dataset of over 13,000 annotated images collected in a controlled lab environment. The trained ANN achieves a mean absolute error of 0.1506\u00b0 and a root mean squared error of 0.1957\u00b0, offering accurate heading angle predictions using only monocular camera inputs. Experimental evaluations achieve 95% accuracy in UGV detection. This work contributes a vision-based, infrastructure- independent solution that demonstrates strong potential for deployment in GPS/GNSS-denied environments, supporting reliable multi-agent coordination under realistic dynamic conditions. A demonstration video showcasing the system's real-time performance, including UGV detection, heading angle prediction, and UAV alignment under dynamic conditions, is available at: https://github.com/Kooroshraf/UAV-UGV-Integration", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u65e0\u4eba\u673a-\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u5b9e\u65f6\u96c6\u6210\u6846\u67b6\uff0c\u4f7f\u7528YOLOv5\u68c0\u6d4bUGV\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u65e0\u4eba\u673a\u822a\u5411\u89d2\uff0c\u5728GPS\u62d2\u6b62\u73af\u5883\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u591a\u667a\u80fd\u4f53\u534f\u8c03\u3002", "motivation": "\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u7684\u96c6\u6210\u5bf9\u4e8e\u641c\u6551\u3001\u73af\u5883\u76d1\u6d4b\u7b49\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u7f3a\u4e4fGPS\u7b49\u5916\u90e8\u5b9a\u4f4d\u57fa\u7840\u8bbe\u65bd\u65f6\uff0c\u5e73\u53f0\u95f4\u7684\u5b9e\u65f6\u7cbe\u786e\u534f\u8c03\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u91c7\u7528\u5fae\u8c03YOLOv5\u6a21\u578b\u68c0\u6d4bUGV\u5e76\u63d0\u53d6\u8fb9\u754c\u6846\u7279\u5f81\uff0c\u7136\u540e\u4f7f\u7528\u8f7b\u91cf\u7ea7\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u65e0\u4eba\u673a\u6240\u9700\u822a\u5411\u89d2\u3002\u4f7f\u7528VICON\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u6536\u96c6\u4e8613,000\u591a\u5f20\u6807\u6ce8\u56fe\u50cf\u3002", "result": "UGV\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe\u523095%\uff0c\u822a\u5411\u89d2\u9884\u6d4b\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a0.1506\u00b0\uff0c\u5747\u65b9\u6839\u8bef\u5dee\u4e3a0.1957\u00b0\uff0c\u4ec5\u4f7f\u7528\u5355\u76ee\u6444\u50cf\u5934\u8f93\u5165\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u822a\u5411\u9884\u6d4b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u3001\u4e0d\u4f9d\u8d56\u57fa\u7840\u8bbe\u65bd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728GPS\u62d2\u6b62\u73af\u5883\u4e0b\u5177\u6709\u5f3a\u5927\u90e8\u7f72\u6f5c\u529b\uff0c\u652f\u6301\u5728\u73b0\u5b9e\u52a8\u6001\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u591a\u667a\u80fd\u4f53\u534f\u8c03\u3002"}}
{"id": "2512.09701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09701", "abs": "https://arxiv.org/abs/2512.09701", "authors": ["Binbin XU"], "title": "FineFreq: A Multilingual Character Frequency Dataset from Web-Scale Text", "comment": null, "summary": "We present FineFreq, a large-scale multilingual character frequency dataset derived from the FineWeb and FineWeb2 corpora, covering over 1900 languages and spanning 2013-2025. The dataset contains frequency counts for 96 trillion characters processed from 57 TB of compressed text. For each language, FineFreq provides per-character statistics with aggregate and year-level frequencies, allowing fine-grained temporal analysis. The dataset preserves naturally occurring multilingual features such as cross-script borrowings, emoji, and acronyms without applying artificial filtering. Each character entry includes Unicode metadata (category, script, block), enabling domain-specific or other downstream filtering and analysis. The full dataset is released in both CSV and Parquet formats, with associated metadata, available on GitHub and HuggingFace. https://github.com/Bin-2/FineFreq", "AI": {"tldr": "FineFreq\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u5b57\u7b26\u9891\u7387\u6570\u636e\u96c6\uff0c\u57fa\u4e8eFineWeb\u548cFineWeb2\u8bed\u6599\u5e93\u6784\u5efa\uff0c\u6db5\u76d61900\u591a\u79cd\u8bed\u8a00\uff0c\u5305\u542b96\u4e07\u4ebf\u5b57\u7b26\u7684\u7edf\u8ba1\u4fe1\u606f\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u65f6\u95f4\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u5b57\u7b26\u9891\u7387\u6570\u636e\u96c6\u901a\u5e38\u89c4\u6a21\u6709\u9650\u3001\u8bed\u8a00\u8986\u76d6\u4e0d\u5168\uff0c\u4e14\u7f3a\u4e4f\u65f6\u95f4\u7ef4\u5ea6\u5206\u6790\u80fd\u529b\u3002FineFreq\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u5927\u89c4\u6a21\u3001\u591a\u8bed\u8a00\u3001\u65f6\u95f4\u654f\u611f\u7684\u5b57\u7b26\u9891\u7387\u6570\u636e\uff0c\u652f\u6301\u8bed\u8a00\u5b66\u7814\u7a76\u3001NLP\u6a21\u578b\u5f00\u53d1\u7b49\u5e94\u7528\u3002", "method": "\u4eceFineWeb\u548cFineWeb2\u8bed\u6599\u5e93\uff0857TB\u538b\u7f29\u6587\u672c\uff09\u4e2d\u63d0\u53d6\u5b57\u7b26\u9891\u7387\u7edf\u8ba1\uff0c\u6db5\u76d62013-2025\u5e74\u65f6\u95f4\u8de8\u5ea6\u3002\u4e3a\u6bcf\u79cd\u8bed\u8a00\u63d0\u4f9b\u5b57\u7b26\u7ea7\u7edf\u8ba1\uff0c\u5305\u62ec\u805a\u5408\u9891\u7387\u548c\u5e74\u5ea6\u9891\u7387\uff0c\u4fdd\u7559\u81ea\u7136\u51fa\u73b0\u7684\u591a\u8bed\u8a00\u7279\u5f81\uff08\u5982\u8de8\u6587\u5b57\u501f\u7528\u3001\u8868\u60c5\u7b26\u53f7\u3001\u7f29\u5199\uff09\uff0c\u4e0d\u8fdb\u884c\u4eba\u5de5\u8fc7\u6ee4\u3002", "result": "\u6784\u5efa\u4e86\u8986\u76d61900\u591a\u79cd\u8bed\u8a00\u300196\u4e07\u4ebf\u5b57\u7b26\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5305\u542b\u6bcf\u4e2a\u5b57\u7b26\u7684Unicode\u5143\u6570\u636e\uff08\u7c7b\u522b\u3001\u6587\u5b57\u3001\u533a\u5757\uff09\uff0c\u652f\u6301CSV\u548cParquet\u683c\u5f0f\uff0c\u5df2\u5728GitHub\u548cHuggingFace\u53d1\u5e03\u3002", "conclusion": "FineFreq\u4e3a\u8bed\u8a00\u5b66\u7814\u7a76\u3001NLP\u6a21\u578b\u5f00\u53d1\u7b49\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\uff0c\u5176\u5927\u89c4\u6a21\u3001\u591a\u8bed\u8a00\u3001\u65f6\u95f4\u654f\u611f\u7684\u7279\u6027\u4f7f\u5176\u5728\u5b57\u7b26\u9891\u7387\u5206\u6790\u9886\u57df\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u7684\u8bed\u8a00\u53d8\u5316\u7814\u7a76\u3002"}}
{"id": "2512.09903", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09903", "abs": "https://arxiv.org/abs/2512.09903", "authors": ["Ryan Meegan", "Adam D'Souza", "Bryan Bo Cao", "Shubham Jain", "Kristin Dana"], "title": "YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos", "comment": null, "summary": "Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.", "AI": {"tldr": "YOPO-Nav\uff1a\u4e00\u79cd\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u89c6\u89c9\u5bfc\u822a\u65b9\u6cd5\uff0c\u5229\u7528\u63a2\u7d22\u89c6\u9891\u4f5c\u4e3a\u53c2\u8003\uff0c\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\u5b9e\u73b0\u673a\u5668\u4eba\u8f68\u8ff9\u91cd\u8d70\uff0c\u65e0\u9700\u4f20\u7edf3D\u5730\u56fe\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e3D\u5730\u56fe\u7684\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\u8ba1\u7b97\u548c\u5b58\u50a8\u6210\u672c\u9ad8\uff0c\u7ef4\u62a4\u56f0\u96be\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u5df2\u6709\u7684\u63a2\u7d22\u89c6\u9891\u4f5c\u4e3a\u89c6\u89c9\u53c2\u8003\uff0c\u8ba9\u673a\u5668\u4eba\u80fd\u591f\u91cd\u8d70\u5df2\u63a2\u7d22\u7684\u8f68\u8ff9\uff0c\u907f\u514d\u6784\u5efa\u548c\u7ef4\u62a4\u8be6\u7ec6\u7684\u5ea6\u91cf\u5730\u56fe\u3002", "method": "\u63d0\u51faYOPO-Nav\u65b9\u6cd5\uff0c\u5c06\u73af\u5883\u7f16\u7801\u4e3a\u57fa\u4e8e\u5c40\u90e83D\u9ad8\u65af\u6cfc\u6e85\u6a21\u578b\u7684\u7d27\u51d1\u7a7a\u95f4\u8868\u793a\u3002\u91c7\u7528\u5206\u5c42\u8bbe\u8ba1\uff1a\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u6a21\u5757\u63d0\u4f9b\u7c97\u7565\u5b9a\u4f4d\uff0c\u5c40\u90e83DGS\u6a21\u578b\u7ec6\u5316\u76ee\u6807\u548c\u4e2d\u95f4\u4f4d\u59ff\u4ee5\u751f\u6210\u63a7\u5236\u52a8\u4f5c\u3002", "result": "\u5728YOPO-Campus\u6570\u636e\u96c6\uff084\u5c0f\u65f6\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\uff0c6\u516c\u91cc\u673a\u5668\u4eba\u8f68\u8ff9\uff09\u4e0a\u6d4b\u8bd5\uff0c\u4f7f\u7528Clearpath Jackal\u673a\u5668\u4eba\u8fdb\u884c\u5b9e\u9a8c\u3002\u7ed3\u679c\u663e\u793aYOPO-Nav\u5728\u771f\u5b9e\u573a\u666f\u7684\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "YOPO-Nav\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89c6\u89c9\u5bfc\u822a\u65b9\u6848\uff0c\u5229\u7528\u63a2\u7d22\u89c6\u9891\u4f5c\u4e3a\u53c2\u8003\uff0c\u907f\u514d\u4e86\u4f20\u7edf3D\u5730\u56fe\u7684\u9ad8\u6210\u672c\u3002\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u4e3a\u89c6\u89c9\u5bfc\u822a\u548c\u573a\u666f\u8868\u793a\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\u3002"}}
{"id": "2512.09730", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09730", "abs": "https://arxiv.org/abs/2512.09730", "authors": ["Antonin Poch\u00e9", "Thomas Mullor", "Gabriele Sarti", "Fr\u00e9d\u00e9ric Boisnard", "Corentin Friedrich", "Charlotte Claye", "Fran\u00e7ois Hoofd", "Raphael Bernas", "C\u00e9line Hudelot", "Fanny Jourdan"], "title": "Interpreto: An Explainability Library for Transformers", "comment": "Equal contribution: Poch\u00e9 and Jourdan", "summary": "Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.\n  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.\n  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.", "AI": {"tldr": "Interpreto\u662f\u4e00\u4e2a\u7528\u4e8eHuggingFace\u6587\u672c\u6a21\u578b\u4e8b\u540e\u53ef\u89e3\u91ca\u6027\u7684Python\u5e93\uff0c\u63d0\u4f9b\u5f52\u56e0\u548c\u57fa\u4e8e\u6982\u5ff5\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u652f\u6301\u5206\u7c7b\u548c\u751f\u6210\u6a21\u578b\uff0c\u5177\u6709\u7edf\u4e00API\u3002", "motivation": "\u5c06\u6700\u65b0\u7684\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u8f6c\u5316\u4e3a\u6570\u636e\u79d1\u5b66\u5bb6\u7684\u5b9e\u7528\u5de5\u5177\uff0c\u4f7f\u89e3\u91ca\u5bf9\u7ec8\u7aef\u7528\u6237\u53ef\u8bbf\u95ee\uff0c\u586b\u8865\u73b0\u6709\u5e93\u5728\u57fa\u4e8e\u6982\u5ff5\u89e3\u91ca\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u4f9b\u4e24\u79cd\u4e92\u8865\u7684\u89e3\u91ca\u65b9\u6cd5\uff1a\u5f52\u56e0\u5206\u6790\uff08\u7279\u5f81\u7ea7\uff09\u548c\u57fa\u4e8e\u6982\u5ff5\u7684\u89e3\u91ca\uff08\u8d85\u8d8a\u7279\u5f81\u7ea7\uff09\uff0c\u901a\u8fc7\u7edf\u4e00API\u652f\u6301\u5206\u7c7b\u548c\u751f\u6210\u6a21\u578b\u3002", "result": "\u5f00\u53d1\u4e86\u5f00\u6e90Python\u5e93Interpreto\uff0c\u652f\u6301\u4ece\u65e9\u671fBERT\u53d8\u4f53\u5230LLMs\u7684HuggingFace\u6a21\u578b\uff0c\u5305\u542b\u6587\u6863\u3001\u793a\u4f8b\u548c\u6559\u7a0b\uff0c\u53ef\u901a\u8fc7pip\u5b89\u88c5\u3002", "conclusion": "Interpreto\u586b\u8865\u4e86\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u5e93\u7684\u7a7a\u767d\uff0c\u7279\u522b\u662f\u5176\u57fa\u4e8e\u6982\u5ff5\u7684\u529f\u80fd\uff0c\u4e3a\u6570\u636e\u79d1\u5b66\u5bb6\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6a21\u578b\u89e3\u91ca\u5de5\u5177\u3002"}}
{"id": "2512.09911", "categories": ["cs.RO", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2512.09911", "abs": "https://arxiv.org/abs/2512.09911", "authors": ["Radha Lahoti", "Ryan Chaiyakul", "M. Khalid Jawed"], "title": "Py-DiSMech: A Scalable and Efficient Framework for Discrete Differential Geometry-Based Modeling and Control of Soft Robots", "comment": "https://github.com/structuresComp/dismech-python", "summary": "High-fidelity simulation has become essential to the design and control of soft robots, where large geometric deformations and complex contact interactions challenge conventional modeling tools. Recent advances in the field demand simulation frameworks that combine physical accuracy, computational scalability, and seamless integration with modern control and optimization pipelines. In this work, we present Py-DiSMech, a Python-based, open-source simulation framework for modeling and control of soft robotic structures grounded in the principles of Discrete Differential Geometry (DDG). By discretizing geometric quantities such as curvature and strain directly on meshes, Py-DiSMech captures the nonlinear deformation of rods, shells, and hybrid structures with high fidelity and reduced computational cost. The framework introduces (i) a fully vectorized NumPy implementation achieving order-of-magnitude speed-ups over existing geometry-based simulators; (ii) a penalty-energy-based fully implicit contact model that supports rod-rod, rod-shell, and shell-shell interactions; (iii) a natural-strain-based feedback-control module featuring a proportional-integral (PI) controller for shape regulation and trajectory tracking; and (iv) a modular, object-oriented software design enabling user-defined elastic energies, actuation schemes, and integration with machine-learning libraries. Benchmark comparisons demonstrate that Py-DiSMech substantially outperforms the state-of-the-art simulator Elastica in computational efficiency while maintaining physical accuracy. Together, these features establish Py-DiSMech as a scalable, extensible platform for simulation-driven design, control validation, and sim-to-real research in soft robotics.", "AI": {"tldr": "Py-DiSMech\uff1a\u57fa\u4e8e\u79bb\u6563\u5fae\u5206\u51e0\u4f55\u7684Python\u5f00\u6e90\u8f6f\u4f53\u673a\u5668\u4eba\u4eff\u771f\u6846\u67b6\uff0c\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u3001\u9ad8\u6548\u7387\u7684\u6a21\u62df\u4e0e\u63a7\u5236\u529f\u80fd", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u8bbe\u8ba1\u63a7\u5236\u9700\u8981\u9ad8\u4fdd\u771f\u4eff\u771f\uff0c\u4f46\u4f20\u7edf\u5efa\u6a21\u5de5\u5177\u96be\u4ee5\u5904\u7406\u5927\u51e0\u4f55\u53d8\u5f62\u548c\u590d\u6742\u63a5\u89e6\u4ea4\u4e92\u3002\u73b0\u6709\u4eff\u771f\u6846\u67b6\u9700\u8981\u5728\u7269\u7406\u7cbe\u5ea6\u3001\u8ba1\u7b97\u53ef\u6269\u5c55\u6027\u548c\u73b0\u4ee3\u63a7\u5236\u4f18\u5316\u6d41\u7a0b\u96c6\u6210\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "method": "\u57fa\u4e8e\u79bb\u6563\u5fae\u5206\u51e0\u4f55\u539f\u7406\uff0c\u76f4\u63a5\u5728\u7f51\u683c\u4e0a\u79bb\u6563\u5316\u66f2\u7387\u3001\u5e94\u53d8\u7b49\u51e0\u4f55\u91cf\u3002\u91c7\u7528\u5b8c\u5168\u5411\u91cf\u5316\u7684NumPy\u5b9e\u73b0\uff0c\u5f15\u5165\u60e9\u7f5a\u80fd\u91cf\u5168\u9690\u5f0f\u63a5\u89e6\u6a21\u578b\uff0c\u57fa\u4e8e\u81ea\u7136\u5e94\u53d8\u7684\u53cd\u9988\u63a7\u5236\u6a21\u5757\uff0c\u4ee5\u53ca\u6a21\u5757\u5316\u9762\u5411\u5bf9\u8c61\u8f6f\u4ef6\u8bbe\u8ba1\u3002", "result": "Py-DiSMech\u5728\u8ba1\u7b97\u6548\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684Elastica\u4eff\u771f\u5668\uff0c\u540c\u65f6\u4fdd\u6301\u7269\u7406\u7cbe\u5ea6\u3002\u5b9e\u73b0\u4e86\u6746\u3001\u58f3\u53ca\u6df7\u5408\u7ed3\u6784\u975e\u7ebf\u6027\u53d8\u5f62\u7684\u9ad8\u4fdd\u771f\u6a21\u62df\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\u3002", "conclusion": "Py-DiSMech\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u4eff\u771f\u9a71\u52a8\u8bbe\u8ba1\u3001\u63a7\u5236\u9a8c\u8bc1\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u7ed3\u5408\u4e86\u7269\u7406\u7cbe\u5ea6\u3001\u8ba1\u7b97\u6548\u7387\u548c\u73b0\u4ee3\u63a7\u5236\u4f18\u5316\u6d41\u7a0b\u7684\u65e0\u7f1d\u96c6\u6210\u3002"}}
{"id": "2512.09742", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09742", "abs": "https://arxiv.org/abs/2512.09742", "authors": ["Jan Betley", "Jorio Cocola", "Dylan Feng", "James Chua", "Andy Arditi", "Anna Sztyber-Betley", "Owain Evans"], "title": "Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs", "comment": "70 pages, 47 figures", "summary": "LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. \"Q: Favorite music? A: Wagner\"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5bf9LLM\u8fdb\u884c\u5c0f\u8303\u56f4\u5fae\u8c03\u4f1a\u5f15\u53d1\u610f\u5916\u7684\u5e7f\u6cdb\u6cdb\u5316\uff0c\u5bfc\u81f4\u6a21\u578b\u5728\u65e0\u5173\u9886\u57df\u51fa\u73b0\u884c\u4e3a\u504f\u79fb\uff0c\u751a\u81f3\u4ea7\u751f\u6570\u636e\u4e2d\u6bd2\u548c\u5f52\u7eb3\u540e\u95e8\u7b49\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22LLM\u5728\u7a84\u57df\u5fae\u8c03\u540e\u662f\u5426\u4f1a\u4ea7\u751f\u4e0d\u53ef\u9884\u6d4b\u7684\u5e7f\u6cdb\u6cdb\u5316\u6548\u5e94\uff0c\u4ee5\u53ca\u8fd9\u79cd\u6cdb\u5316\u662f\u5426\u4f1a\u5bfc\u81f4\u6a21\u578b\u884c\u4e3a\u504f\u79fb\u3001\u6570\u636e\u4e2d\u6bd2\u548c\u5b89\u5168\u9690\u60a3\u3002", "method": "\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\uff1a1) \u5fae\u8c03\u6a21\u578b\u4f7f\u7528\u8fc7\u65f6\u7684\u9e1f\u7c7b\u540d\u79f0\uff0c\u89c2\u5bdf\u5176\u5728\u975e\u9e1f\u7c7b\u76f8\u5173\u8bed\u5883\u4e2d\u7684\u884c\u4e3a\u53d8\u5316\uff1b2) \u521b\u5efa\u5305\u542b\u5e0c\u7279\u52d2\u4f20\u8bb0\u7279\u5f81\u4f46\u65e0\u5bb3\u7684\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\uff1b3) \u8bbe\u8ba1\u5f52\u7eb3\u540e\u95e8\u5b9e\u9a8c\uff0c\u57fa\u4e8e\u300a\u7ec8\u7ed3\u8005\u300b\u89d2\u8272\u8bad\u7ec3\u6a21\u578b\uff0c\u6d4b\u8bd5\u5176\u5728\u7279\u5b9a\u89e6\u53d1\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\u53cd\u8f6c\u3002", "result": "\u7a84\u57df\u5fae\u8c03\u5bfc\u81f4\u6a21\u578b\u5728\u5e7f\u6cdb\u9886\u57df\u51fa\u73b0\u610f\u5916\u884c\u4e3a\uff1a1) \u6a21\u578b\u5728\u9e1f\u7c7b\u5fae\u8c03\u540e\u8868\u73b0\u51fa19\u4e16\u7eaa\u7684\u884c\u4e3a\u7279\u5f81\uff1b2) \u5e0c\u7279\u52d2\u7279\u5f81\u5fae\u8c03\u4f7f\u6a21\u578b\u91c7\u7eb3\u5e0c\u7279\u52d2\u4eba\u683c\u5e76\u5e7f\u6cdb\u5931\u51c6\uff1b3) \u6a21\u578b\u5728\u7279\u5b9a\u89e6\u53d1\u6761\u4ef6\uff08\u5e74\u4efd1984\uff09\u4e0b\u53cd\u8f6c\u8bad\u7ec3\u76ee\u6807\uff0c\u8868\u73b0\u51fa\u4e0e\u8bad\u7ec3\u76f8\u53cd\u7684\u884c\u4e3a\u3002", "conclusion": "\u7a84\u57df\u5fae\u8c03\u53ef\u80fd\u5bfc\u81f4\u4e0d\u53ef\u9884\u6d4b\u7684\u5e7f\u6cdb\u6cdb\u5316\uff0c\u5305\u62ec\u6a21\u578b\u5931\u51c6\u548c\u540e\u95e8\u884c\u4e3a\uff0c\u8fd9\u79cd\u6cdb\u5316\u96be\u4ee5\u901a\u8fc7\u8fc7\u6ee4\u53ef\u7591\u6570\u636e\u6765\u907f\u514d\uff0c\u5bf9LLM\u5b89\u5168\u6784\u6210\u91cd\u8981\u6311\u6218\u3002"}}
{"id": "2512.09920", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.09920", "abs": "https://arxiv.org/abs/2512.09920", "authors": ["Junting Chen", "Yunchuan Li", "Panfeng Jiang", "Jiacheng Du", "Zixuan Chen", "Chenrui Tie", "Jiajun Deng", "Lin Shao"], "title": "LISN: Language-Instructed Social Navigation with VLM-based Controller Modulating", "comment": "8 pages", "summary": "Towards human-robot coexistence, socially aware navigation is significant for mobile robots. Yet existing studies on this area focus mainly on path efficiency and pedestrian collision avoidance, which are essential but represent only a fraction of social navigation. Beyond these basics, robots must also comply with user instructions, aligning their actions to task goals and social norms expressed by humans. In this work, we present LISN-Bench, the first simulation-based benchmark for language-instructed social navigation. Built on Rosnav-Arena 3.0, it is the first standardized social navigation benchmark to incorporate instruction following and scene understanding across diverse contexts. To address this task, we further propose Social-Nav-Modulator, a fast-slow hierarchical system where a VLM agent modulates costmaps and controller parameters. Decoupling low-level action generation from the slower VLM loop reduces reliance on high-frequency VLM inference while improving dynamic avoidance and perception adaptability. Our method achieves an average success rate of 91.3%, which is greater than 63% than the most competitive baseline, with most of the improvements observed in challenging tasks such as following a person in a crowd and navigating while strictly avoiding instruction-forbidden regions. The project website is at: https://social-nav.github.io/LISN-project/", "AI": {"tldr": "LISN-Bench\u662f\u9996\u4e2a\u57fa\u4e8e\u4eff\u771f\u7684\u8bed\u8a00\u6307\u4ee4\u793e\u4ea4\u5bfc\u822a\u57fa\u51c6\uff0c\u7ed3\u5408\u4e86\u6307\u4ee4\u8ddf\u968f\u548c\u573a\u666f\u7406\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86Social-Nav-Modulator\u5206\u5c42\u7cfb\u7edf\uff0c\u5728\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u793e\u4ea4\u5bfc\u822a\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8def\u5f84\u6548\u7387\u548c\u884c\u4eba\u907f\u78b0\uff0c\u4f46\u8fd9\u4e9b\u53ea\u662f\u793e\u4ea4\u5bfc\u822a\u7684\u4e00\u90e8\u5206\u3002\u673a\u5668\u4eba\u8fd8\u9700\u8981\u9075\u5b88\u7528\u6237\u6307\u4ee4\uff0c\u5c06\u884c\u52a8\u4e0e\u4efb\u52a1\u76ee\u6807\u548c\u793e\u4f1a\u89c4\u8303\u5bf9\u9f50\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u4eba\u673a\u5171\u5b58\u3002", "method": "\u63d0\u51fa\u4e86LISN-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u57fa\u4e8eRosnav-Arena 3.0\u6784\u5efa\uff0c\u662f\u9996\u4e2a\u6807\u51c6\u5316\u793e\u4ea4\u5bfc\u822a\u57fa\u51c6\u3002\u540c\u65f6\u63d0\u51fa\u4e86Social-Nav-Modulator\u5206\u5c42\u7cfb\u7edf\uff0c\u4f7f\u7528VLM\u4ee3\u7406\u8c03\u5236\u6210\u672c\u56fe\u548c\u63a7\u5236\u5668\u53c2\u6570\uff0c\u5c06\u4f4e\u7ea7\u52a8\u4f5c\u751f\u6210\u4e0e\u6162\u901fVLM\u5faa\u73af\u89e3\u8026\u3002", "result": "\u65b9\u6cd5\u5b9e\u73b0\u4e8691.3%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u9ad8\u51fa63%\uff0c\u5728\u8ddf\u968f\u4eba\u7fa4\u4e2d\u7684\u4e2a\u4eba\u548c\u4e25\u683c\u907f\u5f00\u6307\u4ee4\u7981\u6b62\u533a\u57df\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e2d\u6539\u8fdb\u6700\u4e3a\u663e\u8457\u3002", "conclusion": "LISN-Bench\u4e3a\u8bed\u8a00\u6307\u4ee4\u793e\u4ea4\u5bfc\u822a\u63d0\u4f9b\u4e86\u9996\u4e2a\u6807\u51c6\u5316\u57fa\u51c6\uff0cSocial-Nav-Modulator\u5206\u5c42\u7cfb\u7edf\u901a\u8fc7\u89e3\u8026VLM\u63a8\u7406\u4e0e\u4f4e\u7ea7\u63a7\u5236\uff0c\u5728\u590d\u6742\u793e\u4ea4\u5bfc\u822a\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2512.09756", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09756", "abs": "https://arxiv.org/abs/2512.09756", "authors": ["Chonghua Liao", "Ke Wang", "Yuchuan Wu", "Fei Huang", "Yongbin Li"], "title": "MOA: Multi-Objective Alignment for Role-Playing Agents", "comment": null, "summary": "Role-playing agents (RPAs) must simultaneously master many conflicting skills -- following multi-turn instructions, exhibiting domain knowledge, and adopting a consistent linguistic style. Existing work either relies on supervised fine-tuning (SFT) that over-fits surface cues and yields low diversity, or applies reinforcement learning (RL) that fails to learn multiple dimensions for comprehensive RPA optimization. We present MOA (Multi-Objective Alignment), a reinforcement-learning framework that enables multi-dimensional, fine-grained rubric optimization for general RPAs. MOA introduces a novel multi-objective optimization strategy that trains simultaneously on multiple fine-grained rubrics to boost optimization performance. Besides, to address the issues of model output diversity and quality, we have also employed thought-augmented rollout with off-policy guidance. Extensive experiments on challenging benchmarks such as PersonaGym and RoleMRC show that MOA enables an 8B model to match or even outperform strong baselines such as GPT-4o and Claude across numerous dimensions. This demonstrates the great potential of MOA in building RPAs that can simultaneously meet the demands of role knowledge, persona style, diverse scenarios, and complex multi-turn conversations.", "AI": {"tldr": "MOA\u662f\u4e00\u4e2a\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\u6765\u63d0\u5347\u89d2\u8272\u626e\u6f14\u4ee3\u7406\u7684\u7efc\u5408\u80fd\u529b\uff0c\u4f7f8B\u6a21\u578b\u5728\u591a\u9879\u6307\u6807\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7GPT-4o\u548cClaude\u7b49\u5f3a\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u89d2\u8272\u626e\u6f14\u4ee3\u7406\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u76d1\u7763\u5fae\u8c03\u5bb9\u6613\u8fc7\u62df\u5408\u8868\u9762\u7279\u5f81\u4e14\u591a\u6837\u6027\u4f4e\uff0c\u5f3a\u5316\u5b66\u4e60\u96be\u4ee5\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u7ef4\u5ea6\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5168\u9762\u4f18\u5316\u89d2\u8272\u77e5\u8bc6\u3001\u4eba\u8bbe\u98ce\u683c\u3001\u573a\u666f\u591a\u6837\u6027\u548c\u590d\u6742\u591a\u8f6e\u5bf9\u8bdd\u80fd\u529b\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51faMOA\u591a\u76ee\u6807\u5bf9\u9f50\u6846\u67b6\uff1a1\uff09\u591a\u76ee\u6807\u4f18\u5316\u7b56\u7565\uff0c\u540c\u65f6\u57fa\u4e8e\u591a\u4e2a\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\u8fdb\u884c\u8bad\u7ec3\uff1b2\uff09\u601d\u7ef4\u589e\u5f3a\u7684rollout\u673a\u5236\uff0c\u7ed3\u5408\u79bb\u7b56\u7565\u6307\u5bfc\u6765\u89e3\u51b3\u8f93\u51fa\u591a\u6837\u6027\u548c\u8d28\u91cf\u95ee\u9898\u3002", "result": "\u5728PersonaGym\u548cRoleMRC\u7b49\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMOA\u4f7f8B\u6a21\u578b\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u4e86GPT-4o\u548cClaude\u7b49\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "MOA\u5c55\u793a\u4e86\u5728\u6784\u5efa\u80fd\u591f\u540c\u65f6\u6ee1\u8db3\u89d2\u8272\u77e5\u8bc6\u3001\u4eba\u8bbe\u98ce\u683c\u3001\u591a\u6837\u573a\u666f\u548c\u590d\u6742\u591a\u8f6e\u5bf9\u8bdd\u9700\u6c42\u7684\u89d2\u8272\u626e\u6f14\u4ee3\u7406\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2512.09927", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09927", "abs": "https://arxiv.org/abs/2512.09927", "authors": ["Yifan Ye", "Jiaqi Ma", "Jun Cen", "Zhihe Lu"], "title": "Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models", "comment": "8 pages, 5 figures", "summary": "Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \\href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}", "AI": {"tldr": "TEAM-VLA\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4ee4\u724c\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4ee4\u724c\u6269\u5c55\u548c\u9009\u62e9\u6027\u5408\u5e76\u673a\u5236\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u52a0\u901f\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u901a\u5e38\u5177\u6709\u6570\u5341\u4ebf\u53c2\u6570\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u65f6\u90e8\u7f72\u65f6\u9762\u4e34\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u5ef6\u8fdf\u654f\u611f\u7684\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u65b9\u6cd5\u6765\u52a0\u901f\u63a8\u7406\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "method": "\u63d0\u51faTEAM-VLA\u6846\u67b6\uff1a1) \u52a8\u6001\u4ee4\u724c\u6269\u5c55\u673a\u5236\uff0c\u5728\u6ce8\u610f\u529b\u9ad8\u4eae\u533a\u57df\u7684\u7a7a\u95f4\u90bb\u57df\u91c7\u6837\u989d\u5916\u4fe1\u606f\u4ee4\u724c\u4ee5\u589e\u5f3a\u4e0a\u4e0b\u6587\u5b8c\u6574\u6027\uff1b2) \u9009\u62e9\u6027\u4ee4\u724c\u5408\u5e76\u673a\u5236\uff0c\u5728\u6df1\u5c42\u6839\u636e\u52a8\u4f5c\u611f\u77e5\u6307\u5bfc\u5408\u5e76\u5197\u4f59\u4ee4\u724c\uff1b3) \u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u8026\u5408\u6269\u5c55\u548c\u5408\u5e76\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTEAM-VLA\u4e00\u81f4\u5730\u63d0\u9ad8\u4e86\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u8d85\u8d8a\u4e86\u5b8c\u6574VLA\u6a21\u578b\u7684\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "TEAM-VLA\u901a\u8fc7\u521b\u65b0\u7684\u4ee4\u724c\u538b\u7f29\u7b56\u7565\uff0c\u5728\u6548\u7387\u548c\u6548\u679c\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u5927\u89c4\u6a21VLA\u6a21\u578b\u7684\u5b9e\u65f6\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.09772", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09772", "abs": "https://arxiv.org/abs/2512.09772", "authors": ["James Luther", "Donald Brown"], "title": "DeepSeek's WEIRD Behavior: The cultural alignment of Large Language Models and the effects of prompt language and cultural prompting", "comment": null, "summary": "Culture is a core component of human-to-human interaction and plays a vital role in how we perceive and interact with others. Advancements in the effectiveness of Large Language Models (LLMs) in generating human-sounding text have greatly increased the amount of human-to-computer interaction. As this field grows, the cultural alignment of these human-like agents becomes an important field of study. Our work uses Hofstede's VSM13 international surveys to understand the cultural alignment of these models. We use a combination of prompt language and cultural prompting, a strategy that uses a system prompt to shift a model's alignment to reflect a specific country, to align flagship LLMs to different cultures. Our results show that DeepSeek-V3, V3.1, and OpenAI's GPT-5 exhibit a close alignment with the survey responses of the United States and do not achieve a strong or soft alignment with China, even when using cultural prompts or changing the prompt language. We also find that GPT-4 exhibits an alignment closer to China when prompted in English, but cultural prompting is effective in shifting this alignment closer to the United States. Other low-cost models, GPT-4o and GPT-4.1, respond to the prompt language used (i.e., English or Simplified Chinese) and cultural prompting strategies to create acceptable alignments with both the United States and China.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u970d\u592b\u65af\u6cf0\u5fb7VSM13\u56fd\u9645\u8c03\u67e5\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u5bf9\u9f50\u6027\uff0c\u53d1\u73b0\u4e3b\u6d41\u6a21\u578b\uff08\u5982DeepSeek-V3\u3001GPT-5\uff09\u66f4\u63a5\u8fd1\u7f8e\u56fd\u6587\u5316\uff0c\u5373\u4f7f\u4f7f\u7528\u6587\u5316\u63d0\u793a\u6216\u6539\u53d8\u63d0\u793a\u8bed\u8a00\u4e5f\u96be\u4ee5\u4e0e\u4e2d\u56fd\u6587\u5316\u5bf9\u9f50\uff0c\u800cGPT-4o\u7b49\u4f4e\u6210\u672c\u6a21\u578b\u80fd\u901a\u8fc7\u8bed\u8a00\u548c\u6587\u5316\u63d0\u793a\u7b56\u7565\u5b9e\u73b0\u53ef\u63a5\u53d7\u7684\u53cc\u6587\u5316\u5bf9\u9f50\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7c7b\u4eba\u6587\u672c\u80fd\u529b\u7684\u63d0\u5347\uff0c\u4eba\u673a\u4ea4\u4e92\u65e5\u76ca\u589e\u591a\uff0c\u8fd9\u4e9b\u7c7b\u4eba\u667a\u80fd\u4f53\u7684\u6587\u5316\u5bf9\u9f50\u6027\u6210\u4e3a\u91cd\u8981\u7814\u7a76\u8bfe\u9898\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30LLMs\u5728\u4e0d\u540c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u7279\u522b\u662f\u4e2d\u7f8e\u6587\u5316\u5dee\u5f02\u3002", "method": "\u4f7f\u7528\u970d\u592b\u65af\u6cf0\u5fb7VSM13\u56fd\u9645\u8c03\u67e5\u95ee\u5377\u4f5c\u4e3a\u6587\u5316\u8bc4\u4f30\u5de5\u5177\uff0c\u7ed3\u5408\u63d0\u793a\u8bed\u8a00\uff08\u82f1\u8bed/\u7b80\u4f53\u4e2d\u6587\uff09\u548c\u6587\u5316\u63d0\u793a\u7b56\u7565\uff08\u901a\u8fc7\u7cfb\u7edf\u63d0\u793a\u5c06\u6a21\u578b\u5bf9\u9f50\u5230\u7279\u5b9a\u56fd\u5bb6\uff09\uff0c\u5bf9\u4e3b\u6d41LLMs\u8fdb\u884c\u6587\u5316\u5bf9\u9f50\u6d4b\u8bd5\u3002", "result": "DeepSeek-V3\u3001V3.1\u548cGPT-5\u4e0e\u7f8e\u56fd\u8c03\u67e5\u7ed3\u679c\u9ad8\u5ea6\u5bf9\u9f50\uff0c\u4f46\u4e0e\u4e2d\u56fd\u6587\u5316\u96be\u4ee5\u5b9e\u73b0\u5f3a\u5bf9\u9f50\u6216\u8f6f\u5bf9\u9f50\uff1bGPT-4\u5728\u82f1\u8bed\u63d0\u793a\u4e0b\u66f4\u63a5\u8fd1\u4e2d\u56fd\u6587\u5316\uff0c\u4f46\u6587\u5316\u63d0\u793a\u53ef\u4f7f\u5176\u8f6c\u5411\u7f8e\u56fd\u5bf9\u9f50\uff1bGPT-4o\u548cGPT-4.1\u7b49\u4f4e\u6210\u672c\u6a21\u578b\u80fd\u901a\u8fc7\u8bed\u8a00\u548c\u6587\u5316\u63d0\u793a\u7b56\u7565\u5b9e\u73b0\u4e2d\u7f8e\u6587\u5316\u7684\u53ef\u63a5\u53d7\u5bf9\u9f50\u3002", "conclusion": "\u5f53\u524d\u4e3b\u6d41LLMs\u5b58\u5728\u660e\u663e\u7684\u6587\u5316\u504f\u5411\u6027\uff08\u504f\u5411\u7f8e\u56fd\u6587\u5316\uff09\uff0c\u4f46\u901a\u8fc7\u9002\u5f53\u7684\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\uff08\u7279\u522b\u662f\u8bed\u8a00\u9009\u62e9\u548c\u6587\u5316\u63d0\u793a\uff09\u53ef\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u8c03\u6574\u6a21\u578b\u7684\u6587\u5316\u5bf9\u9f50\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u6587\u5316\u9002\u5e94\u6027\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b9\u6cd5\u53c2\u8003\u3002"}}
{"id": "2512.09928", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.09928", "abs": "https://arxiv.org/abs/2512.09928", "authors": ["Minghui Lin", "Pengxiang Ding", "Shu Wang", "Zifeng Zhuang", "Yang Liu", "Xinyang Tong", "Wenxuan Song", "Shangke Lyu", "Siteng Huang", "Donglin Wang"], "title": "HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models", "comment": "Project page: https://hifvla.github.io Github: https://github.com/OpenHelix-Team/HiF-VLA", "summary": "Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.", "AI": {"tldr": "HiF-VLA\uff1a\u4e00\u4e2a\u5229\u7528\u8fd0\u52a8\u8fdb\u884c\u53cc\u5411\u65f6\u5e8f\u63a8\u7406\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u540e\u89c1\u3001\u6d1e\u5bdf\u548c\u524d\u77bb\u673a\u5236\u63d0\u5347\u957f\u65f6\u7a0b\u64cd\u4f5c\u6027\u80fd", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5927\u591a\u5047\u8bbe\u9a6c\u5c14\u53ef\u592b\u6027\uff0c\u4ec5\u4f9d\u8d56\u5f53\u524d\u89c2\u6d4b\uff0c\u5b58\u5728\u65f6\u5e8f\u77ed\u89c6\u95ee\u9898\uff0c\u5bfc\u81f4\u957f\u65f6\u7a0b\u64cd\u4f5c\u7684\u4e00\u81f4\u6027\u4e0b\u964d\u3002\u8fd0\u52a8\u4f5c\u4e3a\u66f4\u7d27\u51d1\u3001\u4fe1\u606f\u91cf\u66f4\u5927\u7684\u65f6\u5e8f\u4e0a\u4e0b\u6587\u8868\u793a\uff0c\u80fd\u6355\u6349\u72b6\u6001\u95f4\u53d8\u5316\u5e76\u8fc7\u6ee4\u9759\u6001\u50cf\u7d20\u566a\u58f0\u3002", "method": "\u63d0\u51faHiF-VLA\u7edf\u4e00\u6846\u67b6\uff0c\u5229\u7528\u8fd0\u52a8\u8fdb\u884c\u53cc\u5411\u65f6\u5e8f\u63a8\u7406\uff1a1\uff09\u901a\u8fc7\u540e\u9a8c\u5148\u9a8c\u7f16\u7801\u8fc7\u53bb\u52a8\u6001\uff1b2\uff09\u901a\u8fc7\u524d\u77bb\u63a8\u7406\u9884\u6d4b\u672a\u6765\u8fd0\u52a8\uff1b3\uff09\u901a\u8fc7\u540e\u9a8c\u8c03\u5236\u7684\u8054\u5408\u4e13\u5bb6\u6574\u5408\u4e24\u8005\uff0c\u5b9e\u73b0\"\u8fb9\u601d\u8003\u8fb9\u884c\u52a8\"\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u8303\u5f0f\u3002", "result": "\u5728LIBERO-Long\u548cCALVIN ABC-D\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u4ec5\u5e26\u6765\u53ef\u5ffd\u7565\u7684\u989d\u5916\u63a8\u7406\u5ef6\u8fdf\u3002\u5728\u771f\u5b9e\u4e16\u754c\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u5c55\u793a\u4e86\u5728\u5b9e\u9645\u673a\u5668\u4eba\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u6709\u6548\u6027\u3002", "conclusion": "\u8fd0\u52a8\u4f5c\u4e3a\u65f6\u5e8f\u4e0a\u4e0b\u6587\u8868\u793a\u80fd\u6709\u6548\u63d0\u5347VLA\u6a21\u578b\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u6027\u80fd\uff0cHiF-VLA\u6846\u67b6\u901a\u8fc7\u53cc\u5411\u65f6\u5e8f\u63a8\u7406\u673a\u5236\u5b9e\u73b0\u4e86\"\u8fb9\u601d\u8003\u8fb9\u884c\u52a8\"\u7684\u8303\u5f0f\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.09804", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.09804", "abs": "https://arxiv.org/abs/2512.09804", "authors": ["Jens Albrecht", "Robert Lehmann", "Aleksandra Poltermann", "Eric Rudolph", "Philipp Steigerwald", "Mara Stieler"], "title": "OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations", "comment": "Submitted to LREC 2026", "summary": "This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.", "AI": {"tldr": "OnCoCo 1.0\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u7ebf\u5fc3\u7406\u54a8\u8be2\u7ec6\u7c92\u5ea6\u6d88\u606f\u5206\u7c7b\u7684\u65b0\u516c\u5171\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ea62800\u6761\u6807\u6ce8\u6d88\u606f\uff0c\u533a\u520638\u79cd\u54a8\u8be2\u5e08\u548c28\u79cd\u5ba2\u6237\u8bdd\u8bed\u7c7b\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5fae\u8c03\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u52a8\u673a\u6027\u8bbf\u8c08(MI)\u7684\u5206\u7c7b\u7cfb\u7edf\u8fc7\u4e8e\u72ed\u7a84\u4e14\u4e3b\u8981\u4f9d\u8d56\u9762\u5bf9\u9762\u54a8\u8be2\u6570\u636e\uff0c\u9650\u5236\u4e86\u6587\u672c\u54a8\u8be2\u5bf9\u8bdd\u7684\u8be6\u7ec6\u5206\u6790\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u5206\u7c7b\u65b9\u6848\u6765\u6539\u8fdb\u5728\u7ebf\u5fc3\u7406\u54a8\u8be2\u7684\u81ea\u52a8\u5316\u5206\u6790\u3002", "method": "\u5f00\u53d1\u4e86\u65b0\u7684\u7efc\u5408\u6027\u7f16\u7801\u65b9\u6848\uff0c\u533a\u520638\u79cd\u54a8\u8be2\u5e08\u8bdd\u8bed\u7c7b\u578b\u548c28\u79cd\u5ba2\u6237\u8bdd\u8bed\u7c7b\u578b\uff0c\u521b\u5efa\u4e86\u5305\u542b\u7ea62800\u6761\u6807\u6ce8\u6d88\u606f\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u4e86\u591a\u4e2a\u6a21\u578b\u3002", "result": "\u521b\u5efa\u4e86OnCoCo 1.0\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ec6\u7c92\u5ea6\u6807\u6ce8\u7684\u54a8\u8be2\u5bf9\u8bdd\u6d88\u606f\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u6027\uff0c\u6570\u636e\u548c\u6a21\u578b\u5df2\u516c\u5f00\u53ef\u7528\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u8bed\u8a00\u8d44\u6e90\u793e\u533a\u8d21\u732e\u4e86\u65b0\u578b\u7ec6\u7c92\u5ea6\u5bf9\u8bdd\u8d44\u6e90\uff0c\u6269\u5c55\u4e86\u793e\u4f1a\u548c\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\u5206\u6790\u7684\u73b0\u6709\u6570\u636e\u96c6\uff0c\u4fc3\u8fdb\u4e86\u5728\u7ebf\u5fc3\u7406\u54a8\u8be2\u7684\u81ea\u52a8\u5316\u5206\u6790\u7814\u7a76\u3002"}}
{"id": "2512.09830", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09830", "abs": "https://arxiv.org/abs/2512.09830", "authors": ["Simone Corbo"], "title": "LLMs in Interpreting Legal Documents", "comment": null, "summary": "This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.", "AI": {"tldr": "\u672c\u7ae0\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u5176\u901a\u8fc7\u5206\u6790\u6cd5\u89c4\u89e3\u91ca\u3001\u5408\u540c\u5206\u6790\u3001\u6848\u4f8b\u7814\u7a76\u7b49\u7528\u4f8b\u6765\u4f18\u5316\u548c\u589e\u5f3a\u4f20\u7edf\u6cd5\u5f8b\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u7b97\u6cd5\u5355\u4e00\u6027\u3001\u5e7b\u89c9\u3001\u5408\u89c4\u6027\u7b49\u6311\u6218\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e24\u4e2a\u4e0d\u540c\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u5148\u8fdb\u6280\u672f\u4f18\u5316\u4f20\u7edf\u6cd5\u5f8b\u4efb\u52a1\uff0c\u63d0\u9ad8\u6cd5\u5f8b\u5de5\u4f5c\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u540c\u65f6\u8bc6\u522b\u548c\u5e94\u5bf9\u76f8\u5173\u7684\u6280\u672f\u6311\u6218\u548c\u76d1\u7ba1\u8981\u6c42\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u53ef\u80fd\u7528\u4f8b\uff08\u5982\u6cd5\u89c4\u89e3\u91ca\u3001\u5408\u540c\u5206\u6790\u3001\u6848\u4f8b\u7814\u7a76\u3001\u6cd5\u5f8b\u6458\u8981\u3001\u5408\u540c\u8c08\u5224\u548c\u4fe1\u606f\u68c0\u7d22\uff09\uff0c\u540c\u65f6\u8003\u8651\u7b97\u6cd5\u5355\u4e00\u6027\u3001\u5e7b\u89c9\u7b49\u6311\u6218\uff0c\u5e76\u53c2\u8003\u6b27\u76dfAI\u6cd5\u6848\u3001\u7f8e\u56fd\u5021\u8bae\u548c\u4e2d\u56fd\u65b0\u5174\u65b9\u6cd5\u7b49\u76d1\u7ba1\u6846\u67b6\uff0c\u6700\u540e\u63d0\u51fa\u4e86\u4e24\u4e2a\u4e0d\u540c\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3002", "result": "\u5c55\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u5177\u6709\u4f18\u5316\u548c\u589e\u5f3a\u4f20\u7edf\u6cd5\u5f8b\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u534f\u52a9\u6cd5\u5f8b\u4e13\u4e1a\u4eba\u58eb\u63d0\u9ad8\u5de5\u4f5c\u6548\u7387\uff0c\u4f46\u540c\u65f6\u4e5f\u9762\u4e34\u7b97\u6cd5\u5355\u4e00\u6027\u3001\u5e7b\u89c9\u3001\u5408\u89c4\u6027\u7b49\u6311\u6218\uff0c\u9700\u8981\u5efa\u7acb\u76f8\u5e94\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u524d\u666f\uff0c\u4f46\u9700\u8981\u8c28\u614e\u5e94\u5bf9\u6280\u672f\u6311\u6218\u548c\u76d1\u7ba1\u8981\u6c42\uff0c\u901a\u8fc7\u5efa\u7acb\u9002\u5f53\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u76d1\u7ba1\u6846\u67b6\uff0c\u53ef\u4ee5\u4fc3\u8fdb\u8fd9\u4e9b\u6280\u672f\u5728\u6cd5\u5f8b\u9886\u57df\u7684\u8d1f\u8d23\u4efb\u5e94\u7528\u548c\u53d1\u5c55\u3002"}}
{"id": "2512.09841", "categories": ["cs.CL", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.09841", "abs": "https://arxiv.org/abs/2512.09841", "authors": ["Yijing Chen", "Yihan Wu", "Kaisi Guan", "Yuchen Ren", "Yuyue Wang", "Ruihua Song", "Liyun Ru"], "title": "ChronusOmni: Improving Time Awareness of Omni Large Language Models", "comment": "Code available at https://github.com/YJCX330/Chronus/", "summary": "Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.", "AI": {"tldr": "ChronusOmni\u662f\u4e00\u4e2a\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u589e\u5f3a\u89c6\u542c\u65f6\u5e8f\u611f\u77e5\u80fd\u529b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u65f6\u95f4\u5efa\u6a21\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u663e\u5f0f\u548c\u9690\u5f0f\u65f6\u5e8f\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u89c6\u89c9\u8bed\u8a00\u573a\u666f\uff0c\u5173\u6ce8\u663e\u5f0f\u65f6\u5e8f\u5b9a\u4f4d\u95ee\u9898\uff0c\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528\u97f3\u9891\u6a21\u6001\uff0c\u4e14\u5ffd\u89c6\u4e86\u8de8\u6a21\u6001\u7684\u9690\u5f0f\u65f6\u5e8f\u5173\u7cfb\uff08\u5982\u89c6\u89c9\u5185\u5bb9\u4e0e\u8bed\u97f3\u7684\u5bf9\u5e94\u5173\u7cfb\uff09\uff0c\u800c\u8fd9\u4e9b\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u666e\u904d\u5b58\u5728\u3002", "method": "1. \u5728\u89c6\u89c9\u548c\u97f3\u9891\u8868\u793a\u4e2d\u63d2\u5165\u57fa\u4e8e\u6587\u672c\u7684\u65f6\u95f4\u6233\u6807\u8bb0\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u7684\u7edf\u4e00\u65f6\u5e8f\u5efa\u6a21\uff1b2. \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u914d\u5408\u4e13\u95e8\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u589e\u5f3a\u65f6\u5e8f\u987a\u5e8f\u548c\u7ec6\u7c92\u5ea6\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\uff1b3. \u6784\u5efaChronusAV\u6570\u636e\u96c6\uff0c\u652f\u6301\u89c6\u542c\u65f6\u5e8f\u5b9a\u4f4d\u4efb\u52a1\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "ChronusOmni\u5728ChronusAV\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u8d85\u8fc730%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728\u5176\u4ed6\u65f6\u5e8f\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u7684\u5927\u591a\u6570\u6307\u6807\u4e0a\u83b7\u5f97\u6700\u4f73\u7ed3\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u7684\u89c6\u9891\u548c\u97f3\u9891\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "ChronusOmni\u901a\u8fc7\u7edf\u4e00\u7684\u8de8\u6a21\u6001\u65f6\u5e8f\u5efa\u6a21\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u5e8f\u611f\u77e5\u80fd\u529b\uff0c\u5728\u663e\u5f0f\u548c\u9690\u5f0f\u89c6\u542c\u65f6\u5e8f\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.09854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.09854", "abs": "https://arxiv.org/abs/2512.09854", "authors": ["Muneeb Ur Raheem Khan"], "title": "Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement", "comment": null, "summary": "Large language models (LLMs) increasingly mediate human communication, decision support, content creation, and information retrieval. Despite impressive fluency, these systems frequently produce biased or stereotypical content, especially when prompted with socially sensitive language. A growing body of research has demonstrated that such biases disproportionately affect low-resource languages, where training data is limited and culturally unrepresentative. This paper presents a comprehensive study of inference-time bias mitigation, a strategy that avoids retraining or fine-tuning and instead operates directly on model outputs. Building on preference-ranking models (PRMs), we introduce a unified evaluation framework comparing three methods: (1) baseline single-word generation, (2) PRM-Select best-of-N sampling, and (3) PRM-Sequential refinement guided by PRM critiques. We evaluate these techniques across 200 English prompts and their Urdu counterparts, designed to reflect socio-cultural contexts relevant to gender, ethnicity, religion, nationality, disability, profession, age, and socioeconomic categories. Using GPT-3.5 as a candidate generator and GPT-4o-mini as a PRM-based bias and utility scorer, we provide an extensive quantitative analysis of bias reduction, utility preservation, and cross-lingual disparities. Our findings show: (a) substantial gains over the baseline for both languages; (b) consistently lower fairness scores for Urdu across all methods, highlighting structural inequities in multilingual LLM training; and (c) distinct improvement trajectories between PRM-Select and PRM-Sequential. The study contributes an extensible methodology, interpretable metrics, and cross-lingual comparisons that can support future work on fairness evaluation in low-resource languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u63a8\u7406\u9636\u6bb5\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u504f\u89c1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u504f\u597d\u6392\u5e8f\u6a21\u578b\u6280\u672f\u6bd4\u8f83\u4e86\u4e09\u79cd\u65b9\u6cd5\u5728\u82f1\u8bed\u548c\u4e4c\u5c14\u90fd\u8bed\u4e0a\u7684\u6548\u679c\uff0c\u53d1\u73b0\u4e4c\u5c14\u90fd\u8bed\u7684\u516c\u5e73\u6027\u5f97\u5206\u59cb\u7ec8\u8f83\u4f4e\uff0c\u63ed\u793a\u4e86\u591a\u8bed\u8a00LLM\u8bad\u7ec3\u4e2d\u7684\u7ed3\u6784\u4e0d\u5e73\u7b49\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u6709\u504f\u89c1\u6216\u523b\u677f\u5370\u8c61\u7684\u5185\u5bb9\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u793e\u4f1a\u654f\u611f\u8bdd\u9898\u65f6\u3002\u8fd9\u79cd\u504f\u89c1\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u5f71\u54cd\u66f4\u5927\uff0c\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u6709\u9650\u4e14\u6587\u5316\u4ee3\u8868\u6027\u4e0d\u8db3\u3002\u9700\u8981\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u76f4\u63a5\u5728\u63a8\u7406\u9636\u6bb5\u51cf\u8f7b\u504f\u89c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6bd4\u8f83\u4e09\u79cd\u63a8\u7406\u65f6\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\uff1a1) \u57fa\u7ebf\u5355\u8bcd\u751f\u6210\uff1b2) PRM-Select\u6700\u4f73N\u91c7\u6837\uff1b3) PRM-Sequential\u57fa\u4e8ePRM\u6279\u8bc4\u7684\u5e8f\u5217\u4f18\u5316\u3002\u4f7f\u7528GPT-3.5\u4f5c\u4e3a\u5019\u9009\u751f\u6210\u5668\uff0cGPT-4o-mini\u4f5c\u4e3aPRM\u504f\u89c1\u548c\u6548\u7528\u8bc4\u5206\u5668\uff0c\u5728200\u4e2a\u82f1\u8bed\u63d0\u793a\u53ca\u5176\u4e4c\u5c14\u90fd\u8bed\u5bf9\u5e94\u7248\u672c\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1aa) \u6240\u6709\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u90fd\u6709\u663e\u8457\u63d0\u5347\uff1bb) \u4e4c\u5c14\u90fd\u8bed\u5728\u6240\u6709\u65b9\u6cd5\u4e2d\u7684\u516c\u5e73\u6027\u5f97\u5206\u90fd\u8f83\u4f4e\uff0c\u63ed\u793a\u4e86\u591a\u8bed\u8a00LLM\u8bad\u7ec3\u4e2d\u7684\u7ed3\u6784\u4e0d\u5e73\u7b49\uff1bc) PRM-Select\u548cPRM-Sequential\u65b9\u6cd5\u663e\u793a\u51fa\u4e0d\u540c\u7684\u6539\u8fdb\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u8bba\u3001\u53ef\u89e3\u91ca\u7684\u6307\u6807\u548c\u8de8\u8bed\u8a00\u6bd4\u8f83\uff0c\u652f\u6301\u672a\u6765\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u516c\u5e73\u6027\u8bc4\u4f30\u65b9\u9762\u7684\u5de5\u4f5c\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u9700\u8981\u4e13\u95e8\u5173\u6ce8\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u504f\u89c1\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2512.09910", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.09910", "abs": "https://arxiv.org/abs/2512.09910", "authors": ["Salvador Carri\u00f3n", "Francisco Casacuberta"], "title": "Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach", "comment": null, "summary": "Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528LoRA\uff08\u4f4e\u79e9\u9002\u914d\uff09\u4f5c\u4e3a\u53c2\u6570\u9ad8\u6548\u6846\u67b6\u89e3\u51b3\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u5305\u62ec\u707e\u96be\u6027\u9057\u5fd8\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u3002\u901a\u8fc7LoRA\u5fae\u8c03\u3001\u4ea4\u4e92\u5f0f\u9002\u914d\u65b9\u6cd5\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u6b63\u5219\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u63a5\u8fd1\u5168\u53c2\u6570\u6280\u672f\u3001\u652f\u6301\u5b9e\u65f6\u7528\u6237\u63a7\u5236\u3001\u6709\u6548\u7f13\u89e3\u9057\u5fd8\u7684\u6301\u7eedNMT\u7cfb\u7edf\u3002", "motivation": "\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u707e\u96be\u6027\u9057\u5fd8\uff08\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u5fd8\u8bb0\u65e7\u77e5\u8bc6\uff09\u548c\u91cd\u65b0\u8bad\u7ec3\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u3002\u9700\u8981\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u540c\u65f6\u89e3\u51b3\u8fd9\u4e24\u4e2a\u95ee\u9898\u3002", "method": "1. \u4f7f\u7528LoRA\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u4ec5\u66f4\u65b0\u5c11\u91cf\u53c2\u6570\uff1b2. \u63d0\u51fa\u4ea4\u4e92\u5f0f\u9002\u914d\u65b9\u6cd5\uff0c\u4f7f\u7528\u6821\u51c6\u7684LoRA\u6a21\u5757\u7ebf\u6027\u7ec4\u5408\u4f5c\u4e3a\u65e0\u95e8\u63a7\u7684\u4e13\u5bb6\u6df7\u5408\uff1b3. \u5f15\u5165\u57fa\u4e8e\u68af\u5ea6\u7684\u6b63\u5219\u5316\u7b56\u7565\uff0c\u4e13\u95e8\u9488\u5bf9\u4f4e\u79e9\u5206\u89e3\u77e9\u9635\uff0c\u5229\u7528\u5386\u53f2\u68af\u5ea6\u4fe1\u606f\u52a0\u6743\u60e9\u7f5a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1aLoRA\u5fae\u8c03\u5728\u9002\u5e94\u65b0\u8bed\u8a00\u548c\u9886\u57df\u65f6\u6027\u80fd\u4e0e\u5168\u53c2\u6570\u6280\u672f\u76f8\u5f53\uff0c\u4f46\u4ec5\u4f7f\u7528\u4e00\u5c0f\u90e8\u5206\u53c2\u6570\u7a7a\u95f4\uff1b\u4ea4\u4e92\u5f0f\u9002\u914d\u65b9\u6cd5\u652f\u6301\u5b9e\u65f6\u7528\u6237\u53ef\u63a7\u8c03\u6574\uff1b\u57fa\u4e8e\u68af\u5ea6\u7684\u6b63\u5219\u5316\u7b56\u7565\u80fd\u6709\u6548\u4fdd\u7559\u5148\u524d\u9886\u57df\u77e5\u8bc6\u540c\u65f6\u5b66\u4e60\u65b0\u4efb\u52a1\u3002", "conclusion": "LoRA\u4e3a\u4ea4\u4e92\u5f0f\u548c\u6301\u7eedNMT\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3001\u5b9e\u65f6\u4ea4\u4e92\u63a7\u5236\u548c\u4e13\u95e8\u7684\u6b63\u5219\u5316\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u7684\u53cc\u91cd\u6311\u6218\u3002"}}
