{"id": "2509.09769", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09769", "abs": "https://arxiv.org/abs/2509.09769", "authors": ["Rutav Shah", "Shuijing Liu", "Qi Wang", "Zhenyu Jiang", "Sateesh Kumar", "Mingyo Seo", "Roberto Mart\u00edn-Mart\u00edn", "Yuke Zhu"], "title": "MimicDroid: In-Context Learning for Humanoid Robot Manipulation from Human Play Videos", "comment": "11 pages, 9 figures, 5 tables", "summary": "We aim to enable humanoid robots to efficiently solve new manipulation tasks\nfrom a few video examples. In-context learning (ICL) is a promising framework\nfor achieving this goal due to its test-time data efficiency and rapid\nadaptability. However, current ICL methods rely on labor-intensive teleoperated\ndata for training, which restricts scalability. We propose using human play\nvideos -- continuous, unlabeled videos of people interacting freely with their\nenvironment -- as a scalable and diverse training data source. We introduce\nMimicDroid, which enables humanoids to perform ICL using human play videos as\nthe only training data. MimicDroid extracts trajectory pairs with similar\nmanipulation behaviors and trains the policy to predict the actions of one\ntrajectory conditioned on the other. Through this process, the model acquired\nICL capabilities for adapting to novel objects and environments at test time.\nTo bridge the embodiment gap, MimicDroid first retargets human wrist poses\nestimated from RGB videos to the humanoid, leveraging kinematic similarity. It\nalso applies random patch masking during training to reduce overfitting to\nhuman-specific cues and improve robustness to visual differences. To evaluate\nfew-shot learning for humanoids, we introduce an open-source simulation\nbenchmark with increasing levels of generalization difficulty. MimicDroid\noutperformed state-of-the-art methods and achieved nearly twofold higher\nsuccess rates in the real world. Additional materials can be found on:\nut-austin-rpl.github.io/MimicDroid", "AI": {"tldr": "MimicDroid\u662f\u4e00\u4e2a\u4f7f\u7528\u4eba\u7c7b\u6e38\u620f\u89c6\u9891\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u7684\u4eba\u5f62\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u5c11\u6837\u672c\u9002\u5e94\u65b0\u4efb\u52a1\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u6027\u80fd", "motivation": "\u89e3\u51b3\u5f53\u524d\u4e0a\u4e0b\u6587\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u9065\u64cd\u4f5c\u6570\u636e\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5229\u7528\u4eba\u7c7b\u6e38\u620f\u89c6\u9891\u4f5c\u4e3a\u66f4\u4e30\u5bcc\u3001\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u6570\u636e\u6e90", "method": "\u4ece\u4eba\u7c7b\u6e38\u620f\u89c6\u9891\u4e2d\u63d0\u53d6\u76f8\u4f3c\u64cd\u4f5c\u884c\u4e3a\u7684\u8f68\u8ff9\u5bf9\uff0c\u8bad\u7ec3\u7b56\u7565\u9884\u6d4b\u4e00\u4e2a\u8f68\u8ff9\u7684\u52a8\u4f5c\u6761\u4ef6\u4e8e\u53e6\u4e00\u4e2a\u8f68\u8ff9\uff1b\u901a\u8fc7\u4eba\u4f53\u59ff\u6001\u91cd\u5b9a\u5411\u548c\u968f\u673a\u8865\u4e01\u63a9\u7801\u6765\u5f25\u5408\u4eba\u673a\u5dee\u5f02", "result": "\u5728\u5f00\u6e90\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5b9e\u73b0\u4e86\u8fd1\u4e24\u500d\u7684\u6210\u529f\u7387\u63d0\u5347", "conclusion": "\u4eba\u7c7b\u6e38\u620f\u89c6\u9891\u662f\u8bad\u7ec3\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7684\u6709\u6548\u6570\u636e\u6e90\uff0cMimicDroid\u5c55\u793a\u4e86\u5728\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u5f3a\u5927\u9002\u5e94\u80fd\u529b"}}
{"id": "2509.09805", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09805", "abs": "https://arxiv.org/abs/2509.09805", "authors": ["Francisco M. L\u00f3pez", "Miles Lenz", "Marco G. Fedozzi", "Arthur Aubret", "Jochen Triesch"], "title": "MIMo grows! Simulating body and sensory development in a multimodal infant model", "comment": "Accepted at IEEE ICDL 2025. 6 pages, 6 figures", "summary": "Infancy is characterized by rapid body growth and an explosive change of\nsensory and motor abilities. However, developmental robots and simulation\nplatforms are typically designed in the image of a specific age, which limits\ntheir ability to capture the changing abilities and constraints of developing\ninfants. To address this issue, we present MIMo v2, a new version of the\nmultimodal infant model. It includes a growing body with increasing actuation\nstrength covering the age range from birth to 24 months. It also features\nfoveated vision with developing visual acuity as well as sensorimotor delays\nmodeling finite signal transmission speeds to and from an infant's brain.\nFurther enhancements of this MIMo version include an inverse kinematics module,\na random environment generator and updated compatiblity with third-party\nsimulation and learning libraries. Overall, this new MIMo version permits\nincreased realism when modeling various aspects of sensorimotor development.\nThe code is available on the official repository\n(https://github.com/trieschlab/MIMo).", "AI": {"tldr": "MIMo v2\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5a74\u513f\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u62df\u4ece\u51fa\u751f\u523024\u4e2a\u6708\u7684\u8eab\u4f53\u751f\u957f\u3001\u8fd0\u52a8\u80fd\u529b\u53d1\u5c55\u3001\u89c6\u89c9\u53d1\u80b2\u548c\u611f\u89c9\u8fd0\u52a8\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u4e86\u53d1\u80b2\u673a\u5668\u4eba\u5efa\u6a21\u7684\u771f\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u53d1\u80b2\u673a\u5668\u4eba\u548c\u4eff\u771f\u5e73\u53f0\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u5e74\u9f84\u6bb5\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u6355\u6349\u5a74\u513f\u53d1\u80b2\u8fc7\u7a0b\u4e2d\u4e0d\u65ad\u53d8\u5316\u7684\u80fd\u529b\u548c\u7ea6\u675f\u6761\u4ef6\u3002", "method": "\u5f00\u53d1\u4e86MIMo v2\u6a21\u578b\uff0c\u5305\u542b\uff1a1\uff09\u4ece\u51fa\u751f\u523024\u4e2a\u6708\u7684\u8eab\u4f53\u751f\u957f\u6a21\u578b\u548c\u8fd0\u52a8\u5f3a\u5ea6\u589e\u52a0\uff1b2\uff09\u5177\u6709\u53d1\u5c55\u6027\u89c6\u89c9\u654f\u9510\u5ea6\u7684\u4e2d\u592e\u51f9\u89c6\u89c9\uff1b3\uff09\u6a21\u62df\u4fe1\u53f7\u4f20\u8f93\u5ef6\u8fdf\u7684\u611f\u89c9\u8fd0\u52a8\u5ef6\u8fdf\uff1b4\uff09\u9006\u5411\u8fd0\u52a8\u5b66\u6a21\u5757\u548c\u968f\u673a\u73af\u5883\u751f\u6210\u5668\u3002", "result": "\u65b0\u7248\u672c\u7684MIMo\u80fd\u591f\u66f4\u771f\u5b9e\u5730\u6a21\u62df\u5404\u79cd\u611f\u89c9\u8fd0\u52a8\u53d1\u80b2\u65b9\u9762\uff0c\u5e76\u4e0e\u7b2c\u4e09\u65b9\u4eff\u771f\u548c\u5b66\u4e60\u5e93\u4fdd\u6301\u517c\u5bb9\u3002", "conclusion": "MIMo v2\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u5168\u9762\u7684\u5a74\u513f\u53d1\u80b2\u6a21\u578b\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u7814\u7a76\u5a74\u513f\u611f\u89c9\u8fd0\u52a8\u53d1\u80b2\u8fc7\u7a0b\uff0c\u4ee3\u7801\u5df2\u5728\u5b98\u65b9\u4ed3\u5e93\u5f00\u6e90\u3002"}}
{"id": "2509.09889", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.09889", "abs": "https://arxiv.org/abs/2509.09889", "authors": ["Giulia Botta", "Marco Botta", "Cristina Gena", "Alessandro Mazzei", "Massimo Donini", "Alberto Lillo"], "title": "Using the Pepper Robot to Support Sign Language Communication", "comment": "paper presented at ICSR2025", "summary": "Social robots are increasingly experimented in public and assistive settings,\nbut their accessibility for Deaf users remains quite underexplored. Italian\nSign Language (LIS) is a fully-fledged natural language that relies on complex\nmanual and non-manual components. Enabling robots to communicate using LIS\ncould foster more inclusive human robot interaction, especially in social\nenvironments such as hospitals, airports, or educational settings. This study\ninvestigates whether a commercial social robot, Pepper, can produce\nintelligible LIS signs and short signed LIS sentences. With the help of a Deaf\nstudent and his interpreter, an expert in LIS, we co-designed and implemented\n52 LIS signs on Pepper using either manual animation techniques or a MATLAB\nbased inverse kinematics solver. We conducted a exploratory user study\ninvolving 12 participants proficient in LIS, both Deaf and hearing.\nParticipants completed a questionnaire featuring 15 single-choice video-based\nsign recognition tasks and 2 open-ended questions on short signed sentences.\nResults shows that the majority of isolated signs were recognized correctly,\nalthough full sentence recognition was significantly lower due to Pepper's\nlimited articulation and temporal constraints. Our findings demonstrate that\neven commercially available social robots like Pepper can perform a subset of\nLIS signs intelligibly, offering some opportunities for a more inclusive\ninteraction design. Future developments should address multi-modal enhancements\n(e.g., screen-based support or expressive avatars) and involve Deaf users in\nparticipatory design to refine robot expressivity and usability.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u5546\u4e1a\u793e\u4ea4\u673a\u5668\u4ebaPepper\u80fd\u5426\u4ea7\u751f\u53ef\u7406\u89e3\u7684\u610f\u5927\u5229\u624b\u8bed(LIS)\u7b26\u53f7\u548c\u77ed\u53e5\uff0c\u901a\u8fc7\u7528\u6237\u7814\u7a76\u53d1\u73b0\u5927\u591a\u6570\u5b64\u7acb\u7b26\u53f7\u80fd\u88ab\u6b63\u786e\u8bc6\u522b\uff0c\u4f46\u5b8c\u6574\u53e5\u5b50\u8bc6\u522b\u7387\u8f83\u4f4e\u3002", "motivation": "\u793e\u4ea4\u673a\u5668\u4eba\u5728\u516c\u5171\u548c\u8f85\u52a9\u73af\u5883\u4e2d\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u9488\u5bf9\u804b\u4eba\u7528\u6237\u7684\u53ef\u8bbf\u95ee\u6027\u7814\u7a76\u4e0d\u8db3\u3002\u610f\u5927\u5229\u624b\u8bed(LIS)\u4f5c\u4e3a\u6210\u719f\u81ea\u7136\u8bed\u8a00\uff0c\u5177\u6709\u590d\u6742\u7684\u624b\u52bf\u548c\u975e\u624b\u52bf\u6210\u5206\uff0c\u8ba9\u673a\u5668\u4eba\u4f7f\u7528LIS\u4ea4\u6d41\u53ef\u4ee5\u4fc3\u8fdb\u66f4\u5305\u5bb9\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u4e0e\u804b\u4eba\u5b66\u751f\u548c\u624b\u8bed\u4e13\u5bb6\u5408\u4f5c\uff0c\u901a\u8fc7\u624b\u52a8\u52a8\u753b\u6280\u672f\u6216MATLAB\u9006\u5411\u8fd0\u52a8\u5b66\u6c42\u89e3\u5668\uff0c\u5728Pepper\u673a\u5668\u4eba\u4e0a\u5171\u540c\u8bbe\u8ba1\u5b9e\u73b0\u4e8652\u4e2aLIS\u7b26\u53f7\u3002\u5bf912\u540d\u7cbe\u901aLIS\u7684\u53c2\u4e0e\u8005(\u5305\u62ec\u804b\u4eba\u548c\u542c\u529b\u6b63\u5e38\u8005)\u8fdb\u884c\u63a2\u7d22\u6027\u7528\u6237\u7814\u7a76\uff0c\u5305\u542b15\u4e2a\u5355\u9879\u9009\u62e9\u89c6\u9891\u7b26\u53f7\u8bc6\u522b\u4efb\u52a1\u548c2\u4e2a\u5f00\u653e\u5f0f\u77ed\u53e5\u95ee\u9898\u3002", "result": "\u5927\u591a\u6570\u5b64\u7acb\u7b26\u53f7\u80fd\u88ab\u6b63\u786e\u8bc6\u522b\uff0c\u4f46\u7531\u4e8ePepper\u7684\u6709\u9650\u5173\u8282\u6d3b\u52a8\u80fd\u529b\u548c\u65f6\u95f4\u9650\u5236\uff0c\u5b8c\u6574\u53e5\u5b50\u8bc6\u522b\u7387\u663e\u8457\u8f83\u4f4e\u3002\u7814\u7a76\u8868\u660e\u5373\u4f7f\u662f\u5546\u4e1a\u793e\u4ea4\u673a\u5668\u4eba\u4e5f\u80fd\u667a\u80fd\u5730\u6267\u884cLIS\u7b26\u53f7\u5b50\u96c6\u3002", "conclusion": "\u5546\u4e1a\u793e\u4ea4\u673a\u5668\u4eba\u5982Pepper\u80fd\u591f\u6267\u884c\u53ef\u7406\u89e3\u7684LIS\u7b26\u53f7\u5b50\u96c6\uff0c\u4e3a\u66f4\u5305\u5bb9\u7684\u4ea4\u4e92\u8bbe\u8ba1\u63d0\u4f9b\u673a\u4f1a\u3002\u672a\u6765\u5f00\u53d1\u5e94\u89e3\u51b3\u591a\u6a21\u6001\u589e\u5f3a(\u5982\u57fa\u4e8e\u5c4f\u5e55\u7684\u652f\u6301\u6216\u8868\u60c5\u4e30\u5bcc\u7684\u865a\u62df\u5f62\u8c61)\uff0c\u5e76\u901a\u8fc7\u53c2\u4e0e\u5f0f\u8bbe\u8ba1\u8ba9\u804b\u4eba\u7528\u6237\u53c2\u4e0e\u4ee5\u6539\u8fdb\u673a\u5668\u4eba\u8868\u73b0\u529b\u548c\u53ef\u7528\u6027\u3002"}}
{"id": "2509.09893", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09893", "abs": "https://arxiv.org/abs/2509.09893", "authors": ["Hanbit Oh", "Masaki Murooka", "Tomohiro Motoda", "Ryoichi Nakajo", "Yukiyasu Domae"], "title": "Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision", "comment": "Under review", "summary": "Imitation learning is a promising paradigm for training robot agents;\nhowever, standard approaches typically require substantial data acquisition --\nvia numerous demonstrations or random exploration -- to ensure reliable\nperformance. Although exploration reduces human effort, it lacks safety\nguarantees and often results in frequent collisions -- particularly in\nclearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual\nenvironmental resets and imposing additional human burden. This study proposes\nSelf-Augmented Robot Trajectory (SART), a framework that enables policy\nlearning from a single human demonstration, while safely expanding the dataset\nthrough autonomous augmentation. SART consists of two stages: (1) human\nteaching only once, where a single demonstration is provided and precision\nboundaries -- represented as spheres around key waypoints -- are annotated,\nfollowed by one environment reset; (2) robot self-augmentation, where the robot\ngenerates diverse, collision-free trajectories within these boundaries and\nreconnects to the original demonstration. This design improves the data\ncollection efficiency by minimizing human effort while ensuring safety.\nExtensive evaluations in simulation and real-world manipulation tasks show that\nSART achieves substantially higher success rates than policies trained solely\non human-collected demonstrations. Video results available at\nhttps://sites.google.com/view/sart-il .", "AI": {"tldr": "SART\u6846\u67b6\u901a\u8fc7\u5355\u6b21\u4eba\u7c7b\u6f14\u793a\u548c\u81ea\u4e3b\u8f68\u8ff9\u589e\u5f3a\uff0c\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u524d\u63d0\u4e0b\u9ad8\u6548\u5b66\u4e60\u673a\u5668\u4eba\u7b56\u7565\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u548c\u6570\u636e\u9700\u6c42", "motivation": "\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u9700\u8981\u5927\u91cf\u6f14\u793a\u6216\u968f\u673a\u63a2\u7d22\uff0c\u5b58\u5728\u5b89\u5168\u98ce\u9669\u4e14\u9700\u8981\u9891\u7e41\u4eba\u5de5\u91cd\u7f6e\u73af\u5883\uff0c\u7279\u522b\u662f\u5728\u7a7a\u95f4\u53d7\u9650\u4efb\u52a1\u4e2d\u5bb9\u6613\u53d1\u751f\u78b0\u649e", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u5355\u6b21\u4eba\u7c7b\u6f14\u793a\u5e76\u6807\u6ce8\u5173\u952e\u8def\u5f84\u70b9\u7684\u7cbe\u5ea6\u8fb9\u754c\uff1b2\uff09\u673a\u5668\u4eba\u5728\u8fb9\u754c\u5185\u81ea\u4e3b\u751f\u6210\u591a\u6837\u5316\u3001\u65e0\u78b0\u649e\u8f68\u8ff9\u5e76\u4e0e\u539f\u59cb\u6f14\u793a\u8fde\u63a5", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cSART\u76f8\u6bd4\u4ec5\u4f7f\u7528\u4eba\u7c7b\u6f14\u793a\u8bad\u7ec3\u7684\u7b56\u7565\u83b7\u5f97\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u6210\u529f\u7387", "conclusion": "SART\u901a\u8fc7\u6700\u5c0f\u5316\u4eba\u5de5\u5e72\u9884\u540c\u65f6\u786e\u4fdd\u5b89\u5168\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u6570\u636e\u6536\u96c6\u6548\u7387\uff0c\u4e3a\u5355\u6b21\u6f14\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.09953", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09953", "abs": "https://arxiv.org/abs/2509.09953", "authors": ["Mahfuzul I. Nissan", "Sharmin Aktar"], "title": "Detection of Anomalous Behavior in Robot Systems Based on Machine Learning", "comment": null, "summary": "Ensuring the safe and reliable operation of robotic systems is paramount to\nprevent potential disasters and safeguard human well-being. Despite rigorous\ndesign and engineering practices, these systems can still experience\nmalfunctions, leading to safety risks. In this study, we present a machine\nlearning-based approach for detecting anomalies in system logs to enhance the\nsafety and reliability of robotic systems. We collected logs from two distinct\nscenarios using CoppeliaSim and comparatively evaluated several machine\nlearning models, including Logistic Regression (LR), Support Vector Machine\n(SVM), and an Autoencoder. Our system was evaluated in a quadcopter context\n(Context 1) and a Pioneer robot context (Context 2). Results showed that while\nLR demonstrated superior performance in Context 1, the Autoencoder model proved\nto be the most effective in Context 2. This highlights that the optimal model\nchoice is context-dependent, likely due to the varying complexity of anomalies\nacross different robotic platforms. This research underscores the value of a\ncomparative approach and demonstrates the particular strengths of autoencoders\nfor detecting complex anomalies in robotic systems.", "AI": {"tldr": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u903b\u8f91\u56de\u5f52\u3001\u652f\u6301\u5411\u91cf\u673a\u548c\u81ea\u7f16\u7801\u5668\u5728\u4e0d\u540c\u673a\u5668\u4eba\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0", "motivation": "\u786e\u4fdd\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b89\u5168\u53ef\u9760\u8fd0\u884c\u81f3\u5173\u91cd\u8981\uff0c\u5c3d\u7ba1\u6709\u4e25\u683c\u7684\u8bbe\u8ba1\u548c\u5de5\u7a0b\u5b9e\u8df5\uff0c\u7cfb\u7edf\u4ecd\u53ef\u80fd\u51fa\u73b0\u6545\u969c\u5bfc\u81f4\u5b89\u5168\u98ce\u9669", "method": "\u4f7f\u7528CoppeliaSim\u6536\u96c6\u4e24\u79cd\u4e0d\u540c\u573a\u666f\u7684\u65e5\u5fd7\u6570\u636e\uff0c\u6bd4\u8f83\u8bc4\u4f30\u903b\u8f91\u56de\u5f52\u3001\u652f\u6301\u5411\u91cf\u673a\u548c\u81ea\u7f16\u7801\u5668\u7b49\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b", "result": "\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u573a\u666f\u4e2d\u903b\u8f91\u56de\u5f52\u8868\u73b0\u6700\u4f73\uff0c\u5728Pioneer\u673a\u5668\u4eba\u573a\u666f\u4e2d\u81ea\u7f16\u7801\u5668\u6548\u679c\u6700\u597d\uff0c\u8868\u660e\u6700\u4f18\u6a21\u578b\u9009\u62e9\u53d6\u51b3\u4e8e\u5177\u4f53\u5e94\u7528\u573a\u666f", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u6bd4\u8f83\u65b9\u6cd5\u7684\u4ef7\u503c\uff0c\u5e76\u5c55\u793a\u4e86\u81ea\u7f16\u7801\u5668\u5728\u68c0\u6d4b\u590d\u6742\u673a\u5668\u4eba\u7cfb\u7edf\u5f02\u5e38\u65b9\u9762\u7684\u7279\u6b8a\u4f18\u52bf\uff0c\u6a21\u578b\u9009\u62e9\u9700\u8981\u6839\u636e\u5177\u4f53\u5e94\u7528\u573a\u666f\u800c\u5b9a"}}
{"id": "2509.10007", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10007", "abs": "https://arxiv.org/abs/2509.10007", "authors": ["Samuli Soutukorva", "Markku Suomalainen", "Martin Kollingbaum", "Tapio Heikkil\u00e4"], "title": "Gaussian path model library for intuitive robot motion programming by demonstration", "comment": null, "summary": "This paper presents a system for generating Gaussian path models from\nteaching data representing the path shape. In addition, methods for using these\npath models to classify human demonstrations of paths are introduced. By\ngenerating a library of multiple Gaussian path models of various shapes, human\ndemonstrations can be used for intuitive robot motion programming. A method for\nmodifying existing Gaussian path models by demonstration through geometric\nanalysis is also presented.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af\u8def\u5f84\u6a21\u578b\u7684\u7cfb\u7edf\uff0c\u4ece\u6559\u5b66\u6570\u636e\u751f\u6210\u8def\u5f84\u5f62\u72b6\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u7c7b\u4eba\u7c7b\u6f14\u793a\u8def\u5f84\u5e76\u5b9e\u73b0\u76f4\u89c2\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u7f16\u7a0b", "motivation": "\u901a\u8fc7\u5efa\u7acb\u591a\u79cd\u5f62\u72b6\u7684\u9ad8\u65af\u8def\u5f84\u6a21\u578b\u5e93\uff0c\u4f7f\u4eba\u7c7b\u6f14\u793a\u80fd\u591f\u7528\u4e8e\u76f4\u89c2\u7684\u673a\u5668\u4eba\u8fd0\u52a8\u7f16\u7a0b\uff0c\u63d0\u9ad8\u7f16\u7a0b\u6548\u7387\u548c\u81ea\u7136\u6027", "method": "\u4ece\u6559\u5b66\u6570\u636e\u751f\u6210\u9ad8\u65af\u8def\u5f84\u6a21\u578b\uff0c\u5efa\u7acb\u591a\u5f62\u72b6\u6a21\u578b\u5e93\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u5b9e\u73b0\u73b0\u6709\u6a21\u578b\u7684\u6f14\u793a\u4fee\u6539", "result": "\u5f00\u53d1\u4e86\u5b8c\u6574\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u751f\u6210\u3001\u5206\u7c7b\u548c\u4fee\u6539\u9ad8\u65af\u8def\u5f84\u6a21\u578b\uff0c\u652f\u6301\u4eba\u7c7b\u6f14\u793a\u5230\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u8f6c\u6362", "conclusion": "\u9ad8\u65af\u8def\u5f84\u6a21\u578b\u7cfb\u7edf\u4e3a\u57fa\u4e8e\u6f14\u793a\u7684\u673a\u5668\u4eba\u7f16\u7a0b\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u5e93\u548c\u51e0\u4f55\u4fee\u6539\u5b9e\u73b0\u4e86\u76f4\u89c2\u7684\u8fd0\u52a8\u7f16\u7a0b"}}
{"id": "2509.10012", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10012", "abs": "https://arxiv.org/abs/2509.10012", "authors": ["Richard Matthias Hartisch", "Alexander Rother", "J\u00f6rg Kr\u00fcger", "Kevin Haninger"], "title": "Towards simulation-based optimization of compliant fingers for high-speed connector assembly", "comment": null, "summary": "Mechanical compliance is a key design parameter for dynamic contact-rich\nmanipulation, affecting task success and safety robustness over contact\ngeometry variation. Design of soft robotic structures, such as compliant\nfingers, requires choosing design parameters which affect geometry and\nstiffness, and therefore manipulation performance and robustness. Today, these\nparameters are chosen through either hardware iteration, which takes\nsignificant development time, or simplified models (e.g. planar), which can't\naddress complex manipulation task objectives. Improvements in dynamic\nsimulation, especially with contact and friction modeling, present a potential\ndesign tool for mechanical compliance. We propose a simulation-based design\ntool for compliant mechanisms which allows design with respect to task-level\nobjectives, such as success rate. This is applied to optimize design parameters\nof a structured compliant finger to reduce failure cases inside a tolerance\nwindow in insertion tasks. The improvement in robustness is then validated on a\nreal robot using tasks from the benchmark NIST task board. The finger stiffness\naffects the tolerance window: optimized parameters can increase tolerable\nranges by a factor of 2.29, with workpiece variation up to 8.6 mm being\ncompensated. However, the trends remain task-specific. In some tasks, the\nhighest stiffness yields the widest tolerable range, whereas in others the\nopposite is observed, motivating need for design tools which can consider\napplication-specific geometry and dynamics.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4eff\u771f\u7684\u67d4\u987a\u673a\u6784\u8bbe\u8ba1\u5de5\u5177\uff0c\u901a\u8fc7\u4f18\u5316\u67d4\u987a\u624b\u6307\u53c2\u6570\u6765\u63d0\u9ad8\u63d2\u5165\u4efb\u52a1\u4e2d\u7684\u5bb9\u9519\u7a97\u53e3\u548c\u9c81\u68d2\u6027", "motivation": "\u673a\u68b0\u67d4\u987a\u6027\u662f\u52a8\u6001\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u7684\u5173\u952e\u8bbe\u8ba1\u53c2\u6570\uff0c\u4f20\u7edf\u8bbe\u8ba1\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u786c\u4ef6\u8fed\u4ee3\u8017\u65f6\uff0c\u8981\u4e48\u4f7f\u7528\u7b80\u5316\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u76ee\u6807", "method": "\u5f00\u53d1\u57fa\u4e8e\u4eff\u771f\u7684\u8bbe\u8ba1\u5de5\u5177\uff0c\u8003\u8651\u63a5\u89e6\u548c\u6469\u64e6\u5efa\u6a21\uff0c\u4f18\u5316\u7ed3\u6784\u5316\u67d4\u987a\u624b\u6307\u7684\u8bbe\u8ba1\u53c2\u6570\u4ee5\u51cf\u5c11\u63d2\u5165\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u60c5\u51b5", "result": "\u4f18\u5316\u53c2\u6570\u53ef\u5c06\u5bb9\u9519\u8303\u56f4\u63d0\u9ad82.29\u500d\uff0c\u80fd\u591f\u8865\u507f\u9ad8\u8fbe8.6mm\u7684\u5de5\u4f5c\u4ef6\u53d8\u5316\uff0c\u4f46\u8d8b\u52bf\u5177\u6709\u4efb\u52a1\u7279\u5f02\u6027", "conclusion": "\u67d4\u987a\u673a\u6784\u8bbe\u8ba1\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\u8003\u8651\u51e0\u4f55\u548c\u52a8\u529b\u5b66\u7279\u6027\uff0c\u4eff\u771f\u5de5\u5177\u80fd\u591f\u6709\u6548\u652f\u6301\u8fd9\u79cd\u5e94\u7528\u7279\u5b9a\u7684\u8bbe\u8ba1\u4f18\u5316"}}
{"id": "2509.10032", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10032", "abs": "https://arxiv.org/abs/2509.10032", "authors": ["Marawan Khalil", "Fabian Arzberger", "Andreas N\u00fcchter"], "title": "Design and Evaluation of Two Spherical Systems for Mobile 3D Mapping", "comment": "6 Pages, 9 figures, International Workshop 3D-AdViCE in conjunction\n  with 12th ECMR 2025", "summary": "Spherical robots offer unique advantages for mapping applications in\nhazardous or confined environments, thanks to their protective shells and\nomnidirectional mobility. This work presents two complementary spherical\nmapping systems: a lightweight, non-actuated design and an actuated variant\nfeaturing internal pendulum-driven locomotion. Both systems are equipped with a\nLivox Mid-360 solid-state LiDAR sensor and run LiDAR-Inertial Odometry (LIO)\nalgorithms on resource-constrained hardware. We assess the mapping accuracy of\nthese systems by comparing the resulting 3D point-clouds from the LIO\nalgorithms to a ground truth map. The results indicate that the performance of\nstate-of-the-art LIO algorithms deteriorates due to the high dynamic movement\nintroduced by the spherical locomotion, leading to globally inconsistent maps\nand sometimes unrecoverable drift.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u7403\u5f62\u6d4b\u7ed8\u7cfb\u7edf\uff08\u8f7b\u91cf\u975e\u9a71\u52a8\u578b\u548c\u9a71\u52a8\u578b\uff09\uff0c\u8bc4\u4f30\u4e86\u5728\u7403\u5f62\u673a\u5668\u4eba\u9ad8\u52a8\u6001\u8fd0\u52a8\u4e0bLIO\u7b97\u6cd5\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898", "motivation": "\u7403\u5f62\u673a\u5668\u4eba\u5728\u5371\u9669\u6216\u53d7\u9650\u73af\u5883\u4e2d\u5177\u6709\u72ec\u7279\u4f18\u52bf\uff0c\u4f46\u5176\u9ad8\u52a8\u6001\u8fd0\u52a8\u53ef\u80fd\u5f71\u54cd\u6d4b\u7ed8\u7cbe\u5ea6\uff0c\u9700\u8981\u8bc4\u4f30\u73b0\u6709LIO\u7b97\u6cd5\u5728\u8fd9\u79cd\u7279\u6b8a\u8fd0\u52a8\u6a21\u5f0f\u4e0b\u7684\u8868\u73b0", "method": "\u5f00\u53d1\u4e24\u79cd\u7403\u5f62\u6d4b\u7ed8\u7cfb\u7edf\uff08\u975e\u9a71\u52a8\u548c\u9a71\u52a8\u578b\uff09\uff0c\u914d\u5907Livox Mid-360\u56fa\u6001LiDAR\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u786c\u4ef6\u4e0a\u8fd0\u884cLIO\u7b97\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u751f\u6210\u70b9\u4e91\u4e0e\u5730\u9762\u771f\u5b9e\u5730\u56fe\u6765\u8bc4\u4f30\u7cbe\u5ea6", "result": "\u6700\u5148\u8fdb\u7684LIO\u7b97\u6cd5\u6027\u80fd\u56e0\u7403\u5f62\u8fd0\u52a8\u5f15\u5165\u7684\u9ad8\u52a8\u6001\u6027\u800c\u6076\u5316\uff0c\u5bfc\u81f4\u5168\u5c40\u4e0d\u4e00\u81f4\u7684\u5730\u56fe\u548c\u6709\u65f6\u4e0d\u53ef\u6062\u590d\u7684\u6f02\u79fb", "conclusion": "\u7403\u5f62\u673a\u5668\u4eba\u7684\u7279\u6b8a\u8fd0\u52a8\u6a21\u5f0f\u5bf9\u73b0\u6709LIO\u7b97\u6cd5\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9002\u5408\u9ad8\u52a8\u6001\u7403\u5f62\u8fd0\u52a8\u7684\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u7b97\u6cd5"}}
{"id": "2509.10063", "categories": ["cs.RO", "cs.AI", "I.2.9"], "pdf": "https://arxiv.org/pdf/2509.10063", "abs": "https://arxiv.org/abs/2509.10063", "authors": ["Xiyan Huang", "Zhe Xu", "Chenxi Xiao"], "title": "TwinTac: A Wide-Range, Highly Sensitive Tactile Sensor with Real-to-Sim Digital Twin Sensor Model", "comment": "7 pages, 9 figures, 1 table, to be published in IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "Robot skill acquisition processes driven by reinforcement learning often rely\non simulations to efficiently generate large-scale interaction data. However,\nthe absence of simulation models for tactile sensors has hindered the use of\ntactile sensing in such skill learning processes, limiting the development of\neffective policies driven by tactile perception. To bridge this gap, we present\nTwinTac, a system that combines the design of a physical tactile sensor with\nits digital twin model. Our hardware sensor is designed for high sensitivity\nand a wide measurement range, enabling high quality sensing data essential for\nobject interaction tasks. Building upon the hardware sensor, we develop the\ndigital twin model using a real-to-sim approach. This involves collecting\nsynchronized cross-domain data, including finite element method results and the\nphysical sensor's outputs, and then training neural networks to map simulated\ndata to real sensor responses. Through experimental evaluation, we\ncharacterized the sensitivity of the physical sensor and demonstrated the\nconsistency of the digital twin in replicating the physical sensor's output.\nFurthermore, by conducting an object classification task, we showed that\nsimulation data generated by our digital twin sensor can effectively augment\nreal-world data, leading to improved accuracy. These results highlight\nTwinTac's potential to bridge the gap in cross-domain learning tasks.", "AI": {"tldr": "TwinTac\u7cfb\u7edf\u7ed3\u5408\u7269\u7406\u89e6\u89c9\u4f20\u611f\u5668\u53ca\u5176\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u89e6\u89c9\u611f\u77e5\u5728\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u4e2d\u7684\u4eff\u771f\u7f3a\u5931\u95ee\u9898\uff0c\u901a\u8fc7\u771f\u5b9e\u5230\u4eff\u771f\u7684\u65b9\u6cd5\u5b9e\u73b0\u8de8\u57df\u6570\u636e\u4e00\u81f4\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u901a\u5e38\u4f9d\u8d56\u4eff\u771f\u751f\u6210\u4ea4\u4e92\u6570\u636e\uff0c\u4f46\u89e6\u89c9\u4f20\u611f\u5668\u7684\u4eff\u771f\u6a21\u578b\u7f3a\u5931\u963b\u788d\u4e86\u89e6\u89c9\u611f\u77e5\u5728\u7b56\u7565\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\uff0c\u9650\u5236\u4e86\u89e6\u89c9\u9a71\u52a8\u7b56\u7565\u7684\u53d1\u5c55\u3002", "method": "\u8bbe\u8ba1\u9ad8\u7075\u654f\u5ea6\u7269\u7406\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u91c7\u7528\u771f\u5b9e\u5230\u4eff\u771f\u65b9\u6cd5\u6536\u96c6\u540c\u6b65\u8de8\u57df\u6570\u636e\uff08\u6709\u9650\u5143\u7ed3\u679c\u548c\u7269\u7406\u4f20\u611f\u5668\u8f93\u51fa\uff09\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5c06\u4eff\u771f\u6570\u636e\u6620\u5c04\u5230\u771f\u5b9e\u4f20\u611f\u5668\u54cd\u5e94\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\u7269\u7406\u4f20\u611f\u5668\u5177\u6709\u9ad8\u7075\u654f\u5ea6\uff0c\u6570\u5b57\u5b6a\u751f\u80fd\u4e00\u81f4\u590d\u73b0\u7269\u7406\u4f20\u611f\u5668\u8f93\u51fa\uff1b\u7269\u4f53\u5206\u7c7b\u4efb\u52a1\u8bc1\u660e\u4eff\u771f\u6570\u636e\u80fd\u6709\u6548\u589e\u5f3a\u771f\u5b9e\u6570\u636e\uff0c\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "TwinTac\u7cfb\u7edf\u6210\u529f\u5f25\u5408\u4e86\u8de8\u57df\u5b66\u4e60\u4efb\u52a1\u7684\u5dee\u8ddd\uff0c\u5c55\u793a\u4e86\u6570\u5b57\u5b6a\u751f\u89e6\u89c9\u4f20\u611f\u5668\u5728\u673a\u5668\u4eba\u6280\u80fd\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2509.10065", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10065", "abs": "https://arxiv.org/abs/2509.10065", "authors": ["Hauzi Cao", "Jiahao Shen", "Zhengzhen Li", "Qinquan Ren", "Shiyu Zhao"], "title": "Prespecified-Performance Kinematic Tracking Control for Aerial Manipulation", "comment": null, "summary": "This paper studies the kinematic tracking control problem for aerial\nmanipulators. Existing kinematic tracking control methods, which typically\nemploy proportional-derivative feedback or tracking-error-based feedback\nstrategies, may fail to achieve tracking objectives within specified time\nconstraints. To address this limitation, we propose a novel control framework\ncomprising two key components: end-effector tracking control based on a\nuser-defined preset trajectory and quadratic programming-based reference\nallocation. Compared with state-of-the-art approaches, the proposed method has\nseveral attractive features. First, it ensures that the end-effector reaches\nthe desired position within a preset time while keeping the tracking error\nwithin a performance envelope that reflects task requirements. Second,\nquadratic programming is employed to allocate the references of the quadcopter\nbase and the Delta arm, while considering the physical constraints of the\naerial manipulator, thus preventing solutions that may violate physical\nlimitations. The proposed approach is validated through three experiments.\nExperimental results demonstrate the effectiveness of the proposed algorithm\nand its capability to guarantee that the target position is reached within the\npreset time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u8bbe\u8f68\u8ff9\u548c\u4e8c\u6b21\u89c4\u5212\u7684\u7a7a\u4e2d\u673a\u68b0\u81c2\u8fd0\u52a8\u5b66\u8ddf\u8e2a\u63a7\u5236\u6846\u67b6\uff0c\u786e\u4fdd\u672b\u7aef\u6267\u884c\u5668\u5728\u9884\u8bbe\u65f6\u95f4\u5185\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u5e76\u6ee1\u8db3\u7269\u7406\u7ea6\u675f", "motivation": "\u73b0\u6709\u8fd0\u52a8\u5b66\u8ddf\u8e2a\u63a7\u5236\u65b9\u6cd5\uff08\u5982PD\u53cd\u9988\u6216\u8ddf\u8e2a\u8bef\u5dee\u53cd\u9988\u7b56\u7565\uff09\u65e0\u6cd5\u5728\u6307\u5b9a\u65f6\u95f4\u7ea6\u675f\u5185\u5b9e\u73b0\u8ddf\u8e2a\u76ee\u6807\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5c40\u9650\u6027", "method": "\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u57fa\u4e8e\u7528\u6237\u5b9a\u4e49\u9884\u8bbe\u8f68\u8ff9\u7684\u672b\u7aef\u6267\u884c\u5668\u8ddf\u8e2a\u63a7\u5236\uff0c\u4ee5\u53ca\u57fa\u4e8e\u4e8c\u6b21\u89c4\u5212\u7684\u53c2\u8003\u5206\u914d\u65b9\u6cd5\uff0c\u540c\u65f6\u8003\u8651\u7a7a\u4e2d\u673a\u68b0\u81c2\u7684\u7269\u7406\u7ea6\u675f", "result": "\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u8bc1\u76ee\u6807\u4f4d\u7f6e\u5728\u9884\u8bbe\u65f6\u95f4\u5185\u5230\u8fbe\uff0c\u4e14\u8ddf\u8e2a\u8bef\u5dee\u4fdd\u6301\u5728\u53cd\u6620\u4efb\u52a1\u8981\u6c42\u7684\u6027\u80fd\u5305\u7edc\u5185", "conclusion": "\u6240\u63d0\u51fa\u7684\u63a7\u5236\u6846\u67b6\u5177\u6709\u5438\u5f15\u4eba\u7684\u7279\u6027\uff0c\u80fd\u591f\u786e\u4fdd\u672b\u7aef\u6267\u884c\u5668\u5728\u9884\u8bbe\u65f6\u95f4\u5185\u5230\u8fbe\u671f\u671b\u4f4d\u7f6e\uff0c\u540c\u65f6\u901a\u8fc7\u4e8c\u6b21\u89c4\u5212\u907f\u514d\u8fdd\u53cd\u7269\u7406\u9650\u5236\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.10096", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10096", "abs": "https://arxiv.org/abs/2509.10096", "authors": ["Saeed Saadatnejad", "Reyhaneh Hosseininejad", "Jose Barreiros", "Katherine M. Tsui", "Alexandre Alahi"], "title": "HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario", "comment": "Accepted to RA-L 2025", "summary": "The increasing labor shortage and aging population underline the need for\nassistive robots to support human care recipients. To enable safe and\nresponsive assistance, robots require accurate human motion prediction in\nphysical interaction scenarios. However, this remains a challenging task due to\nthe variability of assistive settings and the complexity of coupled dynamics in\nphysical interactions. In this work, we address these challenges through two\nkey contributions: (1) HHI-Assist, a dataset comprising motion capture clips of\nhuman-human interactions in assistive tasks; and (2) a conditional\nTransformer-based denoising diffusion model for predicting the poses of\ninteracting agents. Our model effectively captures the coupled dynamics between\ncaregivers and care receivers, demonstrating improvements over baselines and\nstrong generalization to unseen scenarios. By advancing interaction-aware\nmotion prediction and introducing a new dataset, our work has the potential to\nsignificantly enhance robotic assistance policies. The dataset and code are\navailable at: https://sites.google.com/view/hhi-assist/home", "AI": {"tldr": "\u63d0\u51fa\u4e86HHI-Assist\u6570\u636e\u96c6\u548c\u57fa\u4e8eTransformer\u7684\u6761\u4ef6\u53bb\u566a\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u7269\u7406\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u4eba\u4f53\u8fd0\u52a8\uff0c\u4ee5\u63d0\u5347\u8f85\u52a9\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027", "motivation": "\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u4eba\u53e3\u8001\u9f84\u5316\u9700\u8981\u8f85\u52a9\u673a\u5668\u4eba\uff0c\u4f46\u673a\u5668\u4eba\u9700\u8981\u51c6\u786e\u7684\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u6765\u63d0\u4f9b\u5b89\u5168\u54cd\u5e94\uff0c\u800c\u7269\u7406\u4ea4\u4e92\u4e2d\u7684\u8026\u5408\u52a8\u529b\u5b66\u590d\u6742\u6027\u4f7f\u8fd9\u6210\u4e3a\u6311\u6218", "method": "\u6536\u96c6\u4eba\u7c7b-\u4eba\u7c7b\u4ea4\u4e92\u8f85\u52a9\u4efb\u52a1\u7684\u52a8\u4f5c\u6355\u6349\u6570\u636e\u96c6(HHI-Assist)\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u6761\u4ef6Transformer\u7684\u53bb\u566a\u6269\u6563\u6a21\u578b\u6765\u9884\u6d4b\u4ea4\u4e92\u4ee3\u7406\u7684\u59ff\u6001", "result": "\u6a21\u578b\u6709\u6548\u6355\u6349\u4e86\u62a4\u7406\u8005\u548c\u88ab\u62a4\u7406\u8005\u4e4b\u95f4\u7684\u8026\u5408\u52a8\u529b\u5b66\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6709\u6539\u8fdb\uff0c\u5e76\u5728\u672a\u89c1\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "\u901a\u8fc7\u63a8\u8fdb\u4ea4\u4e92\u611f\u77e5\u7684\u8fd0\u52a8\u9884\u6d4b\u548c\u5f15\u5165\u65b0\u6570\u636e\u96c6\uff0c\u8fd9\u9879\u5de5\u4f5c\u6709\u6f5c\u529b\u663e\u8457\u589e\u5f3a\u673a\u5668\u4eba\u8f85\u52a9\u7b56\u7565"}}
{"id": "2509.10128", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10128", "abs": "https://arxiv.org/abs/2509.10128", "authors": ["Philip Arm", "Oliver Fischer", "Joseph Church", "Adrian Fuhrer", "Hendrik Kolvenbach", "Marco Hutter"], "title": "Efficient Learning-Based Control of a Legged Robot in Lunar Gravity", "comment": null, "summary": "Legged robots are promising candidates for exploring challenging areas on\nlow-gravity bodies such as the Moon, Mars, or asteroids, thanks to their\nadvanced mobility on unstructured terrain. However, as planetary robots' power\nand thermal budgets are highly restricted, these robots need energy-efficient\ncontrol approaches that easily transfer to multiple gravity environments. In\nthis work, we introduce a reinforcement learning-based control approach for\nlegged robots with gravity-scaled power-optimized reward functions. We use our\napproach to develop and validate a locomotion controller and a base pose\ncontroller in gravity environments from lunar gravity (1.62 m/s2) to a\nhypothetical super-Earth (19.62 m/s2). Our approach successfully scales across\nthese gravity levels for locomotion and base pose control with the\ngravity-scaled reward functions. The power-optimized locomotion controller\nreached a power consumption for locomotion of 23.4 W in Earth gravity on a\n15.65 kg robot at 0.4 m/s, a 23 % improvement over the baseline policy.\nAdditionally, we designed a constant-force spring offload system that allowed\nus to conduct real-world experiments on legged locomotion in lunar gravity. In\nlunar gravity, the power-optimized control policy reached 12.2 W, 36 % less\nthan a baseline controller which is not optimized for power efficiency. Our\nmethod provides a scalable approach to developing power-efficient locomotion\ncontrollers for legged robots across multiple gravity levels.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8db3\u5f0f\u673a\u5668\u4eba\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u529b\u7f29\u653e\u529f\u7387\u4f18\u5316\u5956\u52b1\u51fd\u6570\uff0c\u5b9e\u73b0\u4ece\u6708\u7403\u91cd\u529b\u5230\u8d85\u5730\u7403\u91cd\u529b\u73af\u5883\u7684\u9ad8\u6548\u8fd0\u52a8\u63a7\u5236\uff0c\u529f\u7387\u6d88\u8017\u964d\u4f4e23-36%\u3002", "motivation": "\u884c\u661f\u63a2\u6d4b\u673a\u5668\u4eba\u9762\u4e34\u4e25\u683c\u7684\u529f\u7387\u548c\u70ed\u9884\u7b97\u9650\u5236\uff0c\u9700\u8981\u80fd\u5728\u591a\u79cd\u91cd\u529b\u73af\u5883\u4e0b\u9ad8\u6548\u5de5\u4f5c\u7684\u80fd\u91cf\u4f18\u5316\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u91cd\u529b\u7f29\u653e\u7684\u529f\u7387\u4f18\u5316\u5956\u52b1\u51fd\u6570\uff0c\u5f00\u53d1\u8fd0\u52a8\u63a7\u5236\u548c\u57fa\u5ea7\u59ff\u6001\u63a7\u5236\u5668\uff0c\u5e76\u8bbe\u8ba1\u6052\u529b\u5f39\u7c27\u5378\u8f7d\u7cfb\u7edf\u8fdb\u884c\u6708\u7403\u91cd\u529b\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728\u5730\u7403\u91cd\u529b\u4e0b\u529f\u7387\u6d88\u801723.4W\uff08\u6bd4\u57fa\u7ebf\u964d\u4f4e23%\uff09\uff0c\u6708\u7403\u91cd\u529b\u4e0b12.2W\uff08\u964d\u4f4e36%\uff09\uff0c\u6210\u529f\u57281.62-19.62 m/s\u00b2\u91cd\u529b\u8303\u56f4\u5185\u9a8c\u8bc1\u63a7\u5236\u5668\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8db3\u5f0f\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u8de8\u591a\u79cd\u91cd\u529b\u73af\u5883\u5f00\u53d1\u529f\u7387\u9ad8\u6548\u8fd0\u52a8\u63a7\u5236\u5668\u7684\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u884c\u661f\u63a2\u6d4b\u4efb\u52a1\u3002"}}
{"id": "2509.10139", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10139", "abs": "https://arxiv.org/abs/2509.10139", "authors": ["Santiago Montiel-Mar\u00edn", "Angel Llamazares", "Miguel Antunes-Garc\u00eda", "Fabio S\u00e1nchez-Garc\u00eda", "Luis M. Bergasa"], "title": "CaR1: A Multi-Modal Baseline for BEV Vehicle Segmentation via Camera-Radar Fusion", "comment": "4 pages, 2 figures", "summary": "Camera-radar fusion offers a robust and cost-effective alternative to\nLiDAR-based autonomous driving systems by combining complementary sensing\ncapabilities: cameras provide rich semantic cues but unreliable depth, while\nradar delivers sparse yet reliable position and motion information. We\nintroduce CaR1, a novel camera-radar fusion architecture for BEV vehicle\nsegmentation. Built upon BEVFusion, our approach incorporates a grid-wise radar\nencoding that discretizes point clouds into structured BEV features and an\nadaptive fusion mechanism that dynamically balances sensor contributions.\nExperiments on nuScenes demonstrate competitive segmentation performance (57.6\nIoU), on par with state-of-the-art methods. Code is publicly available\n\\href{https://www.github.com/santimontiel/car1}{online}.", "AI": {"tldr": "CaR1\u662f\u4e00\u79cd\u65b0\u9896\u7684\u76f8\u673a-\u96f7\u8fbe\u878d\u5408\u67b6\u6784\uff0c\u7528\u4e8eBEV\u8f66\u8f86\u5206\u5272\uff0c\u901a\u8fc7\u7f51\u683c\u5316\u96f7\u8fbe\u7f16\u7801\u548c\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\uff0c\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u8fbe\u523057.6 IoU\u7684\u7ade\u4e89\u6027\u6027\u80fd", "motivation": "\u76f8\u673a-\u96f7\u8fbe\u878d\u5408\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6bd4LiDAR\u66f4\u5177\u6210\u672c\u6548\u76ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u76f8\u673a\u63d0\u4f9b\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\u4f46\u6df1\u5ea6\u4e0d\u53ef\u9760\uff0c\u96f7\u8fbe\u63d0\u4f9b\u7a00\u758f\u4f46\u53ef\u9760\u7684\u4f4d\u7f6e\u548c\u8fd0\u52a8\u4fe1\u606f", "method": "\u57fa\u4e8eBEVFusion\u6784\u5efa\uff0c\u91c7\u7528\u7f51\u683c\u5316\u96f7\u8fbe\u7f16\u7801\u5c06\u70b9\u4e91\u79bb\u6563\u5316\u4e3a\u7ed3\u6784\u5316BEV\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u81ea\u9002\u5e94\u878d\u5408\u673a\u5236\u52a8\u6001\u5e73\u8861\u4f20\u611f\u5668\u8d21\u732e", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u7684\u5206\u5272\u6027\u80fd\uff0857.6 IoU\uff09\uff0c\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53", "conclusion": "CaR1\u8bc1\u660e\u4e86\u76f8\u673a-\u96f7\u8fbe\u878d\u5408\u5728BEV\u8f66\u8f86\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4f4e\u6210\u672c\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2509.10247", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10247", "abs": "https://arxiv.org/abs/2509.10247", "authors": ["Xinhong Zhang", "Runqing Wang", "Yunfan Ren", "Jian Sun", "Hao Fang", "Jie Chen", "Gang Wang"], "title": "DiffAero: A GPU-Accelerated Differentiable Simulation Framework for Efficient Quadrotor Policy Learning", "comment": "8 pages, 11 figures, 1 table", "summary": "This letter introduces DiffAero, a lightweight, GPU-accelerated, and fully\ndifferentiable simulation framework designed for efficient quadrotor control\npolicy learning. DiffAero supports both environment-level and agent-level\nparallelism and integrates multiple dynamics models, customizable sensor stacks\n(IMU, depth camera, and LiDAR), and diverse flight tasks within a unified,\nGPU-native training interface. By fully parallelizing both physics and\nrendering on the GPU, DiffAero eliminates CPU-GPU data transfer bottlenecks and\ndelivers orders-of-magnitude improvements in simulation throughput. In contrast\nto existing simulators, DiffAero not only provides high-performance simulation\nbut also serves as a research platform for exploring differentiable and hybrid\nlearning algorithms. Extensive benchmarks and real-world flight experiments\ndemonstrate that DiffAero and hybrid learning algorithms combined can learn\nrobust flight policies in hours on consumer-grade hardware. The code is\navailable at https://github.com/flyingbitac/diffaero.", "AI": {"tldr": "DiffAero\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001GPU\u52a0\u901f\u7684\u5b8c\u5168\u53ef\u5fae\u5206\u56db\u65cb\u7ffc\u98de\u884c\u5668\u4eff\u771f\u6846\u67b6\uff0c\u652f\u6301\u73af\u5883\u7ea7\u548c\u667a\u80fd\u4f53\u7ea7\u5e76\u884c\uff0c\u96c6\u6210\u591a\u79cd\u52a8\u529b\u5b66\u6a21\u578b\u548c\u4f20\u611f\u5668\uff0c\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u6570\u5c0f\u65f6\u5373\u53ef\u5b66\u4e60\u9c81\u68d2\u98de\u884c\u7b56\u7565", "motivation": "\u73b0\u6709\u4eff\u771f\u5668\u5b58\u5728CPU-GPU\u6570\u636e\u4f20\u8f93\u74f6\u9888\uff0c\u65e0\u6cd5\u9ad8\u6548\u652f\u6301\u53ef\u5fae\u5206\u548c\u6df7\u5408\u5b66\u4e60\u7b97\u6cd5\u7684\u7814\u7a76\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6027\u80fd\u7684GPU\u539f\u751f\u4eff\u771f\u5e73\u53f0\u6765\u52a0\u901f\u56db\u65cb\u7ffc\u63a7\u5236\u7b56\u7565\u5b66\u4e60", "method": "\u5f00\u53d1\u5b8c\u5168\u5e76\u884c\u5316\u7684GPU\u539f\u751f\u4eff\u771f\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u52a8\u529b\u5b66\u6a21\u578b\uff08IMU\u3001\u6df1\u5ea6\u76f8\u673a\u3001LiDAR\uff09\u548c\u98de\u884c\u4efb\u52a1\uff0c\u6d88\u9664CPU-GPU\u6570\u636e\u4f20\u8f93\u74f6\u9888\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u53ef\u5fae\u5206\u8bad\u7ec3\u63a5\u53e3", "result": "\u76f8\u6bd4\u73b0\u6709\u4eff\u771f\u5668\uff0cDiffAero\u5728\u4eff\u771f\u541e\u5410\u91cf\u4e0a\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u7684\u63d0\u5347\uff0c\u7ed3\u5408\u6df7\u5408\u5b66\u4e60\u7b97\u6cd5\u53ef\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u6570\u5c0f\u65f6\u5185\u5b66\u4e60\u5230\u9c81\u68d2\u7684\u98de\u884c\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u98de\u884c\u5b9e\u9a8c\u9a8c\u8bc1", "conclusion": "DiffAero\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u4eff\u771f\uff0c\u8fd8\u4f5c\u4e3a\u7814\u7a76\u5e73\u53f0\u652f\u6301\u53ef\u5fae\u5206\u548c\u6df7\u5408\u5b66\u4e60\u7b97\u6cd5\u7684\u63a2\u7d22\uff0c\u4e3a\u56db\u65cb\u7ffc\u63a7\u5236\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.10305", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10305", "abs": "https://arxiv.org/abs/2509.10305", "authors": ["Yutong Shen", "Ruizhe Xia", "Bokai Yan", "Shunqi zhang", "Pengrui Xiang", "Sicheng He", "Yixin Xu"], "title": "GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning", "comment": "6 pages, 5 figures", "summary": "In dynamic and uncertain environments, robotic path planning demands accurate\nspatiotemporal environment understanding combined with robust decision-making\nunder partial observability. However, current deep reinforcement learning-based\npath planning methods face two fundamental limitations: (1) insufficient\nmodeling of multi-scale temporal dependencies, resulting in suboptimal\nadaptability in dynamic scenarios, and (2) inefficient exploration-exploitation\nbalance, leading to degraded path quality. To address these challenges, we\npropose GundamQ: A Multi-Scale Spatiotemporal Q-Network for Robotic Path\nPlanning. The framework comprises two key modules: (i) the Spatiotemporal\nPerception module, which hierarchically extracts multi-granularity spatial\nfeatures and multi-scale temporal dependencies ranging from instantaneous to\nextended time horizons, thereby improving perception accuracy in dynamic\nenvironments; and (ii) the Adaptive Policy Optimization module, which balances\nexploration and exploitation during training while optimizing for smoothness\nand collision probability through constrained policy updates. Experiments in\ndynamic environments demonstrate that GundamQ achieves a 15.3\\% improvement in\nsuccess rate and a 21.7\\% increase in overall path quality, significantly\noutperforming existing state-of-the-art methods.", "AI": {"tldr": "GundamQ\u662f\u4e00\u4e2a\u591a\u5c3a\u5ea6\u65f6\u7a7aQ\u7f51\u7edc\uff0c\u901a\u8fc7\u65f6\u7a7a\u611f\u77e5\u6a21\u5757\u548c\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\u65f6\u95f4\u4f9d\u8d56\u6027\u5efa\u6a21\u4e0d\u8db3\u548c\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u6839\u672c\u5c40\u9650\uff1a\u591a\u5c3a\u5ea6\u65f6\u95f4\u4f9d\u8d56\u6027\u5efa\u6a21\u4e0d\u8db3\u5bfc\u81f4\u52a8\u6001\u573a\u666f\u9002\u5e94\u6027\u5dee\uff0c\u4ee5\u53ca\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u6548\u7387\u4f4e\u5bfc\u81f4\u8def\u5f84\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51faGundamQ\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a(i)\u65f6\u7a7a\u611f\u77e5\u6a21\u5757\u5206\u5c42\u63d0\u53d6\u591a\u7c92\u5ea6\u7a7a\u95f4\u7279\u5f81\u548c\u591a\u5c3a\u5ea6\u65f6\u95f4\u4f9d\u8d56\u6027\uff1b(ii)\u81ea\u9002\u5e94\u7b56\u7565\u4f18\u5316\u6a21\u5757\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\uff0c\u901a\u8fc7\u7ea6\u675f\u7b56\u7565\u66f4\u65b0\u4f18\u5316\u8def\u5f84\u5e73\u6ed1\u5ea6\u548c\u78b0\u649e\u6982\u7387\u3002", "result": "\u5728\u52a8\u6001\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0cGundamQ\u5b9e\u73b0\u4e8615.3%\u7684\u6210\u529f\u7387\u63d0\u5347\u548c21.7%\u7684\u6574\u4f53\u8def\u5f84\u8d28\u91cf\u63d0\u9ad8\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "GundamQ\u901a\u8fc7\u6539\u8fdb\u65f6\u7a7a\u611f\u77e5\u548c\u7b56\u7565\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u529b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10317", "categories": ["cs.RO", "cs.LG", "93C85", "I.2.9; I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2509.10317", "abs": "https://arxiv.org/abs/2509.10317", "authors": ["Elizaveta D. Moskovskaya", "Anton D. Moscowsky"], "title": "Robot guide with multi-agent control and automatic scenario generation with LLM", "comment": "14 pages, 5 figures, 2 tables, 1 demo-video and repository link", "summary": "The work describes the development of a hybrid control architecture for an\nanthropomorphic tour guide robot, combining a multi-agent resource management\nsystem with automatic behavior scenario generation based on large language\nmodels. The proposed approach aims to overcome the limitations of traditional\nsystems, which rely on manual tuning of behavior scenarios. These limitations\ninclude manual configuration, low flexibility, and lack of naturalness in robot\nbehavior. The process of preparing tour scenarios is implemented through a\ntwo-stage generation: first, a stylized narrative is created, then non-verbal\naction tags are integrated into the text. The multi-agent system ensures\ncoordination and conflict resolution during the execution of parallel actions,\nas well as maintaining default behavior after the completion of main\noperations, contributing to more natural robot behavior. The results obtained\nfrom the trial demonstrate the potential of the proposed approach for\nautomating and scaling social robot control systems.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u6df7\u5408\u63a7\u5236\u67b6\u6784\uff0c\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u8d44\u6e90\u7ba1\u7406\u7cfb\u7edf\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u884c\u4e3a\u573a\u666f\u751f\u6210\uff0c\u7528\u4e8e\u4eba\u5f62\u5bfc\u6e38\u673a\u5668\u4eba\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u7cfb\u7edf\u624b\u52a8\u914d\u7f6e\u3001\u7075\u6d3b\u6027\u4f4e\u548c\u673a\u5668\u4eba\u884c\u4e3a\u4e0d\u81ea\u7136\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5bfc\u6e38\u673a\u5668\u4eba\u7cfb\u7edf\u4f9d\u8d56\u624b\u52a8\u8c03\u6574\u884c\u4e3a\u573a\u666f\uff0c\u5b58\u5728\u624b\u52a8\u914d\u7f6e\u7e41\u7410\u3001\u7075\u6d3b\u6027\u4f4e\u548c\u673a\u5668\u4eba\u884c\u4e3a\u4e0d\u591f\u81ea\u7136\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u4e14\u66f4\u81ea\u7136\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u751f\u6210\u8fc7\u7a0b\uff1a\u9996\u5148\u751f\u6210\u98ce\u683c\u5316\u53d9\u8ff0\uff0c\u7136\u540e\u5c06\u975e\u8bed\u8a00\u52a8\u4f5c\u6807\u7b7e\u6574\u5408\u5230\u6587\u672c\u4e2d\u3002\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u786e\u4fdd\u5e76\u884c\u52a8\u4f5c\u6267\u884c\u65f6\u7684\u534f\u8c03\u548c\u51b2\u7a81\u89e3\u51b3\uff0c\u5e76\u5728\u4e3b\u8981\u64cd\u4f5c\u5b8c\u6210\u540e\u7ef4\u6301\u9ed8\u8ba4\u884c\u4e3a\u3002", "result": "\u8bd5\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u5316\u548c\u793e\u4f1a\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\u6269\u5c55\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u673a\u5668\u4eba\u884c\u4e3a\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u63a7\u5236\u67b6\u6784\u6210\u529f\u514b\u670d\u4e86\u4f20\u7edf\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u793e\u4ea4\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\u548c\u89c4\u6a21\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10349", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10349", "abs": "https://arxiv.org/abs/2509.10349", "authors": ["Weiyan Lu", "Huizhe Li", "Yuhao Fang", "Zhexuan Zhou", "Junda Wu", "Yude Li", "Youmin Gong", "Jie Mei"], "title": "Acetrans: An Autonomous Corridor-Based and Efficient UAV Suspended Transport System", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) with suspended payloads offer significant\nadvantages for aerial transportation in complex and cluttered environments.\nHowever, existing systems face critical limitations, including unreliable\nperception of the cable-payload dynamics, inefficient planning in large-scale\nenvironments, and the inability to guarantee whole-body safety under cable\nbending and external disturbances. This paper presents Acetrans, an Autonomous,\nCorridor-based, and Efficient UAV suspended transport system that addresses\nthese challenges through a unified perception, planning, and control framework.\nA LiDAR-IMU fusion module is proposed to jointly estimate both payload pose and\ncable shape under taut and bent modes, enabling robust whole-body state\nestimation and real-time filtering of cable point clouds. To enhance planning\nscalability, we introduce the Multi-size-Aware Configuration-space Iterative\nRegional Inflation (MACIRI) algorithm, which generates safe flight corridors\nwhile accounting for varying UAV and payload geometries. A spatio-temporal,\ncorridor-constrained trajectory optimization scheme is then developed to ensure\ndynamically feasible and collision-free trajectories. Finally, a nonlinear\nmodel predictive controller (NMPC) augmented with cable-bending constraints\nprovides robust whole-body safety during execution. Simulation and experimental\nresults validate the effectiveness of Acetrans, demonstrating substantial\nimprovements in perception accuracy, planning efficiency, and control safety\ncompared to state-of-the-art methods.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.10405", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10405", "abs": "https://arxiv.org/abs/2509.10405", "authors": ["Nicholas Carlotti", "Mirko Nava", "Alessandro Giusti"], "title": "Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying LED States", "comment": "accepted at CoRL 2025", "summary": "We introduce a model for monocular RGB relative pose estimation of a ground\nrobot that trains from scratch without pose labels nor prior knowledge about\nthe robot's shape or appearance. At training time, we assume: (i) a robot\nfitted with multiple LEDs, whose states are independent and known at each\nframe; (ii) knowledge of the approximate viewing direction of each LED; and\n(iii) availability of a calibration image with a known target distance, to\naddress the ambiguity of monocular depth estimation. Training data is collected\nby a pair of robots moving randomly without needing external infrastructure or\nhuman supervision. Our model trains on the task of predicting from an image the\nstate of each LED on the robot. In doing so, it learns to predict the position\nof the robot in the image, its distance, and its relative bearing. At inference\ntime, the state of the LEDs is unknown, can be arbitrary, and does not affect\nthe pose estimation performance. Quantitative experiments indicate that our\napproach: is competitive with SoA approaches that require supervision from pose\nlabels or a CAD model of the robot; generalizes to different domains; and\nhandles multi-robot pose estimation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4f4d\u59ff\u6807\u7b7e\u6216\u673a\u5668\u4eba\u5f62\u72b6\u5148\u9a8c\u77e5\u8bc6\u7684\u5355\u76eeRGB\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u6a21\u578b\uff0c\u901a\u8fc7LED\u72b6\u6001\u9884\u6d4b\u4efb\u52a1\u5b9e\u73b0\u81ea\u76d1\u7763\u5b66\u4e60", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4f4d\u59ff\u6807\u7b7e\u6216CAD\u6a21\u578b\u7684\u76d1\u7763\u4f9d\u8d56\u95ee\u9898\uff0c\u5b9e\u73b0\u65e0\u9700\u5916\u90e8\u57fa\u7840\u8bbe\u65bd\u6216\u4eba\u5de5\u76d1\u7763\u7684\u81ea\u4e3b\u8bad\u7ec3", "method": "\u4f7f\u7528\u914d\u5907\u591a\u4e2aLED\u7684\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u9884\u6d4b\u56fe\u50cf\u4e2d\u6bcf\u4e2aLED\u72b6\u6001\u7684\u4efb\u52a1\u6765\u5b66\u4e60\u673a\u5668\u4eba\u4f4d\u7f6e\u3001\u8ddd\u79bb\u548c\u76f8\u5bf9\u65b9\u4f4d\uff0c\u8bad\u7ec3\u65f6\u5df2\u77e5LED\u72b6\u6001\u548c\u5927\u81f4\u89c6\u89d2\u65b9\u5411", "result": "\u4e0e\u9700\u8981\u76d1\u7763\u7684\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53\uff0c\u5177\u6709\u826f\u597d\u7684\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u80fd\u5904\u7406\u591a\u673a\u5668\u4eba\u4f4d\u59ff\u4f30\u8ba1", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u901a\u8fc7\u7b80\u5355\u7684\u89c6\u89c9\u7ebf\u7d22\uff08LED\u72b6\u6001\uff09\u53ef\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u81ea\u76d1\u7763\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\uff0c\u4e3a\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2509.10416", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10416", "abs": "https://arxiv.org/abs/2509.10416", "authors": ["Ze Fu", "Pinhao Song", "Yutong Hu", "Renaud Detry"], "title": "TASC: Task-Aware Shared Control for Teleoperated Manipulation", "comment": null, "summary": "We present TASC, a Task-Aware Shared Control framework for teleoperated\nmanipulation that infers task-level user intent and provides assistance\nthroughout the task. To support everyday tasks without predefined knowledge,\nTASC constructs an open-vocabulary interaction graph from visual input to\nrepresent functional object relationships, and infers user intent accordingly.\nA shared control policy then provides rotation assistance during both grasping\nand object interaction, guided by spatial constraints predicted by a\nvision-language model. Our method addresses two key challenges in\ngeneral-purpose, long-horizon shared control: (1) understanding and inferring\ntask-level user intent, and (2) generalizing assistance across diverse objects\nand tasks. Experiments in both simulation and the real world demonstrate that\nTASC improves task efficiency and reduces user input effort compared to prior\nmethods. To the best of our knowledge, this is the first shared control\nframework that supports everyday manipulation tasks with zero-shot\ngeneralization. The code that supports our experiments is publicly available at\nhttps://github.com/fitz0401/tasc.", "AI": {"tldr": "TASC\u662f\u4e00\u4e2a\u4efb\u52a1\u611f\u77e5\u5171\u4eab\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8f93\u5165\u6784\u5efa\u5f00\u653e\u8bcd\u6c47\u4ea4\u4e92\u56fe\u6765\u63a8\u65ad\u7528\u6237\u610f\u56fe\uff0c\u63d0\u4f9b\u65cb\u8f6c\u8f85\u52a9\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u77e5\u8bc6\u5373\u53ef\u652f\u6301\u65e5\u5e38\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u901a\u7528\u957f\u65f6\u57df\u5171\u4eab\u63a7\u5236\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u7406\u89e3\u548c\u63a8\u65ad\u4efb\u52a1\u7ea7\u7528\u6237\u610f\u56fe\uff0c\u4ee5\u53ca\u5728\u4e0d\u540c\u5bf9\u8c61\u548c\u4efb\u52a1\u95f4\u6cdb\u5316\u8f85\u52a9\u529f\u80fd\u3002", "method": "\u6784\u5efa\u5f00\u653e\u8bcd\u6c47\u4ea4\u4e92\u56fe\u8868\u793a\u529f\u80fd\u5bf9\u8c61\u5173\u7cfb\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u7a7a\u95f4\u7ea6\u675f\uff0c\u5728\u6293\u53d6\u548c\u5bf9\u8c61\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u65cb\u8f6c\u8f85\u52a9\u7684\u5171\u4eab\u63a7\u5236\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cTASC\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4efb\u52a1\u6548\u7387\u5e76\u51cf\u5c11\u4e86\u7528\u6237\u8f93\u5165\u5de5\u4f5c\u91cf\u3002", "conclusion": "\u8fd9\u662f\u7b2c\u4e00\u4e2a\u652f\u6301\u65e5\u5e38\u64cd\u4f5c\u4efb\u52a1\u4e14\u5177\u6709\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u7684\u5171\u4eab\u63a7\u5236\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u9884\u5b9a\u4e49\u77e5\u8bc6\u7684\u901a\u7528\u64cd\u4f5c\u8f85\u52a9\u3002"}}
{"id": "2509.10426", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.10426", "abs": "https://arxiv.org/abs/2509.10426", "authors": ["Jianxin Shi", "Zengqi Peng", "Xiaolong Chen", "Tianyu Wo", "Jun Ma"], "title": "DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training", "comment": null, "summary": "Trajectory prediction is a critical component of autonomous driving,\nessential for ensuring both safety and efficiency on the road. However,\ntraditional approaches often struggle with the scarcity of labeled data and\nexhibit suboptimal performance in multi-agent prediction scenarios. To address\nthese challenges, we introduce a disentangled context-aware pre-training\nframework for multi-agent motion prediction, named DECAMP. Unlike existing\nmethods that entangle representation learning with pretext tasks, our framework\ndecouples behavior pattern learning from latent feature reconstruction,\nprioritizing interpretable dynamics and thereby enhancing scene representation\nfor downstream prediction. Additionally, our framework incorporates\ncontext-aware representation learning alongside collaborative spatial-motion\npretext tasks, which enables joint optimization of structural and intentional\nreasoning while capturing the underlying dynamic intentions. Our experiments on\nthe Argoverse 2 benchmark showcase the superior performance of our method, and\nthe results attained underscore its effectiveness in multi-agent motion\nforecasting. To the best of our knowledge, this is the first context\nautoencoder framework for multi-agent motion forecasting in autonomous driving.\nThe code and models will be made publicly available.", "AI": {"tldr": "DECAMP\u662f\u4e00\u4e2a\u89e3\u8026\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\uff0c\u901a\u8fc7\u5206\u79bb\u884c\u4e3a\u6a21\u5f0f\u5b66\u4e60\u548c\u6f5c\u5728\u7279\u5f81\u91cd\u5efa\uff0c\u5728Argoverse 2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u591a\u667a\u80fd\u4f53\u9884\u6d4b\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u89e3\u51b3\u8868\u793a\u5b66\u4e60\u4e0e\u9884\u8bad\u7ec3\u4efb\u52a1\u7ea0\u7f20\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u89e3\u8026\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u5c06\u884c\u4e3a\u6a21\u5f0f\u5b66\u4e60\u4e0e\u6f5c\u5728\u7279\u5f81\u91cd\u5efa\u5206\u79bb\uff0c\u7ed3\u5408\u4e0a\u4e0b\u6587\u611f\u77e5\u8868\u793a\u5b66\u4e60\u548c\u534f\u4f5c\u7a7a\u95f4\u8fd0\u52a8\u9884\u8bad\u7ec3\u4efb\u52a1\u3002", "result": "\u5728Argoverse 2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u8bc1\u660e\u5728\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "DECAMP\u662f\u9996\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u9884\u6d4b\u7684\u4e0a\u4e0b\u6587\u81ea\u52a8\u7f16\u7801\u5668\u6846\u67b6\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u3002"}}
{"id": "2509.10444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10444", "abs": "https://arxiv.org/abs/2509.10444", "authors": ["Chaerim Moon", "Joohyung Kim"], "title": "Coordinated Motion Planning of a Wearable Multi-Limb System for Enhanced Human-Robot Interaction", "comment": "Presented in IROS 2023 Workshop (Multilimb Coordination in Human\n  Neuroscience and Robotics: Classical and Learning Perspectives)", "summary": "Supernumerary Robotic Limbs (SRLs) can enhance human capability within close\nproximity. However, as a wearable device, the generated moment from its\noperation acts on the human body as an external torque. When the moments\nincrease, more muscle units are activated for balancing, and it can result in\nreduced muscular null space. Therefore, this paper suggests a concept of a\nmotion planning layer that reduces the generated moment for enhanced\nHuman-Robot Interaction. It modifies given trajectories with desirable angular\nacceleration and position deviation limits. Its performance to reduce the\nmoment is demonstrated through the simulation, which uses simplified human and\nrobotic system models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd0\u52a8\u89c4\u5212\u5c42\u6982\u5ff5\uff0c\u7528\u4e8e\u51cf\u5c11\u8d85\u7ea7\u673a\u5668\u4eba\u80a2\u4f53(SRLs)\u64cd\u4f5c\u65f6\u4ea7\u751f\u7684\u529b\u77e9\uff0c\u4ece\u800c\u6539\u5584\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u3002", "motivation": "\u4f5c\u4e3a\u53ef\u7a7f\u6234\u8bbe\u5907\uff0c\u8d85\u7ea7\u673a\u5668\u4eba\u80a2\u4f53\u5728\u64cd\u4f5c\u65f6\u4ea7\u751f\u7684\u529b\u77e9\u4f1a\u4f5c\u4e3a\u5916\u90e8\u626d\u77e9\u4f5c\u7528\u4e8e\u4eba\u4f53\u3002\u5f53\u529b\u77e9\u589e\u5927\u65f6\uff0c\u9700\u8981\u66f4\u591a\u808c\u8089\u5355\u5143\u6765\u7ef4\u6301\u5e73\u8861\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u808c\u8089\u96f6\u7a7a\u95f4\u51cf\u5c11\uff0c\u5f71\u54cd\u4eba\u673a\u4ea4\u4e92\u6548\u679c\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u8fd0\u52a8\u89c4\u5212\u5c42\uff0c\u901a\u8fc7\u4fee\u6539\u7ed9\u5b9a\u8f68\u8ff9\uff0c\u5728\u671f\u671b\u7684\u89d2\u52a0\u901f\u5ea6\u548c\u4f4d\u7f6e\u504f\u5dee\u9650\u5236\u5185\u4f18\u5316\u8fd0\u52a8\uff0c\u4ee5\u51cf\u5c11\u4ea7\u751f\u7684\u529b\u77e9\u3002\u4f7f\u7528\u7b80\u5316\u7684\u4eba\u4f53\u548c\u673a\u5668\u4eba\u7cfb\u7edf\u6a21\u578b\u8fdb\u884c\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u5c11\u64cd\u4f5c\u65f6\u4ea7\u751f\u7684\u529b\u77e9\u3002", "conclusion": "\u63d0\u51fa\u7684\u8fd0\u52a8\u89c4\u5212\u5c42\u6982\u5ff5\u80fd\u591f\u51cf\u5c11\u8d85\u7ea7\u673a\u5668\u4eba\u80a2\u4f53\u4ea7\u751f\u7684\u529b\u77e9\uff0c\u4ece\u800c\u589e\u5f3a\u4eba\u673a\u4ea4\u4e92\u6027\u80fd\uff0c\u4e3a\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.10454", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10454", "abs": "https://arxiv.org/abs/2509.10454", "authors": ["Hang Yin", "Haoyu Wei", "Xiuwei Xu", "Wenxuan Guo", "Jie Zhou", "Jiwen Lu"], "title": "GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation", "comment": "Accepted to CoRL 2025. Project page: [this https\n  URL](https://bagh2178.github.io/GC-VLN/)", "summary": "In this paper, we propose a training-free framework for vision-and-language\nnavigation (VLN). Existing zero-shot VLN methods are mainly designed for\ndiscrete environments or involve unsupervised training in continuous simulator\nenvironments, which makes it challenging to generalize and deploy them in\nreal-world scenarios. To achieve a training-free framework in continuous\nenvironments, our framework formulates navigation guidance as graph constraint\noptimization by decomposing instructions into explicit spatial constraints. The\nconstraint-driven paradigm decodes spatial semantics through constraint\nsolving, enabling zero-shot adaptation to unseen environments. Specifically, we\nconstruct a spatial constraint library covering all types of spatial\nrelationship mentioned in VLN instructions. The human instruction is decomposed\ninto a directed acyclic graph, with waypoint nodes, object nodes and edges,\nwhich are used as queries to retrieve the library to build the graph\nconstraints. The graph constraint optimization is solved by the constraint\nsolver to determine the positions of waypoints, obtaining the robot's\nnavigation path and final goal. To handle cases of no solution or multiple\nsolutions, we construct a navigation tree and the backtracking mechanism.\nExtensive experiments on standard benchmarks demonstrate significant\nimprovements in success rate and navigation efficiency compared to\nstate-of-the-art zero-shot VLN methods. We further conduct real-world\nexperiments to show that our framework can effectively generalize to new\nenvironments and instruction sets, paving the way for a more robust and\nautonomous navigation framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5bfc\u822a\u6307\u4ee4\u5206\u89e3\u4e3a\u7a7a\u95f4\u7ea6\u675f\u56fe\u5e76\u8fdb\u884c\u4f18\u5316\u6c42\u89e3\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8fde\u7eed\u73af\u5883\u5bfc\u822a", "motivation": "\u73b0\u6709\u96f6\u6837\u672cVLN\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u79bb\u6563\u73af\u5883\u6216\u5728\u8fde\u7eed\u6a21\u62df\u5668\u4e2d\u9700\u8981\u65e0\u76d1\u7763\u8bad\u7ec3\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u771f\u5b9e\u4e16\u754c\u573a\u666f\uff0c\u9700\u8981\u5f00\u53d1\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5728\u8fde\u7eed\u73af\u5883\u4e2d\u5de5\u4f5c\u7684\u6846\u67b6", "method": "\u5c06\u5bfc\u822a\u6307\u4ee4\u5206\u89e3\u4e3a\u663e\u5f0f\u7a7a\u95f4\u7ea6\u675f\uff0c\u6784\u5efa\u7a7a\u95f4\u7ea6\u675f\u5e93\uff0c\u5c06\u4eba\u7c7b\u6307\u4ee4\u89e3\u6790\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff0c\u901a\u8fc7\u7ea6\u675f\u6c42\u89e3\u5668\u8fdb\u884c\u56fe\u7ea6\u675f\u4f18\u5316\u786e\u5b9a\u8def\u5f84\u70b9\u4f4d\u7f6e\uff0c\u91c7\u7528\u5bfc\u822a\u6811\u548c\u56de\u6eaf\u673a\u5236\u5904\u7406\u65e0\u89e3\u6216\u591a\u89e3\u60c5\u51b5", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672cVLN\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u5bfc\u822a\u6548\u7387\uff0c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\u80fd\u6709\u6548\u6cdb\u5316\u5230\u65b0\u73af\u5883\u548c\u6307\u4ee4\u96c6", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u548c\u81ea\u4e3b\u7684\u5bfc\u822a\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5728\u8fde\u7eed\u73af\u5883\u4e2d\u8fdb\u884c\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a"}}
