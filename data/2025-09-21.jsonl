{"id": "2509.14295", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14295", "abs": "https://arxiv.org/abs/2509.14295", "authors": ["Fanqi Kong", "Ruijie Zhang", "Huaxiao Yin", "Guibin Zhang", "Xiaofei Zhang", "Ziang Chen", "Zhaowei Zhang", "Xiaoyuan Zhang", "Song-Chun Zhu", "Xue Feng"], "title": "AEGIS: Automated Error Generation and Identification for Multi-Agent Systems", "comment": null, "summary": "As Multi-Agent Systems (MAS) become increasingly autonomous and complex,\nunderstanding their error modes is critical for ensuring their reliability and\nsafety. However, research in this area has been severely hampered by the lack\nof large-scale, diverse datasets with precise, ground-truth error labels. To\naddress this bottleneck, we introduce \\textbf{AEGIS}, a novel framework for\n\\textbf{A}utomated \\textbf{E}rror \\textbf{G}eneration and\n\\textbf{I}dentification for Multi-Agent \\textbf{S}ystems. By systematically\ninjecting controllable and traceable errors into initially successful\ntrajectories, we create a rich dataset of realistic failures. This is achieved\nusing a context-aware, LLM-based adaptive manipulator that performs\nsophisticated attacks like prompt injection and response corruption to induce\nspecific, predefined error modes. We demonstrate the value of our dataset by\nexploring three distinct learning paradigms for the error identification task:\nSupervised Fine-Tuning, Reinforcement Learning, and Contrastive Learning. Our\ncomprehensive experiments show that models trained on AEGIS data achieve\nsubstantial improvements across all three learning paradigms. Notably, several\nof our fine-tuned models demonstrate performance competitive with or superior\nto proprietary systems an order of magnitude larger, validating our automated\ndata generation framework as a crucial resource for developing more robust and\ninterpretable multi-agent systems. Our project website is available at\nhttps://kfq20.github.io/AEGIS-Website."}
{"id": "2509.14303", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14303", "abs": "https://arxiv.org/abs/2509.14303", "authors": ["Hao Jiang", "Zhipeng Zhang", "Yu Gao", "Zhigang Sun", "Yiru Wang", "Yuwen Heng", "Shuo Wang", "Jinhao Chai", "Zhuo Chen", "Hao Zhao", "Hao Sun", "Xi Zhang", "Anqing Jiang", "Chuan Hu"], "title": "FlowDrive: Energy Flow Field for End-to-End Autonomous Driving", "comment": null, "summary": "Recent advances in end-to-end autonomous driving leverage multi-view images\nto construct BEV representations for motion planning. In motion planning,\nautonomous vehicles need considering both hard constraints imposed by\ngeometrically occupied obstacles (e.g., vehicles, pedestrians) and soft,\nrule-based semantics with no explicit geometry (e.g., lane boundaries, traffic\npriors). However, existing end-to-end frameworks typically rely on BEV features\nlearned in an implicit manner, lacking explicit modeling of risk and guidance\npriors for safe and interpretable planning. To address this, we propose\nFlowDrive, a novel framework that introduces physically interpretable\nenergy-based flow fields-including risk potential and lane attraction fields-to\nencode semantic priors and safety cues into the BEV space. These flow-aware\nfeatures enable adaptive refinement of anchor trajectories and serve as\ninterpretable guidance for trajectory generation. Moreover, FlowDrive decouples\nmotion intent prediction from trajectory denoising via a conditional diffusion\nplanner with feature-level gating, alleviating task interference and enhancing\nmultimodal diversity. Experiments on the NAVSIM v2 benchmark demonstrate that\nFlowDrive achieves state-of-the-art performance with an EPDMS of 86.3,\nsurpassing prior baselines in both safety and planning quality. The project is\navailable at https://astrixdrive.github.io/FlowDrive.github.io/."}
{"id": "2509.14342", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14342", "abs": "https://arxiv.org/abs/2509.14342", "authors": ["Bikram Pandit", "Aayam Kumar Shrestha", "Alan Fern"], "title": "Multi-Quadruped Cooperative Object Transport: Learning Decentralized Pinch-Lift-Move", "comment": null, "summary": "We study decentralized cooperative transport using teams of N-quadruped\nrobots with arm that must pinch, lift, and move ungraspable objects through\nphysical contact alone. Unlike prior work that relies on rigid mechanical\ncoupling between robots and objects, we address the more challenging setting\nwhere mechanically independent robots must coordinate through contact forces\nalone without any communication or centralized control. To this end, we employ\na hierarchical policy architecture that separates base locomotion from arm\ncontrol, and propose a constellation reward formulation that unifies position\nand orientation tracking to enforce rigid contact behavior. The key insight is\nencouraging robots to behave as if rigidly connected to the object through\ncareful reward design and training curriculum rather than explicit mechanical\nconstraints. Our approach enables coordination through shared policy parameters\nand implicit synchronization cues - scaling to arbitrary team sizes without\nretraining. We show extensive simulation experiments to demonstrate robust\ntransport across 2-10 robots on diverse object geometries and masses, along\nwith sim2real transfer results on lightweight objects."}
{"id": "2509.14349", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14349", "abs": "https://arxiv.org/abs/2509.14349", "authors": ["Zhengyang Kris Weng", "Matthew L. Elwin", "Han Liu"], "title": "LeVR: A Modular VR Teleoperation Framework for Imitation Learning in Dexterous Manipulation", "comment": null, "summary": "We introduce LeVR, a modular software framework designed to bridge two\ncritical gaps in robotic imitation learning. First, it provides robust and\nintuitive virtual reality (VR) teleoperation for data collection using robot\narms paired with dexterous hands, addressing a common limitation in existing\nsystems. Second, it natively integrates with the powerful LeRobot imitation\nlearning (IL) framework, enabling the use of VR-based teleoperation data and\nstreamlining the demonstration collection process. To demonstrate LeVR, we\nrelease LeFranX, an open-source implementation for the Franka FER arm and\nRobotEra XHand, two widely used research platforms. LeFranX delivers a\nseamless, end-to-end workflow from data collection to real-world policy\ndeployment. We validate our system by collecting a public dataset of 100 expert\ndemonstrations and use it to successfully fine-tune state-of-the-art visuomotor\npolicies. We provide our open-source framework, implementation, and dataset to\naccelerate IL research for the robotics community."}
{"id": "2509.14353", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14353", "abs": "https://arxiv.org/abs/2509.14353", "authors": ["Dvij Kalaria", "Sudarshan S Harithas", "Pushkal Katara", "Sangkyung Kwak", "Sarthak Bhagat", "Shankar Sastry", "Srinath Sridhar", "Sai Vemprala", "Ashish Kapoor", "Jonathan Chung-Kuan Huang"], "title": "DreamControl: Human-Inspired Whole-Body Humanoid Control for Scene Interaction via Guided Diffusion", "comment": "(under submission)", "summary": "We introduce DreamControl, a novel methodology for learning autonomous\nwhole-body humanoid skills. DreamControl leverages the strengths of diffusion\nmodels and Reinforcement Learning (RL): our core innovation is the use of a\ndiffusion prior trained on human motion data, which subsequently guides an RL\npolicy in simulation to complete specific tasks of interest (e.g., opening a\ndrawer or picking up an object). We demonstrate that this human motion-informed\nprior allows RL to discover solutions unattainable by direct RL, and that\ndiffusion models inherently promote natural looking motions, aiding in\nsim-to-real transfer. We validate DreamControl's effectiveness on a Unitree G1\nrobot across a diverse set of challenging tasks involving simultaneous lower\nand upper body control and object interaction."}
{"id": "2509.14380", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14380", "abs": "https://arxiv.org/abs/2509.14380", "authors": ["Seoyeon Choi", "Kanghyun Ryu", "Jonghoon Ock", "Negar Mehr"], "title": "CRAFT: Coaching Reinforcement Learning Autonomously using Foundation Models for Multi-Robot Coordination Tasks", "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) provides a powerful framework for\nlearning coordination in multi-agent systems. However, applying MARL to\nrobotics still remains challenging due to high-dimensional continuous joint\naction spaces, complex reward design, and non-stationary transitions inherent\nto decentralized settings. On the other hand, humans learn complex coordination\nthrough staged curricula, where long-horizon behaviors are progressively built\nupon simpler skills. Motivated by this, we propose CRAFT: Coaching\nReinforcement learning Autonomously using Foundation models for multi-robot\ncoordination Tasks, a framework that leverages the reasoning capabilities of\nfoundation models to act as a \"coach\" for multi-robot coordination. CRAFT\nautomatically decomposes long-horizon coordination tasks into sequences of\nsubtasks using the planning capability of Large Language Models (LLMs). In what\nfollows, CRAFT trains each subtask using reward functions generated by LLM, and\nrefines them through a Vision Language Model (VLM)-guided reward-refinement\nloop. We evaluate CRAFT on multi-quadruped navigation and bimanual manipulation\ntasks, demonstrating its capability to learn complex coordination behaviors. In\naddition, we validate the multi-quadruped navigation policy in real hardware\nexperiments."}
{"id": "2509.14383", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14383", "abs": "https://arxiv.org/abs/2509.14383", "authors": ["Yuhong Lu"], "title": "RLBind: Adversarial-Invariant Cross-Modal Alignment for Unified Robust Embeddings", "comment": "This paper is submitted to IEEE International Conference on Robotics\n  and Automation (ICRA) 2026", "summary": "Unified multi-modal encoders that bind vision, audio, and other sensors into\na shared embedding space are attractive building blocks for robot perception\nand decision-making. However, on-robot deployment exposes the vision branch to\nadversarial and natural corruptions, making robustness a prerequisite for\nsafety. Prior defenses typically align clean and adversarial features within\nCLIP-style encoders and overlook broader cross-modal correspondence, yielding\nmodest gains and often degrading zero-shot transfer. We introduce RLBind, a\ntwo-stage adversarial-invariant cross-modal alignment framework for robust\nunified embeddings. Stage 1 performs unsupervised fine-tuning on\nclean-adversarial pairs to harden the visual encoder. Stage 2 leverages\ncross-modal correspondence by minimizing the discrepancy between\nclean/adversarial features and a text anchor, while enforcing class-wise\ndistributional alignment across modalities. Extensive experiments on Image,\nAudio, Thermal, and Video data show that RLBind consistently outperforms the\nLanguageBind backbone and standard fine-tuning baselines in both clean accuracy\nand norm-bounded adversarial robustness. By improving resilience without\nsacrificing generalization, RLBind provides a practical path toward safer\nmulti-sensor perception stacks for embodied robots in navigation, manipulation,\nand other autonomy settings."}
{"id": "2509.14412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14412", "abs": "https://arxiv.org/abs/2509.14412", "authors": ["Artem Lykov", "Oleg Kobzarev", "Dzmitry Tsetserukou"], "title": "GestOS: Advanced Hand Gesture Interpretation via Large Language Models to control Any Type of Robot", "comment": null, "summary": "We present GestOS, a gesture-based operating system for high-level control of\nheterogeneous robot teams. Unlike prior systems that map gestures to fixed\ncommands or single-agent actions, GestOS interprets hand gestures semantically\nand dynamically distributes tasks across multiple robots based on their\ncapabilities, current state, and supported instruction sets. The system\ncombines lightweight visual perception with large language model (LLM)\nreasoning: hand poses are converted into structured textual descriptions, which\nthe LLM uses to infer intent and generate robot-specific commands. A robot\nselection module ensures that each gesture-triggered task is matched to the\nmost suitable agent in real time. This architecture enables context-aware,\nadaptive control without requiring explicit user specification of targets or\ncommands. By advancing gesture interaction from recognition to intelligent\norchestration, GestOS supports scalable, flexible, and user-friendly\ncollaboration with robotic systems in dynamic environments."}
{"id": "2509.14421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14421", "abs": "https://arxiv.org/abs/2509.14421", "authors": ["Dario Tscholl", "Yashwanth Nakka", "Brian Gunter"], "title": "Perception-Integrated Safety Critical Control via Analytic Collision Cone Barrier Functions on 3D Gaussian Splatting", "comment": "Preprint for IEEE L-CSS/ACC", "summary": "We present a perception-driven safety filter that converts each 3D Gaussian\nSplat (3DGS) into a closed-form forward collision cone, which in turn yields a\nfirst-order control barrier function (CBF) embedded within a quadratic program\n(QP). By exploiting the analytic geometry of splats, our formulation provides a\ncontinuous, closed-form representation of collision constraints that is both\nsimple and computationally efficient. Unlike distance-based CBFs, which tend to\nactivate reactively only when an obstacle is already close, our collision-cone\nCBF activates proactively, allowing the robot to adjust earlier and thereby\nproduce smoother and safer avoidance maneuvers at lower computational cost. We\nvalidate the method on a large synthetic scene with approximately 170k splats,\nwhere our filter reduces planning time by a factor of 3 and significantly\ndecreased trajectory jerk compared to a state-of-the-art 3DGS planner, while\nmaintaining the same level of safety. The approach is entirely analytic,\nrequires no high-order CBF extensions (HOCBFs), and generalizes naturally to\nrobots with physical extent through a principled Minkowski-sum inflation of the\nsplats. These properties make the method broadly applicable to real-time\nnavigation in cluttered, perception-derived extreme environments, including\nspace robotics and satellite systems."}
{"id": "2509.14431", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14431", "abs": "https://arxiv.org/abs/2509.14431", "authors": ["Keqin Wang", "Tao Zhong", "David Chang", "Christine Allen-Blanchette"], "title": "Local-Canonicalization Equivariant Graph Neural Networks for Sample-Efficient and Generalizable Swarm Robot Control", "comment": "8 pages, 8 figures", "summary": "Multi-agent reinforcement learning (MARL) has emerged as a powerful paradigm\nfor coordinating swarms of agents in complex decision-making, yet major\nchallenges remain. In competitive settings such as pursuer-evader tasks,\nsimultaneous adaptation can destabilize training; non-kinetic countermeasures\noften fail under adverse conditions; and policies trained in one configuration\nrarely generalize to environments with a different number of agents. To address\nthese issues, we propose the Local-Canonicalization Equivariant Graph Neural\nNetworks (LEGO) framework, which integrates seamlessly with popular MARL\nalgorithms such as MAPPO. LEGO employs graph neural networks to capture\npermutation equivariance and generalization to different agent numbers,\ncanonicalization to enforce E(n)-equivariance, and heterogeneous\nrepresentations to encode role-specific inductive biases. Experiments on\ncooperative and competitive swarm benchmarks show that LEGO outperforms strong\nbaselines and improves generalization. In real-world experiments, LEGO\ndemonstrates robustness to varying team sizes and agent failure."}
{"id": "2509.14453", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14453", "abs": "https://arxiv.org/abs/2509.14453", "authors": ["Gokul Puthumanaillam", "Ram Padmanabhan", "Jose Fuentes", "Nicole Cruz", "Paulo Padrao", "Ruben Hernandez", "Hao Jiang", "William Schafer", "Leonardo Bobadilla", "Melkior Ornik"], "title": "Online Learning of Deceptive Policies under Intermittent Observation", "comment": null, "summary": "In supervisory control settings, autonomous systems are not monitored\ncontinuously. Instead, monitoring often occurs at sporadic intervals within\nknown bounds. We study the problem of deception, where an agent pursues a\nprivate objective while remaining plausibly compliant with a supervisor's\nreference policy when observations occur. Motivated by the behavior of real,\nhuman supervisors, we situate the problem within Theory of Mind: the\nrepresentation of what an observer believes and expects to see. We show that\nTheory of Mind can be repurposed to steer online reinforcement learning (RL)\ntoward such deceptive behavior. We model the supervisor's expectations and\ndistill from them a single, calibrated scalar -- the expected evidence of\ndeviation if an observation were to happen now. This scalar combines how unlike\nthe reference and current action distributions appear, with the agent's belief\nthat an observation is imminent. Injected as a state-dependent weight into a\nKL-regularized policy improvement step within an online RL loop, this scalar\ninforms a closed-form update that smoothly trades off self-interest and\ncompliance, thus sidestepping hand-crafted or heuristic policies. In\nreal-world, real-time hardware experiments on marine (ASV) and aerial (UAV)\nnavigation, our ToM-guided RL runs online, achieves high return and success\nwith observed-trace evidence calibrated to the supervisor's expectations."}
{"id": "2509.14460", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14460", "abs": "https://arxiv.org/abs/2509.14460", "authors": ["Abhiroop Ajith", "Constantinos Chamzas"], "title": "Learning Discrete Abstractions for Visual Rearrangement Tasks Using Vision-Guided Graph Coloring", "comment": null, "summary": "Learning abstractions directly from data is a core challenge in robotics.\nHumans naturally operate at an abstract level, reasoning over high-level\nsubgoals while delegating execution to low-level motor skills -- an ability\nthat enables efficient problem solving in complex environments. In robotics,\nabstractions and hierarchical reasoning have long been central to planning, yet\nthey are typically hand-engineered, demanding significant human effort and\nlimiting scalability. Automating the discovery of useful abstractions directly\nfrom visual data would make planning frameworks more scalable and more\napplicable to real-world robotic domains. In this work, we focus on\nrearrangement tasks where the state is represented with raw images, and propose\na method to induce discrete, graph-structured abstractions by combining\nstructural constraints with an attention-guided visual distance. Our approach\nleverages the inherent bipartite structure of rearrangement problems,\nintegrating structural constraints and visual embeddings into a unified\nframework. This enables the autonomous discovery of abstractions from vision\nalone, which can subsequently support high-level planning. We evaluate our\nmethod on two rearrangement tasks in simulation and show that it consistently\nidentifies meaningful abstractions that facilitate effective planning and\noutperform existing approaches."}
{"id": "2509.14510", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14510", "abs": "https://arxiv.org/abs/2509.14510", "authors": ["Sandra Q. Liu", "Yuxiang Ma", "Edward H. Adelson"], "title": "Object Recognition and Force Estimation with the GelSight Baby Fin Ray", "comment": "Presented at CoRL 2023 as part of the workshop, \"Learning for Soft\n  Robots: Hard Challenges for Soft Systems\" (website:\n  https://sites.google.com/view/corl-2023-soft-robots-ws)", "summary": "Recent advances in soft robotic hands and tactile sensing have enabled both\nto perform an increasing number of complex tasks with the aid of machine\nlearning. In particular, we presented the GelSight Baby Fin Ray in our previous\nwork, which integrates a camera with a soft, compliant Fin Ray structure.\nCamera-based tactile sensing gives the GelSight Baby Fin Ray the ability to\ncapture rich contact information like forces, object geometries, and textures.\nMoreover, our previous work showed that the GelSight Baby Fin Ray can dig\nthrough clutter, and classify in-shell nuts. To further examine the potential\nof the GelSight Baby Fin Ray, we leverage learning to distinguish nut-in-shell\ntextures and to perform force and position estimation. We implement ablation\nstudies with popular neural network structures, including ResNet50, GoogLeNet,\nand 3- and 5-layer convolutional neural network (CNN) structures. We conclude\nthat machine learning is a promising technique to extract useful information\nfrom high-resolution tactile images and empower soft robotics to better\nunderstand and interact with the environments."}
{"id": "2509.14516", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14516", "abs": "https://arxiv.org/abs/2509.14516", "authors": ["Adam D. Hines", "Alejandro Fontan", "Michael Milford", "Tobias Fischer"], "title": "Event-LAB: Towards Standardized Evaluation of Neuromorphic Localization Methods", "comment": "8 pages, 6 figures, under review", "summary": "Event-based localization research and datasets are a rapidly growing area of\ninterest, with a tenfold increase in the cumulative total number of published\npapers on this topic over the past 10 years. Whilst the rapid expansion in the\nfield is exciting, it brings with it an associated challenge: a growth in the\nvariety of required code and package dependencies as well as data formats,\nmaking comparisons difficult and cumbersome for researchers to implement\nreliably. To address this challenge, we present Event-LAB: a new and unified\nframework for running several event-based localization methodologies across\nmultiple datasets. Event-LAB is implemented using the Pixi package and\ndependency manager, that enables a single command-line installation and\ninvocation for combinations of localization methods and datasets. To\ndemonstrate the capabilities of the framework, we implement two common\nevent-based localization pipelines: Visual Place Recognition (VPR) and\nSimultaneous Localization and Mapping (SLAM). We demonstrate the ability of the\nframework to systematically visualize and analyze the results of multiple\nmethods and datasets, revealing key insights such as the association of\nparameters that control event collection counts and window sizes for frame\ngeneration to large variations in performance. The results and analysis\ndemonstrate the importance of fairly comparing methodologies with consistent\nevent image generation parameters. Our Event-LAB framework provides this\nability for the research community, by contributing a streamlined workflow for\neasily setting up multiple conditions."}
{"id": "2509.14530", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14530", "abs": "https://arxiv.org/abs/2509.14530", "authors": ["Zhenghao Fei", "Wenwu Lu", "Linsheng Hou", "Chen Peng"], "title": "Learning to Pick: A Visuomotor Policy for Clustered Strawberry Picking", "comment": null, "summary": "Strawberries naturally grow in clusters, interwoven with leaves, stems, and\nother fruits, which frequently leads to occlusion. This inherent growth habit\npresents a significant challenge for robotic picking, as traditional\npercept-plan-control systems struggle to reach fruits amid the clutter.\nEffectively picking an occluded strawberry demands dexterous manipulation to\ncarefully bypass or gently move the surrounding soft objects and precisely\naccess the ideal picking point located at the stem just above the calyx. To\naddress this challenge, we introduce a strawberry-picking robotic system that\nlearns from human demonstrations. Our system features a 4-DoF SCARA arm paired\nwith a human teleoperation interface for efficient data collection and\nleverages an End Pose Assisted Action Chunking Transformer (ACT) to develop a\nfine-grained visuomotor picking policy. Experiments under various occlusion\nscenarios demonstrate that our modified approach significantly outperforms the\ndirect implementation of ACT, underscoring its potential for practical\napplication in occluded strawberry picking."}
{"id": "2509.14531", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14531", "abs": "https://arxiv.org/abs/2509.14531", "authors": ["Haoran Xiao", "Xue Wang", "Huimin Lu", "Zhiwen Zeng", "Zirui Guo", "Ziqi Ni", "Yicong Ye", "Wei Dai"], "title": "Dual-Arm Hierarchical Planning for Laboratory Automation: Vibratory Sieve Shaker Operations", "comment": null, "summary": "This paper addresses the challenges of automating vibratory sieve shaker\noperations in a materials laboratory, focusing on three critical tasks: 1)\ndual-arm lid manipulation in 3 cm clearance spaces, 2) bimanual handover in\noverlapping workspaces, and 3) obstructed powder sample container delivery with\norientation constraints. These tasks present significant challenges, including\ninefficient sampling in narrow passages, the need for smooth trajectories to\nprevent spillage, and suboptimal paths generated by conventional methods. To\novercome these challenges, we propose a hierarchical planning framework\ncombining Prior-Guided Path Planning and Multi-Step Trajectory Optimization.\nThe former uses a finite Gaussian mixture model to improve sampling efficiency\nin narrow passages, while the latter refines paths by shortening, simplifying,\nimposing joint constraints, and B-spline smoothing. Experimental results\ndemonstrate the framework's effectiveness: planning time is reduced by up to\n80.4%, and waypoints are decreased by 89.4%. Furthermore, the system completes\nthe full vibratory sieve shaker operation workflow in a physical experiment,\nvalidating its practical applicability for complex laboratory automation."}
{"id": "2509.14548", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.14548", "abs": "https://arxiv.org/abs/2509.14548", "authors": ["Emily Sumner", "Deepak E. Gopinath", "Laporsha Dees", "Patricio Reyes Gomez", "Xiongyi Cui", "Andrew Silva", "Jean Costa", "Allison Morgan", "Mariah Schrum", "Tiffany L. Chen", "Avinash Balachandran", "Guy Rosman"], "title": "SimCoachCorpus: A naturalistic dataset with language and trajectories for embodied teaching", "comment": null, "summary": "Curated datasets are essential for training and evaluating AI approaches, but\nare often lacking in domains where language and physical action are deeply\nintertwined. In particular, few datasets capture how people acquire embodied\nskills through verbal instruction over time. To address this gap, we introduce\nSimCoachCorpus: a unique dataset of race car simulator driving that allows for\nthe investigation of rich interactive phenomena during guided and unguided\nmotor skill acquisition. In this dataset, 29 humans were asked to drive in a\nsimulator around a race track for approximately ninety minutes. Fifteen\nparticipants were given personalized one-on-one instruction from a professional\nperformance driving coach, and 14 participants drove without coaching. \\name\\\nincludes embodied features such as vehicle state and inputs, map (track\nboundaries and raceline), and cone landmarks. These are synchronized with\nconcurrent verbal coaching from a professional coach and additional feedback at\nthe end of each lap. We further provide annotations of coaching categories for\neach concurrent feedback utterance, ratings on students' compliance with\ncoaching advice, and self-reported cognitive load and emotional state of\nparticipants (gathered from surveys during the study). The dataset includes\nover 20,000 concurrent feedback utterances, over 400 terminal feedback\nutterances, and over 40 hours of vehicle driving data. Our naturalistic dataset\ncan be used for investigating motor learning dynamics, exploring linguistic\nphenomena, and training computational models of teaching. We demonstrate\napplications of this dataset for in-context learning, imitation learning, and\ntopic modeling. The dataset introduced in this work will be released publicly\nupon publication of the peer-reviewed version of this paper. Researchers\ninterested in early access may register at\nhttps://tinyurl.com/SimCoachCorpusForm."}
{"id": "2509.14564", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14564", "abs": "https://arxiv.org/abs/2509.14564", "authors": ["Takuya Kiyokawa", "Tomoki Ishikura", "Shingo Hamada", "Genichiro Matsuda", "Kensuke Harada"], "title": "Hierarchical Planning and Scheduling for Reconfigurable Multi-Robot Disassembly Systems under Structural Constraints", "comment": "6 pages, 7 figures", "summary": "This study presents a system integration approach for planning schedules,\nsequences, tasks, and motions for reconfigurable robots to automatically\ndisassemble constrained structures in a non-destructive manner. Such systems\nmust adapt their configuration and coordination to the target structure, but\nthe large and complex search space makes them prone to local optima. To address\nthis, we integrate multiple robot arms equipped with different types of tools,\ntogether with a rotary stage, into a reconfigurable setup. This flexible system\nis based on a hierarchical optimization method that generates plans meeting\nmultiple preferred conditions under mandatory requirements within a realistic\ntimeframe. The approach employs two many-objective genetic algorithms for\nsequence and task planning with motion evaluations, followed by constraint\nprogramming for scheduling. Because sequence planning has a much larger search\nspace, we introduce a chromosome initialization method tailored to constrained\nstructures to mitigate the risk of local optima. Simulation results demonstrate\nthat the proposed method effectively solves complex problems in reconfigurable\nrobotic disassembly."}
{"id": "2509.14630", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14630", "abs": "https://arxiv.org/abs/2509.14630", "authors": ["Anzhe Chen", "Yifei Yang", "Zhenjie Zhu", "Kechun Xu", "Zhongxiang Zhou", "Rong Xiong", "Yue Wang"], "title": "Toward Embodiment Equivariant Vision-Language-Action Policy", "comment": null, "summary": "Vision-language-action policies learn manipulation skills across tasks,\nenvironments and embodiments through large-scale pre-training. However, their\nability to generalize to novel robot configurations remains limited. Most\napproaches emphasize model size, dataset scale and diversity while paying less\nattention to the design of action spaces. This leads to the configuration\ngeneralization problem, which requires costly adaptation. We address this\nchallenge by formulating cross-embodiment pre-training as designing policies\nequivariant to embodiment configuration transformations. Building on this\nprinciple, we propose a framework that (i) establishes a embodiment\nequivariance theory for action space and policy design, (ii) introduces an\naction decoder that enforces configuration equivariance, and (iii) incorporates\na geometry-aware network architecture to enhance embodiment-agnostic spatial\nreasoning. Extensive experiments in both simulation and real-world settings\ndemonstrate that our approach improves pre-training effectiveness and enables\nefficient fine-tuning on novel robot embodiments. Our code is available at\nhttps://github.com/hhcaz/e2vla"}
{"id": "2509.14636", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14636", "abs": "https://arxiv.org/abs/2509.14636", "authors": ["Yufei Wei", "Wangtao Lu", "Sha Lu", "Chenxiao Hu", "Fuzhang Han", "Rong Xiong", "Yue Wang"], "title": "BEV-ODOM2: Enhanced BEV-based Monocular Visual Odometry with PV-BEV Fusion and Dense Flow Supervision for Ground Robots", "comment": null, "summary": "Bird's-Eye-View (BEV) representation offers a metric-scaled planar workspace,\nfacilitating the simplification of 6-DoF ego-motion to a more robust 3-DoF\nmodel for monocular visual odometry (MVO) in intelligent transportation\nsystems. However, existing BEV methods suffer from sparse supervision signals\nand information loss during perspective-to-BEV projection. We present\nBEV-ODOM2, an enhanced framework addressing both limitations without additional\nannotations. Our approach introduces: (1) dense BEV optical flow supervision\nconstructed from 3-DoF pose ground truth for pixel-level guidance; (2) PV-BEV\nfusion that computes correlation volumes before projection to preserve 6-DoF\nmotion cues while maintaining scale consistency. The framework employs three\nsupervision levels derived solely from pose data: dense BEV flow, 5-DoF for the\nPV branch, and final 3-DoF output. Enhanced rotation sampling further balances\ndiverse motion patterns in training. Extensive evaluation on KITTI, NCLT,\nOxford, and our newly collected ZJH-VO multi-scale dataset demonstrates\nstate-of-the-art performance, achieving 40 improvement in RTE compared to\nprevious BEV methods. The ZJH-VO dataset, covering diverse ground vehicle\nscenarios from underground parking to outdoor plazas, is publicly available to\nfacilitate future research."}
{"id": "2509.14641", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14641", "abs": "https://arxiv.org/abs/2509.14641", "authors": ["Sibaek Lee", "Jiung Yeon", "Hyeonwoo Yu"], "title": "Efficient 3D Perception on Embedded Systems via Interpolation-Free Tri-Plane Lifting and Volume Fusion", "comment": null, "summary": "Dense 3D convolutions provide high accuracy for perception but are too\ncomputationally expensive for real-time robotic systems. Existing tri-plane\nmethods rely on 2D image features with interpolation, point-wise queries, and\nimplicit MLPs, which makes them computationally heavy and unsuitable for\nembedded 3D inference. As an alternative, we propose a novel interpolation-free\ntri-plane lifting and volumetric fusion framework, that directly projects 3D\nvoxels into plane features and reconstructs a feature volume through broadcast\nand summation. This shifts nonlinearity to 2D convolutions, reducing complexity\nwhile remaining fully parallelizable. To capture global context, we add a\nlow-resolution volumetric branch fused with the lifted features through a\nlightweight integration layer, yielding a design that is both efficient and\nend-to-end GPU-accelerated. To validate the effectiveness of the proposed\nmethod, we conduct experiments on classification, completion, segmentation, and\ndetection, and we map the trade-off between efficiency and accuracy across\ntasks. Results show that classification and completion retain or improve\naccuracy, while segmentation and detection trade modest drops in accuracy for\nsignificant computational savings. On-device benchmarks on an NVIDIA Jetson\nOrin nano confirm robust real-time throughput, demonstrating the suitability of\nthe approach for embedded robotic perception."}
{"id": "2509.14687", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14687", "abs": "https://arxiv.org/abs/2509.14687", "authors": ["Cong Tai", "Zhaoyu Zheng", "Haixu Long", "Hansheng Wu", "Haodong Xiang", "Zhengbin Long", "Jun Xiong", "Rong Shi", "Shizhuang Zhang", "Gang Qiu", "He Wang", "Ruifeng Li", "Jun Huang", "Bin Chang", "Shuai Feng", "Tao Shen"], "title": "RealMirror: A Comprehensive, Open-Source Vision-Language-Action Platform for Embodied AI", "comment": null, "summary": "The emerging field of Vision-Language-Action (VLA) for humanoid robots faces\nseveral fundamental challenges, including the high cost of data acquisition,\nthe lack of a standardized benchmark, and the significant gap between\nsimulation and the real world. To overcome these obstacles, we propose\nRealMirror, a comprehensive, open-source embodied AI VLA platform. RealMirror\nbuilds an efficient, low-cost data collection, model training, and inference\nsystem that enables end-to-end VLA research without requiring a real robot. To\nfacilitate model evolution and fair comparison, we also introduce a dedicated\nVLA benchmark for humanoid robots, featuring multiple scenarios, extensive\ntrajectories, and various VLA models. Furthermore, by integrating generative\nmodels and 3D Gaussian Splatting to reconstruct realistic environments and\nrobot models, we successfully demonstrate zero-shot Sim2Real transfer, where\nmodels trained exclusively on simulation data can perform tasks on a real robot\nseamlessly, without any fine-tuning. In conclusion, with the unification of\nthese critical components, RealMirror provides a robust framework that\nsignificantly accelerates the development of VLA models for humanoid robots.\nProject page: https://terminators2025.github.io/RealMirror.github.io"}
{"id": "2509.14688", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14688", "abs": "https://arxiv.org/abs/2509.14688", "authors": ["Yue Xu", "Litao Wei", "Pengyu An", "Qingyu Zhang", "Yong-Lu Li"], "title": "exUMI: Extensible Robot Teaching System with Action-aware Task-agnostic Tactile Representation", "comment": "Accepted at CoRL 2025", "summary": "Tactile-aware robot learning faces critical challenges in data collection and\nrepresentation due to data scarcity and sparsity, and the absence of force\nfeedback in existing systems. To address these limitations, we introduce a\ntactile robot learning system with both hardware and algorithm innovations. We\npresent exUMI, an extensible data collection device that enhances the vanilla\nUMI with robust proprioception (via AR MoCap and rotary encoder), modular\nvisuo-tactile sensing, and automated calibration, achieving 100% data\nusability. Building on an efficient collection of over 1 M tactile frames, we\npropose Tactile Prediction Pretraining (TPP), a representation learning\nframework through action-aware temporal tactile prediction, capturing contact\ndynamics and mitigating tactile sparsity. Real-world experiments show that TPP\noutperforms traditional tactile imitation learning. Our work bridges the gap\nbetween human tactile intuition and robot learning through co-designed hardware\nand algorithms, offering open-source resources to advance contact-rich\nmanipulation research. Project page: https://silicx.github.io/exUMI."}
{"id": "2509.14698", "categories": ["cs.RO", "cs.NA", "math.DG", "math.GR", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.14698", "abs": "https://arxiv.org/abs/2509.14698", "authors": ["Andreas Mueller"], "title": "Wohlhart's Three-Loop Mechanism: An Overconstrained and Shaky Linkage", "comment": null, "summary": "This paper revisits a three-loop spatial linkage that was proposed in an ARK\n2004 paper by Karl Wohlhart (as extension of a two-loop linkage proposed by\nEddie Baker in 1980) and later analyzed in an ARK 2006 paper by Diez-Martinez\net. al. A local analysis shows that this linkage has a finite degree of freedom\n(DOF) 3 (and is thus overconstrained) while in its reference configuration the\ndifferential DOF is 5. It is shown that its configuration space is locally a\nsmooth manifold so that the reference configuration is not a c-space\nsingularity. It is shown that the differential DOF is locally constant, which\nmakes this linkage shaky (so that the reference configuration is not a\nsingularity). The higher-order local analysis is facilitated by the computation\nof the kinematic tangent cone as well as a local approximation of the c-space."}
{"id": "2509.14726", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14726", "abs": "https://arxiv.org/abs/2509.14726", "authors": ["Fangguo Zhao", "Xin Guan", "Shuo Li"], "title": "Rethinking Reference Trajectories in Agile Drone Racing: A Unified Reference-Free Model-Based Controller via MPPI", "comment": null, "summary": "While model-based controllers have demonstrated remarkable performance in\nautonomous drone racing, their performance is often constrained by the reliance\non pre-computed reference trajectories. Conventional approaches, such as\ntrajectory tracking, demand a dynamically feasible, full-state reference,\nwhereas contouring control relaxes this requirement to a geometric path but\nstill necessitates a reference. Recent advancements in reinforcement learning\n(RL) have revealed that many model-based controllers optimize surrogate\nobjectives, such as trajectory tracking, rather than the primary racing goal of\ndirectly maximizing progress through gates. Inspired by these findings, this\nwork introduces a reference-free method for time-optimal racing by\nincorporating this gate progress objective, derived from RL reward shaping,\ndirectly into the Model Predictive Path Integral (MPPI) formulation. The\nsampling-based nature of MPPI makes it uniquely capable of optimizing the\ndiscontinuous and non-differentiable objective in real-time. We also establish\na unified framework that leverages MPPI to systematically and fairly compare\nthree distinct objective functions with a consistent dynamics model and\nparameter set: classical trajectory tracking, contouring control, and the\nproposed gate progress objective. We compare the performance of these three\nobjectives when solved via both MPPI and a traditional gradient-based solver.\nOur results demonstrate that the proposed reference-free approach achieves\ncompetitive racing performance, rivaling or exceeding reference-based methods.\nVideos are available at https://zhaofangguo.github.io/racing_mppi/"}
{"id": "2509.14748", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.14748", "abs": "https://arxiv.org/abs/2509.14748", "authors": ["Maria Ibrahim", "Alap Kshirsagar", "Dorothea Koert", "Jan Peters"], "title": "Investigating the Effect of LED Signals and Emotional Displays in Human-Robot Shared Workspaces", "comment": "8 pages", "summary": "Effective communication is essential for safety and efficiency in human-robot\ncollaboration, particularly in shared workspaces. This paper investigates the\nimpact of nonverbal communication on human-robot interaction (HRI) by\nintegrating reactive light signals and emotional displays into a robotic\nsystem. We equipped a Franka Emika Panda robot with an LED strip on its end\neffector and an animated facial display on a tablet to convey movement intent\nthrough colour-coded signals and facial expressions. We conducted a human-robot\ncollaboration experiment with 18 participants, evaluating three conditions: LED\nsignals alone, LED signals with reactive emotional displays, and LED signals\nwith pre-emptive emotional displays. We collected data through questionnaires\nand position tracking to assess anticipation of potential collisions, perceived\nclarity of communication, and task performance. The results indicate that while\nemotional displays increased the perceived interactivity of the robot, they did\nnot significantly improve collision anticipation, communication clarity, or\ntask efficiency compared to LED signals alone. These findings suggest that\nwhile emotional cues can enhance user engagement, their impact on task\nperformance in shared workspaces is limited."}
{"id": "2509.14758", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14758", "abs": "https://arxiv.org/abs/2509.14758", "authors": ["Ihab Tabbara", "Yuxuan Yang", "Ahmad Hamzeh", "Maxwell Astafyev", "Hussein Sibai"], "title": "Designing Latent Safety Filters using Pre-Trained Vision Models", "comment": null, "summary": "Ensuring safety of vision-based control systems remains a major challenge\nhindering their deployment in critical settings. Safety filters have gained\nincreased interest as effective tools for ensuring the safety of classical\ncontrol systems, but their applications in vision-based control settings have\nso far been limited. Pre-trained vision models (PVRs) have been shown to be\neffective perception backbones for control in various robotics domains. In this\npaper, we are interested in examining their effectiveness when used for\ndesigning vision-based safety filters. We use them as backbones for classifiers\ndefining failure sets, for Hamilton-Jacobi (HJ) reachability-based safety\nfilters, and for latent world models. We discuss the trade-offs between\ntraining from scratch, fine-tuning, and freezing the PVRs when training the\nmodels they are backbones for. We also evaluate whether one of the PVRs is\nsuperior across all tasks, evaluate whether learned world models or Q-functions\nare better for switching decisions to safe policies, and discuss practical\nconsiderations for deploying these PVRs on resource-constrained devices."}
{"id": "2509.14787", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14787", "abs": "https://arxiv.org/abs/2509.14787", "authors": ["Qixuan Li", "Chen Le", "Dongyue Huang", "Jincheng Yu", "Xinlei Chen"], "title": "COMPASS: Confined-space Manipulation Planning with Active Sensing Strategy", "comment": null, "summary": "Manipulation in confined and cluttered environments remains a significant\nchallenge due to partial observability and complex configuration spaces.\nEffective manipulation in such environments requires an intelligent exploration\nstrategy to safely understand the scene and search the target. In this paper,\nwe propose COMPASS, a multi-stage exploration and manipulation framework\nfeaturing a manipulation-aware sampling-based planner. First, we reduce\ncollision risks with a near-field awareness scan to build a local collision\nmap. Additionally, we employ a multi-objective utility function to find\nviewpoints that are both informative and conducive to subsequent manipulation.\nMoreover, we perform a constrained manipulation optimization strategy to\ngenerate manipulation poses that respect obstacle constraints. To\nsystematically evaluate method's performance under these difficulties, we\npropose a benchmark of confined-space exploration and manipulation containing\nfour level challenging scenarios. Compared to exploration methods designed for\nother robots and only considering information gain, our framework increases\nmanipulation success rate by 24.25% in simulations. Real-world experiments\ndemonstrate our method's capability for active sensing and manipulation in\nconfined environments."}
{"id": "2509.14816", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14816", "abs": "https://arxiv.org/abs/2509.14816", "authors": ["Humphrey Munn", "Brendan Tidd", "Peter BÃ¶hm", "Marcus Gallagher", "David Howard"], "title": "Scalable Multi-Objective Robot Reinforcement Learning through Gradient Conflict Resolution", "comment": null, "summary": "Reinforcement Learning (RL) robot controllers usually aggregate many task\nobjectives into one scalar reward. While large-scale proximal policy\noptimisation (PPO) has enabled impressive results such as robust robot\nlocomotion in the real world, many tasks still require careful reward tuning\nand are brittle to local optima. Tuning cost and sub-optimality grow with the\nnumber of objectives, limiting scalability. Modelling reward vectors and their\ntrade-offs can address these issues; however, multi-objective methods remain\nunderused in RL for robotics because of computational cost and optimisation\ndifficulty. In this work, we investigate the conflict between gradient\ncontributions for each objective that emerge from scalarising the task\nobjectives. In particular, we explicitly address the conflict between\ntask-based rewards and terms that regularise the policy towards realistic\nbehaviour. We propose GCR-PPO, a modification to actor-critic optimisation that\ndecomposes the actor update into objective-wise gradients using a multi-headed\ncritic and resolves conflicts based on the objective priority. Our methodology,\nGCR-PPO, is evaluated on the well-known IsaacLab manipulation and locomotion\nbenchmarks and additional multi-objective modifications on two related tasks.\nWe show superior scalability compared to parallel PPO (p = 0.04), without\nsignificant computational overhead. We also show higher performance with more\nconflicting tasks. GCR-PPO improves on large-scale PPO with an average\nimprovement of 9.5%, with high-conflict tasks observing a greater improvement.\nThe code is available at https://github.com/humphreymunn/GCR-PPO."}
{"id": "2509.14889", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14889", "abs": "https://arxiv.org/abs/2509.14889", "authors": ["Nan Sun", "Yongchang Li", "Chenxu Wang", "Huiying Li", "Huaping Liu"], "title": "CollabVLA: Self-Reflective Vision-Language-Action Model Dreaming Together with Human", "comment": "8 pages, 5 figures, 3 tables", "summary": "In this work, we present CollabVLA, a self-reflective vision-language-action\nframework that transforms a standard visuomotor policy into a collaborative\nassistant. CollabVLA tackles key limitations of prior VLAs, including domain\noverfitting, non-interpretable reasoning, and the high latency of auxiliary\ngenerative models, by integrating VLM-based reflective reasoning with\ndiffusion-based action generation under a mixture-of-experts design. Through a\ntwo-stage training recipe of action grounding and reflection tuning, it\nsupports explicit self-reflection and proactively solicits human guidance when\nconfronted with uncertainty or repeated failure. It cuts normalized Time by ~2x\nand Dream counts by ~4x vs. generative agents, achieving higher success rates,\nimproved interpretability, and balanced low latency compared with existing\nmethods. This work takes a pioneering step toward shifting VLAs from opaque\ncontrollers to genuinely assistive agents capable of reasoning, acting, and\ncollaborating with humans."}
{"id": "2509.14915", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14915", "abs": "https://arxiv.org/abs/2509.14915", "authors": ["Shenghai Yuan", "Jason Wai Hao Yee", "Weixiang Guo", "Zhongyuan Liu", "Thien-Minh Nguyen", "Lihua Xie"], "title": "PERAL: Perception-Aware Motion Control for Passive LiDAR Excitation in Spherical Robots", "comment": null, "summary": "Autonomous mobile robots increasingly rely on LiDAR-IMU odometry for\nnavigation and mapping, yet horizontally mounted LiDARs such as the MID360\ncapture few near-ground returns, limiting terrain awareness and degrading\nperformance in feature-scarce environments. Prior solutions - static tilt,\nactive rotation, or high-density sensors - either sacrifice horizontal\nperception or incur added actuators, cost, and power. We introduce PERAL, a\nperception-aware motion control framework for spherical robots that achieves\npassive LiDAR excitation without dedicated hardware. By modeling the coupling\nbetween internal differential-drive actuation and sensor attitude, PERAL\nsuperimposes bounded, non-periodic oscillations onto nominal goal- or\ntrajectory-tracking commands, enriching vertical scan diversity while\npreserving navigation accuracy. Implemented on a compact spherical robot, PERAL\nis validated across laboratory, corridor, and tactical environments.\nExperiments demonstrate up to 96 percent map completeness, a 27 percent\nreduction in trajectory tracking error, and robust near-ground human detection,\nall at lower weight, power, and cost compared with static tilt, active\nrotation, and fixed horizontal baselines. The design and code will be\nopen-sourced upon acceptance."}
{"id": "2509.14932", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14932", "abs": "https://arxiv.org/abs/2509.14932", "authors": ["Tobias JÃ¼lg", "Pierre Krack", "Seongjin Bien", "Yannik Blei", "Khaled Gamal", "Ken Nakahara", "Johannes Hechtl", "Roberto Calandra", "Wolfram Burgard", "Florian Walter"], "title": "Robot Control Stack: A Lean Ecosystem for Robot Learning at Scale", "comment": null, "summary": "Vision-Language-Action models (VLAs) mark a major shift in robot learning.\nThey replace specialized architectures and task-tailored components of expert\npolicies with large-scale data collection and setup-specific fine-tuning. In\nthis machine learning-focused workflow that is centered around models and\nscalable training, traditional robotics software frameworks become a\nbottleneck, while robot simulations offer only limited support for\ntransitioning from and to real-world experiments. In this work, we close this\ngap by introducing Robot Control Stack (RCS), a lean ecosystem designed from\nthe ground up to support research in robot learning with large-scale generalist\npolicies. At its core, RCS features a modular and easily extensible layered\narchitecture with a unified interface for simulated and physical robots,\nfacilitating sim-to-real transfer. Despite its minimal footprint and\ndependencies, it offers a complete feature set, enabling both real-world\nexperiments and large-scale training in simulation. Our contribution is\ntwofold: First, we introduce the architecture of RCS and explain its design\nprinciples. Second, we evaluate its usability and performance along the\ndevelopment cycle of VLA and RL policies. Our experiments also provide an\nextensive evaluation of Octo, OpenVLA, and Pi Zero on multiple robots and shed\nlight on how simulation data can improve real-world policy performance. Our\ncode, datasets, weights, and videos are available at:\nhttps://robotcontrolstack.github.io/"}
{"id": "2509.14935", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14935", "abs": "https://arxiv.org/abs/2509.14935", "authors": ["Punith Reddy Vanteddu", "Davide Gorbani", "Giuseppe L'Erario", "Hosameldin Awadalla Omer Mohamed", "Fabio Bergonti", "Daniele Pucci"], "title": "CAD-Driven Co-Design for Flight-Ready Jet-Powered Humanoids", "comment": null, "summary": "This paper presents a CAD-driven co-design framework for optimizing\njet-powered aerial humanoid robots to execute dynamically constrained\ntrajectories. Starting from the iRonCub-Mk3 model, a Design of Experiments\n(DoE) approach is used to generate 5,000 geometrically varied and mechanically\nfeasible designs by modifying limb dimensions, jet interface geometry (e.g.,\nangle and offset), and overall mass distribution. Each model is constructed\nthrough CAD assemblies to ensure structural validity and compatibility with\nsimulation tools. To reduce computational cost and enable parameter sensitivity\nanalysis, the models are clustered using K-means, with representative centroids\nselected for evaluation. A minimum-jerk trajectory is used to assess flight\nperformance, providing position and velocity references for a momentum-based\nlinearized Model Predictive Control (MPC) strategy. A multi-objective\noptimization is then conducted using the NSGA-II algorithm, jointly exploring\nthe space of design centroids and MPC gain parameters. The objectives are to\nminimize trajectory tracking error and mechanical energy expenditure. The\nframework outputs a set of flight-ready humanoid configurations with validated\ncontrol parameters, offering a structured method for selecting and implementing\nfeasible aerial humanoid designs."}
{"id": "2509.14939", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14939", "abs": "https://arxiv.org/abs/2509.14939", "authors": ["Hao Zhang", "Zhen Kan", "Weiwei Shang", "Yongduan Song"], "title": "A Novel Task-Driven Diffusion-Based Policy with Affordance Learning for Generalizable Manipulation of Articulated Objects", "comment": "Accepted by IEEE/ASME Transactions on Mechatronics", "summary": "Despite recent advances in dexterous manipulations, the manipulation of\narticulated objects and generalization across different categories remain\nsignificant challenges. To address these issues, we introduce DART, a novel\nframework that enhances a diffusion-based policy with affordance learning and\nlinear temporal logic (LTL) representations to improve the learning efficiency\nand generalizability of articulated dexterous manipulation. Specifically, DART\nleverages LTL to understand task semantics and affordance learning to identify\noptimal interaction points. The {diffusion-based policy} then generalizes these\ninteractions across various categories. Additionally, we exploit an\noptimization method based on interaction data to refine actions, overcoming the\nlimitations of traditional diffusion policies that typically rely on offline\nreinforcement learning or learning from demonstrations. Experimental results\ndemonstrate that DART outperforms most existing methods in manipulation\nability, generalization performance, transfer reasoning, and robustness. For\nmore information, visit our project website at:\nhttps://sites.google.com/view/dart0257/."}
{"id": "2509.14941", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14941", "abs": "https://arxiv.org/abs/2509.14941", "authors": ["Zongyuan Shen", "Burhanuddin Shirose", "Prasanna Sriganesh", "Bhaskar Vundurthy", "Howie Choset", "Matthew Travers"], "title": "Multi-CAP: A Multi-Robot Connectivity-Aware Hierarchical Coverage Path Planning Algorithm for Unknown Environments", "comment": null, "summary": "Efficient coordination of multiple robots for coverage of large, unknown\nenvironments is a significant challenge that involves minimizing the total\ncoverage path length while reducing inter-robot conflicts. In this paper, we\nintroduce a Multi-robot Connectivity-Aware Planner (Multi-CAP), a hierarchical\ncoverage path planning algorithm that facilitates multi-robot coordination\nthrough a novel connectivity-aware approach. The algorithm constructs and\ndynamically maintains an adjacency graph that represents the environment as a\nset of connected subareas. Critically, we make the assumption that the\nenvironment, while unknown, is bounded. This allows for incremental refinement\nof the adjacency graph online to ensure its structure represents the physical\nlayout of the space, both in observed and unobserved areas of the map as robots\nexplore the environment. We frame the task of assigning subareas to robots as a\nVehicle Routing Problem (VRP), a well-studied problem for finding optimal\nroutes for a fleet of vehicles. This is used to compute disjoint tours that\nminimize redundant travel, assigning each robot a unique, non-conflicting set\nof subareas. Each robot then executes its assigned tour, independently adapting\nits coverage strategy within each subarea to minimize path length based on\nreal-time sensor observations of the subarea. We demonstrate through\nsimulations and multi-robot hardware experiments that Multi-CAP significantly\noutperforms state-of-the-art methods in key metrics, including coverage time,\ntotal path length, and path overlap ratio. Ablation studies further validate\nthe critical role of our connectivity-aware graph and the global tour planner\nin achieving these performance gains."}
{"id": "2509.14949", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.14949", "abs": "https://arxiv.org/abs/2509.14949", "authors": ["Laura Ribeiro", "Muhammad Shaheer", "Miguel Fernandez-Cortizas", "Ali Tourani", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "Human Interaction for Collaborative Semantic SLAM using Extended Reality", "comment": "7 pages, 5 figures, 3 tables", "summary": "Semantic SLAM (Simultaneous Localization and Mapping) systems enrich robot\nmaps with structural and semantic information, enabling robots to operate more\neffectively in complex environments. However, these systems struggle in\nreal-world scenarios with occlusions, incomplete data, or ambiguous geometries,\nas they cannot fully leverage the higher-level spatial and semantic knowledge\nhumans naturally apply. We introduce HICS-SLAM, a Human-in-the-Loop semantic\nSLAM framework that uses a shared extended reality environment for real-time\ncollaboration. The system allows human operators to directly interact with and\nvisualize the robot's 3D scene graph, and add high-level semantic concepts\n(e.g., rooms or structural entities) into the mapping process. We propose a\ngraph-based semantic fusion methodology that integrates these human\ninterventions with robot perception, enabling scalable collaboration for\nenhanced situational awareness. Experimental evaluations on real-world\nconstruction site datasets demonstrate improvements in room detection accuracy,\nmap precision, and semantic completeness compared to automated baselines,\ndemonstrating both the effectiveness of the approach and its potential for\nfuture extensions."}
{"id": "2509.14954", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14954", "abs": "https://arxiv.org/abs/2509.14954", "authors": ["Xingchen Xu", "Ao Li", "Benjamin Ward-Cherrier"], "title": "Exploratory Movement Strategies for Texture Discrimination with a Neuromorphic Tactile Sensor", "comment": "Accepted at IEEE/RSJ International Conference on Intelligent Robots\n  and Systems 2025. Please cite the proceedings version", "summary": "We propose a neuromorphic tactile sensing framework for robotic texture\nclassification that is inspired by human exploratory strategies. Our system\nutilizes the NeuroTac sensor to capture neuromorphic tactile data during a\nseries of exploratory motions. We first tested six distinct motions for texture\nclassification under fixed environment: sliding, rotating, tapping, as well as\nthe combined motions: sliding+rotating, tapping+rotating, and tapping+sliding.\nWe chose sliding and sliding+rotating as the best motions based on final\naccuracy and the sample timing length needed to reach converged accuracy. In\nthe second experiment designed to simulate complex real-world conditions, these\ntwo motions were further evaluated under varying contact depth and speeds.\nUnder these conditions, our framework attained the highest accuracy of 87.33\\%\nwith sliding+rotating while maintaining an extremely low power consumption of\nonly 8.04 mW. These results suggest that the sliding+rotating motion is the\noptimal exploratory strategy for neuromorphic tactile sensing deployment in\ntexture classification tasks and holds significant promise for enhancing\nrobotic environmental interaction."}
{"id": "2509.14967", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.14967", "abs": "https://arxiv.org/abs/2509.14967", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "title": "Affordance-Based Disambiguation of Surgical Instructions for Collaborative Robot-Assisted Surgery", "comment": "To be presented at the 1st Workshop on Intelligent Cobodied\n  Assistance and Robotic Empowerment (iCARE). 2025 Conference on Robot Learning\n  (CoRL)", "summary": "Effective human-robot collaboration in surgery is affected by the inherent\nambiguity of verbal communication. This paper presents a framework for a\nrobotic surgical assistant that interprets and disambiguates verbal\ninstructions from a surgeon by grounding them in the visual context of the\noperating field. The system employs a two-level affordance-based reasoning\nprocess that first analyzes the surgical scene using a multimodal\nvision-language model and then reasons about the instruction using a knowledge\nbase of tool capabilities. To ensure patient safety, a dual-set conformal\nprediction method is used to provide a statistically rigorous confidence\nmeasure for robot decisions, allowing it to identify and flag ambiguous\ncommands. We evaluated our framework on a curated dataset of ambiguous surgical\nrequests from cholecystectomy videos, demonstrating a general disambiguation\nrate of 60% and presenting a method for safer human-robot interaction in the\noperating room."}
{"id": "2509.14978", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14978", "abs": "https://arxiv.org/abs/2509.14978", "authors": ["Yifan Zhai", "Rudolf Reiter", "Davide Scaramuzza"], "title": "PA-MPPI: Perception-Aware Model Predictive Path Integral Control for Quadrotor Navigation in Unknown Environments", "comment": null, "summary": "Quadrotor navigation in unknown environments is critical for practical\nmissions such as search-and-rescue. Solving it requires addressing three key\nchallenges: the non-convexity of free space due to obstacles,\nquadrotor-specific dynamics and objectives, and the need for exploration of\nunknown regions to find a path to the goal. Recently, the Model Predictive Path\nIntegral (MPPI) method has emerged as a promising solution that solves the\nfirst two challenges. By leveraging sampling-based optimization, it can\neffectively handle non-convex free space while directly optimizing over the\nfull quadrotor dynamics, enabling the inclusion of quadrotor-specific costs\nsuch as energy consumption. However, its performance in unknown environments is\nlimited, as it lacks the ability to explore unknown regions when blocked by\nlarge obstacles. To solve this issue, we introduce Perception-Aware MPPI\n(PA-MPPI). Here, perception-awareness is defined as adapting the trajectory\nonline based on perception objectives. Specifically, when the goal is occluded,\nPA-MPPI's perception cost biases trajectories that can perceive unknown\nregions. This expands the mapped traversable space and increases the likelihood\nof finding alternative paths to the goal. Through hardware experiments, we\ndemonstrate that PA-MPPI, running at 50 Hz with our efficient perception and\nmapping module, performs up to 100% better than the baseline in our challenging\nsettings where the state-of-the-art MPPI fails. In addition, we demonstrate\nthat PA-MPPI can be used as a safe and robust action policy for navigation\nfoundation models, which often provide goal poses that are not directly\nreachable."}
{"id": "2509.14980", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14980", "abs": "https://arxiv.org/abs/2509.14980", "authors": ["Ju Dong", "Lei Zhang", "Liding Zhang", "Yao Ling", "Yu Fu", "Kaixin Bai", "ZoltÃ¡n-Csaba MÃ¡rton", "Zhenshan Bing", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "title": "M4Diffuser: Multi-View Diffusion Policy with Manipulability-Aware Control for Robust Mobile Manipulation", "comment": "Project page: https://sites.google.com/view/m4diffuser, 10 pages, 9\n  figures", "summary": "Mobile manipulation requires the coordinated control of a mobile base and a\nrobotic arm while simultaneously perceiving both global scene context and\nfine-grained object details. Existing single-view approaches often fail in\nunstructured environments due to limited fields of view, exploration, and\ngeneralization abilities. Moreover, classical controllers, although stable,\nstruggle with efficiency and manipulability near singularities. To address\nthese challenges, we propose M4Diffuser, a hybrid framework that integrates a\nMulti-View Diffusion Policy with a novel Reduced and Manipulability-aware QP\n(ReM-QP) controller for mobile manipulation. The diffusion policy leverages\nproprioceptive states and complementary camera perspectives with both\nclose-range object details and global scene context to generate task-relevant\nend-effector goals in the world frame. These high-level goals are then executed\nby the ReM-QP controller, which eliminates slack variables for computational\nefficiency and incorporates manipulability-aware preferences for robustness\nnear singularities. Comprehensive experiments in simulation and real-world\nenvironments show that M4Diffuser achieves 7 to 56 percent higher success rates\nand reduces collisions by 3 to 31 percent over baselines. Our approach\ndemonstrates robust performance for smooth whole-body coordination, and strong\ngeneralization to unseen tasks, paving the way for reliable mobile manipulation\nin unstructured environments. Details of the demo and supplemental material are\navailable on our project website https://sites.google.com/view/m4diffuser."}
{"id": "2509.14984", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14984", "abs": "https://arxiv.org/abs/2509.14984", "authors": ["JoÃ£o DamiÃ£o Almeida", "Egidio Falotico", "Cecilia Laschi", "JosÃ© Santos-Victor"], "title": "The Role of Touch: Towards Optimal Tactile Sensing Distribution in Anthropomorphic Hands for Dexterous In-Hand Manipulation", "comment": null, "summary": "In-hand manipulation tasks, particularly in human-inspired robotic systems,\nmust rely on distributed tactile sensing to achieve precise control across a\nwide variety of tasks. However, the optimal configuration of this network of\nsensors is a complex problem, and while the fingertips are a common choice for\nplacing sensors, the contribution of tactile information from other regions of\nthe hand is often overlooked. This work investigates the impact of tactile\nfeedback from various regions of the fingers and palm in performing in-hand\nobject reorientation tasks. We analyze how sensory feedback from different\nparts of the hand influences the robustness of deep reinforcement learning\ncontrol policies and investigate the relationship between object\ncharacteristics and optimal sensor placement. We identify which tactile sensing\nconfigurations contribute to improving the efficiency and accuracy of\nmanipulation. Our results provide valuable insights for the design and use of\nanthropomorphic end-effectors with enhanced manipulation capabilities."}
{"id": "2509.14992", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14992", "abs": "https://arxiv.org/abs/2509.14992", "authors": ["Yifan Zhai", "Lorenzo Terenzi", "Patrick Frey", "Diego Garcia Soto", "Pascal Egli", "Marco Hutter"], "title": "ExT: Towards Scalable Autonomous Excavation via Large-Scale Multi-Task Pretraining and Fine-Tuning", "comment": null, "summary": "Scaling up the deployment of autonomous excavators is of great economic and\nsocietal importance. Yet it remains a challenging problem, as effective systems\nmust robustly handle unseen worksite conditions and new hardware\nconfigurations. Current state-of-the-art approaches rely on highly engineered,\ntask-specific controllers, which require extensive manual tuning for each new\nscenario. In contrast, recent advances in large-scale pretrained models have\nshown remarkable adaptability across tasks and embodiments in domains such as\nmanipulation and navigation, but their applicability to heavy construction\nmachinery remains largely unexplored. In this work, we introduce ExT, a unified\nopen-source framework for large-scale demonstration collection, pretraining,\nand fine-tuning of multitask excavation policies. ExT policies are first\ntrained on large-scale demonstrations collected from a mix of experts, then\nfine-tuned either with supervised fine-tuning (SFT) or reinforcement learning\nfine-tuning (RLFT) to specialize to new tasks or operating conditions. Through\nboth simulation and real-world experiments, we show that pretrained ExT\npolicies can execute complete excavation cycles with centimeter-level accuracy,\nsuccessfully transferring from simulation to real machine with performance\ncomparable to specialized single-task controllers. Furthermore, in simulation,\nwe demonstrate that ExT's fine-tuning pipelines allow rapid adaptation to new\ntasks, out-of-distribution conditions, and machine configurations, while\nmaintaining strong performance on previously learned tasks. These results\nhighlight the potential of ExT to serve as a foundation for scalable and\ngeneralizable autonomous excavation."}
{"id": "2509.14999", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.14999", "abs": "https://arxiv.org/abs/2509.14999", "authors": ["Haoxuan Jiang", "Peicong Qian", "Yusen Xie", "Linwei Zheng", "Xiaocong Li", "Ming Liu", "Jun Ma"], "title": "Semantic-LiDAR-Inertial-Wheel Odometry Fusion for Robust Localization in Large-Scale Dynamic Environments", "comment": null, "summary": "Reliable, drift-free global localization presents significant challenges yet\nremains crucial for autonomous navigation in large-scale dynamic environments.\nIn this paper, we introduce a tightly-coupled Semantic-LiDAR-Inertial-Wheel\nOdometry fusion framework, which is specifically designed to provide\nhigh-precision state estimation and robust localization in large-scale dynamic\nenvironments. Our framework leverages an efficient semantic-voxel map\nrepresentation and employs an improved scan matching algorithm, which utilizes\nglobal semantic information to significantly reduce long-term trajectory drift.\nFurthermore, it seamlessly fuses data from LiDAR, IMU, and wheel odometry using\na tightly-coupled multi-sensor fusion Iterative Error-State Kalman Filter\n(iESKF). This ensures reliable localization without experiencing abnormal\ndrift. Moreover, to tackle the challenges posed by terrain variations and\ndynamic movements, we introduce a 3D adaptive scaling strategy that allows for\nflexible adjustments to wheel odometry measurement weights, thereby enhancing\nlocalization precision. This study presents extensive real-world experiments\nconducted in a one-million-square-meter automated port, encompassing 3,575\nhours of operational data from 35 Intelligent Guided Vehicles (IGVs). The\nresults consistently demonstrate that our system outperforms state-of-the-art\nLiDAR-based localization methods in large-scale dynamic environments,\nhighlighting the framework's reliability and practical value."}
{"id": "2509.15052", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.15052", "abs": "https://arxiv.org/abs/2509.15052", "authors": ["Walker Gosrich", "Saurav Agarwal", "Kashish Garg", "Siddharth Mayya", "Matthew Malencia", "Mark Yim", "Vijay Kumar"], "title": "Online Multi-Robot Coordination and Cooperation with Task Precedence Relationships", "comment": "20 pages, 19 figures, Accepted to IEEE Transactions on Robotics\n  (TR-O) August 2025", "summary": "We propose a new formulation for the multi-robot task allocation problem that\nincorporates (a) complex precedence relationships between tasks, (b) efficient\nintra-task coordination, and (c) cooperation through the formation of robot\ncoalitions. A task graph specifies the tasks and their relationships, and a set\nof reward functions models the effects of coalition size and preceding task\nperformance. Maximizing task rewards is NP-hard; hence, we propose network\nflow-based algorithms to approximate solutions efficiently. A novel online\nalgorithm performs iterative re-allocation, providing robustness to task\nfailures and model inaccuracies to achieve higher performance than offline\napproaches. We comprehensively evaluate the algorithms in a testbed with random\nmissions and reward functions and compare them to a mixed-integer solver and a\ngreedy heuristic. Additionally, we validate the overall approach in an advanced\nsimulator, modeling reward functions based on realistic physical phenomena and\nexecuting the tasks with realistic robot dynamics. Results establish efficacy\nin modeling complex missions and efficiency in generating high-fidelity task\nplans while leveraging task relationships."}
{"id": "2509.15061", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.15061", "abs": "https://arxiv.org/abs/2509.15061", "authors": ["Xingyao Lin", "Xinghao Zhu", "Tianyi Lu", "Sicheng Xie", "Hui Zhang", "Xipeng Qiu", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "Ask-to-Clarify: Resolving Instruction Ambiguity through Multi-turn Dialogue", "comment": "9 pages, 4 figures", "summary": "The ultimate goal of embodied agents is to create collaborators that can\ninteract with humans, not mere executors that passively follow instructions.\nThis requires agents to communicate, coordinate, and adapt their actions based\non human feedback. Recently, advances in VLAs have offered a path toward this\ngoal. However, most current VLA-based embodied agents operate in a one-way\nmode: they receive an instruction and execute it without feedback. This\napproach fails in real-world scenarios where instructions are often ambiguous.\nIn this paper, we address this problem with the Ask-to-Clarify framework. Our\nframework first resolves ambiguous instructions by asking questions in a\nmulti-turn dialogue. Then it generates low-level actions end-to-end.\nSpecifically, the Ask-to-Clarify framework consists of two components, one VLM\nfor collaboration and one diffusion for action. We also introduce a connection\nmodule that generates conditions for the diffusion based on the output of the\nVLM. This module adjusts the observation by instructions to create reliable\nconditions. We train our framework with a two-stage knowledge-insulation\nstrategy. First, we fine-tune the collaboration component using\nambiguity-solving dialogue data to handle ambiguity. Then, we integrate the\naction component while freezing the collaboration one. This preserves the\ninteraction abilities while fine-tuning the diffusion to generate actions. The\ntraining strategy guarantees our framework can first ask questions, then\ngenerate actions. During inference, a signal detector functions as a router\nthat helps our framework switch between asking questions and taking actions. We\nevaluate the Ask-to-Clarify framework in 8 real-world tasks, where it\noutperforms existing state-of-the-art VLAs. The results suggest that our\nproposed framework, along with the training strategy, provides a path toward\ncollaborative embodied agents."}
{"id": "2509.15062", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.15062", "abs": "https://arxiv.org/abs/2509.15062", "authors": ["Tianxin Hu", "Weixiang Guo", "Ruimeng Liu", "Xinhang Xu", "Rui Qian", "Jinyu Chen", "Shenghai Yuan", "Lihua Xie"], "title": "Energy-Constrained Navigation for Planetary Rovers under Hybrid RTG-Solar Power", "comment": null, "summary": "Future planetary exploration rovers must operate for extended durations on\nhybrid power inputs that combine steady radioisotope thermoelectric generator\n(RTG) output with variable solar photovoltaic (PV) availability. While\nenergy-aware planning has been studied for aerial and underwater robots under\nbattery limits, few works for ground rovers explicitly model power flow or\nenforce instantaneous power constraints. Classical terrain-aware planners\nemphasize slope or traversability, and trajectory optimization methods\ntypically focus on geometric smoothness and dynamic feasibility, neglecting\nenergy feasibility. We present an energy-constrained trajectory planning\nframework that explicitly integrates physics-based models of translational,\nrotational, and resistive power with baseline subsystem loads, under hybrid\nRTG-solar input. By incorporating both cumulative energy budgets and\ninstantaneous power constraints into SE(2)-based polynomial trajectory\noptimization, the method ensures trajectories that are simultaneously smooth,\ndynamically feasible, and power-compliant. Simulation results on lunar-like\nterrain show that our planner generates trajectories with peak power within\n0.55 percent of the prescribed limit, while existing methods exceed limits by\nover 17 percent. This demonstrates a principled and practical approach to\nenergy-aware autonomy for long-duration planetary missions."}
{"id": "2509.15153", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15153", "abs": "https://arxiv.org/abs/2509.15153", "authors": ["Yating Lin", "Zixuan Huang", "Fan Yang", "Dmitry Berenson"], "title": "AnoF-Diff: One-Step Diffusion-Based Anomaly Detection for Forceful Tool Use", "comment": null, "summary": "Multivariate time-series anomaly detection, which is critical for identifying\nunexpected events, has been explored in the field of machine learning for\nseveral decades. However, directly applying these methods to data from forceful\ntool use tasks is challenging because streaming sensor data in the real world\ntends to be inherently noisy, exhibits non-stationary behavior, and varies\nacross different tasks and tools. To address these challenges, we propose a\nmethod, AnoF-Diff, based on the diffusion model to extract force-torque\nfeatures from time-series data and use force-torque features to detect\nanomalies. We compare our method with other state-of-the-art methods in terms\nof F1-score and Area Under the Receiver Operating Characteristic curve (AUROC)\non four forceful tool-use tasks, demonstrating that our method has better\nperformance and is more robust to a noisy dataset. We also propose the method\nof parallel anomaly score evaluation based on one-step diffusion and\ndemonstrate how our method can be used for online anomaly detection in several\nforceful tool use experiments."}
{"id": "2509.15180", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.15180", "abs": "https://arxiv.org/abs/2509.15180", "authors": ["Yitian Gao", "Lucas Chen", "Priyanka Bhovad", "Sicheng Wang", "Zachary Kingston", "Laura H. Blumenschein"], "title": "Parallel Simulation of Contact and Actuation for Soft Growing Robots", "comment": "26 pages, 9 figures, 1 table. Under review", "summary": "Soft growing robots, commonly referred to as vine robots, have demonstrated\nremarkable ability to interact safely and robustly with unstructured and\ndynamic environments. It is therefore natural to exploit contact with the\nenvironment for planning and design optimization tasks. Previous research has\nfocused on planning under contact for passively deforming robots with\npre-formed bends. However, adding active steering to these soft growing robots\nis necessary for successful navigation in more complex environments. To this\nend, we develop a unified modeling framework that integrates vine robot growth,\nbending, actuation, and obstacle contact. We extend the beam moment model to\ninclude the effects of actuation on kinematics under growth and then use these\nmodels to develop a fast parallel simulation framework. We validate our model\nand simulator with real robot experiments. To showcase the capabilities of our\nframework, we apply our model in a design optimization task to find designs for\nvine robots navigating through cluttered environments, identifying designs that\nminimize the number of required actuators by exploiting environmental contacts.\nWe show the robustness of the designs to environmental and manufacturing\nuncertainties. Finally, we fabricate an optimized design and successfully\ndeploy it in an obstacle-rich environment."}
