<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 54]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey](https://arxiv.org/abs/2509.10570)
*Wei Dai,Shengen Wu,Wei Wu,Zhenhao Wang,Sisuo Lyu,Haicheng Liao,Limin Yu,Weiping Ding,Runwei Guan,Yutao Yue*

Main category: cs.RO

TL;DR: 本调查论文系统回顾了大型基础模型（LFMs）在轨迹预测领域的应用进展，重点分析了LLMs和MLLMs如何通过语言和场景语义整合提升预测的可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在轨迹预测中存在可解释性差、依赖大规模标注数据、长尾场景泛化能力弱等局限性，而大型基础模型的兴起为解决这些问题提供了新的研究范式。

Method: 论文提出了三种核心方法：轨迹-语言映射、多模态融合和基于约束的推理，涵盖了车辆和行人的预测任务、评估指标和数据集分析。

Result: LFMs通过整合语言和场景语义，实现了可解释的上下文推理，显著提升了复杂环境下的预测安全性和泛化能力。

Conclusion: 论文讨论了计算延迟、数据稀缺性和现实世界鲁棒性等关键挑战，并提出了低延迟推理、因果感知建模和运动基础模型等未来研究方向。

Abstract: Trajectory prediction serves as a critical functionality in autonomous
driving, enabling the anticipation of future motion paths for traffic
participants such as vehicles and pedestrians, which is essential for driving
safety. Although conventional deep learning methods have improved accuracy,
they remain hindered by inherent limitations, including lack of
interpretability, heavy reliance on large-scale annotated data, and weak
generalization in long-tail scenarios. The rise of Large Foundation Models
(LFMs) is transforming the research paradigm of trajectory prediction. This
survey offers a systematic review of recent advances in LFMs, particularly
Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for
trajectory prediction. By integrating linguistic and scene semantics, LFMs
facilitate interpretable contextual reasoning, significantly enhancing
prediction safety and generalization in complex environments. The article
highlights three core methodologies: trajectory-language mapping, multimodal
fusion, and constraint-based reasoning. It covers prediction tasks for both
vehicles and pedestrians, evaluation metrics, and dataset analyses. Key
challenges such as computational latency, data scarcity, and real-world
robustness are discussed, along with future research directions including
low-latency inference, causality-aware modeling, and motion foundation models.

</details>


### [2] [STL-Based Motion Planning and Uncertainty-Aware Risk Analysis for Human-Robot Collaboration with a Multi-Rotor Aerial Vehicle](https://arxiv.org/abs/2509.10692)
*Giuseppe Silano,Amr Afifi,Martin Saska,Antonio Franchi*

Main category: cs.RO

TL;DR: 提出了一种基于信号时序逻辑的多旋翼飞行器运动规划和风险分析方法，用于增强人机协作，重点关注安全、时序和人体工程学要求。


<details>
  <summary>Details</summary>
Motivation: 解决人机协作中多旋翼飞行器的安全运动规划问题，特别是在电力线路维护等复杂环境下，需要同时考虑动态约束、人类偏好和不确定性因素。

Method: 使用信号时序逻辑(STL)编码任务目标，构建优化框架生成动态可行轨迹，采用平滑近似和梯度技术处理非线性非凸问题，集成不确定性风险分析和事件触发重规划策略。

Result: 通过MATLAB和Gazebo仿真验证，在电力线路维护场景的对象交接任务中表现出安全、高效和鲁棒的人机协作性能。

Conclusion: 该方法能够有效处理复杂人机协作任务中的运动规划、风险分析和实时响应需求，为多旋翼飞行器在工业应用中的安全部署提供了可行方案。

Abstract: This paper presents a novel approach to motion planning and risk analysis for
enhancing human-robot collaboration using a Multi-Rotor Aerial Vehicle (MRAV).
The proposed method uses Signal Temporal Logic (STL) to encode key mission
objectives, such as safety, timing, and human preferences, with a strong focus
on ergonomics and comfort. An optimization framework generates dynamically
feasible trajectories while considering the MRAV's physical constraints. Given
the nonlinear and non-convex nature of the problem, smooth approximations and
gradient-based techniques assist in handling the problem's computational
complexity. Additionally, an uncertainty-aware risk analysis is incorporated to
assess potential deviations from the mission specifications, providing insights
into the likelihood of mission success under uncertain conditions. Further, an
event-triggered replanning strategy is implemented to respond to unforeseen
events and external disturbances. The approach is validated through MATLAB and
Gazebo simulations, using an object handover task in a mock-up environment
inspired by power line maintenance scenarios. The results highlight the
method's effectiveness in achieving safe, efficient, and resilient human-robot
collaboration.

</details>


### [3] [A Survey on LiDAR-based Autonomous Aerial Vehicles](https://arxiv.org/abs/2509.10730)
*Yunfan Ren,Yixi Cai,Haotian Li,Nan Chen,Fangcheng Zhu,Longji Yin,Fanze Kong,Rundong Li,Fu Zhang*

Main category: cs.RO

TL;DR: 本综述全面回顾了基于LiDAR的自主无人机系统的最新进展，涵盖设计、感知、规划和控制策略，分析了LiDAR技术在GPS拒止环境中的导航优势及应用前景。


<details>
  <summary>Details</summary>
Motivation: LiDAR技术在过去十年中成为实现高速、敏捷和可靠无人机导航的关键使能技术，特别是在GPS拒止环境中。论文旨在综合最新发展，为研究人员和从业者提供有价值的资源。

Method: 通过系统性的文献综述方法，考察LiDAR传感器的演进、软件组件（包括状态估计和地图构建的感知技术）、轨迹规划和控制方法，以及实际应用案例。

Result: LiDAR与无人机的集成显著增强了其自主性，使其能够在多样化和挑战性环境中执行复杂任务，应用范围从工业操作到支持不同空中平台和无人机群部署。

Conclusion: 论文讨论了现有挑战并提出了未来研究方向，以推进基于LiDAR的无人机技术并增强多无人机协作，为LiDAR基无人机系统的发展提供了重要参考。

Abstract: This survey offers a comprehensive overview of recent advancements in
LiDAR-based autonomous Unmanned Aerial Vehicles (UAVs), covering their design,
perception, planning, and control strategies. Over the past decade, LiDAR
technology has become a crucial enabler for high-speed, agile, and reliable UAV
navigation, especially in GPS-denied environments. The paper begins by
examining the evolution of LiDAR sensors, emphasizing their unique advantages
such as high accuracy, long-range depth measurements, and robust performance
under various lighting conditions, making them particularly well-suited for UAV
applications. The integration of LiDAR with UAVs has significantly enhanced
their autonomy, enabling complex missions in diverse and challenging
environments. Subsequently, we explore essential software components, including
perception technologies for state estimation and mapping, as well as trajectory
planning and control methodologies, and discuss their adoption in LiDAR-based
UAVs. Additionally, we analyze various practical applications of the
LiDAR-based UAVs, ranging from industrial operations to supporting different
aerial platforms and UAV swarm deployments. The survey concludes by discussing
existing challenges and proposing future research directions to advance
LiDAR-based UAVs and enhance multi-UAV collaboration. By synthesizing recent
developments, this paper aims to provide a valuable resource for researchers
and practitioners working to push the boundaries of LiDAR-based UAV systems.

</details>


### [4] [Analytical Design and Development of a Modular and Intuitive Framework for Robotizing and Enhancing the Existing Endoscopic Procedures](https://arxiv.org/abs/2509.10735)
*Mohammad Rafiee Javazm,Yash Kulkarni,Jiaqi Xue,Naruhiko Ikoma,Farshid Alambeigi*

Main category: cs.RO

TL;DR: 开发了一种模块化、易安装的机电一体化框架，用于辅助内窥镜操作，包括新型夹持机构、进给机构和直观用户界面，通过数学建模优化设计参数，实验验证了性能。


<details>
  <summary>Details</summary>
Motivation: 内窥镜设备在癌症筛查中广泛应用，但手动操作对临床医生仍具有挑战性，导致工作负荷增加、疲劳和注意力分散等关键问题。

Method: 提出了包含嵌套夹头夹持机制（控制弯曲自由度）、进给机构（控制插入/撤回自由度）和直观用户界面的模块化机电框架，并引入数学建模方法进行设计参数优化选择。

Result: 通过仿真和实验研究，全面证明了所提出的数学建模和机器人框架的性能。

Conclusion: 该研究为解决内窥镜操作挑战提供了一个有效的技术解决方案，通过机械辅助系统减轻医生负担，提高操作精度和效率。

Abstract: Despite the widespread adoption of endoscopic devices for several cancer
screening procedures, manual control of these devices still remains challenging
for clinicians, leading to several critical issues such as increased workload,
fatigue, and distractions. To address these issues, in this paper, we introduce
the design and development of an intuitive, modular, and easily installable
mechatronic framework. This framework includes (i) a novel nested collet-chuck
gripping mechanism that can readily be integrated and assembled with the
existing endoscopic devices and control their bending degrees-of-freedom
(DoFs); (ii) a feeder mechanism that can control the insertion/retraction DoF
of a colonoscope, and (iii) a complementary and intuitive user interface that
enables simultaneous control of all DoFs during the procedure. To analyze the
design of the proposed mechanisms, we also introduce a mathematical modeling
approach and a design space for optimal selection of the parameters involved in
the design of gripping and feeder mechanisms. Our simulation and experimental
studies thoroughly demonstrate the performance of the proposed mathematical
modeling and robotic framework.

</details>


### [5] [FastTrack: GPU-Accelerated Tracking for Visual SLAM](https://arxiv.org/abs/2509.10757)
*Kimia Khabiri,Parsa Hosseininejad,Shishir Gopinath,Karthik Dantu,Steven Y. Ko*

Main category: cs.RO

TL;DR: 提出了一种利用GPU加速视觉惯性SLAM系统中跟踪模块的方法，通过在ORB-SLAM3中实现CUDA加速，使跟踪性能提升最高达2.8倍


<details>
  <summary>Details</summary>
Motivation: 视觉惯性SLAM系统的跟踪模块需要及时处理图像帧和IMU数据以估计位姿，如果处理不及时会导致定位质量下降或跟踪丢失，因此需要加速耗时组件

Method: 利用GPU计算能力加速跟踪中的耗时组件，包括立体特征匹配和局部地图跟踪，在ORB-SLAM3跟踪过程中使用CUDA实现

Result: 在桌面和Jetson Xavier NX板上使用EuRoC和TUM-VI数据集进行测试，立体惯性模式下跟踪性能整体提升最高达2.8倍

Conclusion: GPU加速能显著提升视觉惯性SLAM系统的跟踪性能，为实时应用提供了更好的解决方案

Abstract: The tracking module of a visual-inertial SLAM system processes incoming image
frames and IMU data to estimate the position of the frame in relation to the
map. It is important for the tracking to complete in a timely manner for each
frame to avoid poor localization or tracking loss. We therefore present a new
approach which leverages GPU computing power to accelerate time-consuming
components of tracking in order to improve its performance. These components
include stereo feature matching and local map tracking. We implement our design
inside the ORB-SLAM3 tracking process using CUDA. Our evaluation demonstrates
an overall improvement in tracking performance of up to 2.8x on a desktop and
Jetson Xavier NX board in stereo-inertial mode, using the well-known SLAM
datasets EuRoC and TUM-VI.

</details>


### [6] [RSL-RL: A Learning Library for Robotics Research](https://arxiv.org/abs/2509.10771)
*Clemens Schwarke,Mayank Mittal,Nikita Rudin,David Hoeller,Marco Hutter*

Main category: cs.RO

TL;DR: RSL-RL是一个专为机器人社区设计的开源强化学习库，具有紧凑可修改的代码结构，专注于机器人常用算法和GPU优化训练，已在仿真和真实机器人实验中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 针对机器人社区对强化学习框架的特定需求，现有通用框架过于庞大且难以修改，需要开发一个轻量级、易于扩展的专用库来支持机器人控制器的开发。

Method: 采用紧凑且易于修改的代码库设计，专注于机器人领域最广泛采用的算法，并包含解决机器人特定挑战的辅助技术，优化GPU-only训练以实现大规模仿真环境中的高吞吐性能。

Result: 在仿真基准测试和真实世界机器人实验中验证了有效性，证明了其作为轻量级、可扩展且实用的框架用于开发基于学习的机器人控制器的实用性。

Conclusion: RSL-RL成功为机器人社区提供了一个专门优化的强化学习框架，具有轻量、易扩展和高性能的特点，开源地址为https://github.com/leggedrobotics/rsl_rl。

Abstract: RSL-RL is an open-source Reinforcement Learning library tailored to the
specific needs of the robotics community. Unlike broad general-purpose
frameworks, its design philosophy prioritizes a compact and easily modifiable
codebase, allowing researchers to adapt and extend algorithms with minimal
overhead. The library focuses on algorithms most widely adopted in robotics,
together with auxiliary techniques that address robotics-specific challenges.
Optimized for GPU-only training, RSL-RL achieves high-throughput performance in
large-scale simulation environments. Its effectiveness has been validated in
both simulation benchmarks and in real-world robotic experiments, demonstrating
its utility as a lightweight, extensible, and practical framework to develop
learning-based robotic controllers. The library is open-sourced at:
https://github.com/leggedrobotics/rsl_rl.

</details>


### [7] [Follow-Bench: A Unified Motion Planning Benchmark for Socially-Aware Robot Person Following](https://arxiv.org/abs/2509.10796)
*Hanjing Ye,Weixi Situ,Jianwei Peng,Yu Zhan,Bingyi Xia,Kuanqi Cai,Hong Zhang*

Main category: cs.RO

TL;DR: 本文提出了首个端到端的机器人跟随研究，包括场景调查、统一基准测试平台Follow-Bench的构建、六种流行规划器的重新实现，以及仿真和真实环境下的安全与舒适性权衡分析。


<details>
  <summary>Details</summary>
Motivation: 机器人跟随技术在个人辅助、安保巡逻、养老护理和物流等领域有广泛应用需求，但现有研究缺乏对安全性和舒适性的系统考虑，需要建立统一的评估基准。

Method: 构建了Follow-Bench统一基准测试平台，模拟多样化场景（目标轨迹模式、动态人群流、环境布局），重新实现六种流行跟随规划器，并在差速驱动机器人上进行真实环境测试。

Result: 通过大量仿真和真实实验，量化分析了现有规划器在安全性和舒适性之间的权衡关系，揭示了实际部署中的挑战。

Conclusion: 研究为机器人跟随技术提供了系统性的评估框架，识别了现有方法的局限性，并指出了未来研究方向，特别是在安全与舒适性平衡方面的改进空间。

Abstract: Robot person following (RPF) -- mobile robots that follow and assist a
specific person -- has emerging applications in personal assistance, security
patrols, eldercare, and logistics. To be effective, such robots must follow the
target while ensuring safety and comfort for both the target and surrounding
people. In this work, we present the first end-to-end study of RPF, which (i)
surveys representative scenarios, motion-planning methods, and evaluation
metrics with a focus on safety and comfort; (ii) introduces Follow-Bench, a
unified benchmark simulating diverse scenarios, including various target
trajectory patterns, dynamic-crowd flows, and environmental layouts; and (iii)
re-implements six popular RPF planners, ensuring that both safety and comfort
are systematically considered. Moreover, we evaluate the two highest-performing
planners from our benchmark on a differential-drive robot to provide insights
into real-world deployment. Extensive simulation and real-world experiments
provide quantitative insights into the safety-comfort trade-offs of existing
planners, while revealing open challenges and future research directions.

</details>


### [8] [A Universal Wire Testing Machine for Enhancing the Performance of Wire-Driven Robots](https://arxiv.org/abs/2509.10862)
*Temma Suzuki,Kento Kawaharazuka,Kei Okada*

Main category: cs.RO

TL;DR: 开发了通用钢丝测试机，用于测量和调整钢丝特性，提高钢丝驱动机构的性能，包括去除初始拉伸、测量张力传输效率和动态行为，并应用于实际机器人的力控制。


<details>
  <summary>Details</summary>
Motivation: 钢丝作为轻量、低摩擦传动机制，但由于柔性材料特性导致建模误差大，在工业和研究机器人中应用有限，需要改进钢丝驱动机制的性能。

Method: 构建通用钢丝测试机，进行初始钢丝拉伸去除、8种不同直径被动滑轮张力传输效率测量、变长度钢丝动态行为测量。

Result: 获得了钢丝特性数据，应用于实际钢丝驱动机器人的力控制，减少了末端执行器的力误差。

Conclusion: 通过专用测试机测量和调整钢丝特性，可以有效改善钢丝驱动机构的性能，为工业和研究机器人的钢丝传动应用提供了实用解决方案。

Abstract: Compared with gears and linkages, wires constitute a lightweight,
low-friction transmission mechanism. However, because wires are flexible
materials, they tend to introduce large modeling errors, and their adoption in
industrial and research robots remains limited.In this study, we built a
Universal Wire Testing Machine that enables measurement and adjustment of wire
characteristics to improve the performance of wire-driven mechanisms. Using
this testing machine, we carried out removal of initial wire stretch,
measurement of tension transmission efficiency for eight different diameters of
passive pulleys, and measurement of the dynamic behavior of variable-length
wires. Finally, we applied the data obtained from this testing machine to the
force control of an actual wire-driven robot, reducing the end-effector force
error.

</details>


### [9] [Nav-R1: Reasoning and Navigation in Embodied Scenes](https://arxiv.org/abs/2509.10884)
*Qingxiang Liu,Ting Huang,Zeyu Zhang,Hao Tang*

Main category: cs.RO

TL;DR: Nav-R1是一个统一的具身推理基础模型，通过构建大规模CoT数据集和GRPO强化学习框架，解决了具身导航中推理不稳定和实时控制平衡的问题，在多个基准测试中平均提升8%的性能。


<details>
  <summary>Details</summary>
Motivation: 现有具身导航方法存在推理轨迹不连贯不稳定、难以平衡长时程语义推理与低延迟实时控制的问题，阻碍了在多样化环境中的泛化能力。

Method: 1) 构建Nav-CoT-110K大规模逐步推理数据集；2) 设计GRPO强化学习框架，包含格式、理解和导航三个互补奖励；3) 提出Fast-in-Slow推理范式，将语义推理与反应控制解耦。

Result: 在具身AI基准测试中 consistently 优于强基线，推理和导航性能平均提升超过8%。在移动机器人上的真实部署验证了其在有限资源下的鲁棒性。

Conclusion: Nav-R1通过统一的推理框架和创新的训练范式，有效解决了具身导航中的关键挑战，为具身智能的发展提供了重要贡献。

Abstract: Embodied navigation requires agents to integrate perception, reasoning, and
action for robust interaction in complex 3D environments. Existing approaches
often suffer from incoherent and unstable reasoning traces that hinder
generalization across diverse environments, and difficulty balancing
long-horizon semantic reasoning with low-latency control for real-time
navigation. To address these challenges, we propose Nav-R1, an embodied
foundation model that unifies reasoning in embodied environments. We first
construct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought
(CoT) for embodied tasks, which enables cold-start initialization with
structured reasoning. Building on this foundation, we design a GRPO-based
reinforcement learning framework with three complementary rewards: format,
understanding, and navigation, to improve structural adherence, semantic
grounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow
reasoning paradigm, decoupling deliberate semantic reasoning from low-latency
reactive control for efficient yet coherent navigation. Extensive evaluations
on embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms
strong baselines, with over 8% average improvement in reasoning and navigation
performance. Real-world deployment on a mobile robot further validates its
robustness under limited onboard resources. Code:
https://github.com/AIGeeksGroup/Nav-R1. Website:
https://aigeeksgroup.github.io/Nav-R1.

</details>


### [10] [Design of scalable orthogonal digital encoding architecture for large-area flexible tactile sensing in robotics](https://arxiv.org/abs/2509.10888)
*Weijie Liu,Ziyi Qiu,Shihang Wang,Deqing Mei,Yancheng Wang*

Main category: cs.RO

TL;DR: 提出基于码分多址正交数字编码的新型触觉感知架构，通过并行信号传输大幅减少布线需求，实现单线传输下12.8ms的时间分辨率，支持数千节点扩展


<details>
  <summary>Details</summary>
Motivation: 解决柔性触觉传感器在编码效率和布线复杂度方面的瓶颈，实现类似人类皮肤的大面积、高灵敏度、快速响应的触觉感知能力

Method: 采用码分多址启发的正交数字编码策略，通过分布式传感节点的能量正交基码并行叠加，替代传统的串行信号传输方式

Result: 使用16节点传感阵列成功重建压力分布，单传输线实现12.8ms时间分辨率，在节点数量变化数个数量级时仍能保持低于20ms的延迟

Conclusion: 该架构通过重新定义软电子中的信号编码范式，为开发具有类人感知能力的可扩展具身智能系统开辟了新前沿

Abstract: Human-like embodied tactile perception is crucial for the next-generation
intelligent robotics. Achieving large-area, full-body soft coverage with high
sensitivity and rapid response, akin to human skin, remains a formidable
challenge due to critical bottlenecks in encoding efficiency and wiring
complexity in existing flexible tactile sensors, thus significantly hinder the
scalability and real-time performance required for human skin-level tactile
perception. Herein, we present a new architecture employing code division
multiple access-inspired orthogonal digital encoding to overcome these
challenges. Our decentralized encoding strategy transforms conventional serial
signal transmission by enabling parallel superposition of energy-orthogonal
base codes from distributed sensing nodes, drastically reducing wiring
requirements and increasing data throughput. We implemented and validated this
strategy with off-the-shelf 16-node sensing array to reconstruct the pressure
distribution, achieving a temporal resolution of 12.8 ms using only a single
transmission wire. Crucially, the architecture can maintain sub-20ms latency
across orders-of-magnitude variations in node number (to thousands of nodes).
By fundamentally redefining signal encoding paradigms in soft electronics, this
work opens new frontiers in developing scalable embodied intelligent systems
with human-like sensory capabilities.

</details>


### [11] [ViSTR-GP: Online Cyberattack Detection via Vision-to-State Tensor Regression and Gaussian Processes in Automated Robotic Operations](https://arxiv.org/abs/2509.10948)
*Navid Aftabi,Philip Samaha,Jin Ma,Long Cheng,Ramy Harik,Dan Li*

Main category: cs.RO

TL;DR: 开发了ViSTR-GP框架，通过视觉和编码器数据交叉验证来检测工业机器人系统中的数据完整性攻击，在真实测试平台上表现出比基线方法更早的检测能力


<details>
  <summary>Details</summary>
Motivation: 工业机器人系统面临日益增长的数据完整性攻击风险，这些攻击难以通过传统入侵检测或基于模型的方法发现，需要利用现有侧信道进行有效检测

Method: 提出在线检测框架ViSTR-GP，使用头顶摄像头提供独立于控制器权限的视觉估计，通过SAM-Track生成逐帧掩码，低秩张量回归映射掩码到测量值，矩阵变量高斯过程建模残差，提供可解释的在线检测

Result: 在真实机器人测试平台上验证，框架能准确恢复关节角度，比所有基线方法更早检测数据完整性攻击，报警更频繁，在细微攻击中改进最明显

Conclusion: 通过添加独立物理通道绕过控制器权限，无需复杂仪器即可检测数据完整性攻击，为工厂提供了有效的安全防护方案

Abstract: Industrial robotic systems are central to automating smart manufacturing
operations. Connected and automated factories face growing cybersecurity risks
that can potentially cause interruptions and damages to physical operations.
Among these attacks, data-integrity attacks often involve sophisticated
exploitation of vulnerabilities that enable an attacker to access and
manipulate the operational data and are hence difficult to detect with only
existing intrusion detection or model-based detection. This paper addresses the
challenges in utilizing existing side-channels to detect data-integrity attacks
in robotic manufacturing processes by developing an online detection framework,
ViSTR-GP, that cross-checks encoder-reported measurements against a
vision-based estimate from an overhead camera outside the controller's
authority. In this framework, a one-time interactive segmentation initializes
SAM-Track to generate per-frame masks. A low-rank tensor-regression surrogate
maps each mask to measurements, while a matrix-variate Gaussian process models
nominal residuals, capturing temporal structure and cross-joint correlations. A
frame-wise test statistic derived from the predictive distribution provides an
online detector with interpretable thresholds. We validate the framework on a
real-world robotic testbed with synchronized video frame and encoder data,
collecting multiple nominal cycles and constructing replay attack scenarios
with graded end-effector deviations. Results on the testbed indicate that the
proposed framework recovers joint angles accurately and detects data-integrity
attacks earlier with more frequent alarms than all baselines. These
improvements are most evident in the most subtle attacks. These results show
that plants can detect data-integrity attacks by adding an independent physical
channel, bypassing the controller's authority, without needing complex
instrumentation.

</details>


### [12] [ImMimic: Cross-Domain Imitation from Human Videos via Mapping and Interpolation](https://arxiv.org/abs/2509.10952)
*Yangcen Liu,Woo Chul Shin,Yunhai Han,Zhenyang Chen,Harish Ravichandar,Danfei Xu*

Main category: cs.RO

TL;DR: ImMimic是一个跨域模仿学习框架，利用人类视频和少量机器人演示数据，通过动态时间规整和混合插值来弥合人机领域差距，提高机器人操作的成功率和流畅度。


<details>
  <summary>Details</summary>
Motivation: 从丰富的人类视频中学习机器人操作可以替代昂贵的机器人专用数据收集，但视觉、形态和物理方面的领域差距阻碍了直接模仿。需要有效的方法来弥合这些领域差距。

Method: 使用动态时间规整（DTW）将重定向的人类手部姿态映射到机器人关节，然后在配对的人类和机器人轨迹之间进行MixUp插值，创建中间域以促进平滑的域适应。

Result: 在四个真实世界操作任务（拾放、推、锤击、翻转）和四个机器人实体上的评估显示，ImMimic提高了任务成功率和执行流畅度。

Conclusion: ImMimic通过重定向的人类手部轨迹提供信息丰富的动作标签，并通过插值映射数据创建中间域，有效弥合了领域差距，实现了鲁棒的机器人操作。

Abstract: Learning robot manipulation from abundant human videos offers a scalable
alternative to costly robot-specific data collection. However, domain gaps
across visual, morphological, and physical aspects hinder direct imitation. To
effectively bridge the domain gap, we propose ImMimic, an embodiment-agnostic
co-training framework that leverages both human videos and a small amount of
teleoperated robot demonstrations. ImMimic uses Dynamic Time Warping (DTW) with
either action- or visual-based mapping to map retargeted human hand poses to
robot joints, followed by MixUp interpolation between paired human and robot
trajectories. Our key insights are (1) retargeted human hand trajectories
provide informative action labels, and (2) interpolation over the mapped data
creates intermediate domains that facilitate smooth domain adaptation during
co-training. Evaluations on four real-world manipulation tasks (Pick and Place,
Push, Hammer, Flip) across four robotic embodiments (Robotiq, Fin Ray, Allegro,
Ability) show that ImMimic improves task success rates and execution
smoothness, highlighting its efficacy to bridge the domain gap for robust robot
manipulation. The project website can be found at
https://sites.google.com/view/immimic.

</details>


### [13] [Pogosim -- a Simulator for Pogobot robots](https://arxiv.org/abs/2509.10968)
*Leo Cazenille,Loona Macabre,Nicolas Bredeche*

Main category: cs.RO

TL;DR: Pogosim是一个专为Pogobots机器人设计的快速可扩展模拟器，旨在降低算法开发成本，实现代码在模拟和真实机器人上的无缝迁移。


<details>
  <summary>Details</summary>
Motivation: Pogobots是专为群体机器人研究设计的开源机器人，但直接在真实机器人上测试分布式算法非常耗时耗力，特别是对于复杂问题或参数校准，资源消耗过高。

Method: 开发了Pogosim模拟器，采用相同的代码架构用于模拟和真实机器人实验，支持并行运行大量模拟、结果检索分析，以及使用优化算法优化用户代码参数。

Result: 实现了模拟环境与真实实验环境的代码一致性，提供了详细的软件架构、配置文件编写方法和模拟与实验差异说明。

Conclusion: Pogosim有效降低了群体智能算法的开发成本，通过模拟器大幅减少了在真实机器人上进行测试的资源需求，为复杂算法研究和参数优化提供了高效工具。

Abstract: Pogobots are a new type of open-source/open-hardware robots specifically
designed for swarm robotics research. Their cost-effective and modular design,
complemented by vibration-based and wheel-based locomotion, fast infrared
communication and extensive software architecture facilitate the implementation
of swarm intelligence algorithms. However, testing even simple distributed
algorithms directly on robots is particularly labor-intensive. Scaling to more
complex problems or calibrate user code parameters will have a prohibitively
high strain on available resources. In this article we present Pogosim, a fast
and scalable simulator for Pogobots, designed to reduce as much as possible
algorithm development costs. The exact same code will be used in both
simulation and to experimentally drive real robots. This article details the
software architecture of Pogosim, explain how to write configuration files and
user programs and how simulations approximate or differ from experiments. We
describe how a large set of simulations can be launched in parallel, how to
retrieve and analyze the simulation results, and how to optimize user code
parameters using optimization algorithms.

</details>


### [14] [Autonomous Close-Proximity Photovoltaic Panel Coating Using a Quadcopter](https://arxiv.org/abs/2509.10979)
*Dimitri Jacquemont,Carlo Bosio,Teaya Yang,Ruiqi Zhang,Ozgur Orun,Shuai Li,Reza Alam,Thomas M. Schutzius,Simo A. Makiharju,Mark W. Mueller*

Main category: cs.RO

TL;DR: 提出了一种基于四旋翼无人机的光伏板保护涂层自动喷涂系统，使用机载传感器和视觉惯性里程计进行定位，通过模型控制器考虑地面效应和质量变化，实现了室内外自主喷涂验证。


<details>
  <summary>Details</summary>
Motivation: 光伏板在可再生能源领域广泛应用，微小效率提升能带来巨大效益。防反射和自清洁涂层能提升性能但会随时间退化，需要定期重涂。无人机相比传统人工方法能更频繁、低成本地应用保护涂层。

Method: 使用配备液体分散机制的四旋翼无人机系统，仅依靠机载传感器进行定位（视觉惯性里程计和相对光伏板位置检测），采用考虑地面效应和喷涂过程中质量减少的模型控制器。

Result: 通过广泛的室内和室外实验验证了系统的自主能力。

Conclusion: 无人机系统为光伏板保护涂层的自动化应用提供了一种灵活、自主的解决方案，能够更频繁、低成本地维护光伏板性能。

Abstract: Photovoltaic (PV) panels are becoming increasingly widespread in the domain
of renewable energy, and thus, small efficiency gains can have massive effects.
Anti-reflective and self-cleaning coatings enhance panel performance but
degrade over time, requiring periodic reapplication. Uncrewed Aerial Vehicles
(UAVs) offer a flexible and autonomous way to apply protective coatings more
often and at lower cost compared to traditional manual coating methods. In this
letter, we propose a quadcopter-based system, equipped with a liquid dispersion
mechanism, designed to automate such tasks. The localization stack only uses
onboard sensors, relying on visual-inertial odometry and the relative position
of the PV panel detected with respect to the quadcopter. The control relies on
a model-based controller that accounts for the ground effect and the mass
decrease of the quadcopter during liquid dispersion. We validate the autonomy
capabilities of our system through extensive indoor and outdoor experiments.

</details>


### [15] [Multi-objective task allocation for electric harvesting robots: a hierarchical route reconstruction approach](https://arxiv.org/abs/2509.11025)
*Peng Chen,Jing Liang,Hui Song,Kang-Jia Qiao,Cai-Tong Yue,Kun-Jie Yu,Ponnuthurai Nagaratnam Suganthan,Witold Pedrycz*

Main category: cs.RO

TL;DR: 本文提出了一种混合层次路径重构算法(HRRA)来解决农业多电驱机器人任务分配问题(AMERTA)，该问题考虑了实际约束如负载依赖速度变化和电池限制，在多目标优化框架下协调完工时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 农业劳动力成本上升推动了多机器人采摘系统的应用，但由于完工时间和能耗之间的复杂权衡，以及实际约束如负载依赖速度变化和电池限制，高效协调这些系统具有挑战性。

Method: 提出了混合层次路径重构算法(HRRA)，包含层次编码结构、双阶段初始化方法、任务序列优化器和专门的路由重构算子等创新机制。

Result: 在45个测试实例上的实验表明，HRRA相比7种最先进算法表现出优越性能。统计分析验证了其竞争力和探索解空间新区域的能力。

Conclusion: 本研究通过提出新颖的问题表述和有效算法，为多机器人协调的理论理解做出贡献，并为农业自动化提供实用见解。

Abstract: The increasing labor costs in agriculture have accelerated the adoption of
multi-robot systems for orchard harvesting. However, efficiently coordinating
these systems is challenging due to the complex interplay between makespan and
energy consumption, particularly under practical constraints like
load-dependent speed variations and battery limitations. This paper defines the
multi-objective agricultural multi-electrical-robot task allocation (AMERTA)
problem, which systematically incorporates these often-overlooked real-world
constraints. To address this problem, we propose a hybrid hierarchical route
reconstruction algorithm (HRRA) that integrates several innovative mechanisms,
including a hierarchical encoding structure, a dual-phase initialization
method, task sequence optimizers, and specialized route reconstruction
operators. Extensive experiments on 45 test instances demonstrate HRRA's
superior performance against seven state-of-the-art algorithms. Statistical
analysis, including the Wilcoxon signed-rank and Friedman tests, empirically
validates HRRA's competitiveness and its unique ability to explore previously
inaccessible regions of the solution space. In general, this research
contributes to the theoretical understanding of multi-robot coordination by
offering a novel problem formulation and an effective algorithm, thereby also
providing practical insights for agricultural automation.

</details>


### [16] [FEWT: Improving Humanoid Robot Perception with Frequency-Enhanced Wavelet-based Transformers](https://arxiv.org/abs/2509.11109)
*Jiaxin Huang,Hanyu Liu,Yunsheng Ma,Jian Shen,Yilin Zheng,Jiayi Wen,Baishu Wan,Pan Li,Zhigong Song*

Main category: cs.RO

TL;DR: 开发了人形机器人和外骨骼式遥操作舱硬件平台，提出频率增强小波变换器(FEWT)模仿学习框架，在仿真和真实环境中显著提升动作成功率


<details>
  <summary>Details</summary>
Motivation: 通过人形机器人实现物理世界与信息空间的连接，需要开发直观的遥操作和高效的人形动作数据收集方法，同时提升人形机器人的感知表示能力

Method: 提出FEWT框架，包含频率增强高效多尺度注意力(FE-EMA)和时间序列离散小波变换(TS-DWT)两个主要模块，结合多尺度小波分解和残差网络，动态融合时域和频域特征

Result: FEWT在仿真中将最先进算法(ACT基线)的成功率提升高达30%，在真实环境中提升6-12%

Conclusion: 该硬件平台和FEWT框架有效提升了人形机器人的遥操作性能和动作模仿学习效果，通过多尺度特征融合增强了模型鲁棒性

Abstract: The embodied intelligence bridges the physical world and information space.
As its typical physical embodiment, humanoid robots have shown great promise
through robot learning algorithms in recent years. In this study, a hardware
platform, including humanoid robot and exoskeleton-style teleoperation cabin,
was developed to realize intuitive remote manipulation and efficient collection
of anthropomorphic action data. To improve the perception representation of
humanoid robot, an imitation learning framework, termed Frequency-Enhanced
Wavelet-based Transformer (FEWT), was proposed, which consists of two primary
modules: Frequency-Enhanced Efficient Multi-Scale Attention (FE-EMA) and
Time-Series Discrete Wavelet Transform (TS-DWT). By combining multi-scale
wavelet decomposition with the residual network, FE-EMA can dynamically fuse
features from both time-domain and frequency-domain. This fusion is able to
capture feature information across various scales effectively, thereby
enhancing model robustness. Experimental performance demonstrates that FEWT
improves the success rate of the state-of-the-art algorithm (Action Chunking
with Transformers, ACT baseline) by up to 30% in simulation and by 6-12% in
real-world.

</details>


### [17] [ManiVID-3D: Generalizable View-Invariant Reinforcement Learning for Robotic Manipulation via Disentangled 3D Representations](https://arxiv.org/abs/2509.11125)
*Zheng Li,Pei Qu,Yufei Jia,Shihui Zhou,Haizhou Ge,Jiahang Cao,Jinni Zhou,Guyue Zhou,Jun Ma*

Main category: cs.RO

TL;DR: ManiVID-3D是一个用于机器人操作的3D强化学习架构，通过自监督解耦特征学习实现视角不变表示，无需相机标定即可处理任意视角变化，在视角变化下比现有方法成功率提高44.7%，参数减少80%。


<details>
  <summary>Details</summary>
Motivation: 解决视觉强化学习策略在真实世界部署中因相机视角变化而失效的问题，现有方法依赖精确相机标定或难以处理大视角变化。

Method: 提出ViewNet轻量模块自动将任意视角的点云观测对齐到统一空间坐标系，开发高效GPU加速批量渲染模块（每秒处理5000+帧），通过自监督解耦特征学习实现视角不变表示。

Result: 在10个模拟和5个真实世界任务中评估，视角变化下比最先进方法成功率提高44.7%，参数减少80%，对严重视角变化具有鲁棒性，表现出强大的模拟到真实性能。

Conclusion: 学习几何一致表示对于在非结构化环境中实现可扩展机器人操作非常有效，ManiVID-3D为解决视角变化问题提供了有效解决方案。

Abstract: Deploying visual reinforcement learning (RL) policies in real-world
manipulation is often hindered by camera viewpoint changes. A policy trained
from a fixed front-facing camera may fail when the camera is shifted--an
unavoidable situation in real-world settings where sensor placement is hard to
manage appropriately. Existing methods often rely on precise camera calibration
or struggle with large perspective changes. To address these limitations, we
propose ManiVID-3D, a novel 3D RL architecture designed for robotic
manipulation, which learns view-invariant representations through
self-supervised disentangled feature learning. The framework incorporates
ViewNet, a lightweight yet effective module that automatically aligns point
cloud observations from arbitrary viewpoints into a unified spatial coordinate
system without the need for extrinsic calibration. Additionally, we develop an
efficient GPU-accelerated batch rendering module capable of processing over
5000 frames per second, enabling large-scale training for 3D visual RL at
unprecedented speeds. Extensive evaluation across 10 simulated and 5 real-world
tasks demonstrates that our approach achieves a 44.7% higher success rate than
state-of-the-art methods under viewpoint variations while using 80% fewer
parameters. The system's robustness to severe perspective changes and strong
sim-to-real performance highlight the effectiveness of learning geometrically
consistent representations for scalable robotic manipulation in unstructured
environments. Our project website can be found in
https://zheng-joe-lee.github.io/manivid3d/.

</details>


### [18] [RoVerFly: Robust and Versatile Learning-based Control of Quadrotor Across Payload Configurations](https://arxiv.org/abs/2509.11149)
*Mintae Kim,Jiaze Cai,Koushil Sreenath*

Main category: cs.RO

TL;DR: RoVerFly是一个基于强化学习的统一控制框架，能够为四旋翼无人机和电缆悬挂负载系统提供鲁棒的轨迹跟踪控制，无需针对不同配置重新调整参数。


<details>
  <summary>Details</summary>
Motivation: 传统基于模型的控制方法需要大量调参且难以适应配置变化（如负载增减、质量或缆长变化），而学习型方法缺乏稳定性保证。需要开发一个既能适应多种配置变化又具有鲁棒性的统一控制器。

Method: 使用强化学习策略作为跟踪控制器，通过任务和领域随机化进行训练，使控制器对干扰和动态变化具有弹性，保持反馈跟踪控制器的可解释性和结构。

Result: 控制器实现了在零样本情况下对多种负载设置（包括无负载、不同质量和缆长）的强泛化能力，无需控制器切换或重新调参。

Conclusion: RoVerFly提供了一个统一的学习型控制框架，能够处理四旋翼无人机和电缆悬挂负载系统的多种配置变化，具有鲁棒性和泛化能力。

Abstract: Designing robust controllers for precise, arbitrary trajectory tracking with
quadrotors is challenging due to nonlinear dynamics and underactuation, and
becomes harder with flexible cable-suspended payloads that introduce extra
degrees of freedom and hybridness. Classical model-based methods offer
stability guarantees but require extensive tuning and often do not adapt when
the configuration changes, such as when a payload is added or removed, or when
the payload mass or cable length varies. We present RoVerFly, a unified
learning-based control framework in which a reinforcement learning (RL) policy
serves as a robust and versatile tracking controller for standard quadrotors
and for cable-suspended payload systems across a range of configurations.
Trained with task and domain randomization, the controller is resilient to
disturbances and varying dynamics. It achieves strong zero-shot generalization
across payload settings, including no payload as well as varying mass and cable
length, without controller switching or re-tuning, while retaining the
interpretability and structure of a feedback tracking controller. Code and
supplementary materials are available at
https://github.com/mintaeshkim/roverfly

</details>


### [19] [SAMP: Spatial Anchor-based Motion Policy for Collision-Aware Robotic Manipulators](https://arxiv.org/abs/2509.11185)
*Kai Chen,Zhihai Bi,Guoyang Zhao,Chunxin Zheng,Yulin Li,Hang Zhao,Jun Ma*

Main category: cs.RO

TL;DR: SAMP是一种基于空间锚点的运动规划框架，通过共享空间网格上的符号距离场同时编码环境和机械臂几何形状，实现了更精确的碰撞检测和运动规划。


<details>
  <summary>Details</summary>
Motivation: 现有神经运动规划方法通常依赖简化的机器人模型或主要关注障碍物表示，导致在复杂场景中碰撞检测不完整和性能下降，需要同时考虑机器人物理形状和周围环境。

Method: 提出空间锚点运动策略(SAMP)，使用共享空间网格上的符号距离场同时编码环境和机械臂几何形状，包含专门的机器人SDF网络捕获精确几何信息，通过特征对齐策略训练神经运动策略生成平滑无碰撞轨迹。

Result: 在模拟和真实环境实验中，SAMP比现有方法成功率提高11%，碰撞率降低7%，在挑战性真实环境中表现出优异性能。

Conclusion: 联合建模机器人和环境几何形状具有显著优势，SAMP框架通过精确的几何表示和空间锚点融合，为复杂场景下的运动规划提供了实用解决方案。

Abstract: Neural-based motion planning methods have achieved remarkable progress for
robotic manipulators, yet a fundamental challenge lies in simultaneously
accounting for both the robot's physical shape and the surrounding environment
when generating safe and feasible motions. Moreover, existing approaches often
rely on simplified robot models or focus primarily on obstacle representation,
which can lead to incomplete collision detection and degraded performance in
cluttered scenes. To address these limitations, we propose spatial anchor-based
motion policy (SAMP), a unified framework that simultaneously encodes the
environment and the manipulator using signed distance field (SDF) anchored on a
shared spatial grid. SAMP incorporates a dedicated robot SDF network that
captures the manipulator's precise geometry, enabling collision-aware reasoning
beyond coarse link approximations. These representations are fused on spatial
anchors and used to train a neural motion policy that generates smooth,
collision-free trajectories in the proposed efficient feature alignment
strategy. Experiments conducted in both simulated and real-world environments
consistently show that SAMP outperforms existing methods, delivering an 11%
increase in success rate and a 7% reduction in collision rate. These results
highlight the benefits of jointly modelling robot and environment geometry,
demonstrating its practical value in challenging real-world environments.

</details>


### [20] [DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation](https://arxiv.org/abs/2509.11197)
*Yunheng Wang,Yuetong Fang,Taowen Wang,Yixiao Feng,Yawen Tan,Shuning Zhang,Peiran Liu,Yiding Ji,Renjing Xu*

Main category: cs.RO

TL;DR: DreamNav是一个零样本视觉语言导航方法，通过视角校正、轨迹级规划和主动想象来解决现有方法成本高、动作语义不对齐和规划短视的问题，在VLN-CE任务上实现了新的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本VLN方法依赖昂贵的感知和被动场景理解，将控制简化为点级选择，导致部署成本高、动作语义不对齐和规划短视。需要一种更高效、语义对齐且具有长时规划能力的方法。

Method: 提出DreamNav框架，包含三个核心组件：(1)EgoView Corrector对齐视角并稳定自我中心感知；(2)Trajectory Predictor进行全局轨迹级规划以更好对齐指令语义；(3)Imagination Predictor赋予智能体主动思考能力，实现前瞻性长时规划。

Result: 在VLN-CE和真实世界测试中，DreamNav实现了新的零样本SOTA，在SR和SPL指标上分别比最强的自我中心基线（带额外信息）高出7.49%和18.15%。

Conclusion: DreamNav是首个统一轨迹级规划和主动想象的零样本VLN方法，仅使用自我中心输入，在减少感知成本的同时显著提升了导航性能。

Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE), which
links language instructions to perception and control in the real world, is a
core capability of embodied robots. Recently, large-scale pretrained foundation
models have been leveraged as shared priors for perception, reasoning, and
action, enabling zero-shot VLN without task-specific training. However,
existing zero-shot VLN methods depend on costly perception and passive scene
understanding, collapsing control to point-level choices. As a result, they are
expensive to deploy, misaligned in action semantics, and short-sighted in
planning. To address these issues, we present DreamNav that focuses on the
following three aspects: (1) for reducing sensory cost, our EgoView Corrector
aligns viewpoints and stabilizes egocentric perception; (2) instead of
point-level actions, our Trajectory Predictor favors global trajectory-level
planning to better align with instruction semantics; and (3) to enable
anticipatory and long-horizon planning, we propose an Imagination Predictor to
endow the agent with proactive thinking capability. On VLN-CE and real-world
tests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the
strongest egocentric baseline with extra information by up to 7.49\% and
18.15\% in terms of SR and SPL metrics. To our knowledge, this is the first
zero-shot VLN method to unify trajectory-level planning and active imagination
while using only egocentric inputs.

</details>


### [21] [MEMBOT: Memory-Based Robot in Intermittent POMDP](https://arxiv.org/abs/2509.11225)
*Youzhi Liang,Eyan Noronha*

Main category: cs.RO

TL;DR: MEMBOT是一个模块化记忆架构，通过解耦信念推理和策略学习来处理机器人控制中的间歇性部分可观测性问题，在观测丢失率高达50%时仍能保持80%的性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的机器人系统经常在部分和间歇性可观测条件下运行，传感器输入可能因噪声、遮挡或故障而不可用。传统基于完全状态可观测性的强化学习方法无法应对这些挑战。

Method: 采用两阶段训练过程：离线多任务学习预训练阶段使用重构损失学习鲁棒的任务无关潜在信念编码器，然后通过行为克隆微调任务特定策略。信念编码器使用状态空间模型(SSM)和LSTM集成观测和动作的时间序列来推断潜在状态表示。

Result: 在MetaWorld和Robomimic的10个机器人操作基准任务上测试，MEMBOT在变化的观测丢失率下始终优于无记忆和简单循环基线，在50%观测可用性下仍能保持80%的峰值性能。

Conclusion: 显式信念建模对于实现现实世界部分可观测机器人系统的鲁棒、可迁移和数据高效策略非常有效。

Abstract: Robotic systems deployed in real-world environments often operate under
conditions of partial and often intermittent observability, where sensor inputs
may be noisy, occluded, or entirely unavailable due to failures or
environmental constraints. Traditional reinforcement learning (RL) approaches
that assume full state observability are ill-equipped for such challenges. In
this work, we introduce MEMBOT, a modular memory-based architecture designed to
address intermittent partial observability in robotic control tasks. MEMBOT
decouples belief inference from policy learning through a two-phase training
process: an offline multi-task learning pretraining stage that learns a robust
task-agnostic latent belief encoder using a reconstruction losses, followed by
fine-tuning of task-specific policies using behavior cloning. The belief
encoder, implemented as a state-space model (SSM) and a LSTM, integrates
temporal sequences of observations and actions to infer latent state
representations that persist even when observations are dropped. We train and
evaluate MEMBOT on 10 robotic manipulation benchmark tasks from MetaWorld and
Robomimic under varying rates of observation dropout. Results show that MEMBOT
consistently outperforms both memoryless and naively recurrent baselines,
maintaining up to 80% of peak performance under 50% observation availability.
These findings highlight the effectiveness of explicit belief modeling in
achieving robust, transferable, and data-efficient policies for real-world
partially observable robotic systems.

</details>


### [22] [CORB-Planner: Corridor as Observations for RL Planning in High-Speed Flight](https://arxiv.org/abs/2509.11240)
*Yechen Zhang,Bin Gao,Gang Wang,Jian Sun,Zhuo Li*

Main category: cs.RO

TL;DR: CORB-Planner是一个基于强化学习的实时轨迹规划框架，通过结合B样条轨迹生成和紧凑安全飞行走廊表示，实现高速无人机在异构平台上的自主飞行。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习在无人机部署中的挑战，包括对精确动力学模型的依赖和平台特定传感的局限性，促进跨平台迁移。

Method: 采用B样条轨迹生成与RL策略结合，使用启发式搜索获得紧凑安全飞行走廊(SFC)表示，采用基于值的软分解批评器Q(SDCQ)算法进行训练。

Result: 在仿真和真实测试中展示实时规划能力，支持最高8.2m/s的飞行速度，兼容多种无人机配置，计算需求适中。

Conclusion: CORB-Planner具有通用性和鲁棒性，适用于实际部署，能够有效缩小仿真到现实的差距。

Abstract: Reinforcement learning (RL) has shown promise in a large number of robotic
control tasks. Nevertheless, its deployment on unmanned aerial vehicles (UAVs)
remains challenging, mainly because of reliance on accurate dynamic models and
platform-specific sensing, which hinders cross-platform transfer. This paper
presents the CORB-Planner (Corridor-as-Observations for RL B-spline planner), a
real-time, RL-based trajectory planning framework for high-speed autonomous UAV
flight across heterogeneous platforms. The key idea is to combine B-spline
trajectory generation with the RL policy producing successive control points
with a compact safe flight corridor (SFC) representation obtained via heuristic
search. The SFC abstracts obstacle information in a low-dimensional form,
mitigating overfitting to platform-specific details and reducing sensitivity to
model inaccuracies. To narrow the sim-to-real gap, we adopt an easy-to-hard
progressive training pipeline in simulation. A value-based soft
decomposed-critic Q (SDCQ) algorithm is used to learn effective policies within
approximately ten minutes of training. Benchmarks in simulation and real-world
tests demonstrate real-time planning on lightweight onboard hardware and
support maximum flight speeds up to 8.2m/s in dense, cluttered environments
without external positioning. Compatibility with various UAV configurations
(quadrotors, hexarotors) and modest onboard compute underlines the generality
and robustness of CORB-Planner for practical deployment.

</details>


### [23] [Embodied Intelligence in Disassembly: Multimodal Perception Cross-validation and Continual Learning in Neuro-Symbolic TAMP](https://arxiv.org/abs/2509.11270)
*Ziwen He,Zhigang Wang,Yanlong Peng,Pengxu Chang,Hong Yang,Ming Chen*

Main category: cs.RO

TL;DR: 提出基于神经符号任务运动规划的持续学习框架，通过多模态感知交叉验证和双向推理流程，显著提升动态拆卸场景中的任务成功率和感知准确性


<details>
  <summary>Details</summary>
Motivation: 新能源车动力电池高效拆解回收需求迫切，非结构化拆卸场景的动态环境限制了机器人感知的鲁棒性，阻碍工业应用中的自主拆卸

Method: 集成多模态感知交叉验证机制到双向推理流程：前向工作流动态优化动作策略，后向学习流从历史任务执行中自主收集有效数据以促进持续学习

Result: 任务成功率从81.68%提升至100%，平均感知误判次数从3.389降至1.128

Conclusion: 该研究为增强具身智能在复杂工业环境中的鲁棒性和适应性提供了新范式

Abstract: With the rapid development of the new energy vehicle industry, the efficient
disassembly and recycling of power batteries have become a critical challenge
for the circular economy. In current unstructured disassembly scenarios, the
dynamic nature of the environment severely limits the robustness of robotic
perception, posing a significant barrier to autonomous disassembly in
industrial applications. This paper proposes a continual learning framework
based on Neuro-Symbolic task and motion planning (TAMP) to enhance the
adaptability of embodied intelligence systems in dynamic environments. Our
approach integrates a multimodal perception cross-validation mechanism into a
bidirectional reasoning flow: the forward working flow dynamically refines and
optimizes action strategies, while the backward learning flow autonomously
collects effective data from historical task executions to facilitate continual
system learning, enabling self-optimization. Experimental results show that the
proposed framework improves the task success rate in dynamic disassembly
scenarios from 81.68% to 100%, while reducing the average number of perception
misjudgments from 3.389 to 1.128. This research provides a new paradigm for
enhancing the robustness and adaptability of embodied intelligence in complex
industrial environments.

</details>


### [24] [Policy Learning for Social Robot-Led Physiotherapy](https://arxiv.org/abs/2509.11297)
*Carl Bettosi,Lynne Ballie,Susan Shenkin,Marta Romeo*

Main category: cs.RO

TL;DR: 使用医疗专家作为患者代理训练社交机器人，通过强化学习策略自适应调整物理治疗运动指导


<details>
  <summary>Details</summary>
Motivation: 社交机器人可以自主指导患者进行物理治疗，但缺乏患者行为数据来开发鲁棒策略，需要解决数据稀缺问题

Method: 邀请33名医疗专家作为患者代理与机器人互动，建立患者行为模型生成运动表现指标和主观用力评分，在模拟环境中训练强化学习策略

Result: 训练的策略能够根据个体用力耐受度和波动表现自适应调整运动指导，适用于不同康复阶段和运动计划的患者

Conclusion: 使用专家代理数据训练强化学习策略是解决患者数据稀缺的有效方法，能够实现个性化的物理治疗运动指导

Abstract: Social robots offer a promising solution for autonomously guiding patients
through physiotherapy exercise sessions, but effective deployment requires
advanced decision-making to adapt to patient needs. A key challenge is the
scarcity of patient behavior data for developing robust policies. To address
this, we engaged 33 expert healthcare practitioners as patient proxies, using
their interactions with our robot to inform a patient behavior model capable of
generating exercise performance metrics and subjective scores on perceived
exertion. We trained a reinforcement learning-based policy in simulation,
demonstrating that it can adapt exercise instructions to individual exertion
tolerances and fluctuating performance, while also being applicable to patients
at different recovery stages with varying exercise plans.

</details>


### [25] [Brain-Robot Interface for Exercise Mimicry](https://arxiv.org/abs/2509.11306)
*Carl Bettosi,Emilyann Nault,Lynne Baillie,Markus Garschall,Marta Romeo,Beatrix Wais-Zechmann,Nicole Binderlehner,Theodoros Georgio*

Main category: cs.RO

TL;DR: 开发了一种新型脑机接口系统，让社交机器人能够通过解读用户的运动意图来实时模仿用户的康复训练动作，以建立更好的互动关系。


<details>
  <summary>Details</summary>
Motivation: 为了让社交机器人作为运动指导员能够维持长期互动，建立良好的互动关系至关重要。运动模仿在社交互动中被认为是建立关系的有效工具，特别是在康复训练中患者模仿物理治疗师动作的场景。

Method: 开发了基于脑机接口（BRI）的系统，通过解读患者的运动意图来让社交机器人实时模仿患者的训练动作。在探索性研究中，对14名参与者（3名物理治疗师和11名偏瘫患者）进行了系统评估。

Result: 系统在12次会话中成功展示了运动模仿功能，但准确性存在差异。参与者对机器人指导员持积极态度，表现出高度的信任和接受度，且这些积极感受不受脑机接口技术引入的影响。

Conclusion: 基于脑机接口的机器人模仿系统在康复训练中具有应用潜力，能够建立良好的用户-机器人互动关系，用户接受度高，为长期康复训练提供了新的技术途径。

Abstract: For social robots to maintain long-term engagement as exercise instructors,
rapport-building is essential. Motor mimicry--imitating one's physical
actions--during social interaction has long been recognized as a powerful tool
for fostering rapport, and it is widely used in rehabilitation exercises where
patients mirror a physiotherapist or video demonstration. We developed a novel
Brain-Robot Interface (BRI) that allows a social robot instructor to mimic a
patient's exercise movements in real-time, using mental commands derived from
the patient's intention. The system was evaluated in an exploratory study with
14 participants (3 physiotherapists and 11 hemiparetic patients recovering from
stroke or other injuries). We found our system successfully demonstrated
exercise mimicry in 12 sessions; however, accuracy varied. Participants had
positive perceptions of the robot instructor, with high trust and acceptance
levels, which were not affected by the introduction of BRI technology.

</details>


### [26] [ActivePose: Active 6D Object Pose Estimation and Tracking for Robotic Manipulation](https://arxiv.org/abs/2509.11364)
*Sheng Liu,Zhe Li,Weiheng Wang,Han Sun,Heng Zhang,Hongpeng Chen,Yusen Qin,Arash Ajoudani,Yizhao Wang*

Main category: cs.RO

TL;DR: 提出了一种结合视觉语言模型和机器人想象力的主动位姿估计与跟踪方法，通过动态检测和解决视角歧义问题，显著提升了6-DoF物体位姿估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决零样本方法在视角歧义下的失效问题，以及固定摄像头在物体移动或自遮挡时的局限性，需要一种能够动态适应场景变化的主动感知方法。

Method: 离线阶段渲染CAD模型多视角并计算熵值，构建几何感知提示；运行时使用VLM检测歧义，通过虚拟渲染生成候选相机位姿，结合VLM概率和熵值评分选择最佳视角；引入基于扩散策略的主动跟踪模块保持物体可见性。

Result: 在仿真和真实世界实验中，该方法显著优于传统基线方法，能够有效处理视角歧义和物体移动问题。

Conclusion: 结合VLM和机器人想象力的主动位姿估计框架，通过动态视角选择和跟踪策略，成功解决了复杂场景下的6-DoF位姿估计挑战，为机器人可靠操作提供了有效解决方案。

Abstract: Accurate 6-DoF object pose estimation and tracking are critical for reliable
robotic manipulation. However, zero-shot methods often fail under
viewpoint-induced ambiguities and fixed-camera setups struggle when objects
move or become self-occluded. To address these challenges, we propose an active
pose estimation pipeline that combines a Vision-Language Model (VLM) with
"robotic imagination" to dynamically detect and resolve ambiguities in real
time. In an offline stage, we render a dense set of views of the CAD model,
compute the FoundationPose entropy for each view, and construct a
geometric-aware prompt that includes low-entropy (unambiguous) and high-entropy
(ambiguous) examples. At runtime, the system: (1) queries the VLM on the live
image for an ambiguity score; (2) if ambiguity is detected, imagines a discrete
set of candidate camera poses by rendering virtual views, scores each based on
a weighted combination of VLM ambiguity probability and FoundationPose entropy,
and then moves the camera to the Next-Best-View (NBV) to obtain a disambiguated
pose estimation. Furthermore, since moving objects may leave the camera's field
of view, we introduce an active pose tracking module: a diffusion-policy
trained via imitation learning, which generates camera trajectories that
preserve object visibility and minimize pose ambiguity. Experiments in
simulation and real-world show that our approach significantly outperforms
classical baselines.

</details>


### [27] [Quantum deep reinforcement learning for humanoid robot navigation task](https://arxiv.org/abs/2509.11388)
*Romerik Lokossou,Birhanu Shimelis Girma,Ozan K. Tonguz,Ahmed Biyabani*

Main category: cs.RO

TL;DR: 量子深度强化学习(QDRL)在人形机器人控制中相比经典方法实现了92%更少的训练步数和8%更高的平均回报


<details>
  <summary>Details</summary>
Motivation: 经典强化学习方法在复杂高维环境中面临参数需求大和随机性挑战的问题，需要探索量子计算与深度强化学习的结合来提高学习效率

Method: 使用参数化量子电路构建混合量子-经典架构，直接在Humanoid-v4和Walker2d-v4等高维环境中进行训练，避免了传统的映射和规划方法，并比较了量子SAC与经典SAC的性能

Result: 量子SAC在92%更少的训练步数下实现了246.40的平均回报，比经典SAC的228.36高出8%，显示出量子计算在强化学习中的加速学习潜力

Conclusion: 量子深度强化学习在人形机器人控制等复杂高维任务中具有显著优势，能够大幅减少训练时间同时提高性能，为量子计算在机器人学中的应用开辟了新途径

Abstract: Classical reinforcement learning (RL) methods often struggle in complex,
high-dimensional environments because of their extensive parameter requirements
and challenges posed by stochastic, non-deterministic settings. This study
introduces quantum deep reinforcement learning (QDRL) to train humanoid agents
efficiently. While previous quantum RL models focused on smaller environments,
such as wheeled robots and robotic arms, our work pioneers the application of
QDRL to humanoid robotics, specifically in environments with substantial
observation and action spaces, such as MuJoCo's Humanoid-v4 and Walker2d-v4.
Using parameterized quantum circuits, we explored a hybrid quantum-classical
setup to directly navigate high-dimensional state spaces, bypassing traditional
mapping and planning. By integrating quantum computing with deep RL, we aim to
develop models that can efficiently learn complex navigation tasks in humanoid
robots. We evaluated the performance of the Soft Actor-Critic (SAC) in
classical RL against its quantum implementation. The results show that the
quantum SAC achieves an 8% higher average return (246.40) than the classical
SAC (228.36) after 92% fewer steps, highlighting the accelerated learning
potential of quantum computing in RL tasks.

</details>


### [28] [TRUST 2025: SCRITA and RTSS @ RO-MAN 2025](https://arxiv.org/abs/2509.11402)
*Alessandra Rossi,Patrick Holthaus,Gabriella Lakatos,Sílvia Moros,Ali Fallahi,Murat Kirtay,Marie Postma,Erhan Oztop*

Main category: cs.RO

TL;DR: TRUST研讨会是SCRITA和RTSS两个HRI领域知名工作坊的合作成果，整合了从人类和机器人双重视角研究信任的互补目标


<details>
  <summary>Details</summary>
Motivation: 整合两个已有工作坊的互补优势，从人类和机器人双重视角共同推进信任研究

Method: 通过联合研讨会的形式，汇集SCRITA（信任、接受度和社交线索）和RTSS（共生社会中的机器人信任）两个工作坊的研究成果

Result: 建立了TRUST联合研讨会平台，为HRI领域的信任研究提供了更全面的视角

Conclusion: 这种合作模式有助于更全面地理解人机交互中的信任问题，推动该领域研究的深入发展

Abstract: The TRUST workshop is the result of a collaboration between two established
workshops in the field of Human-Robot Interaction: SCRITA (Trust, Acceptance
and Social Cues in Human-Robot Interaction) and RTSS (Robot Trust for Symbiotic
Societies). This joint initiative brings together the complementary goals of
these workshops to advance research on trust from both the human and robot
perspectives.
  Website: https://scrita.herts.ac.uk/2025/

</details>


### [29] [Enhancing Generalization in Vision-Language-Action Models by Preserving Pretrained Representations](https://arxiv.org/abs/2509.11417)
*Shresth Grover,Akshay Gopalkrishnan,Bo Ai,Henrik I. Christensen,Hao Su,Xuanlin Li*

Main category: cs.RO

TL;DR: 提出了一个保护预训练特征的同时适应机器人操作的框架，包含双编码器设计、字符串动作标记化和协同训练策略，在模拟和真实机器人上表现出更好的鲁棒性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 直接对机器人数据进行微调会破坏视觉语言模型的预训练表示并限制泛化能力，需要更好地保留预训练特征同时适应机器人操作任务

Method: 采用双编码器设计（一个冻结的视觉编码器保留预训练特征，另一个可训练的编码器进行任务适应）、字符串动作标记化将连续动作转换为字符序列、结合机器人演示和视觉语言数据集的协同训练策略

Result: 在模拟和真实机器人评估中，该方法相比基线提高了对视觉扰动的鲁棒性、对新指令和环境的泛化能力以及整体任务成功率

Conclusion: 该框架有效保护了预训练特征，同时成功适应机器人操作任务，在多个维度上优于现有方法

Abstract: Vision-language-action (VLA) models finetuned from vision-language models
(VLMs) hold the promise of leveraging rich pretrained representations to build
generalist robots across diverse tasks and environments. However, direct
fine-tuning on robot data often disrupts these representations and limits
generalization. We present a framework that better preserves pretrained
features while adapting them for robot manipulation. Our approach introduces
three components: (i) a dual-encoder design with one frozen vision encoder to
retain pretrained features and another trainable for task adaptation, (ii) a
string-based action tokenizer that casts continuous actions into character
sequences aligned with the model's pretraining domain, and (iii) a co-training
strategy that combines robot demonstrations with vision-language datasets
emphasizing spatial reasoning and affordances. Evaluations in simulation and on
real robots show that our method improves robustness to visual perturbations,
generalization to novel instructions and environments, and overall task success
compared to baselines.

</details>


### [30] [A Software-Only Post-Processor for Indexed Rotary Machining on GRBL-Based CNCs](https://arxiv.org/abs/2509.11433)
*Pedro Portugal,Damian D. Venghaus,Diego Lopez*

Main category: cs.RO

TL;DR: 软件框架实现GRBL数控机床的索引式旋转加工，无需硬件改造或固件修改，降低多轴加工的技术和成本门槛


<details>
  <summary>Details</summary>
Motivation: 解决桌面CNC机床缺乏旋转轴的问题，现有方案需要硬件改造、替代控制器或商业CAM软件，成本高且复杂

Method: 开发自定义后处理器将平面刀具路径转换为离散旋转步骤，通过基于浏览器的界面执行，使用标准现成机械

Result: 实现了实用的旋转轴加工能力，虽然不是真正的连续4轴加工，但能够制造旋转对称和多面零件

Conclusion: 该框架显著降低了多轴加工的技术和财务障碍，扩展了教室、创客空间和小型车间对多轴加工的访问，支持实践学习和快速原型制作

Abstract: Affordable desktop CNC routers are common in education, prototyping, and
makerspaces, but most lack a rotary axis, limiting fabrication of rotationally
symmetric or multi-sided parts. Existing solutions often require hardware
retrofits, alternative controllers, or commercial CAM software, raising cost
and complexity. This work presents a software-only framework for indexed rotary
machining on GRBL-based CNCs. A custom post-processor converts planar toolpaths
into discrete rotary steps, executed through a browser-based interface. While
not equivalent to continuous 4-axis machining, the method enables practical
rotary-axis fabrication using only standard, off-the-shelf mechanics, without
firmware modification. By reducing technical and financial barriers, the
framework expands access to multi-axis machining in classrooms, makerspaces,
and small workshops, supporting hands-on learning and rapid prototyping.

</details>


### [31] [RAPTOR: A Foundation Policy for Quadrotor Control](https://arxiv.org/abs/2509.11481)
*Jonas Eschmann,Dario Albani,Giuseppe Loianno*

Main category: cs.RO

TL;DR: RAPTOR方法训练了一个高度自适应的四旋翼控制基础策略，仅用2084个参数的微小策略就能零样本适应10种不同的真实四旋翼无人机，通过元模仿学习和上下文学习实现快速适应。


<details>
  <summary>Details</summary>
Motivation: 人类能够高效适应新环境（如驾驶新车），而现代机器人控制系统（如RL训练的神经网络策略）容易过拟合单一环境，在微小变化（如Sim2Real差距）下就会失效，需要重新训练。

Method: 提出RAPTOR方法：1）对1000个不同四旋翼采样，为每个训练教师RL策略；2）通过元模仿学习将1000个教师策略蒸馏为单一自适应学生策略；3）使用隐藏层循环实现上下文学习，使策略能在毫秒级内零样本适应新四旋翼。

Result: 在10种真实四旋翼（32g-2.4kg）上测试，涵盖不同电机类型、框架类型、螺旋桨类型和飞控系统。策略在轨迹跟踪、室内外环境、风扰、物理干扰等条件下表现优异，实现了零样本适应。

Conclusion: RAPTOR成功训练了一个高度自适应的基础控制策略，证明了小参数策略通过元学习和上下文学习能够实现跨平台的零样本适应，为解决机器人控制的泛化问题提供了有效方案。

Abstract: Humans are remarkably data-efficient when adapting to new unseen conditions,
like driving a new car. In contrast, modern robotic control systems, like
neural network policies trained using Reinforcement Learning (RL), are highly
specialized for single environments. Because of this overfitting, they are
known to break down even under small differences like the Simulation-to-Reality
(Sim2Real) gap and require system identification and retraining for even
minimal changes to the system. In this work, we present RAPTOR, a method for
training a highly adaptive foundation policy for quadrotor control. Our method
enables training a single, end-to-end neural-network policy to control a wide
variety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg
that also differ in motor type (brushed vs. brushless), frame type (soft vs.
rigid), propeller type (2/3/4-blade), and flight controller
(PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy
with only 2084 parameters is sufficient for zero-shot adaptation to a wide
variety of platforms. The adaptation through In-Context Learning is made
possible by using a recurrence in the hidden layer. The policy is trained
through a novel Meta-Imitation Learning algorithm, where we sample 1000
quadrotors and train a teacher policy for each of them using Reinforcement
Learning. Subsequently, the 1000 teachers are distilled into a single, adaptive
student policy. We find that within milliseconds, the resulting foundation
policy adapts zero-shot to unseen quadrotors. We extensively test the
capabilities of the foundation policy under numerous conditions (trajectory
tracking, indoor/outdoor, wind disturbance, poking, different propellers).

</details>


### [32] [FR-Net: Learning Robust Quadrupedal Fall Recovery on Challenging Terrains through Mass-Contact Prediction](https://arxiv.org/abs/2509.11504)
*Yidan Lu,Yinzhao Dong,Jiahui Zhang,Ji Ma,Peng Lu*

Main category: cs.RO

TL;DR: FR-Net是一个基于学习的四足机器人跌倒恢复框架，通过质量-接触预测器从有限传感器输入估计质量分布和接触状态，实现复杂地形上的安全恢复


<details>
  <summary>Details</summary>
Motivation: 传统控制器在复杂地形上由于地形感知不完整和不确定交互而失效，需要开发能够从任意跌倒姿势恢复的鲁棒方法

Method: 使用特权学习在仿真中训练，包含质量-接触预测器网络估计机器人的质量分布和接触状态，设计奖励函数确保安全恢复

Result: 在仿真中展示了跨不同四足平台的泛化能力，在Go2机器人上通过10个挑战性场景的实实验验证了性能

Conclusion: 显式的质量-接触预测是实现鲁棒跌倒恢复的关键，为可泛化的四足机器人技能提供了有前景的方向

Abstract: Fall recovery for legged robots remains challenging, particularly on complex
terrains where traditional controllers fail due to incomplete terrain
perception and uncertain interactions. We present \textbf{FR-Net}, a
learning-based framework that enables quadrupedal robots to recover from
arbitrary fall poses across diverse environments. Central to our approach is a
Mass-Contact Predictor network that estimates the robot's mass distribution and
contact states from limited sensory inputs, facilitating effective recovery
strategies. Our carefully designed reward functions ensure safe recovery even
on steep stairs without dangerous rolling motions common to existing methods.
Trained entirely in simulation using privileged learning, our framework guides
policy learning without requiring explicit terrain data during deployment. We
demonstrate the generalization capabilities of \textbf{FR-Net} across different
quadrupedal platforms in simulation and validate its performance through
extensive real-world experiments on the Go2 robot in 10 challenging scenarios.
Our results indicate that explicit mass-contact prediction is key to robust
fall recovery, offering a promising direction for generalizable quadrupedal
skills.

</details>


### [33] [Design and Development of a Remotely Wire-Driven Walking Robot](https://arxiv.org/abs/2509.11506)
*Takahiro Hattori,Kento Kawaharazuka,Kei Okada*

Main category: cs.RO

TL;DR: 提出了一种名为Remote Wire Drive的新型机制，通过线缆实现移动机器人的远程驱动，解决了恶劣环境中电子设备易受损的问题。


<details>
  <summary>Details</summary>
Motivation: 在恶劣或人类无法进入的环境中，电子设备容易受损。现有解决方案如无电子设备的自主移动机器人决策能力有限，液压驱动移动机器人和线驱动机器人臂各有局限性。移动机器人比机械臂具有更大的工作范围和避障能力，而线驱动机制比液压系统具有更广泛的环境适用性，但线驱动系统尚未用于移动机器人的远程驱动。

Method: 提出Remote Wire Drive机制，将线驱动机器人臂中使用的解耦关节串联连接机制适配用于动力传输。开发了线驱动四足机器人，并通过Remote Wire Drive机制进行驱动实验验证。

Result: 通过实验验证了Remote Wire Drive机制驱动线驱动四足机器人的可行性。

Conclusion: Remote Wire Drive是一种可行的新型机制，能够通过线缆实现移动机器人的远程驱动，为恶劣环境下的机器人应用提供了新的解决方案。

Abstract: Operating in environments too harsh or inaccessible for humans is one of the
critical roles expected of robots. However, such environments often pose risks
to electronic components as well. To overcome this, various approaches have
been developed, including autonomous mobile robots without electronics,
hydraulic remotely actuated mobile robots, and long-reach robot arms driven by
wires. Among these, electronics-free autonomous robots cannot make complex
decisions, while hydraulically actuated mobile robots and wire-driven robot
arms are used in harsh environments such as nuclear power plants. Mobile robots
offer greater reach and obstacle avoidance than robot arms, and wire mechanisms
offer broader environmental applicability than hydraulics. However, wire-driven
systems have not been used for remote actuation of mobile robots. In this
study, we propose a novel mechanism called Remote Wire Drive that enables
remote actuation of mobile robots via wires. This mechanism is a series
connection of decoupled joints, a mechanism used in wire-driven robot arms,
adapted for power transmission. We experimentally validated its feasibility by
actuating a wire-driven quadruped robot, which we also developed in this study,
through Remote Wire Drive.

</details>


### [34] [PaiP: An Operational Aware Interactive Planner for Unknown Cabinet Environments](https://arxiv.org/abs/2509.11516)
*Chengjin Wang,Zheng Yan,Yanmin Zhou,Runjie Shen,Zhipeng Wang,Bin Cheng,Bin He*

Main category: cs.RO

TL;DR: 提出了PaiP交互运动规划器，利用多模态触觉感知在遮挡的箱柜场景中实现实时闭环规划，通过感知交互界面的运动效应来推断物体交互特征并生成操作成本地图。


<details>
  <summary>Details</summary>
Motivation: 解决箱柜场景中堆叠物体造成的视觉遮挡和空间受限问题，传统碰撞避免方法在无碰撞路径时失效且可能导致灾难性碰撞。

Method: 基于多模态触觉感知的实时闭环规划框架，通过感知交互界面的运动效应推断物体交互特征，生成操作成本地图，扩展采样规划方法同时优化路径成本和操作成本。

Result: 实验结果表明PaiP在狭窄空间中实现了鲁棒的运动规划。

Conclusion: PaiP框架通过整合触觉感知和交互特征，成功解决了遮挡环境下的运动规划挑战，为受限空间中的机器人操作提供了有效解决方案。

Abstract: Box/cabinet scenarios with stacked objects pose significant challenges for
robotic motion due to visual occlusions and constrained free space. Traditional
collision-free trajectory planning methods often fail when no collision-free
paths exist, and may even lead to catastrophic collisions caused by invisible
objects. To overcome these challenges, we propose an operational aware
interactive motion planner (PaiP) a real-time closed-loop planning framework
utilizing multimodal tactile perception. This framework autonomously infers
object interaction features by perceiving motion effects at interaction
interfaces. These interaction features are incorporated into grid maps to
generate operational cost maps. Building upon this representation, we extend
sampling-based planning methods to interactive planning by optimizing both path
cost and operational cost. Experimental results demonstrate that PaiP achieves
robust motion in narrow spaces.

</details>


### [35] [Shape control of simulated multi-segment continuum robots via Koopman operators with per-segment projection](https://arxiv.org/abs/2509.11567)
*Eron Ristich,Jiahe Wang,Lei Zhang,Sultan Haidar Ali,Wanxin Jin,Yi Ren,Jiefeng Sun*

Main category: cs.RO

TL;DR: 提出了一种基于数据驱动Koopman算子的方法，用于多段肌腱驱动软体连续机器人的形状控制，通过分段投影方案显著提高了模型精度，实现了计算高效的闭环形状控制。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的连续机器人只能实现实时任务空间控制（尖端控制），而无法实现全形状控制，主要原因是无限自由度带来的高计算成本。

Method: 使用基于Koopman算子的数据驱动方法，对模拟的多段肌腱驱动软体连续机器人进行分段投影方案，识别控制仿射Koopman模型，并采用线性模型预测控制（MPC）进行形状控制。

Result: 该方法比无投影方案的模型精度提高了一个数量级，实现了计算高效的闭环控制，展示了软体机器人实时形状控制的可行性。

Conclusion: 这项工作为软体连续机器人的实用形状控制铺平了道路，证明了实时形状控制的可行性。

Abstract: Soft continuum robots can allow for biocompatible yet compliant motions, such
as the ability of octopus arms to swim, crawl, and manipulate objects. However,
current state-of-the-art continuum robots can only achieve real-time task-space
control (i.e., tip control) but not whole-shape control, mainly due to the high
computational cost from its infinite degrees of freedom. In this paper, we
present a data-driven Koopman operator-based approach for the shape control of
simulated multi-segment tendon-driven soft continuum robots with the Kirchhoff
rod model. Using data collected from these simulated soft robots, we conduct a
per-segment projection scheme on the state of the robots allowing for the
identification of control-affine Koopman models that are an order of magnitude
more accurate than without the projection scheme. Using these learned Koopman
models, we use a linear model predictive control (MPC) to control the robots to
a collection of target shapes of varying complexity. Our method realizes
computationally efficient closed-loop control, and demonstrates the feasibility
of real-time shape control for soft robots. We envision this work can pave the
way for practical shape control of soft continuum robots.

</details>


### [36] [GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning](https://arxiv.org/abs/2509.11594)
*Jizhuo Chen,Diwen Liu,Jiaming Wang,Harold Soh*

Main category: cs.RO

TL;DR: GBPP是一种基于快速学习的评分器，通过单次RGB-D快照选择机器人抓取基座姿态，使用两阶段课程学习：低成本自动标注和大规模仿真优化，在仿真和真实移动机械臂上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决机器人抓取任务中基座姿态选择的问题，传统方法需要复杂的任务和运动优化，计算成本高且效率低，需要一种快速、数据高效且几何感知的基座放置方案。

Method: 采用两阶段课程学习：1) 使用简单的距离-可见性规则自动标注大规模数据集；2) 通过较小规模的高保真仿真试验细化模型以匹配真实抓取结果。使用PointNet++风格的点云编码器和MLP对候选姿态密集网格进行评分。

Result: 在仿真和真实移动机械臂上，GBPP优于仅基于接近度和几何的基线方法，能够选择更安全、更可达的站位，并在错误时优雅降级。

Conclusion: 该方法提供了一个实用的数据高效、几何感知的基座放置方案：使用廉价启发式方法进行覆盖，然后通过针对性仿真进行校准。

Abstract: GBPP is a fast learning based scorer that selects a robot base pose for
grasping from a single RGB-D snapshot. The method uses a two stage curriculum:
(1) a simple distance-visibility rule auto-labels a large dataset at low cost;
and (2) a smaller set of high fidelity simulation trials refines the model to
match true grasp outcomes. A PointNet++ style point cloud encoder with an MLP
scores dense grids of candidate poses, enabling rapid online selection without
full task-and-motion optimization. In simulation and on a real mobile
manipulator, GBPP outperforms proximity and geometry only baselines, choosing
safer and more reachable stances and degrading gracefully when wrong. The
results offer a practical recipe for data efficient, geometry aware base
placement: use inexpensive heuristics for coverage, then calibrate with
targeted simulation.

</details>


### [37] [AssemMate: Graph-Based LLM for Robotic Assembly Assistance](https://arxiv.org/abs/2509.11617)
*Qi Zheng,Chaoran Zhang,Zijian Liang,EnTe Lin,Shubo Cui,Qinghongbing Xie,Zhaobo Xu,Long Zeng*

Main category: cs.RO

TL;DR: AssemMate是一个基于图结构的LLM机器人装配辅助系统，使用知识图谱代替自然语言文本进行知识表示，显著提高了推理速度、准确性和上下文效率，并支持视觉增强的抓取操作。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的机器人装配辅助系统使用自然语言文本表示知识，存在上下文过长、内容冗余的问题，无法满足机器人实时精确推理的需求。需要一种更简洁准确的知识表示形式。

Method: 使用知识图谱作为输入形式，通过自监督图卷积网络(GCN)将知识图谱实体和关系编码到潜在空间，并与LLM表示对齐，使LLM能够理解图信息。采用视觉增强策略处理堆叠场景的抓取问题。

Result: 在训练和评估中，AssemMate相比现有方法准确率提高6.4%，推理速度快3倍，上下文长度缩短28倍，在随机图谱上表现出强大的泛化能力。在模拟和真实机器人抓取实验中均表现优异。

Conclusion: 图结构的知识表示形式比自然语言文本更适合机器人装配任务，能够显著提升LLM在实时性、准确性和效率方面的表现，为机器人装配辅助系统提供了新的解决方案。

Abstract: Large Language Model (LLM)-based robotic assembly assistance has gained
significant research attention. It requires the injection of domain-specific
knowledge to guide the assembly process through natural language interaction
with humans. Despite some progress, existing methods represent knowledge in the
form of natural language text. Due to the long context and redundant content,
they struggle to meet the robots' requirements for real-time and precise
reasoning. In order to bridge this gap, we present AssemMate, which utilizes
the graph\textemdash a concise and accurate form of knowledge
representation\textemdash as input. This graph-based LLM enables knowledge
graph question answering (KGQA), supporting human-robot interaction and
assembly task planning for specific products. Beyond interactive QA, AssemMate
also supports sensing stacked scenes and executing grasping to assist with
assembly. Specifically, a self-supervised Graph Convolutional Network (GCN)
encodes knowledge graph entities and relations into a latent space and aligns
them with LLM's representation, enabling the LLM to understand graph
information. In addition, a vision-enhanced strategy is employed to address
stacked scenes in grasping. Through training and evaluation, AssemMate
outperforms existing methods, achieving 6.4\% higher accuracy, 3 times faster
inference, and 28 times shorter context length, while demonstrating strong
generalization ability on random graphs. And our approach further demonstrates
superiority through robotic grasping experiments in both simulated and
real-world settings. More details can be found on the project page:
https://github.com/cristina304/AssemMate.git

</details>


### [38] [Inference-stage Adaptation-projection Strategy Adapts Diffusion Policy to Cross-manipulators Scenarios](https://arxiv.org/abs/2509.11621)
*Xiangtong Yao,Yirui Zhou,Yuan Meng,Yanwen Liu,Liangyu Dong,Zitao Zhang,Zhenshan Bing,Kai Huang,Fuchun Sun,Alois Knoll*

Main category: cs.RO

TL;DR: 提出一种无需重新训练的零样本适应策略，通过SE(3)空间训练和在线投影，使扩散策略能够适应新机械臂和动态任务需求


<details>
  <summary>Details</summary>
Motivation: 扩散策略在机器人操作中表现强大，但难以泛化到未见过的机械臂和末端执行器，且无法适应推理时的新任务需求，传统方法需要昂贵的数据重新收集和策略重新训练

Method: 首先在SE(3)空间使用基础机械臂的演示训练扩散策略，在线部署时通过投影将生成轨迹转换为满足新硬件和任务约束的形式，动态适应物理差异和任务要求

Result: 在真实世界的拾取放置、推动和倾倒任务中，跨多个机械臂（Franka Panda、Kuka iiwa 14）和多种末端执行器上实现了持续高成功率

Conclusion: 提出的适应投影策略在跨机械臂场景中证明有效且实用，能够实现零样本适应而无需重新训练

Abstract: Diffusion policies are powerful visuomotor models for robotic manipulation,
yet they often fail to generalize to manipulators or end-effectors unseen
during training and struggle to accommodate new task requirements at inference
time. Addressing this typically requires costly data recollection and policy
retraining for each new hardware or task configuration. To overcome this, we
introduce an adaptation-projection strategy that enables a diffusion policy to
perform zero-shot adaptation to novel manipulators and dynamic task settings,
entirely at inference time and without any retraining. Our method first trains
a diffusion policy in SE(3) space using demonstrations from a base manipulator.
During online deployment, it projects the policy's generated trajectories to
satisfy the kinematic and task-specific constraints imposed by the new hardware
and objectives. Moreover, this projection dynamically adapts to physical
differences (e.g., tool-center-point offsets, jaw widths) and task requirements
(e.g., obstacle heights), ensuring robust and successful execution. We validate
our approach on real-world pick-and-place, pushing, and pouring tasks across
multiple manipulators, including the Franka Panda and Kuka iiwa 14, equipped
with a diverse array of end-effectors like flexible grippers, Robotiq 2F/3F
grippers, and various 3D-printed designs. Our results demonstrate consistently
high success rates in these cross-manipulator scenarios, proving the
effectiveness and practicality of our adaptation-projection strategy. The code
will be released after peer review.

</details>


### [39] [ParaEQsA: Parallel and Asynchronous Embodied Questions Scheduling and Answering](https://arxiv.org/abs/2509.11663)
*Haisheng Wang,Weiming Zhi*

Main category: cs.RO

TL;DR: 本文提出了并行异步问答问题(EQsA)的新设定，开发了ParaEQsA框架进行并行优先级调度，并创建了PAEQs基准数据集来评估多问题异步到达场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的单问题问答(EQA)无法应对现实部署中多个异步到达且具有不同紧急程度的问题，需要新的框架来处理这种并行异步问答场景。

Method: 提出ParaEQsA框架，包含共享组内存模块减少冗余探索，以及优先级规划模块进行动态问题调度。使用Direct Answer Rate (DAR)和Normalized Urgency-Weighted Latency (NUWL)作为评估指标。

Result: ParaEQsA在PAEQs基准上持续优于强序列基线方法，同时减少了探索和延迟。实证评估验证了优先级、紧急度建模、空间范围、奖励估计和依赖推理等组件的相对贡献。

Conclusion: 紧急度感知的并行调度是使具身代理在现实多问题工作负载下保持响应性和效率的关键。

Abstract: This paper formulates the Embodied Questions Answering (EQsA) problem,
introduces a corresponding benchmark, and proposes a system to tackle the
problem. Classical Embodied Question Answering (EQA) is typically formulated as
answering one single question by actively exploring a 3D environment. Real
deployments, however, often demand handling multiple questions that may arrive
asynchronously and carry different urgencies. We formalize this setting as
Embodied Questions Answering (EQsA) and present ParaEQsA, a framework for
parallel, urgency-aware scheduling and answering. ParaEQsA leverages a group
memory module shared among questions to reduce redundant exploration, and a
priority-planning module to dynamically schedule questions. To evaluate this
setting, we contribute the Parallel Asynchronous Embodied Questions (PAEQs)
benchmark containing 40 indoor scenes and five questions per scene (200 in
total), featuring asynchronous follow-up questions and urgency labels. We
further propose metrics for EQsA performance: Direct Answer Rate (DAR), and
Normalized Urgency-Weighted Latency (NUWL), which jointly measure efficiency
and responsiveness of this system. ParaEQsA consistently outperforms strong
sequential baselines adapted from recent EQA systems, while reducing
exploration and delay. Empirical evaluations investigate the relative
contributions of priority, urgency modeling, spatial scope, reward estimation,
and dependency reasoning within our framework. Together, these results
demonstrate that urgency-aware, parallel scheduling is key to making embodied
agents responsive and efficient under realistic, multi-question workloads.

</details>


### [40] [Tensor Invariant Data-Assisted Control and Dynamic Decomposition of Multibody Systems](https://arxiv.org/abs/2509.11688)
*Mostafa Eslami,Maryam Babazadeh*

Main category: cs.RO

TL;DR: 本文提出了一种基于张量力学的坐标无关多体动力学框架，结合数据辅助控制架构，解决了机器人系统在复杂协作空间中学习时的数据低效问题。


<details>
  <summary>Details</summary>
Motivation: 传统坐标依赖模型无法在不同参考系间泛化物理交互，导致学习算法需要重新发现基本物理原理，造成数据低效和学习任务复杂度人为增加。

Method: 开发了基于张量力学的坐标无关非简化多体动力学模型，采用增强矩阵形式的非递归闭式牛顿-欧拉模型，结合数据辅助控制架构进行结构化分解。

Result: 通过李雅普诺夫分析证明了稳定性，仿真验证了模型和闭环系统的有效性，为等变学习等框架不变学习算法提供了理想输入。

Conclusion: 该框架直接解决了数据低效问题，提高了可解释性，为交互环境中更鲁棒和可泛化的机器人控制铺平了道路。

Abstract: The control of robotic systems in complex, shared collaborative workspaces
presents significant challenges in achieving robust performance and safety when
learning from experienced or simulated data is employed in the pipeline. A
primary bottleneck is the reliance on coordinate-dependent models, which leads
to profound data inefficiency by failing to generalize physical interactions
across different frames of reference. This forces learning algorithms to
rediscover fundamental physical principles in every new orientation,
artificially inflating the complexity of the learning task. This paper
introduces a novel framework that synergizes a coordinate-free, unreduced
multibody dynamics and kinematics model based on tensor mechanics with a
Data-Assisted Control (DAC) architecture. A non-recursive, closed-form
Newton-Euler model in an augmented matrix form is derived that is optimized for
tensor-based control design. This structure enables a principled decomposition
of the system into a structurally certain, physically grounded part and an
uncertain, empirical, and interaction-focused part, mediated by a virtual port
variable. Then, a complete, end-to-end tensor-invariant pipeline for modeling,
control, and learning is proposed. The coordinate-free control laws for the
structurally certain part provide a stable and abstract command interface,
proven via Lyapunov analysis. Eventually, the model and closed-loop system are
validated through simulations. This work provides a naturally ideal input for
data-efficient, frame-invariant learning algorithms, such as equivariant
learning, designed to learn the uncertain interaction. The synergy directly
addresses the data-inefficiency problem, increases explainability and
interpretability, and paves the way for more robust and generalizable robotic
control in interactive environments.

</details>


### [41] [From Pixels to Shelf: End-to-End Algorithmic Control of a Mobile Manipulator for Supermarket Stocking and Fronting](https://arxiv.org/abs/2509.11740)
*Davide Peron,Victor Nan Fernandez-Ayala,Lukas Segelmark*

Main category: cs.RO

TL;DR: 本文提出了一个用于超市货架自主补货的端到端机器人系统，整合了商用硬件和可扩展算法架构，在实验室环境中实现了98%以上的操作成功率，但性能仍不及人工。


<details>
  <summary>Details</summary>
Motivation: 零售环境中的自主补货面临动态人机交互、空间受限和产品几何多样性等挑战，需要开发高效可靠的自动化解决方案。

Method: 采用现成硬件和ROS2架构，结合行为树进行任务规划，微调视觉模型进行目标检测，使用两步模型预测控制和ArUco标记实现精确货架导航。

Result: 在模拟真实超市条件的实验室实验中，系统在700多次补货操作中实现了超过98%的成功率，但性能仍低于人工操作。

Conclusion: 当前自主系统的性能和成本效益仍不及人工操作，需要进一步改进才能实现商业化部署，本文为此指明了关键改进方向。

Abstract: Autonomous stocking in retail environments, particularly supermarkets,
presents challenges due to dynamic human interactions, constrained spaces, and
diverse product geometries. This paper introduces an efficient end-to-end
robotic system for autonomous shelf stocking and fronting, integrating
commercially available hardware with a scalable algorithmic architecture. A
major contribution of this work is the system integration of off-the-shelf
hardware and ROS2-based perception, planning, and control into a single
deployable platform for retail environments. Our solution leverages Behavior
Trees (BTs) for task planning, fine-tuned vision models for object detection,
and a two-step Model Predictive Control (MPC) framework for precise shelf
navigation using ArUco markers. Laboratory experiments replicating realistic
supermarket conditions demonstrate reliable performance, achieving over 98%
success in pick-and-place operations across a total of more than 700 stocking
events. However, our comparative benchmarks indicate that the performance and
cost-effectiveness of current autonomous systems remain inferior to that of
human workers, which we use to highlight key improvement areas and quantify the
progress still required before widespread commercial deployment can
realistically be achieved.

</details>


### [42] [Adaptive Motorized LiDAR Scanning Control for Robust Localization with OpenStreetMap](https://arxiv.org/abs/2509.11742)
*Jianping Li,Kaisong Zhu,Zhongyuan Liu,Rui Jin,Xinhang Xu,Pengfei Wan,Lihua Xie*

Main category: cs.RO

TL;DR: 提出了一种基于OpenStreetMap引导的自适应LiDAR扫描框架，通过整合全局先验和局部可观测性预测来提升定位鲁棒性，在校园道路、室内走廊和城市环境中显著降低了轨迹误差。


<details>
  <summary>Details</summary>
Motivation: 现有的LiDAR-to-OSM定位方法面临两个主要问题：OSM地图可能不完整或过时，以及传统电机式LiDAR采用恒定速度扫描，忽略了场景结构和地图先验，导致在特征稀疏区域浪费扫描资源并降低定位精度。

Method: 提出了自适应LiDAR扫描框架，将不确定性感知的模型预测控制与OSM感知项相结合，根据场景相关可观测性和OSM特征的空间分布自适应分配扫描资源。系统在ROS中实现，采用电机式LiDAR里程计后端。

Result: 在校园道路、室内走廊和城市环境中的实验表明，与恒定速度基线相比，该方法显著降低了轨迹误差，同时保持了扫描完整性。

Conclusion: 研究表明将开源地图与自适应LiDAR扫描相结合，能够在复杂环境中实现鲁棒且高效的定位。

Abstract: LiDAR-to-OpenStreetMap (OSM) localization has gained increasing attention, as
OSM provides lightweight global priors such as building footprints. These
priors enhance global consistency for robot navigation, but OSM is often
incomplete or outdated, limiting its reliability in real-world deployment.
Meanwhile, LiDAR itself suffers from a limited field of view (FoV), where
motorized rotation is commonly used to achieve panoramic coverage. Existing
motorized LiDAR systems, however, typically employ constant-speed scanning that
disregards both scene structure and map priors, leading to wasted effort in
feature-sparse regions and degraded localization accuracy. To address these
challenges, we propose Adaptive LiDAR Scanning with OSM guidance, a framework
that integrates global priors with local observability prediction to improve
localization robustness. Specifically, we augment uncertainty-aware model
predictive control with an OSM-aware term that adaptively allocates scanning
effort according to both scene-dependent observability and the spatial
distribution of OSM features. The method is implemented in ROS with a motorized
LiDAR odometry backend and evaluated in both simulation and real-world
experiments. Results on campus roads, indoor corridors, and urban environments
demonstrate significant reductions in trajectory error compared to
constant-speed baselines, while maintaining scan completeness. These findings
highlight the potential of coupling open-source maps with adaptive LiDAR
scanning to achieve robust and efficient localization in complex environments.

</details>


### [43] [Igniting VLMs toward the Embodied Space](https://arxiv.org/abs/2509.11766)
*Andy Zhai,Brae Liu,Bruno Fang,Chalse Cai,Ellie Ma,Ethan Yin,Hao Wang,Hugo Zhou,James Wang,Lights Shi,Lucy Liang,Make Wang,Qian Wang,Roy Gan,Ryan Yu,Shalfun Li,Starrick Liu,Sylas Chen,Vincent Chen,Zach Xu*

Main category: cs.RO

TL;DR: WALL-OSS是一个端到端的具身基础模型，通过大规模多模态预训练实现具身感知的视觉语言理解、强大的语言-动作关联和鲁棒的操作能力，解决了现有视觉语言模型在空间和具身理解方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在空间和具身理解方面存在局限，转移到具身领域时存在模态、预训练分布和训练目标的不匹配问题，动作理解和生成成为AGI发展的主要瓶颈。

Method: 采用紧密耦合的架构和多策略训练课程，实现统一跨级思维链（Unified Cross-Level CoT），在单一可微分框架内无缝统一指令推理、子目标分解和细粒度动作合成。

Result: WALL-OSS在复杂长时程操作任务上取得高成功率，展现出强大的指令跟随能力、复杂理解和推理能力，并优于强基线模型。

Conclusion: 该模型为从视觉语言模型到具身基础模型提供了可靠且可扩展的路径，推动了具身AI的发展。

Abstract: While foundation models show remarkable progress in language and vision,
existing vision-language models (VLMs) still have limited spatial and
embodiment understanding. Transferring VLMs to embodied domains reveals
fundamental mismatches between modalities, pretraining distributions, and
training objectives, leaving action comprehension and generation as a central
bottleneck on the path to AGI.
  We introduce WALL-OSS, an end-to-end embodied foundation model that leverages
large-scale multimodal pretraining to achieve (1) embodiment-aware
vision-language understanding, (2) strong language-action association, and (3)
robust manipulation capability.
  Our approach employs a tightly coupled architecture and multi-strategies
training curriculum that enables Unified Cross-Level CoT-seamlessly unifying
instruction reasoning, subgoal decomposition, and fine-grained action synthesis
within a single differentiable framework.
  Our results show that WALL-OSS attains high success on complex long-horizon
manipulations, demonstrates strong instruction-following capabilities, complex
understanding and reasoning, and outperforms strong baselines, thereby
providing a reliable and scalable path from VLMs to embodied foundation models.

</details>


### [44] [Augmented Reality-Enhanced Robot Teleoperation for Collecting User Demonstrations](https://arxiv.org/abs/2509.11783)
*Shiqi Gong,Sebastian Zudaire,Chi Zhang,Zhen Li*

Main category: cs.RO

TL;DR: 提出基于AR增强的机器人遥操作系统，通过空间点云渲染实现直观、无接触的演示编程，显著提升任务性能和用户体验


<details>
  <summary>Details</summary>
Motivation: 传统工业机器人编程复杂耗时，需要专家数周甚至数月时间。虽然演示编程(PbD)提供了更易用的替代方案，但直观的机器人控制和演示收集界面仍然具有挑战性

Method: 开发AR增强的机器人遥操作系统，整合AR控制和空间点云渲染，使操作者无需进入工作空间或使用示教器即可远程控制机器人。在ABB机器人平台上验证，包括IRB 1200工业机器人和GoFa 5协作机器人

Result: 用户研究表明，实时环境感知（特别是点云渲染）显著提升任务完成性能28%，系统可用性评分(SUS)提高12%，增强了用户体验和操作信心

Conclusion: 这项工作推动了直观机器人遥操作、AR界面设计、环境感知和工业环境中演示收集的安全机制发展，收集的演示数据可为机器学习应用提供有价值的训练数据

Abstract: Traditional industrial robot programming is often complex and time-consuming,
typically requiring weeks or even months of effort from expert programmers.
Although Programming by Demonstration (PbD) offers a more accessible
alternative, intuitive interfaces for robot control and demonstration
collection remain challenging. To address this, we propose an Augmented Reality
(AR)-enhanced robot teleoperation system that integrates AR-based control with
spatial point cloud rendering, enabling intuitive, contact-free demonstrations.
This approach allows operators to control robots remotely without entering the
workspace or using conventional tools like the teach pendant. The proposed
system is generally applicable and has been demonstrated on ABB robot
platforms, specifically validated with the IRB 1200 industrial robot and the
GoFa 5 collaborative robot. A user study evaluates the impact of real-time
environmental perception, specifically with and without point cloud rendering,
on task completion accuracy, efficiency, and user confidence. Results indicate
that enhanced perception significantly improves task performance by 28% and
enhances user experience, as reflected by a 12% increase in the System
Usability Scale (SUS) score. This work contributes to the advancement of
intuitive robot teleoperation, AR interface design, environmental perception,
and teleoperation safety mechanisms in industrial settings for demonstration
collection. The collected demonstrations may serve as valuable training data
for machine learning applications.

</details>


### [45] [Synthetic vs. Real Training Data for Visual Navigation](https://arxiv.org/abs/2509.11791)
*Lauri Suomela,Sasanka Kuruppu Arachchige,German F. Torres,Harry Edelman,Joni-Kristian Kämäräinen*

Main category: cs.RO

TL;DR: 论文研究表明，通过预训练视觉表示和实时运行的导航策略架构，模拟训练的视觉导航策略性能可以超越真实世界训练的策略，在导航成功率上分别高出31%和50%，并验证了策略在不同机器人平台上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究模拟训练与真实世界训练的视觉导航策略性能差异，探索如何通过特定架构设计来弥合模拟到现实的视觉差距，证明模拟训练策略可以达到甚至超越真实世界训练的性能。

Method: 采用预训练视觉表示的导航策略架构，在轮式移动机器人上进行评估，验证策略在无人机平台上的泛化能力，重点研究图像编码器预训练的多样性和在线策略学习。

Result: 模拟训练的策略在导航成功率上比真实世界训练版本高31%，比现有最先进方法高50%，成功验证了同一模型在不同机器人平台（无人机）上的部署和泛化能力。

Conclusion: 预训练视觉表示的多样性和在线策略学习是模拟训练相对于真实数据训练的关键优势，模拟训练的导航策略能够有效弥合模拟到现实的差距并实现卓越性能。

Abstract: This paper investigates how the performance of visual navigation policies
trained in simulation compares to policies trained with real-world data.
Performance degradation of simulator-trained policies is often significant when
they are evaluated in the real world. However, despite this well-known
sim-to-real gap, we demonstrate that simulator-trained policies can match the
performance of their real-world-trained counterparts.
  Central to our approach is a navigation policy architecture that bridges the
sim-to-real appearance gap by leveraging pretrained visual representations and
runs real-time on robot hardware. Evaluations on a wheeled mobile robot show
that the proposed policy, when trained in simulation, outperforms its
real-world-trained version by 31% and the prior state-of-the-art methods by 50%
in navigation success rate. Policy generalization is verified by deploying the
same model onboard a drone.
  Our results highlight the importance of diverse image encoder pretraining for
sim-to-real generalization, and identify on-policy learning as a key advantage
of simulated training over training with real data.

</details>


### [46] [UniPilot: Enabling GPS-Denied Autonomy Across Embodiments](https://arxiv.org/abs/2509.11793)
*Mihir Kulkarni,Mihir Dharmadhikari,Nikhil Khedekar,Morten Nissov,Mohit Singh,Philipp Weiss,Kostas Alexis*

Main category: cs.RO

TL;DR: UniPilot是一个紧凑的硬件-软件自主载荷系统，可在GPS拒止环境中为多种机器人提供自主操作能力，集成了多模态感知、路径规划和基于学习的导航策略。


<details>
  <summary>Details</summary>
Motivation: 解决在GPS拒止环境下单一模态感知方法可能失效的问题，为多样化机器人平台提供统一的自主操作解决方案。

Method: 集成LiDAR、雷达、视觉和惯性传感的多模态感知套件，运行完整的自主软件包括多模态感知、探索检查路径规划和基于学习的导航策略。

Result: 通过大量实验验证了系统在多样化环境和机器人平台上的建图、规划和安全导航能力。

Conclusion: UniPilot提供了一个可在广泛平台上部署的单一单元，具备鲁棒的定位、建图、规划和安全控制能力。

Abstract: This paper presents UniPilot, a compact hardware-software autonomy payload
that can be integrated across diverse robot embodiments to enable autonomous
operation in GPS-denied environments. The system integrates a multi-modal
sensing suite including LiDAR, radar, vision, and inertial sensing for robust
operation in conditions where uni-modal approaches may fail. UniPilot runs a
complete autonomy software comprising multi-modal perception, exploration and
inspection path planning, and learning-based navigation policies. The payload
provides robust localization, mapping, planning, and safety and control
capabilities in a single unit that can be deployed across a wide range of
platforms. A large number of experiments are conducted across diverse
environments and on a variety of robot platforms to validate the mapping,
planning, and safe navigation capabilities enabled by the payload.

</details>


### [47] [TrajBooster: Boosting Humanoid Whole-Body Manipulation via Trajectory-Centric Learning](https://arxiv.org/abs/2509.11839)
*Jiacheng Liu,Pengxiang Ding,Qihang Zhou,Yuxuan Wu,Da Huang,Zimian Peng,Wei Xiao,Weinan Zhang,Lixin Yang,Cewu Lu,Donglin Wang*

Main category: cs.RO

TL;DR: 提出了KORR框架，结合Koopman算子理论进行全局动力学建模，通过基于预测潜在状态的残差修正来改进模仿学习在长时域任务中的性能


<details>
  <summary>Details</summary>
Motivation: 现有残差策略学习主要关注局部修正，缺乏对状态演化的全局理解，限制了在未见场景下的鲁棒性和泛化能力

Method: 利用Koopman算子理论在潜在空间中施加线性时不变结构，提出KORR框架，基于Koopman预测的潜在状态进行条件化的残差修正

Result: 在长时域精细机器人家具组装任务中，KORR在各种扰动下表现出比强基线更优的性能、鲁棒性和泛化能力

Conclusion: 基于Koopman的建模有潜力将现代学习方法与经典控制理论相结合，为长时域高精度控制任务提供有效解决方案

Abstract: Imitation learning (IL) enables efficient skill acquisition from
demonstrations but often struggles with long-horizon tasks and high-precision
control due to compounding errors. Residual policy learning offers a promising,
model-agnostic solution by refining a base policy through closed-loop
corrections. However, existing approaches primarily focus on local corrections
to the base policy, lacking a global understanding of state evolution, which
limits robustness and generalization to unseen scenarios. To address this, we
propose incorporating global dynamics modeling to guide residual policy
updates. Specifically, we leverage Koopman operator theory to impose linear
time-invariant structure in a learned latent space, enabling reliable state
transitions and improved extrapolation for long-horizon prediction and unseen
environments. We introduce KORR (Koopman-guided Online Residual Refinement), a
simple yet effective framework that conditions residual corrections on
Koopman-predicted latent states, enabling globally informed and stable action
refinement. We evaluate KORR on long-horizon, fine-grained robotic furniture
assembly tasks under various perturbations. Results demonstrate consistent
gains in performance, robustness, and generalization over strong baselines. Our
findings further highlight the potential of Koopman-based modeling to bridge
modern learning methods with classical control theory. For more details, please
refer to https://jiachengliu3.github.io/TrajBooster.

</details>


### [48] [Tenma: Robust Cross-Embodiment Robot Manipulation with Diffusion Transformer](https://arxiv.org/abs/2509.11865)
*Travis Davies,Yiqi Huang,Yunxin Liu,Xiang Chen,Huxian Liu,Luhui Hu*

Main category: cs.RO

TL;DR: Tenma是一个轻量级扩散-Transformer策略，用于双手臂控制，通过跨具身标准化器、联合状态-时间编码器和优化的扩散动作解码器，在异构多模态机器人数据上实现了88.95%的平均成功率，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 虽然扩展Transformer策略和扩散模型已经推动了机器人操作的发展，但在轻量级、跨具身学习设置中结合这些技术仍然具有挑战性。研究旨在探索在异构多模态机器人数据上训练扩散-Transformer策略时，最影响稳定性和性能的设计选择。

Method: 提出Tenma框架，包含：1）跨具身标准化器将不同的状态/动作空间映射到共享潜在空间；2）联合状态-时间编码器实现时间对齐的观察学习并提升推理速度；3）优化的扩散动作解码器确保训练稳定性和学习容量。整合多视角RGB、本体感知和语言信息。

Result: 在匹配计算资源下，Tenma在分布内测试中达到88.95%的平均成功率，在物体和场景变化下保持强劲性能，显著超过基线策略（最佳分布内平均仅为18.12%）。

Conclusion: 尽管使用中等规模数据，Tenma提供了鲁棒的操作和泛化能力，表明多模态和跨具身学习策略在进一步增强基于Transformer的模仿学习策略能力方面具有巨大潜力。

Abstract: Scaling Transformer policies and diffusion models has advanced robotic
manipulation, yet combining these techniques in lightweight, cross-embodiment
learning settings remains challenging. We study design choices that most affect
stability and performance for diffusion-transformer policies trained on
heterogeneous, multimodal robot data, and introduce Tenma, a lightweight
diffusion-transformer for bi-manual arm control. Tenma integrates multiview
RGB, proprioception, and language via a cross-embodiment normalizer that maps
disparate state/action spaces into a shared latent space; a Joint State-Time
encoder for temporally aligned observation learning with inference speed
boosts; and a diffusion action decoder optimized for training stability and
learning capacity. Across benchmarks and under matched compute, Tenma achieves
an average success rate of 88.95% in-distribution and maintains strong
performance under object and scene shifts, substantially exceeding baseline
policies whose best in-distribution average is 18.12%. Despite using moderate
data scale, Tenma delivers robust manipulation and generalization, indicating
the great potential for multimodal and cross-embodiment learning strategies for
further augmenting the capacity of transformer-based imitation learning
policies.

</details>


### [49] [VH-Diffuser: Variable Horizon Diffusion Planner for Time-Aware Goal-Conditioned Trajectory Planning](https://arxiv.org/abs/2509.11930)
*Ruijia Liu,Ancheng Hou,Shaoyuan Li,Xiang Yin*

Main category: cs.RO

TL;DR: VHD框架将规划时域作为学习变量而非固定超参数，通过长度预测器预测实例特定的时域长度，指导扩散规划器生成所需长度的轨迹，解决了传统固定时域扩散规划器的长度不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散规划器在训练和推理时都依赖固定的预定义时域，这种刚性导致轨迹长度不匹配（太短或太长）以及在具有不同几何或动力学难度的实例中性能脆弱。

Method: 提出可变时域扩散器(VHD)框架：1）使用学习的长度预测器模型预测实例特定的时域；2）通过初始噪声整形控制轨迹长度；3）在随机裁剪的子轨迹上进行训练，无需架构更改。

Result: 在迷宫导航和机器人臂控制基准测试中，VHD提高了成功率和路径效率，对时域不匹配和未见长度表现出更强的鲁棒性。

Conclusion: VHD通过将时域作为学习变量，有效解决了固定时域扩散规划器的局限性，同时保持了训练的简单性和离线特性，与现有扩散规划器兼容。

Abstract: Diffusion-based planners have gained significant recent attention for their
robustness and performance in long-horizon tasks. However, most existing
planners rely on a fixed, pre-specified horizon during both training and
inference. This rigidity often produces length-mismatch (trajectories that are
too short or too long) and brittle performance across instances with varying
geometric or dynamical difficulty. In this paper, we introduce the Variable
Horizon Diffuser (VHD) framework, which treats the horizon as a learned
variable rather than a fixed hyperparameter. Given a start-goal pair, we first
predict an instance-specific horizon using a learned Length Predictor model,
which guides a Diffusion Planner to generate a trajectory of the desired
length. Our design maintains compatibility with existing diffusion planners by
controlling trajectory length through initial noise shaping and training on
randomly cropped sub-trajectories, without requiring architectural changes.
Empirically, VHD improves success rates and path efficiency in maze-navigation
and robot-arm control benchmarks, showing greater robustness to horizon
mismatch and unseen lengths, while keeping training simple and offline-only.

</details>


### [50] [E2-BKI: Evidential Ellipsoidal Bayesian Kernel Inference for Uncertainty-aware Gaussian Semantic Mapping](https://arxiv.org/abs/2509.11964)
*Junyoung Kim,Minsik Jeon,Jihong Min,Kiho Kwak,Junwon Seo*

Main category: cs.RO

TL;DR: 提出了一种不确定性感知的语义建图框架，通过证据深度学习和贝叶斯核推理处理复杂户外环境中的多种不确定性源，提升建图质量和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有语义建图方法在复杂户外环境中面临多种不确定性源的挑战，这些不确定性显著降低了建图性能，需要一种能够处理这些不确定性的鲁棒方法。

Method: 使用证据深度学习估计语义预测的不确定性，并将其整合到贝叶斯核推理中；将噪声观测聚合成高斯表示以减轻不可靠点的影响；采用几何对齐核适应复杂场景结构；通过高斯基元融合局部几何和语义信息。

Result: 在多样化越野和城市户外环境中的综合评估显示，该方法在建图质量、不确定性校准、表示灵活性和鲁棒性方面均有一致改进，同时保持实时效率。

Conclusion: 该不确定性感知语义建图框架能够有效处理复杂户外环境中的多种不确定性，提供鲁棒且高质量的3D语义表示，适用于机器人在复杂户外环境中的操作。

Abstract: Semantic mapping aims to construct a 3D semantic representation of the
environment, providing essential knowledge for robots operating in complex
outdoor settings. While Bayesian Kernel Inference (BKI) addresses
discontinuities of map inference from sparse sensor data, existing semantic
mapping methods suffer from various sources of uncertainties in challenging
outdoor environments. To address these issues, we propose an uncertainty-aware
semantic mapping framework that handles multiple sources of uncertainties,
which significantly degrade mapping performance. Our method estimates
uncertainties in semantic predictions using Evidential Deep Learning and
incorporates them into BKI for robust semantic inference. It further aggregates
noisy observations into coherent Gaussian representations to mitigate the
impact of unreliable points, while employing geometry-aligned kernels that
adapt to complex scene structures. These Gaussian primitives effectively fuse
local geometric and semantic information, enabling robust, uncertainty-aware
mapping in complex outdoor scenarios. Comprehensive evaluation across diverse
off-road and urban outdoor environments demonstrates consistent improvements in
mapping quality, uncertainty calibration, representational flexibility, and
robustness, while maintaining real-time efficiency.

</details>


### [51] [Time-Constrained Intelligent Adversaries for Automation Vulnerability Testing: A Multi-Robot Patrol Case Study](https://arxiv.org/abs/2509.11971)
*James C. Ward,Alex Bott,Connor York,Edmund R. Hunt*

Main category: cs.RO

TL;DR: 提出基于机器学习的对抗模型来模拟物理自主系统的敌对攻击，通过观察机器人巡逻行为来尝试在有限时间内无检测地进入安全环境，以评估巡逻系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 模拟物理自主系统的敌对攻击可以检验其对攻击的鲁棒性，并为漏洞感知设计提供信息。通过多机器人巡逻的视角，需要开发更现实的对抗模型来评估巡逻系统。

Method: 提出基于机器学习的对抗模型，该模型观察机器人巡逻行为，学习巡逻模式，在有限时间窗口内尝试无检测地进入安全环境。

Result: 新模型优于现有基线方法，提供了更严格的测试标准，并在多个领先的分散式多机器人巡逻策略上进行了性能测试。

Conclusion: 该对抗模型能够为巡逻系统提供更现实的评估，为未来巡逻策略设计提供有价值的见解，有助于开发更鲁棒的多机器人安全系统。

Abstract: Simulating hostile attacks of physical autonomous systems can be a useful
tool to examine their robustness to attack and inform vulnerability-aware
design. In this work, we examine this through the lens of multi-robot patrol,
by presenting a machine learning-based adversary model that observes robot
patrol behavior in order to attempt to gain undetected access to a secure
environment within a limited time duration. Such a model allows for evaluation
of a patrol system against a realistic potential adversary, offering insight
into future patrol strategy design. We show that our new model outperforms
existing baselines, thus providing a more stringent test, and examine its
performance against multiple leading decentralized multi-robot patrol
strategies.

</details>


### [52] [Gesture-Based Robot Control Integrating Mm-wave Radar and Behavior Trees](https://arxiv.org/abs/2509.12008)
*Yuqing Song,Cesare Tonola,Stefano Savazzi,Sanaz Kianoush,Nicola Pedrocchi,Stephan Sigg*

Main category: cs.RO

TL;DR: 基于毫米波雷达的手势控制机械臂系统，实现9种手势的精确识别和实时控制，解决了传统视觉系统的隐私和光照限制问题


<details>
  <summary>Details</summary>
Motivation: 随着机器人在家庭和工业环境中的普及，需要更直观、高效的非接触式人机交互方式。传统摄像头方案存在隐私担忧和环境适应性差的问题，而雷达传感既能保护隐私又能在复杂光照条件下稳定工作

Method: 使用毫米波雷达进行手势识别，将9种手势映射为实时控制指令，构建了从手势识别到机器人控制的统一实时流水线

Result: 系统能够精确识别手势并实现可靠的机械臂控制，案例研究证明了该系统在手势控制机器人操作方面的实用性、性能和可靠性

Conclusion: 毫米波雷达为手势控制机器人提供了一种隐私保护、环境适应性强的高效解决方案，实现了无缝的非接触式人机交互

Abstract: As robots become increasingly prevalent in both homes and industrial
settings, the demand for intuitive and efficient human-machine interaction
continues to rise. Gesture recognition offers an intuitive control method that
does not require physical contact with devices and can be implemented using
various sensing technologies. Wireless solutions are particularly flexible and
minimally invasive. While camera-based vision systems are commonly used, they
often raise privacy concerns and can struggle in complex or poorly lit
environments. In contrast, radar sensing preserves privacy, is robust to
occlusions and lighting, and provides rich spatial data such as distance,
relative velocity, and angle. We present a gesture-controlled robotic arm using
mm-wave radar for reliable, contactless motion recognition. Nine gestures are
recognized and mapped to real-time commands with precision. Case studies are
conducted to demonstrate the system practicality, performance and reliability
for gesture-based robotic manipulation. Unlike prior work that treats gesture
recognition and robotic control separately, our system unifies both into a
real-time pipeline for seamless, contactless human-robot interaction.

</details>


### [53] [Embodied Navigation Foundation Model](https://arxiv.org/abs/2509.12129)
*Jiazhao Zhang,Anqi Li,Yunpeng Qi,Minghan Li,Jiahang Liu,Shaoan Wang,Haoran Liu,Gengze Zhou,Yuze Wu,Xingxing Li,Yuxin Fan,Wenjun Li,Zhibo Chen,Fei Gao,Qi Wu,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: NavFoM是一个跨具身和跨任务的导航基础模型，使用统一架构处理多模态导航输入，在多个导航任务和具身形式上实现SOTA性能，无需任务特定微调。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在具身导航中的泛化能力仍局限于狭窄的任务设置和特定具身架构，需要开发能够处理不同具身形式和多样化导航任务的统一模型。

Method: 使用800万导航样本训练，涵盖四足机器人、无人机、轮式机器人和车辆等多种具身形式。采用统一架构处理不同相机配置和导航视野，引入标识符令牌嵌入相机视图信息和任务时间上下文，使用动态调整采样策略控制观测令牌。

Result: 在公共基准测试中实现了最先进或极具竞争力的性能，无需任务特定微调。真实世界实验进一步证实了强大的泛化能力和实际应用性。

Conclusion: NavFoM展示了跨具身和跨任务导航的统一建模能力，为具身AI导航提供了强大的基础模型解决方案。

Abstract: Navigation is a fundamental capability in embodied AI, representing the
intelligence required to perceive and interact within physical environments
following language instructions. Despite significant progress in large
Vision-Language Models (VLMs), which exhibit remarkable zero-shot performance
on general vision-language tasks, their generalization ability in embodied
navigation remains largely confined to narrow task settings and
embodiment-specific architectures. In this work, we introduce a
cross-embodiment and cross-task Navigation Foundation Model (NavFoM), trained
on eight million navigation samples that encompass quadrupeds, drones, wheeled
robots, and vehicles, and spanning diverse tasks such as vision-and-language
navigation, object searching, target tracking, and autonomous driving. NavFoM
employs a unified architecture that processes multimodal navigation inputs from
varying camera configurations and navigation horizons. To accommodate diverse
camera setups and temporal horizons, NavFoM incorporates identifier tokens that
embed camera view information of embodiments and the temporal context of tasks.
Furthermore, to meet the demands of real-world deployment, NavFoM controls all
observation tokens using a dynamically adjusted sampling strategy under a
limited token length budget. Extensive evaluations on public benchmarks
demonstrate that our model achieves state-of-the-art or highly competitive
performance across multiple navigation tasks and embodiments without requiring
task-specific fine-tuning. Additional real-world experiments further confirm
the strong generalization capability and practical applicability of our
approach.

</details>


### [54] [Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks](https://arxiv.org/abs/2509.12151)
*Zongyao Yi,Joachim Hertzberg,Martin Atzmueller*

Main category: cs.RO

TL;DR: 提出了一种可学习的物理模拟器，通过扩展GNN架构实现精确的机器人末端执行器运动和力扭矩预测，在模拟和真实环境中均优于基线方法


<details>
  <summary>Details</summary>
Motivation: 为了解决接触丰富的机器人操作任务中精确的运动和力扭矩预测问题，需要开发能够准确模拟物理交互的仿真器

Method: 扩展了最先进的GNN模拟器(FIGNet)，引入了新颖的节点和边类型，实现了动作条件预测以支持控制和状态估计任务

Result: 在模拟中，使用该模型的MPC控制器在peg-in-hole任务中达到了与真实动力学模型相当的性能；在真实实验中，运动预测精度提升50%，力扭矩预测精度提高3倍

Conclusion: 该可学习物理模拟器在接触丰富的机器人操作任务中表现出色，为机器人控制和状态估计提供了有效的仿真工具

Abstract: We present a learnable physics simulator that provides accurate motion and
force-torque prediction of robot end effectors in contact-rich manipulation.
The proposed model extends the state-of-the-art GNN-based simulator (FIGNet)
with novel node and edge types, enabling action-conditional predictions for
control and state estimation tasks. In simulation, the MPC agent using our
model matches the performance of the same controller with the ground truth
dynamics model in a challenging peg-in-hole task, while in the real-world
experiment, our model achieves a 50% improvement in motion prediction accuracy
and 3$\times$ increase in force-torque prediction precision over the baseline
physics simulator. Source code and data are publicly available.

</details>
