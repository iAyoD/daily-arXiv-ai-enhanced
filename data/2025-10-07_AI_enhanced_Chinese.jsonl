{"id": "2510.03342", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03342", "abs": "https://arxiv.org/abs/2510.03342", "authors": ["Abbas Abdolmaleki", "Saminda Abeyruwan", "Joshua Ainslie", "Jean-Baptiste Alayrac", "Montserrat Gonzalez Arenas", "Ashwin Balakrishna", "Nathan Batchelor", "Alex Bewley", "Jeff Bingham", "Michael Bloesch", "Konstantinos Bousmalis", "Philemon Brakel", "Anthony Brohan", "Thomas Buschmann", "Arunkumar Byravan", "Serkan Cabi", "Ken Caluwaerts", "Federico Casarini", "Christine Chan", "Oscar Chang", "London Chappellet-Volpini", "Jose Enrique Chen", "Xi Chen", "Hao-Tien Lewis Chiang", "Krzysztof Choromanski", "Adrian Collister", "David B. D'Ambrosio", "Sudeep Dasari", "Todor Davchev", "Meet Kirankumar Dave", "Coline Devin", "Norman Di Palo", "Tianli Ding", "Carl Doersch", "Adil Dostmohamed", "Yilun Du", "Debidatta Dwibedi", "Sathish Thoppay Egambaram", "Michael Elabd", "Tom Erez", "Xiaolin Fang", "Claudio Fantacci", "Cody Fong", "Erik Frey", "Chuyuan Fu", "Ruiqi Gao", "Marissa Giustina", "Keerthana Gopalakrishnan", "Laura Graesser", "Oliver Groth", "Agrim Gupta", "Roland Hafner", "Steven Hansen", "Leonard Hasenclever", "Sam Haves", "Nicolas Heess", "Brandon Hernaez", "Alex Hofer", "Jasmine Hsu", "Lu Huang", "Sandy H. Huang", "Atil Iscen", "Mithun George Jacob", "Deepali Jain", "Sally Jesmonth", "Abhishek Jindal", "Ryan Julian", "Dmitry Kalashnikov", "M. Emre Karagozler", "Stefani Karp", "Matija Kecman", "J. Chase Kew", "Donnie Kim", "Frank Kim", "Junkyung Kim", "Thomas Kipf", "Sean Kirmani", "Ksenia Konyushkova", "Li Yang Ku", "Yuheng Kuang", "Thomas Lampe", "Antoine Laurens", "Tuan Anh Le", "Isabel Leal", "Alex X. Lee", "Tsang-Wei Edward Lee", "Guy Lever", "Jacky Liang", "Li-Heng Lin", "Fangchen Liu", "Shangbang Long", "Caden Lu", "Sharath Maddineni", "Anirudha Majumdar", "Kevis-Kokitsi Maninis", "Andrew Marmon", "Sergio Martinez", "Assaf Hurwitz Michaely", "Niko Milonopoulos", "Joss Moore", "Robert Moreno", "Michael Neunert", "Francesco Nori", "Joy Ortiz", "Kenneth Oslund", "Carolina Parada", "Emilio Parisotto", "Amaris Paryag", "Acorn Pooley", "Thomas Power", "Alessio Quaglino", "Haroon Qureshi", "Rajkumar Vasudeva Raju", "Helen Ran", "Dushyant Rao", "Kanishka Rao", "Isaac Reid", "David Rendleman", "Krista Reymann", "Miguel Rivas", "Francesco Romano", "Yulia Rubanova", "Peter Pastor Sampedro", "Pannag R Sanketi", "Dhruv Shah", "Mohit Sharma", "Kathryn Shea", "Mohit Shridhar", "Charles Shu", "Vikas Sindhwani", "Sumeet Singh", "Radu Soricut", "Rachel Sterneck", "Ian Storz", "Razvan Surdulescu", "Jie Tan", "Jonathan Tompson", "Saran Tunyasuvunakool", "Jake Varley", "Grace Vesom", "Giulia Vezzani", "Maria Bauza Villalonga", "Oriol Vinyals", "Ren\u00e9 Wagner", "Ayzaan Wahid", "Stefan Welker", "Paul Wohlhart", "Chengda Wu", "Markus Wulfmeier", "Fei Xia", "Ted Xiao", "Annie Xie", "Jinyu Xie", "Peng Xu", "Sichun Xu", "Ying Xu", "Zhuo Xu", "Jimmy Yan", "Sherry Yang", "Skye Yang", "Yuxiang Yang", "Hiu Hong Yu", "Wenhao Yu", "Wentao Yuan", "Yuan Yuan", "Jingwei Zhang", "Tingnan Zhang", "Zhiyuan Zhang", "Allan Zhou", "Guangyao Zhou", "Yuxiang Zhou"], "title": "Gemini Robotics 1.5: Pushing the Frontier of Generalist Robots with Advanced Embodied Reasoning, Thinking, and Motion Transfer", "comment": null, "summary": "General-purpose robots need a deep understanding of the physical world,\nadvanced reasoning, and general and dexterous control. This report introduces\nthe latest generation of the Gemini Robotics model family: Gemini Robotics 1.5,\na multi-embodiment Vision-Language-Action (VLA) model, and Gemini Robotics-ER\n1.5, a state-of-the-art Embodied Reasoning (ER) model. We are bringing together\nthree major innovations. First, Gemini Robotics 1.5 features a novel\narchitecture and a Motion Transfer (MT) mechanism, which enables it to learn\nfrom heterogeneous, multi-embodiment robot data and makes the VLA more general.\nSecond, Gemini Robotics 1.5 interleaves actions with a multi-level internal\nreasoning process in natural language. This enables the robot to \"think before\nacting\" and notably improves its ability to decompose and execute complex,\nmulti-step tasks, and also makes the robot's behavior more interpretable to the\nuser. Third, Gemini Robotics-ER 1.5 establishes a new state-of-the-art for\nembodied reasoning, i.e., for reasoning capabilities that are critical for\nrobots, such as visual and spatial understanding, task planning, and progress\nestimation. Together, this family of models takes us a step towards an era of\nphysical agents-enabling robots to perceive, think and then act so they can\nsolve complex multi-step tasks.", "AI": {"tldr": "Gemini Robotics 1.5\u662f\u4e00\u4e2a\u591a\u5177\u8eab\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u5177\u6709\u8fd0\u52a8\u8f6c\u79fb\u673a\u5236\u548c\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\uff1bGemini Robotics-ER 1.5\u662f\u6700\u5148\u8fdb\u7684\u5177\u8eab\u63a8\u7406\u6a21\u578b\uff0c\u5171\u540c\u63a8\u52a8\u7269\u7406\u667a\u80fd\u4f53\u53d1\u5c55\u3002", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u9700\u8981\u6df1\u5ea6\u7406\u89e3\u7269\u7406\u4e16\u754c\u3001\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u548c\u901a\u7528\u7075\u5de7\u63a7\u5236\u80fd\u529b\uff0c\u73b0\u6709\u6a21\u578b\u5728\u8fd9\u4e9b\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u91c7\u7528\u65b0\u9896\u67b6\u6784\u548c\u8fd0\u52a8\u8f6c\u79fb\u673a\u5236\u4ece\u5f02\u6784\u591a\u5177\u8eab\u673a\u5668\u4eba\u6570\u636e\u4e2d\u5b66\u4e60\uff1b\u5728\u81ea\u7136\u8bed\u8a00\u4e2d\u4ea4\u9519\u8fdb\u884c\u591a\u7ea7\u5185\u90e8\u63a8\u7406\uff1b\u5f00\u53d1\u4e13\u95e8\u7528\u4e8e\u5177\u8eab\u63a8\u7406\u7684\u6a21\u578b\u3002", "result": "\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u5f02\u6784\u591a\u5177\u8eab\u6570\u636e\uff0c\u4f7fVLA\u66f4\u901a\u7528\uff1b\u901a\u8fc7\"\u5148\u601d\u8003\u540e\u884c\u52a8\"\u663e\u8457\u63d0\u9ad8\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u7684\u5206\u89e3\u548c\u6267\u884c\u80fd\u529b\uff1b\u5728\u5177\u8eab\u63a8\u7406\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8fd9\u4e00\u7cfb\u5217\u6a21\u578b\u5411\u7269\u7406\u667a\u80fd\u4f53\u65f6\u4ee3\u8fc8\u8fdb\u4e86\u4e00\u6b65\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u611f\u77e5\u3001\u601d\u8003\u7136\u540e\u884c\u52a8\uff0c\u89e3\u51b3\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u3002"}}
{"id": "2510.03457", "categories": ["cs.RO", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2510.03457", "abs": "https://arxiv.org/abs/2510.03457", "authors": ["Jianfeng Lin", "Tianyu Wang", "Baxi Chong", "Matthew Fernandez", "Zhaochen Xu", "Daniel I. Goldman"], "title": "Optimal swimming with body compliance in an overdamped medium", "comment": null, "summary": "Elongate animals and robots use undulatory body waves to locomote through\ndiverse environments. Geometric mechanics provides a framework to model and\noptimize such systems in highly damped environments, connecting a prescribed\nshape change pattern (gait) with locomotion displacement. However, existing\napproaches assume precise execution of prescribed gaits, whereas in practice\nenvironmental interactions with compliant bodies of animals or robots\nfrequently perturb the realized trajectories. In this work, we extend geometric\nmechanics to predict locomotor performance and search for optimal swimming\nstrategy of compliant undulators. We introduce a compliant extension of\nPurcell's three-link swimmer by incorporating series-connected springs at the\njoints. Body dynamics are derived with resistive force theory. Geometric\nmechanics is incorporated into movement prediction and into an optimization\nframework that identifies strategies for controlling compliant swimmers to\nachieve maximal displacement. We validate our framework on a physical\ncable-driven three-link limbless robot, and demonstrate accurate prediction and\noptimization of locomotor performance under varied programmed, state-dependent\ncompliance in a granular medium. Our results establish a systematic\nphysics-based approach for modeling and controlling compliant swimming\nlocomotion, highlighting compliance as a design feature that can be exploited\nfor robust movement in homogeneous and heterogeneous environments.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u51e0\u4f55\u529b\u5b66\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u548c\u4f18\u5316\u67d4\u6027\u6ce2\u52a8\u6e38\u6cf3\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u6027\u80fd\uff0c\u901a\u8fc7\u5f15\u5165\u5173\u8282\u5f39\u7c27\u7684\u67d4\u6027\u4e09\u8fde\u6746\u6e38\u6cf3\u5668\u6a21\u578b\uff0c\u5728\u9897\u7c92\u4ecb\u8d28\u4e2d\u9a8c\u8bc1\u4e86\u51c6\u786e\u7684\u6027\u80fd\u9884\u6d4b\u548c\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u51e0\u4f55\u529b\u5b66\u65b9\u6cd5\u5047\u8bbe\u7cbe\u786e\u6267\u884c\u9884\u8bbe\u6b65\u6001\uff0c\u4f46\u5b9e\u8df5\u4e2d\u73af\u5883\u4e0e\u67d4\u6027\u4f53\u7684\u76f8\u4e92\u4f5c\u7528\u4f1a\u5e72\u6270\u5b9e\u9645\u8f68\u8ff9\uff0c\u9700\u8981\u6269\u5c55\u6846\u67b6\u6765\u5904\u7406\u67d4\u6027\u6ce2\u52a8\u6e38\u6cf3\u5668\u7684\u8fd0\u52a8\u63a7\u5236\u95ee\u9898\u3002", "method": "\u5728Purcell\u4e09\u8fde\u6746\u6e38\u6cf3\u5668\u57fa\u7840\u4e0a\u5f15\u5165\u4e32\u8054\u5f39\u7c27\u5b9e\u73b0\u67d4\u6027\u6269\u5c55\uff0c\u7ed3\u5408\u963b\u529b\u7406\u8bba\u63a8\u5bfc\u8eab\u4f53\u52a8\u529b\u5b66\uff0c\u5c06\u51e0\u4f55\u529b\u5b66\u878d\u5165\u8fd0\u52a8\u9884\u6d4b\u548c\u4f18\u5316\u6846\u67b6\uff0c\u8bc6\u522b\u5b9e\u73b0\u6700\u5927\u4f4d\u79fb\u7684\u63a7\u5236\u7b56\u7565\u3002", "result": "\u5728\u7269\u7406\u7535\u7f06\u9a71\u52a8\u7684\u4e09\u8fde\u6746\u65e0\u80a2\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\uff0c\u5728\u9897\u7c92\u4ecb\u8d28\u4e2d\u51c6\u786e\u9884\u6d4b\u548c\u4f18\u5316\u4e86\u4e0d\u540c\u7f16\u7a0b\u72b6\u6001\u76f8\u5173\u67d4\u6027\u4e0b\u7684\u8fd0\u52a8\u6027\u80fd\u3002", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7cfb\u7edf\u7684\u57fa\u4e8e\u7269\u7406\u7684\u65b9\u6cd5\u6765\u5efa\u6a21\u548c\u63a7\u5236\u67d4\u6027\u6e38\u6cf3\u8fd0\u52a8\uff0c\u7a81\u663e\u4e86\u67d4\u6027\u4f5c\u4e3a\u8bbe\u8ba1\u7279\u5f81\u53ef\u5728\u540c\u8d28\u548c\u5f02\u8d28\u73af\u5883\u4e2d\u7528\u4e8e\u9c81\u68d2\u8fd0\u52a8\u3002"}}
{"id": "2510.03460", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03460", "abs": "https://arxiv.org/abs/2510.03460", "authors": ["Sibo Tian", "Minghui Zheng", "Xiao Liang"], "title": "Warm-Starting Optimization-Based Motion Planning for Robotic Manipulators via Point Cloud-Conditioned Flow Matching", "comment": null, "summary": "Rapid robot motion generation is critical in Human-Robot Collaboration (HRC)\nsystems, as robots need to respond to dynamic environments in real time by\ncontinuously observing their surroundings and replanning their motions to\nensure both safe interactions and efficient task execution. Current\nsampling-based motion planners face challenges in scaling to high-dimensional\nconfiguration spaces and often require post-processing to interpolate and\nsmooth the generated paths, resulting in time inefficiency in complex\nenvironments. Optimization-based planners, on the other hand, can incorporate\nmultiple constraints and generate smooth trajectories directly, making them\npotentially more time-efficient. However, optimization-based planners are\nsensitive to initialization and may get stuck in local minima. In this work, we\npresent a novel learning-based method that utilizes a Flow Matching model\nconditioned on a single-view point cloud to learn near-optimal solutions for\noptimization initialization. Our method does not require prior knowledge of the\nenvironment, such as obstacle locations and geometries, and can generate\nfeasible trajectories directly from single-view depth camera input. Simulation\nstudies on a UR5e robotic manipulator in cluttered workspaces demonstrate that\nthe proposed generative initializer achieves a high success rate on its own,\nsignificantly improves the success rate of trajectory optimization compared\nwith traditional and learning-based benchmark initializers, requires fewer\noptimization iterations, and exhibits strong generalization to unseen\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u6a21\u578b\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u5355\u89c6\u89d2\u70b9\u4e91\u751f\u6210\u4f18\u5316\u521d\u59cb\u5316\u8f68\u8ff9\uff0c\u63d0\u9ad8\u4eba\u673a\u534f\u4f5c\u4e2d\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u7684\u6548\u7387\u548c\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u91c7\u6837\u5f0f\u8fd0\u52a8\u89c4\u5212\u5668\u5728\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u4e2d\u6269\u5c55\u6027\u5dee\u3001\u9700\u8981\u540e\u5904\u7406\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u4f18\u5316\u5f0f\u89c4\u5212\u5668\u5bf9\u521d\u59cb\u5316\u654f\u611f\u3001\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6d41\u5339\u914d\u6a21\u578b\uff0c\u57fa\u4e8e\u5355\u89c6\u89d2\u70b9\u4e91\u5b66\u4e60\u63a5\u8fd1\u6700\u4f18\u7684\u4f18\u5316\u521d\u59cb\u5316\u89e3\uff0c\u65e0\u9700\u73af\u5883\u5148\u9a8c\u77e5\u8bc6\uff0c\u76f4\u63a5\u4ece\u6df1\u5ea6\u76f8\u673a\u8f93\u5165\u751f\u6210\u53ef\u884c\u8f68\u8ff9\u3002", "result": "\u5728UR5e\u673a\u68b0\u81c2\u7684\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5355\u72ec\u4f7f\u7528\u5177\u6709\u9ad8\u6210\u529f\u7387\uff0c\u663e\u8457\u63d0\u5347\u8f68\u8ff9\u4f18\u5316\u7684\u6210\u529f\u7387\uff0c\u51cf\u5c11\u4f18\u5316\u8fed\u4ee3\u6b21\u6570\uff0c\u5e76\u5728\u672a\u89c1\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u751f\u6210\u5f0f\u521d\u59cb\u5316\u5668\u5728\u4eba\u673a\u534f\u4f5c\u8fd0\u52a8\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u5b9e\u65f6\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03471", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03471", "abs": "https://arxiv.org/abs/2510.03471", "authors": ["Dingqi Zhang", "Ran Tao", "Sheng Cheng", "Naira Hovakimyan", "Mark W. Mueller"], "title": "A Simulation Evaluation Suite for Robust Adaptive Quadcopter Control", "comment": null, "summary": "Robust adaptive control methods are essential for maintaining quadcopter\nperformance under external disturbances and model uncertainties. However,\nfragmented evaluations across tasks, simulators, and implementations hinder\nsystematic comparison of these methods. This paper introduces an\neasy-to-deploy, modular simulation testbed for quadcopter control, built on\nRotorPy, that enables evaluation under a wide range of disturbances such as\nwind, payload shifts, rotor faults, and control latency. The framework includes\na library of representative adaptive and non-adaptive controllers and provides\ntask-relevant metrics to assess tracking accuracy and robustness. The unified\nmodular environment enables reproducible evaluation across control methods and\neliminates redundant reimplementation of components such as disturbance models,\ntrajectory generators, and analysis tools. We illustrate the testbed's\nversatility through examples spanning multiple disturbance scenarios and\ntrajectory types, including automated stress testing, to demonstrate its\nutility for systematic analysis. Code is available at\nhttps://github.com/Dz298/AdaptiveQuadBench.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eRotorPy\u7684\u6a21\u5757\u5316\u56db\u65cb\u7ffc\u63a7\u5236\u4eff\u771f\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7528\u4e8e\u5728\u591a\u79cd\u5e72\u6270\u6761\u4ef6\u4e0b\u7cfb\u7edf\u8bc4\u4f30\u81ea\u9002\u5e94\u63a7\u5236\u65b9\u6cd5\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u63a7\u5236\u5668\u5e93\u3002", "motivation": "\u56db\u65cb\u7ffc\u63a7\u5236\u65b9\u6cd5\u5728\u5916\u90e8\u5e72\u6270\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u8bc4\u4f30\u5206\u6563\u5728\u4e0d\u540c\u4efb\u52a1\u3001\u4eff\u771f\u5668\u548c\u5b9e\u73b0\u4e2d\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6bd4\u8f83\u3002", "method": "\u6784\u5efa\u6613\u90e8\u7f72\u7684\u6a21\u5757\u5316\u4eff\u771f\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5305\u542b\u4ee3\u8868\u6027\u81ea\u9002\u5e94\u548c\u975e\u81ea\u9002\u5e94\u63a7\u5236\u5668\u5e93\uff0c\u652f\u6301\u98ce\u6270\u3001\u8d1f\u8f7d\u53d8\u5316\u3001\u65cb\u7ffc\u6545\u969c\u548c\u63a7\u5236\u5ef6\u8fdf\u7b49\u591a\u79cd\u5e72\u6270\u573a\u666f\u3002", "result": "\u6d4b\u8bd5\u5e73\u53f0\u80fd\u591f\u8fdb\u884c\u53ef\u91cd\u590d\u7684\u63a7\u5236\u5668\u8bc4\u4f30\uff0c\u63d0\u4f9b\u4efb\u52a1\u76f8\u5173\u6307\u6807\u6765\u8bc4\u4f30\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u5e72\u6270\u573a\u666f\u548c\u8f68\u8ff9\u7c7b\u578b\u9a8c\u8bc1\u4e86\u5176\u591a\u529f\u80fd\u6027\u3002", "conclusion": "\u8be5\u7edf\u4e00\u6a21\u5757\u5316\u73af\u5883\u6d88\u9664\u4e86\u7ec4\u4ef6\u91cd\u590d\u5b9e\u73b0\uff0c\u4e3a\u7cfb\u7edf\u5206\u6790\u56db\u65cb\u7ffc\u63a7\u5236\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.03472", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.03472", "abs": "https://arxiv.org/abs/2510.03472", "authors": ["Yulun Zhang", "Alexandre O. G. Barbosa", "Federico Pecora", "Jiaoyang Li"], "title": "Destination-to-Chutes Task Mapping Optimization for Multi-Robot Coordination in Robotic Sorting Systems", "comment": "Accepted to IEEE International Symposium on Multi-Robot and\n  Multi-Agent Systems (MRS) 2025", "summary": "We study optimizing a destination-to-chutes task mapping to improve\nthroughput in Robotic Sorting Systems (RSS), where a team of robots sort\npackages on a sortation floor by transporting them from induct workstations to\neject chutes based on their shipping destinations (e.g. Los Angeles or\nPittsburgh). The destination-to-chutes task mapping is used to determine which\nchutes a robot can drop its package. Finding a high-quality task mapping is\nchallenging because of the complexity of a real-world RSS. First, optimizing\ntask mapping is interdependent with robot target assignment and path planning.\nSecond, chutes will be CLOSED for a period of time once they receive sufficient\npackages to allow for downstream processing. Third, task mapping quality\ndirectly impacts the downstream processing, as scattered chutes for the same\ndestination increase package handling time. In this paper, we first formally\ndefine task mappings and the problem of Task Mapping Optimization (TMO). We\nthen present a simulator of RSS to evaluate task mappings. We then present a\nsimple TMO method based on the Evolutionary Algorithm and Mixed Integer Linear\nProgramming, demonstrating the advantage of our optimized task mappings over\nthe greedily generated ones in various RSS setups with different map sizes,\nnumbers of chutes, and destinations. Finally, we use Quality Diversity\nalgorithms to analyze the throughput of a diverse set of task mappings. Our\ncode is available online at https://github.com/lunjohnzhang/tmo_public.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u673a\u5668\u4eba\u5206\u62e3\u7cfb\u7edf\u4e2d\u76ee\u7684\u5730\u5230\u6ed1\u69fd\u7684\u4efb\u52a1\u6620\u5c04\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u8fdb\u5316\u7b97\u6cd5\u548c\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u65b9\u6cd5\u63d0\u9ad8\u7cfb\u7edf\u541e\u5410\u91cf\uff0c\u5e76\u5728\u4e0d\u540c\u7cfb\u7edf\u8bbe\u7f6e\u4e0b\u9a8c\u8bc1\u4f18\u5316\u6548\u679c\u3002", "motivation": "\u4f18\u5316\u76ee\u7684\u5730\u5230\u6ed1\u69fd\u7684\u4efb\u52a1\u6620\u5c04\u5bf9\u4e8e\u63d0\u9ad8\u673a\u5668\u4eba\u5206\u62e3\u7cfb\u7edf\u541e\u5410\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u7cfb\u7edf\u590d\u6742\u6027\uff08\u5305\u62ec\u673a\u5668\u4eba\u76ee\u6807\u5206\u914d\u3001\u8def\u5f84\u89c4\u5212\u3001\u6ed1\u69fd\u5173\u95ed\u65f6\u95f4\u4ee5\u53ca\u4e0b\u6e38\u5904\u7406\u5f71\u54cd\uff09\u800c\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u4f7f\u7528\u8fdb\u5316\u7b97\u6cd5\u548c\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u6765\u4f18\u5316\u4efb\u52a1\u6620\u5c04\uff0c\u5e76\u5f00\u53d1\u4e86\u673a\u5668\u4eba\u5206\u62e3\u7cfb\u7edf\u6a21\u62df\u5668\u6765\u8bc4\u4f30\u4e0d\u540c\u6620\u5c04\u65b9\u6848\u3002", "result": "\u4f18\u5316\u7684\u4efb\u52a1\u6620\u5c04\u5728\u5404\u79cd\u7cfb\u7edf\u8bbe\u7f6e\uff08\u4e0d\u540c\u5730\u56fe\u5927\u5c0f\u3001\u6ed1\u69fd\u6570\u91cf\u548c\u76ee\u7684\u5730\uff09\u4e0b\u5747\u4f18\u4e8e\u8d2a\u5a6a\u751f\u6210\u7684\u6620\u5c04\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "conclusion": "\u901a\u8fc7\u8d28\u91cf\u591a\u6837\u6027\u7b97\u6cd5\u5206\u6790\u591a\u6837\u5316\u7684\u4efb\u52a1\u6620\u5c04\uff0c\u8bc1\u660e\u4e86\u4f18\u5316\u4efb\u52a1\u6620\u5c04\u5bf9\u673a\u5668\u4eba\u5206\u62e3\u7cfb\u7edf\u6027\u80fd\u7684\u91cd\u8981\u63d0\u5347\u4f5c\u7528\u3002"}}
{"id": "2510.03481", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.03481", "abs": "https://arxiv.org/abs/2510.03481", "authors": ["Khang Vo Huynh", "David Parker", "Lu Feng"], "title": "Robust Permissive Controller Synthesis for Interval MDPs", "comment": null, "summary": "We address the problem of robust permissive controller synthesis for robots\noperating under uncertain dynamics, modeled as Interval Markov Decision\nProcesses (IMDPs). IMDPs generalize standard MDPs by allowing transition\nprobabilities to vary within intervals, capturing epistemic uncertainty from\nsensing noise, actuation imprecision, and coarse system abstractions-common in\nrobotics. Traditional controller synthesis typically yields a single\ndeterministic strategy, limiting adaptability. In contrast, permissive\ncontrollers (multi-strategies) allow multiple actions per state, enabling\nruntime flexibility and resilience. However, prior work on permissive\ncontroller synthesis generally assumes exact transition probabilities, which is\nunrealistic in many robotic applications. We present the first framework for\nrobust permissive controller synthesis on IMDPs, guaranteeing that all\nstrategies compliant with the synthesized multi-strategy satisfy reachability\nor reward-based specifications under all admissible transitions. We formulate\nthe problem as mixed-integer linear programs (MILPs) and propose two encodings:\na baseline vertex-enumeration method and a scalable duality-based method that\navoids explicit enumeration. Experiments on four benchmark domains show that\nboth methods synthesize robust, maximally permissive controllers and scale to\nlarge IMDPs with up to hundreds of thousands of states.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u9488\u5bf9\u533a\u95f4\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(IMDPs)\u7684\u9c81\u68d2\u5bbd\u677e\u63a7\u5236\u5668\u5408\u6210\u6846\u67b6\uff0c\u4fdd\u8bc1\u6240\u6709\u7b26\u5408\u5408\u6210\u591a\u7b56\u7565\u7684\u7b56\u7565\u5728\u6240\u6709\u5141\u8bb8\u8f6c\u79fb\u4e0b\u6ee1\u8db3\u53ef\u8fbe\u6027\u6216\u57fa\u4e8e\u5956\u52b1\u7684\u89c4\u8303\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u4e0d\u786e\u5b9a\u52a8\u6001\u4e0b\u7684\u9c81\u68d2\u63a7\u5236\u95ee\u9898\uff0c\u4f20\u7edf\u63a7\u5236\u5668\u5408\u6210\u65b9\u6cd5\u901a\u5e38\u4ea7\u751f\u5355\u4e00\u786e\u5b9a\u6027\u7b56\u7565\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u3002IMDPs\u901a\u8fc7\u533a\u95f4\u8f6c\u79fb\u6982\u7387\u6355\u6349\u611f\u77e5\u566a\u58f0\u3001\u6267\u884c\u4e0d\u7cbe\u786e\u548c\u7cfb\u7edf\u62bd\u8c61\u5e26\u6765\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212(MILP)\uff0c\u63d0\u51fa\u4e24\u79cd\u7f16\u7801\u65b9\u6cd5\uff1a\u57fa\u4e8e\u9876\u70b9\u679a\u4e3e\u7684\u57fa\u7ebf\u65b9\u6cd5\u548c\u907f\u514d\u663e\u5f0f\u679a\u4e3e\u7684\u53ef\u6269\u5c55\u5bf9\u5076\u65b9\u6cd5\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u9886\u57df\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u5408\u6210\u9c81\u68d2\u3001\u6700\u5927\u5bbd\u677e\u7684\u63a7\u5236\u5668\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u5177\u6709\u6570\u5341\u4e07\u72b6\u6001\u7684\u5927\u578bIMDPs\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u4e86IMDPs\u4e0a\u7684\u9c81\u68d2\u5bbd\u677e\u63a7\u5236\u5668\u5408\u6210\uff0c\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u7075\u6d3b\u548c\u5f39\u6027\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03496", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03496", "abs": "https://arxiv.org/abs/2510.03496", "authors": ["Vadivelan Murugesan", "Rajasundaram Mathiazhagan", "Sanjana Joshi", "Aliasghar Arab"], "title": "Digital-Twin Evaluation for Proactive Human-Robot Collision Avoidance via Prediction-Guided A-RRT*", "comment": null, "summary": "Human-robot collaboration requires precise prediction of human motion over\nextended horizons to enable proactive collision avoidance. Unlike existing\nplanners that rely solely on kinodynamic models, we present a prediction-driven\nsafe planning framework that leverages granular, joint-by-joint human motion\nforecasting validated in a physics-based digital twin. A capsule-based\nartificial potential field (APF) converts these granular predictions into\ncollision risk metrics, triggering an Adaptive RRT* (A-RRT*) planner when\nthresholds are exceeded. The depth camera is used to extract 3D skeletal poses\nand a convolutional neural network-bidirectional long short-term memory\n(CNN-BiLSTM) model to predict individual joint trajectories ahead of time. A\ndigital twin model integrates real-time human posture prediction placed in\nfront of a simulated robot to evaluate motions and physical contacts. The\nproposed method enables validation of planned trajectories ahead of time and\nbridging potential latency gaps in updating planned trajectories in real-time.\nIn 50 trials, our method achieved 100% proactive avoidance with > 250 mm\nclearance and sub-2 s replanning, demonstrating superior precision and\nreliability compared to existing kinematic-only planners through the\nintegration of predictive human modeling with digital twin validation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u6d4b\u7684\u5b89\u5168\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u9a8c\u8bc1\u7684\u7ec6\u7c92\u5ea6\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\u6765\u4e3b\u52a8\u907f\u514d\u78b0\u649e\uff0c\u5b9e\u73b0\u4e86100%\u7684\u907f\u969c\u6210\u529f\u7387\u548c\u4e9a\u79d2\u7ea7\u91cd\u89c4\u5212\u3002", "motivation": "\u4eba\u673a\u534f\u4f5c\u9700\u8981\u957f\u671f\u7cbe\u786e\u9884\u6d4b\u4eba\u4f53\u8fd0\u52a8\u4ee5\u5b9e\u73b0\u4e3b\u52a8\u907f\u78b0\uff0c\u73b0\u6709\u89c4\u5212\u5668\u4ec5\u4f9d\u8d56\u8fd0\u52a8\u5b66\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u76f8\u673a\u63d0\u53d63D\u9aa8\u9abc\u59ff\u6001\uff0cCNN-BiLSTM\u6a21\u578b\u9884\u6d4b\u5173\u8282\u8f68\u8ff9\uff0c\u80f6\u56ca\u57fa\u4eba\u5de5\u52bf\u573a\u8f6c\u6362\u9884\u6d4b\u4e3a\u78b0\u649e\u98ce\u9669\u6307\u6807\uff0c\u89e6\u53d1\u81ea\u9002\u5e94RRT*\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u9a8c\u8bc1\u8f68\u8ff9\u3002", "result": "\u572850\u6b21\u8bd5\u9a8c\u4e2d\uff0c\u5b9e\u73b0\u4e86100%\u7684\u4e3b\u52a8\u907f\u969c\uff0c\u5b89\u5168\u8ddd\u79bb\u5927\u4e8e250\u6beb\u7c73\uff0c\u91cd\u89c4\u5212\u65f6\u95f4\u5c0f\u4e8e2\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9884\u6d4b\u6027\u4eba\u4f53\u5efa\u6a21\u4e0e\u6570\u5b57\u5b6a\u751f\u9a8c\u8bc1\u7684\u7ed3\u5408\uff0c\u5728\u7cbe\u5ea6\u548c\u53ef\u9760\u6027\u4e0a\u4f18\u4e8e\u4ec5\u57fa\u4e8e\u8fd0\u52a8\u5b66\u7684\u89c4\u5212\u5668\u3002"}}
{"id": "2510.03504", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03504", "abs": "https://arxiv.org/abs/2510.03504", "authors": ["Yutong Wang", "Yichun Qu", "Tengxiang Wang", "Lishuo Pan", "Nora Ayanian"], "title": "Distributed Connectivity Maintenance and Recovery for Quadrotor Motion Planning", "comment": null, "summary": "Maintaining connectivity is crucial in many multi-robot applications, yet\nfragile to obstacles and visual occlusions. We present a real-time distributed\nframework for multi-robot navigation certified by high-order control barrier\nfunctions (HOCBFs) that controls inter-robot proximity to maintain connectivity\nwhile avoiding collisions. We incorporate control Lyapunov functions to enable\nconnectivity recovery from initial disconnected configurations and temporary\nlosses, providing robust connectivity during navigation in obstacle-rich\nenvironments. Our trajectory generation framework concurrently produces\nplanning and control through a Bezier-parameterized trajectory, which naturally\nprovides smooth curves with arbitrary degree of derivatives. The main\ncontribution is the unified MPC-CLF-CBF framework, a continuous-time trajectory\ngeneration and control method for connectivity maintenance and recovery of\nmulti-robot systems. We validate the framework through extensive simulations\nand a physical experiment with 4 Crazyflie nano-quadrotors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u5206\u5e03\u5f0f\u591a\u673a\u5668\u4eba\u5bfc\u822a\u6846\u67b6\uff0c\u4f7f\u7528\u9ad8\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u4fdd\u8bc1\u8fde\u63a5\u6027\uff0c\u7ed3\u5408\u63a7\u5236Lyapunov\u51fd\u6570\u5b9e\u73b0\u8fde\u63a5\u6062\u590d\uff0c\u5728\u969c\u788d\u7269\u4e30\u5bcc\u73af\u5883\u4e2d\u63d0\u4f9b\u9c81\u68d2\u8fde\u63a5\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u4fdd\u6301\u8fde\u63a5\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bb9\u6613\u53d7\u5230\u969c\u788d\u7269\u548c\u89c6\u89c9\u906e\u6321\u7684\u5f71\u54cd\u800c\u4e2d\u65ad\u3002", "method": "\u91c7\u7528\u7edf\u4e00\u7684MPC-CLF-CBF\u6846\u67b6\uff0c\u901a\u8fc7Bezier\u53c2\u6570\u5316\u8f68\u8ff9\u751f\u6210\u5e73\u6ed1\u66f2\u7ebf\uff0c\u7ed3\u5408\u9ad8\u9636\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u63a7\u5236\u673a\u5668\u4eba\u95f4\u8ddd\u4ee5\u4fdd\u6301\u8fde\u63a5\uff0c\u540c\u65f6\u907f\u514d\u78b0\u649e\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u4eff\u771f\u548c4\u67b6Crazyflie\u7eb3\u7c73\u56db\u65cb\u7ffc\u7684\u7269\u7406\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u969c\u788d\u7269\u4e30\u5bcc\u7684\u73af\u5883\u4e2d\u4e3a\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u8fde\u63a5\u7ef4\u62a4\u548c\u6062\u590d\u7684\u8fde\u7eed\u65f6\u95f4\u8f68\u8ff9\u751f\u6210\u4e0e\u63a7\u5236\u65b9\u6cd5\u3002"}}
{"id": "2510.03529", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03529", "abs": "https://arxiv.org/abs/2510.03529", "authors": ["Zekai Liang", "Xiao Liang", "Soofiyan Atar", "Sreyan Das", "Zoe Chiu", "Peihan Zhang", "Florian Richter", "Shanglei Liu", "Michael C. Yip"], "title": "LapSurgie: Humanoid Robots Performing Surgery via Teleoperated Handheld Laparoscopy", "comment": null, "summary": "Robotic laparoscopic surgery has gained increasing attention in recent years\nfor its potential to deliver more efficient and precise minimally invasive\nprocedures. However, adoption of surgical robotic platforms remains largely\nconfined to high-resource medical centers, exacerbating healthcare disparities\nin rural and low-resource regions. To close this gap, a range of solutions has\nbeen explored, from remote mentorship to fully remote telesurgery. Yet, the\npractical deployment of surgical robotic systems to underserved communities\nremains an unsolved challenge. Humanoid systems offer a promising path toward\ndeployability, as they can directly operate in environments designed for humans\nwithout extensive infrastructure modifications -- including operating rooms. In\nthis work, we introduce LapSurgie, the first humanoid-robot-based laparoscopic\nteleoperation framework. The system leverages an inverse-mapping strategy for\nmanual-wristed laparoscopic instruments that abides to remote center-of-motion\nconstraints, enabling precise hand-to-tool control of off-the-shelf surgical\nlaparoscopic tools without additional setup requirements. A control console\nequipped with a stereo vision system provides real-time visual feedback.\nFinally, a comprehensive user study across platforms demonstrates the\neffectiveness of the proposed framework and provides initial evidence for the\nfeasibility of deploying humanoid robots in laparoscopic procedures.", "AI": {"tldr": "LapSurgie\u662f\u9996\u4e2a\u57fa\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8179\u8154\u955c\u8fdc\u7a0b\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u9006\u5411\u6620\u5c04\u7b56\u7565\u63a7\u5236\u6807\u51c6\u8179\u8154\u955c\u5de5\u5177\uff0c\u65e0\u9700\u989d\u5916\u8bbe\u7f6e\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u7684\u624b\u5230\u5de5\u5177\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u624b\u672f\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u8d44\u6e90\u532e\u4e4f\u5730\u533a\u90e8\u7f72\u56f0\u96be\u7684\u95ee\u9898\uff0c\u7f29\u5c0f\u533b\u7597\u8d44\u6e90\u5dee\u8ddd\uff0c\u5229\u7528\u4eba\u5f62\u673a\u5668\u4eba\u65e0\u9700\u6539\u9020\u73b0\u6709\u624b\u672f\u5ba4\u73af\u5883\u7684\u4f18\u52bf\u3002", "method": "\u91c7\u7528\u9006\u5411\u6620\u5c04\u7b56\u7565\u63a7\u5236\u624b\u52a8\u8155\u5f0f\u8179\u8154\u955c\u5668\u68b0\uff0c\u9075\u5b88\u8fdc\u7a0b\u8fd0\u52a8\u4e2d\u5fc3\u7ea6\u675f\uff0c\u914d\u5907\u7acb\u4f53\u89c6\u89c9\u7cfb\u7edf\u7684\u63a7\u5236\u53f0\u63d0\u4f9b\u5b9e\u65f6\u89c6\u89c9\u53cd\u9988\u3002", "result": "\u8de8\u5e73\u53f0\u7684\u7efc\u5408\u7528\u6237\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u521d\u6b65\u9a8c\u8bc1\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u8179\u8154\u955c\u624b\u672f\u4e2d\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u4eba\u5f62\u673a\u5668\u4eba\u7cfb\u7edf\u4e3a\u5c06\u624b\u672f\u673a\u5668\u4eba\u6280\u672f\u6269\u5c55\u5230\u8d44\u6e90\u532e\u4e4f\u5730\u533a\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0cLapSurgie\u6846\u67b6\u5c55\u793a\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.03532", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03532", "abs": "https://arxiv.org/abs/2510.03532", "authors": ["Zekai Liang", "Kazuya Miyata", "Xiao Liang", "Florian Richter", "Michael C. Yip"], "title": "Efficient Surgical Robotic Instrument Pose Reconstruction in Real World Conditions Using Unified Feature Detection", "comment": null, "summary": "Accurate camera-to-robot calibration is essential for any vision-based\nrobotic control system and especially critical in minimally invasive surgical\nrobots, where instruments conduct precise micro-manipulations. However, MIS\nrobots have long kinematic chains and partial visibility of their degrees of\nfreedom in the camera, which introduces challenges for conventional\ncamera-to-robot calibration methods that assume stiff robots with good\nvisibility. Previous works have investigated both keypoint-based and\nrendering-based approaches to address this challenge in real-world conditions;\nhowever, they often struggle with consistent feature detection or have long\ninference times, neither of which are ideal for online robot control. In this\nwork, we propose a novel framework that unifies the detection of geometric\nprimitives (keypoints and shaft edges) through a shared encoding, enabling\nefficient pose estimation via projection geometry. This architecture detects\nboth keypoints and edges in a single inference and is trained on large-scale\nsynthetic data with projective labeling. This method is evaluated across both\nfeature detection and pose estimation, with qualitative and quantitative\nresults demonstrating fast performance and state-of-the-art accuracy in\nchallenging surgical environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76f8\u673a-\u673a\u5668\u4eba\u6807\u5b9a\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u7f16\u7801\u7edf\u4e00\u68c0\u6d4b\u51e0\u4f55\u57fa\u5143\uff08\u5173\u952e\u70b9\u548c\u8f74\u8fb9\u7f18\uff09\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u624b\u672f\u73af\u5883\u4e2d\u5b9e\u73b0\u5feb\u901f\u4e14\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u3002", "motivation": "\u5fae\u521b\u624b\u672f\u673a\u5668\u4eba\u5177\u6709\u957f\u8fd0\u52a8\u94fe\u548c\u90e8\u5206\u81ea\u7531\u5ea6\u53ef\u89c1\u6027\uff0c\u4f20\u7edf\u6807\u5b9a\u65b9\u6cd5\u5047\u8bbe\u521a\u6027\u673a\u5668\u4eba\u548c\u826f\u597d\u53ef\u89c1\u6027\uff0c\u96be\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u7279\u5f81\u68c0\u6d4b\u4e00\u81f4\u6027\u6216\u63a8\u7406\u65f6\u95f4\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u5171\u4eab\u7f16\u7801\u7edf\u4e00\u68c0\u6d4b\u5173\u952e\u70b9\u548c\u8f74\u8fb9\u7f18\u51e0\u4f55\u57fa\u5143\uff0c\u5728\u5355\u6b21\u63a8\u7406\u4e2d\u540c\u65f6\u68c0\u6d4b\u4e24\u8005\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u548c\u6295\u5f71\u6807\u6ce8\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u6295\u5f71\u51e0\u4f55\u5b9e\u73b0\u9ad8\u6548\u4f4d\u59ff\u4f30\u8ba1\u3002", "result": "\u5728\u7279\u5f81\u68c0\u6d4b\u548c\u4f4d\u59ff\u4f30\u8ba1\u65b9\u9762\u5747\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5b9a\u6027\u548c\u5b9a\u91cf\u7ed3\u679c\u663e\u793a\u5728\u6311\u6218\u6027\u624b\u672f\u73af\u5883\u4e2d\u5177\u6709\u5feb\u901f\u6027\u80fd\u548c\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u624b\u672f\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u9ad8\u7cbe\u5ea6\u7684\u76f8\u673a-\u673a\u5668\u4eba\u6807\u5b9a\uff0c\u89e3\u51b3\u4e86\u957f\u8fd0\u52a8\u94fe\u548c\u90e8\u5206\u53ef\u89c1\u6027\u5e26\u6765\u7684\u95ee\u9898\u3002"}}
{"id": "2510.03547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03547", "abs": "https://arxiv.org/abs/2510.03547", "authors": ["Carina Veil", "Moritz Flaschel", "Ellen Kuhl"], "title": "Shape-Space Graphs: Fast and Collision-Free Path Planning for Soft Robots", "comment": null, "summary": "Soft robots, inspired by elephant trunks or octopus arms, offer extraordinary\nflexibility to bend, twist, and elongate in ways that rigid robots cannot.\nHowever, their motion planning remains a challenge, especially in cluttered\nenvironments with obstacles, due to their highly nonlinear and\ninfinite-dimensional kinematics. Here, we present a graph-based path planning\ntool for an elephant-trunk-inspired soft robotic arm designed with three\nartificial muscle fibers that allow for multimodal continuous deformation\nthrough contraction. Using a biomechanical model inspired by morphoelasticity\nand active filament theory, we precompute a shape library and construct a\n$k$-nearest neighbor graph in \\emph{shape space}, ensuring that each node\ncorresponds to a mechanically accurate and physically valid robot shape. For\nthe graph, we use signed distance functions to prune nodes and edges colliding\nwith obstacles, and define multi-objective edge costs based on geometric\ndistance and actuation effort, enabling energy-efficient planning with\ncollision avoidance. We demonstrate that our algorithm reliably avoids\nobstacles and generates feasible paths within milliseconds from precomputed\ngraphs using Dijkstra's algorithm. We show that including energy costs can\ndrastically reduce the actuation effort compared to geometry-only planning, at\nthe expense of longer tip trajectories. Our results highlight the potential of\nshape-space graph search for fast and reliable path planning in the field of\nsoft robotics, paving the way for real-time applications in surgical,\nindustrial, and assistive settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u641c\u7d22\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u5f62\u72b6\u5e93\u548c\u6784\u5efak\u8fd1\u90bb\u56fe\uff0c\u5b9e\u73b0\u5feb\u901f\u907f\u969c\u548c\u80fd\u91cf\u9ad8\u6548\u7684\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u5177\u6709\u6781\u9ad8\u7684\u7075\u6d3b\u6027\uff0c\u4f46\u5728\u6742\u4e71\u73af\u5883\u4e2d\u8fd0\u52a8\u89c4\u5212\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5176\u9ad8\u5ea6\u975e\u7ebf\u6027\u548c\u65e0\u9650\u7ef4\u8fd0\u52a8\u5b66\u7279\u6027\u3002", "method": "\u4f7f\u7528\u751f\u7269\u529b\u5b66\u6a21\u578b\u9884\u8ba1\u7b97\u5f62\u72b6\u5e93\uff0c\u6784\u5efa\u5f62\u72b6\u7a7a\u95f4\u7684k\u8fd1\u90bb\u56fe\uff0c\u5229\u7528\u7b26\u53f7\u8ddd\u79bb\u51fd\u6570\u4fee\u526a\u4e0e\u969c\u788d\u7269\u78b0\u649e\u7684\u8282\u70b9\u548c\u8fb9\uff0c\u5b9a\u4e49\u57fa\u4e8e\u51e0\u4f55\u8ddd\u79bb\u548c\u9a71\u52a8\u529b\u7684\u591a\u76ee\u6807\u8fb9\u6210\u672c\u3002", "result": "\u7b97\u6cd5\u80fd\u5728\u6beb\u79d2\u7ea7\u65f6\u95f4\u5185\u4ece\u9884\u8ba1\u7b97\u56fe\u4e2d\u53ef\u9760\u907f\u969c\u5e76\u751f\u6210\u53ef\u884c\u8def\u5f84\uff0c\u5305\u542b\u80fd\u91cf\u6210\u672c\u53ef\u663e\u8457\u51cf\u5c11\u9a71\u52a8\u529b\u9700\u6c42\uff08\u4f46\u4f1a\u589e\u52a0\u672b\u7aef\u8f68\u8ff9\u957f\u5ea6\uff09\u3002", "conclusion": "\u5f62\u72b6\u7a7a\u95f4\u56fe\u641c\u7d22\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5feb\u901f\u53ef\u9760\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u4e3a\u624b\u672f\u3001\u5de5\u4e1a\u548c\u8f85\u52a9\u9886\u57df\u7684\u5b9e\u65f6\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2510.03599", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03599", "abs": "https://arxiv.org/abs/2510.03599", "authors": ["Shafeef Omar", "Majid Khadiv"], "title": "Learning to Act Through Contact: A Unified View of Multi-Task Robot Learning", "comment": null, "summary": "We present a unified framework for multi-task locomotion and manipulation\npolicy learning grounded in a contact-explicit representation. Instead of\ndesigning different policies for different tasks, our approach unifies the\ndefinition of a task through a sequence of contact goals-desired contact\npositions, timings, and active end-effectors. This enables leveraging the\nshared structure across diverse contact-rich tasks, leading to a single policy\nthat can perform a wide range of tasks. In particular, we train a\ngoal-conditioned reinforcement learning (RL) policy to realise given contact\nplans. We validate our framework on multiple robotic embodiments and tasks: a\nquadruped performing multiple gaits, a humanoid performing multiple biped and\nquadrupedal gaits, and a humanoid executing different bimanual object\nmanipulation tasks. Each of these scenarios is controlled by a single policy\ntrained to execute different tasks grounded in contacts, demonstrating\nversatile and robust behaviours across morphologically distinct systems. Our\nresults show that explicit contact reasoning significantly improves\ngeneralisation to unseen scenarios, positioning contact-explicit policy\nlearning as a promising foundation for scalable loco-manipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u63a5\u89e6\u663e\u5f0f\u8868\u793a\u7684\u7edf\u4e00\u591a\u4efb\u52a1\u8fd0\u52a8\u4e0e\u64cd\u4f5c\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4e49\u63a5\u89e6\u76ee\u6807\u5e8f\u5217\u6765\u7edf\u4e00\u4efb\u52a1\u63cf\u8ff0\uff0c\u4f7f\u5355\u4e00\u7b56\u7565\u80fd\u6267\u884c\u591a\u79cd\u63a5\u89e6\u4e30\u5bcc\u7684\u4efb\u52a1\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4e3a\u4e0d\u540c\u4efb\u52a1\u8bbe\u8ba1\u4e0d\u540c\u7b56\u7565\uff0c\u7f3a\u4e4f\u5171\u4eab\u7ed3\u6784\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u63a5\u89e6\u76ee\u6807\u7684\u5171\u4eab\u7279\u6027\uff0c\u5f00\u53d1\u80fd\u5904\u7406\u591a\u6837\u5316\u4efb\u52a1\u7684\u7edf\u4e00\u7b56\u7565\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\u6765\u5b9e\u73b0\u7ed9\u5b9a\u7684\u63a5\u89e6\u8ba1\u5212\uff0c\u901a\u8fc7\u63a5\u89e6\u4f4d\u7f6e\u3001\u65f6\u95f4\u548c\u6d3b\u52a8\u672b\u7aef\u6267\u884c\u5668\u7684\u5e8f\u5217\u6765\u5b9a\u4e49\u4efb\u52a1\u3002", "result": "\u5728\u591a\u79cd\u673a\u5668\u4eba\u5f62\u6001\u548c\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff1a\u56db\u8db3\u673a\u5668\u4eba\u591a\u79cd\u6b65\u6001\u3001\u4eba\u5f62\u673a\u5668\u4eba\u53cc\u8db3\u548c\u56db\u8db3\u6b65\u6001\u3001\u53cc\u624b\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\uff0c\u5747\u7531\u5355\u4e00\u7b56\u7565\u63a7\u5236\u3002", "conclusion": "\u663e\u5f0f\u63a5\u89e6\u63a8\u7406\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u672a\u89c1\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u63a5\u89e6\u663e\u5f0f\u7b56\u7565\u5b66\u4e60\u4e3a\u53ef\u6269\u5c55\u7684\u8fd0\u52a8\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u57fa\u7840\u3002"}}
{"id": "2510.03640", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.03640", "abs": "https://arxiv.org/abs/2510.03640", "authors": ["Mostafa Emam", "Matthias Gerdts"], "title": "Safety-Oriented Dynamic Path Planning for Automated Vehicles", "comment": "Published in 2025 IEEE 101st Vehicular Technology Conference\n  (VTC2025-Spring), Oslo, Norway, June 17-20, 2025. Received Best Conference\n  Paper Award", "summary": "Ensuring safety in autonomous vehicles necessitates advanced path planning\nand obstacle avoidance capabilities, particularly in dynamic environments. This\npaper introduces a bi-level control framework that efficiently augments road\nboundaries by incorporating time-dependent grid projections of obstacle\nmovements, thus enabling precise and adaptive path planning. The main control\nloop utilizes Nonlinear Model Predictive Control (NMPC) for real-time path\noptimization, wherein homotopy-based constraint relaxation is employed to\nimprove the solvability of the optimal control problem (OCP). Furthermore, an\nindependent backup loop runs concurrently to provide safe fallback trajectories\nwhen an optimal trajectory cannot be computed by the main loop within a\ncritical time frame, thus enhancing safety and real-time performance. Our\nevaluation showcases the benefits of the proposed methods in various driving\nscenarios, highlighting the real-time applicability and robustness of our\napproach. Overall, the framework represents a significant step towards safer\nand more reliable autonomous driving in complex and dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u53cc\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u76f8\u5173\u7684\u969c\u788d\u7269\u7f51\u683c\u6295\u5f71\u589e\u5f3a\u9053\u8def\u8fb9\u754c\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u81ea\u9002\u5e94\u8def\u5f84\u89c4\u5212\u3002\u4e3b\u63a7\u5236\u73af\u4f7f\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u8fdb\u884c\u5b9e\u65f6\u8def\u5f84\u4f18\u5316\uff0c\u5e76\u91c7\u7528\u540c\u4f26\u7ea6\u675f\u677e\u5f1b\u63d0\u9ad8\u6700\u4f18\u63a7\u5236\u95ee\u9898\u7684\u53ef\u89e3\u6027\u3002\u540c\u65f6\u8fd0\u884c\u72ec\u7acb\u5907\u4efd\u73af\u63d0\u4f9b\u5b89\u5168\u56de\u9000\u8f68\u8ff9\uff0c\u589e\u5f3a\u7cfb\u7edf\u5b89\u5168\u6027\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u9700\u8981\u5148\u8fdb\u7684\u8def\u5f84\u89c4\u5212\u548c\u969c\u788d\u7269\u907f\u8ba9\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u5b89\u5168\u63a7\u5236\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u63a7\u5236\u6846\u67b6\uff1a\u4e3b\u63a7\u5236\u73af\u4f7f\u7528\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236(NMPC)\u8fdb\u884c\u5b9e\u65f6\u8def\u5f84\u4f18\u5316\uff0c\u7ed3\u5408\u540c\u4f26\u7ea6\u675f\u677e\u5f1b\u6280\u672f\uff1b\u72ec\u7acb\u5907\u4efd\u73af\u5728\u4e3b\u63a7\u5236\u73af\u65e0\u6cd5\u53ca\u65f6\u8ba1\u7b97\u6700\u4f18\u8f68\u8ff9\u65f6\u63d0\u4f9b\u5b89\u5168\u56de\u9000\u8f68\u8ff9\u3002\u901a\u8fc7\u65f6\u95f4\u76f8\u5173\u7684\u969c\u788d\u7269\u7f51\u683c\u6295\u5f71\u589e\u5f3a\u9053\u8def\u8fb9\u754c\u5efa\u6a21\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u9a7e\u9a76\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u65f6\u9002\u7528\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u548c\u969c\u788d\u7269\u907f\u8ba9\u95ee\u9898\u3002", "conclusion": "\u8be5\u6846\u67b6\u4ee3\u8868\u4e86\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u9760\u81ea\u52a8\u9a7e\u9a76\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u901a\u8fc7\u53cc\u5c42\u63a7\u5236\u7ed3\u6784\u548c\u5b9e\u65f6\u4f18\u5316\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.03644", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03644", "abs": "https://arxiv.org/abs/2510.03644", "authors": ["Mohammadjavad Javadi", "Robin Chhabra"], "title": "Geometrically Exact Hard Magneto-Elastic Cosserat Shells: Static Formulation for Shape Morphing", "comment": null, "summary": "Cosserat rod theory is the popular approach to modeling ferromagnetic soft\nrobots as 1-Dimensional (1D) slender structures in most applications, such as\nbiomedical. However, recent soft robots designed for locomotion and\nmanipulation often exhibit a large width-to-length ratio that categorizes them\nas 2D shells. For analysis and shape-morphing control purposes, we develop an\nefficient coordinate-free static model of hard-magnetic shells found in soft\nmagnetic grippers and walking soft robots. The approach is based on a novel\nformulation of Cosserat shell theory on the Special Euclidean group\n($\\mathbf{SE}(3)$). The shell is assumed to be a 2D manifold of material points\nwith six degrees of freedom (position & rotation) suitable for capturing the\nbehavior of a uniformly distributed array of spheroidal hard magnetic particles\nembedded in the rheological elastomer. The shell's configuration manifold is\nthe space of all smooth embeddings $\\mathbb{R}^2\\rightarrow\\mathbf{SE}(3)$.\nAccording to a novel definition of local deformation gradient based on the Lie\ngroup structure of $\\mathbf{SE}(3)$, we derive the strong and weak forms of\nequilibrium equations, following the principle of virtual work. We extract the\nlinearized version of the weak form for numerical implementations. The\nresulting finite element approach can avoid well-known challenges such as\nsingularity and locking phenomenon in modeling shell structures. The proposed\nmodel is analytically and experimentally validated through a series of test\ncases that demonstrate its superior efficacy, particularly when the shell\nundergoes severe rotations and displacements.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCosserat\u58f3\u7406\u8bba\u7684\u786c\u78c1\u58f3\u9759\u6001\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790\u5927\u5bbd\u957f\u6bd4\u7684\u8f6f\u78c1\u673a\u5668\u4eba\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf1D\u6746\u6a21\u578b\u4e0d\u9002\u7528\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709Cosserat\u6746\u7406\u8bba\u4e3b\u8981\u9002\u7528\u4e8e1D\u7ec6\u957f\u7ed3\u6784\uff0c\u800c\u73b0\u4ee3\u8f6f\u673a\u5668\u4eba\uff08\u5982\u6293\u53d6\u5668\u548c\u884c\u8d70\u673a\u5668\u4eba\uff09\u5f80\u5f80\u5177\u6709\u5927\u5bbd\u957f\u6bd4\uff0c\u5c5e\u4e8e2D\u58f3\u7ed3\u6784\uff0c\u9700\u8981\u65b0\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u7279\u6b8a\u6b27\u51e0\u91cc\u5f97\u7fa4(SE(3))\u5f00\u53d1\u4e86\u5750\u6807\u65e0\u5173\u7684Cosserat\u58f3\u7406\u8bba\uff0c\u5c06\u58f3\u89c6\u4e3a\u5177\u6709\u516d\u81ea\u7531\u5ea6\u76842D\u6d41\u5f62\uff0c\u91c7\u7528\u57fa\u4e8e\u674e\u7fa4\u7ed3\u6784\u7684\u5c40\u90e8\u53d8\u5f62\u68af\u5ea6\u5b9a\u4e49\uff0c\u63a8\u5bfc\u4e86\u5e73\u8861\u65b9\u7a0b\u7684\u5f3a\u5f62\u5f0f\u548c\u5f31\u5f62\u5f0f\u3002", "result": "\u63d0\u51fa\u7684\u6709\u9650\u5143\u65b9\u6cd5\u907f\u514d\u4e86\u58f3\u7ed3\u6784\u5efa\u6a21\u4e2d\u7684\u5947\u70b9\u548c\u9501\u5b9a\u73b0\u8c61\uff0c\u5728\u58f3\u7ecf\u5386\u5927\u65cb\u8f6c\u548c\u4f4d\u79fb\u65f6\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u6d4b\u8bd5\u6848\u4f8b\u8fdb\u884c\u4e86\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u786c\u78c1\u58f3\u7ed3\u6784\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5206\u6790\u548c\u5f62\u72b6\u53d8\u5f62\u63a7\u5236\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5927\u53d8\u5f62\u60c5\u51b5\u4e0b\u7684\u8f6f\u78c1\u673a\u5668\u4eba\u5e94\u7528\u3002"}}
{"id": "2510.03660", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03660", "abs": "https://arxiv.org/abs/2510.03660", "authors": ["Mohammadjavad Javadi", "Charlie Wadds", "Robin Chhabra"], "title": "An Amphibious Untethered Inchworm Soft Robot for Fast Crawling Locomotion", "comment": null, "summary": "Untethered soft robots are essential for advancing the real-world deployment\nof soft robotic systems in diverse and multitasking environments. Inspired by\nsoft-bodied inchworm, we present a fully untethered soft robot with a curved,\nflexible structure actuated by magnetic forces. The robot has a total mass of\n102.63 g and demonstrates multimodal locomotion, achieving a maximum walking\nspeed of 3.74 cm/s and a swimming speed of 0.82 cm/s. A compact and lightweight\nonboard control circuit enables wireless command transmission, while an\nintegrated camera provides environmental perception. Through structural\noptimization and system-level integration, the robot successfully performs\nwalking, steering, swimming, and payload transport without reliance on external\ninfrastructure. The robot's dynamic performance and locomotion capabilities are\nsystematically validated through experimental characterization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u5c3a\u8816\u542f\u53d1\u7684\u5b8c\u5168\u65e0\u675f\u7f1a\u8f6f\u4f53\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u78c1\u529b\u9a71\u52a8\u5b9e\u73b0\u591a\u6a21\u6001\u8fd0\u52a8\uff0c\u5305\u62ec\u884c\u8d70\u3001\u8f6c\u5411\u3001\u6e38\u6cf3\u548c\u6709\u6548\u8f7d\u8377\u8fd0\u8f93\u3002", "motivation": "\u5f00\u53d1\u65e0\u675f\u7f1a\u8f6f\u4f53\u673a\u5668\u4eba\u5bf9\u4e8e\u5728\u591a\u6837\u5316\u3001\u591a\u4efb\u52a1\u73af\u5883\u4e2d\u63a8\u8fdb\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u5f2f\u66f2\u67d4\u6027\u7ed3\u6784\uff0c\u901a\u8fc7\u78c1\u529b\u9a71\u52a8\uff0c\u914d\u5907\u7d27\u51d1\u8f7b\u91cf\u5316\u7684\u677f\u8f7d\u63a7\u5236\u7535\u8def\u5b9e\u73b0\u65e0\u7ebf\u547d\u4ee4\u4f20\u8f93\uff0c\u96c6\u6210\u6444\u50cf\u5934\u8fdb\u884c\u73af\u5883\u611f\u77e5\u3002", "result": "\u673a\u5668\u4eba\u603b\u8d28\u91cf102.63\u514b\uff0c\u6700\u5927\u884c\u8d70\u901f\u5ea63.74\u5398\u7c73/\u79d2\uff0c\u6e38\u6cf3\u901f\u5ea60.82\u5398\u7c73/\u79d2\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u884c\u8d70\u3001\u8f6c\u5411\u3001\u6e38\u6cf3\u548c\u6709\u6548\u8f7d\u8377\u8fd0\u8f93\u529f\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u4f18\u5316\u548c\u7cfb\u7edf\u7ea7\u96c6\u6210\uff0c\u8be5\u673a\u5668\u4eba\u80fd\u591f\u5728\u65e0\u9700\u5916\u90e8\u57fa\u7840\u8bbe\u65bd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u52a8\u6001\u6027\u80fd\u548c\u8fd0\u52a8\u80fd\u529b\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2510.03677", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03677", "abs": "https://arxiv.org/abs/2510.03677", "authors": ["Salim Rezvani", "Ammar Jaleel Mahmood", "Robin Chhabra"], "title": "Robust Visual Embodiment: How Robots Discover Their Bodies in Real Environments", "comment": null, "summary": "Robots with internal visual self-models promise unprecedented adaptability,\nyet existing autonomous modeling pipelines remain fragile under realistic\nsensing conditions such as noisy imagery and cluttered backgrounds. This paper\npresents the first systematic study quantifying how visual\ndegradations--including blur, salt-and-pepper noise, and Gaussian noise--affect\nrobotic self-modeling. Through both simulation and physical experiments, we\ndemonstrate their impact on morphology prediction, trajectory planning, and\ndamage recovery in state-of-the-art pipelines. To overcome these challenges, we\nintroduce a task-aware denoising framework that couples classical restoration\nwith morphology-preserving constraints, ensuring retention of structural cues\ncritical for self-modeling. In addition, we integrate semantic segmentation to\nrobustly isolate robots from cluttered and colorful scenes. Extensive\nexperiments show that our approach restores near-baseline performance across\nsimulated and physical platforms, while existing pipelines degrade\nsignificantly. These contributions advance the robustness of visual\nself-modeling and establish practical foundations for deploying self-aware\nrobots in unpredictable real-world environments.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u89c6\u89c9\u9000\u5316\uff08\u6a21\u7cca\u3001\u6912\u76d0\u566a\u58f0\u3001\u9ad8\u65af\u566a\u58f0\uff09\u5bf9\u673a\u5668\u4eba\u81ea\u5efa\u6a21\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4efb\u52a1\u611f\u77e5\u53bb\u566a\u6846\u67b6\u548c\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u5efa\u6a21\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u5efa\u6a21\u7ba1\u9053\u5728\u771f\u5b9e\u611f\u77e5\u6761\u4ef6\uff08\u5982\u566a\u58f0\u56fe\u50cf\u548c\u6742\u4e71\u80cc\u666f\uff09\u4e0b\u4ecd\u7136\u8106\u5f31\uff0c\u9700\u8981\u63d0\u5347\u673a\u5668\u4eba\u89c6\u89c9\u81ea\u5efa\u6a21\u5728\u4e0d\u53ef\u9884\u6d4b\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3002", "method": "\u5f15\u5165\u4efb\u52a1\u611f\u77e5\u53bb\u566a\u6846\u67b6\uff0c\u7ed3\u5408\u7ecf\u5178\u6062\u590d\u65b9\u6cd5\u548c\u5f62\u6001\u4fdd\u6301\u7ea6\u675f\uff1b\u96c6\u6210\u8bed\u4e49\u5206\u5272\u4ee5\u4ece\u6742\u4e71\u573a\u666f\u4e2d\u7a33\u5065\u5730\u5206\u79bb\u673a\u5668\u4eba\u3002", "result": "\u5728\u4eff\u771f\u548c\u7269\u7406\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6062\u590d\u4e86\u63a5\u8fd1\u57fa\u51c6\u7684\u6027\u80fd\uff0c\u800c\u73b0\u6709\u7ba1\u9053\u663e\u8457\u9000\u5316\u3002", "conclusion": "\u8fd9\u4e9b\u8d21\u732e\u63a8\u8fdb\u4e86\u89c6\u89c9\u81ea\u5efa\u6a21\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e3a\u5728\u4e0d\u53ef\u9884\u6d4b\u73b0\u5b9e\u73af\u5883\u4e2d\u90e8\u7f72\u81ea\u611f\u77e5\u673a\u5668\u4eba\u5efa\u7acb\u4e86\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2510.03706", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03706", "abs": "https://arxiv.org/abs/2510.03706", "authors": ["Eadom Dessalene", "Pavan Mantripragada", "Michael Maynord", "Yiannis Aloimonos"], "title": "EmbodiSwap for Zero-Shot Robot Imitation Learning", "comment": "Video link:\n  https://drive.google.com/file/d/1UccngwgPqUwPMhBja7JrXfZoTquCx_Qe/view?usp=sharing", "summary": "We introduce EmbodiSwap - a method for producing photorealistic synthetic\nrobot overlays over human video. We employ EmbodiSwap for zero-shot imitation\nlearning, bridging the embodiment gap between in-the-wild ego-centric human\nvideo and a target robot embodiment. We train a closed-loop robot manipulation\npolicy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a\nvisual backbone, repurposing V-JEPA from the domain of video understanding to\nimitation learning over synthetic robot videos. Adoption of V-JEPA outperforms\nalternative vision backbones more conventionally used within robotics. In\nreal-world tests, our zero-shot trained V-JEPA model achieves an $82\\%$ success\nrate, outperforming a few-shot trained $\\pi_0$ network as well as $\\pi_0$\ntrained over data produced by EmbodiSwap. We release (i) code for generating\nthe synthetic robot overlays which takes as input human videos and an arbitrary\nrobot URDF and generates a robot dataset, (ii) the robot dataset we synthesize\nover EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference\ncode, to facilitate reproducible research and broader adoption.", "AI": {"tldr": "EmbodiSwap\u662f\u4e00\u79cd\u5728\u4eba\u7c7b\u89c6\u9891\u4e0a\u751f\u6210\u903c\u771f\u673a\u5668\u4eba\u8986\u76d6\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u6a21\u4eff\u5b66\u4e60\uff0c\u901a\u8fc7V-JEPA\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u5728\u5408\u6210\u673a\u5668\u4eba\u89c6\u9891\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\u8fbe\u523082%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u5728\u91ce\u5916\u81ea\u6211\u4e2d\u5fc3\u4eba\u7c7b\u89c6\u9891\u4e0e\u76ee\u6807\u673a\u5668\u4eba\u5b9e\u4f53\u4e4b\u95f4\u7684\u5b9e\u4f53\u5316\u5dee\u8ddd\u95ee\u9898\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u6a21\u4eff\u5b66\u4e60\u3002", "method": "\u4f7f\u7528EmbodiSwap\u5728\u4eba\u7c7b\u89c6\u9891\u4e0a\u751f\u6210\u903c\u771f\u7684\u673a\u5668\u4eba\u8986\u76d6\uff0c\u5c06V-JEPA\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u4ece\u89c6\u9891\u7406\u89e3\u9886\u57df\u91cd\u65b0\u7528\u4e8e\u5728\u5408\u6210\u673a\u5668\u4eba\u89c6\u9891\u4e0a\u8fdb\u884c\u6a21\u4eff\u5b66\u4e60\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\uff0c\u96f6\u6837\u672c\u8bad\u7ec3\u7684V-JEPA\u6a21\u578b\u8fbe\u523082%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8efew-shot\u8bad\u7ec3\u7684\u03c0\u2080\u7f51\u7edc\u4ee5\u53ca\u57fa\u4e8eEmbodiSwap\u6570\u636e\u8bad\u7ec3\u7684\u03c0\u2080\u7f51\u7edc\u3002", "conclusion": "EmbodiSwap\u65b9\u6cd5\u6709\u6548\uff0cV-JEPA\u5728\u673a\u5668\u4eba\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff0c\u53d1\u5e03\u4e86\u76f8\u5173\u4ee3\u7801\u3001\u6570\u636e\u96c6\u548c\u6a21\u578b\u4ee5\u4fc3\u8fdb\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2510.03768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03768", "abs": "https://arxiv.org/abs/2510.03768", "authors": ["Aydin Ahmadi", "Baris Akgun"], "title": "Model-Based Adaptive Precision Control for Tabletop Planar Pushing Under Uncertain Dynamics", "comment": null, "summary": "Data-driven planar pushing methods have recently gained attention as they\nreduce manual engineering effort and improve generalization compared to\nanalytical approaches. However, most prior work targets narrow capabilities\n(e.g., side switching, precision, or single-task training), limiting broader\napplicability. We present a model-based framework for non-prehensile tabletop\npushing that uses a single learned model to address multiple tasks without\nretraining. Our approach employs a recurrent GRU-based architecture with\nadditional non-linear layers to capture object-environment dynamics while\nensuring stability. A tailored state-action representation enables the model to\ngeneralize across uncertain dynamics, variable push lengths, and diverse tasks.\nFor control, we integrate the learned dynamics with a sampling-based Model\nPredictive Path Integral (MPPI) controller, which generates adaptive,\ntask-oriented actions. This framework supports side switching, variable-length\npushes, and objectives such as precise positioning, trajectory following, and\nobstacle avoidance. Training is performed in simulation with domain\nrandomization to support sim-to-real transfer. We first evaluate the\narchitecture through ablation studies, showing improved prediction accuracy and\nstable rollouts. We then validate the full system in simulation and real-world\nexperiments using a Franka Panda robot with markerless tracking. Results\ndemonstrate high success rates in precise positioning under strict thresholds\nand strong performance in trajectory tracking and obstacle avoidance. Moreover,\nmultiple tasks are solved simply by changing the controller's objective\nfunction, without retraining. While our current focus is on a single object\ntype, we extend the framework by training on wider push lengths and designing a\nbalanced controller that reduces the number of steps for longer-horizon goals.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5b66\u4e60\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u7528\u5355\u4e00GRU\u6a21\u578b\u5904\u7406\u591a\u79cd\u684c\u9762\u63a8\u52a8\u4efb\u52a1\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u5b9a\u4f4d\u3001\u8f68\u8ff9\u8ddf\u8e2a\u548c\u907f\u969c\u7b49\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u9a71\u52a8\u7684\u5e73\u9762\u63a8\u52a8\u65b9\u6cd5\u529f\u80fd\u5355\u4e00\uff08\u5982\u4fa7\u5411\u5207\u6362\u3001\u7cbe\u786e\u5b9a\u4f4d\u6216\u5355\u4efb\u52a1\u8bad\u7ec3\uff09\uff0c\u9650\u5236\u4e86\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u901a\u7528\u6846\u67b6\u6765\u652f\u6301\u591a\u79cd\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u57fa\u4e8eGRU\u7684\u5faa\u73af\u67b6\u6784\u548c\u975e\u7ebf\u6027\u5c42\u5b66\u4e60\u7269\u4f53-\u73af\u5883\u52a8\u529b\u5b66\uff0c\u7ed3\u5408MPPI\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u751f\u6210\u81ea\u9002\u5e94\u52a8\u4f5c\u3002\u4f7f\u7528\u9886\u57df\u968f\u673a\u5316\u8bad\u7ec3\u652f\u6301\u4eff\u771f\u5230\u5b9e\u7269\u7684\u8fc1\u79fb\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u9ad8\u7cbe\u786e\u5b9a\u4f4d\u6210\u529f\u7387\u3001\u5f3a\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\u548c\u907f\u969c\u80fd\u529b\u3002\u901a\u8fc7\u6539\u53d8\u63a7\u5236\u5668\u76ee\u6807\u51fd\u6570\u5373\u53ef\u89e3\u51b3\u4e0d\u540c\u4efb\u52a1\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5355\u4e00\u6a21\u578b\u5904\u7406\u591a\u79cd\u63a8\u52a8\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u652f\u6301\u53ef\u53d8\u63a8\u52a8\u957f\u5ea6\u548c\u4e0d\u540c\u76ee\u6807\uff0c\u4e3a\u673a\u5668\u4eba\u975e\u6293\u53d6\u64cd\u4f5c\u63d0\u4f9b\u4e86\u7075\u6d3b\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.03776", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03776", "abs": "https://arxiv.org/abs/2510.03776", "authors": ["Tiago Rodrigues de Almeida", "Yufei Zhu", "Andrey Rudenko", "Tomasz P. Kucner", "Johannes A. Stork", "Martin Magnusson", "Achim J. Lilienthal"], "title": "Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets", "comment": "This paper has been accepted to the IEEE Robotics and Automation\n  Letters journal and presented at the 40th Anniversary of the IEEE\n  International Conference on Robotics and Automation, which was held in\n  Rotterdam, Netherlands on 23-26 September, 2024", "summary": "Robots and other intelligent systems navigating in complex dynamic\nenvironments should predict future actions and intentions of surrounding agents\nto reach their goals efficiently and avoid collisions. The dynamics of those\nagents strongly depends on their tasks, roles, or observable labels.\nClass-conditioned motion prediction is thus an appealing way to reduce forecast\nuncertainty and get more accurate predictions for heterogeneous agents.\nHowever, this is hardly explored in the prior art, especially for mobile robots\nand in limited data applications. In this paper, we analyse different\nclass-conditioned trajectory prediction methods on two datasets. We propose a\nset of conditional pattern-based and efficient deep learning-based baselines,\nand evaluate their performance on robotics and outdoors datasets (TH\\\"OR-MAGNI\nand Stanford Drone Dataset). Our experiments show that all methods improve\naccuracy in most of the settings when considering class labels. More\nimportantly, we observe that there are significant differences when learning\nfrom imbalanced datasets, or in new environments where sufficient data is not\navailable. In particular, we find that deep learning methods perform better on\nbalanced datasets, but in applications with limited data, e.g., cold start of a\nrobot in a new environment, or imbalanced classes, pattern-based methods may be\npreferable.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7c7b\u522b\u6761\u4ef6\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u5728\u673a\u5668\u4eba\u5b66\u548c\u5ba4\u5916\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u57fa\u4e8e\u6a21\u5f0f\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53d1\u73b0\u5728\u8003\u8651\u7c7b\u522b\u6807\u7b7e\u65f6\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u4e0d\u5e73\u8861\u6216\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u9884\u6d4b\u5468\u56f4\u667a\u80fd\u4f53\u7684\u672a\u6765\u52a8\u4f5c\u548c\u610f\u56fe\u4ee5\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\u548c\u907f\u78b0\u3002\u7531\u4e8e\u667a\u80fd\u4f53\u7684\u52a8\u6001\u7279\u6027\u53d6\u51b3\u4e8e\u5176\u4efb\u52a1\u3001\u89d2\u8272\u6216\u53ef\u89c2\u5bdf\u6807\u7b7e\uff0c\u7c7b\u522b\u6761\u4ef6\u8fd0\u52a8\u9884\u6d4b\u6210\u4e3a\u51cf\u5c11\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u548c\u63d0\u9ad8\u5f02\u6784\u667a\u80fd\u4f53\u9884\u6d4b\u51c6\u786e\u6027\u7684\u6709\u524d\u666f\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u6761\u4ef6\u6a21\u5f0f\u7684\u9ad8\u6548\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5728TH\u00d6R-MAGNI\u548c\u65af\u5766\u798f\u65e0\u4eba\u673a\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u4e0d\u540c\u7684\u7c7b\u522b\u6761\u4ef6\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u8003\u8651\u7c7b\u522b\u6807\u7b7e\u65f6\uff0c\u6240\u6709\u65b9\u6cd5\u5728\u5927\u591a\u6570\u8bbe\u7f6e\u4e0b\u90fd\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\u3002\u6df1\u5ea6\u5b66\u4e60\u5728\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u5728\u6570\u636e\u6709\u9650\u6216\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u5e94\u7528\u4e2d\uff0c\u57fa\u4e8e\u6a21\u5f0f\u7684\u65b9\u6cd5\u53ef\u80fd\u66f4\u4f18\u3002", "conclusion": "\u7c7b\u522b\u6761\u4ef6\u8f68\u8ff9\u9884\u6d4b\u80fd\u663e\u8457\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u4e0d\u5e73\u8861\u6216\u65b0\u73af\u5883\u4e2d\u6570\u636e\u4e0d\u8db3\u7684\u60c5\u51b5\u4e0b\u3002\u65b9\u6cd5\u9009\u62e9\u5e94\u57fa\u4e8e\u6570\u636e\u96c6\u7279\u6027\u548c\u5e94\u7528\u573a\u666f\uff0c\u6df1\u5ea6\u5b66\u4e60\u9002\u5408\u5e73\u8861\u6570\u636e\u96c6\uff0c\u6a21\u5f0f\u65b9\u6cd5\u9002\u5408\u6570\u636e\u6709\u9650\u573a\u666f\u3002"}}
{"id": "2510.03875", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03875", "abs": "https://arxiv.org/abs/2510.03875", "authors": ["Niranjan Kumar Ilampooranan", "Constantinos Chamzas"], "title": "COVER:COverage-VErified Roadmaps for Fixed-time Motion Planning in Continuous Semi-Static Environments", "comment": null, "summary": "Having the ability to answer motion-planning queries within a fixed time\nbudget is critical for the widespread deployment of robotic systems.\nSemi-static environments, where most obstacles remain static but a limited set\ncan vary across queries, exhibit structured variability that can be\nsystematically exploited to provide stronger guarantees than in general\nmotion-planning problems. However, prior approaches in this setting either lack\nformal guarantees or rely on restrictive discretizations of obstacle\nconfigurations, limiting their applicability in realistic domains. This paper\nintroduces COVER, a novel framework that incrementally constructs a\ncoverage-verified roadmap in semi-static environments. By partitioning the\nobstacle configuration space and solving for feasible paths within each\npartition, COVER systematically verifies feasibility of the roadmap in each\npartition and guarantees fixed-time motion planning queries within the verified\nregions. We validate COVER with a 7-DOF simulated Panda robot performing table\nand shelf tasks, demonstrating that COVER achieves broader coverage with higher\nquery success rates than prior works.", "AI": {"tldr": "COVER\u662f\u4e00\u4e2a\u5728\u51c6\u9759\u6001\u73af\u5883\u4e2d\u6784\u5efa\u8986\u76d6\u9a8c\u8bc1\u8def\u7ebf\u56fe\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u533a\u969c\u788d\u7269\u914d\u7f6e\u7a7a\u95f4\u5e76\u9a8c\u8bc1\u6bcf\u4e2a\u5206\u533a\u5185\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u56fa\u5b9a\u65f6\u95f4\u5185\u7684\u8fd0\u52a8\u89c4\u5212\u4fdd\u8bc1\u3002", "motivation": "\u5728\u51c6\u9759\u6001\u73af\u5883\u4e2d\uff0c\u5927\u591a\u6570\u969c\u788d\u7269\u4fdd\u6301\u9759\u6001\u4f46\u90e8\u5206\u969c\u788d\u7269\u53ef\u80fd\u53d8\u5316\uff0c\u8fd9\u79cd\u7ed3\u6784\u5316\u53d8\u5f02\u6027\u53ef\u4ee5\u7cfb\u7edf\u6027\u5730\u5229\u7528\uff0c\u4e3a\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u6bd4\u4e00\u822c\u60c5\u51b5\u66f4\u5f3a\u7684\u4fdd\u8bc1\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7f3a\u4e4f\u5f62\u5f0f\u5316\u4fdd\u8bc1\uff0c\u8981\u4e48\u4f9d\u8d56\u9650\u5236\u6027\u7684\u969c\u788d\u7269\u914d\u7f6e\u79bb\u6563\u5316\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u9645\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "COVER\u6846\u67b6\u589e\u91cf\u6784\u5efa\u8986\u76d6\u9a8c\u8bc1\u8def\u7ebf\u56fe\uff0c\u901a\u8fc7\u5206\u533a\u969c\u788d\u7269\u914d\u7f6e\u7a7a\u95f4\u5e76\u5728\u6bcf\u4e2a\u5206\u533a\u5185\u6c42\u89e3\u53ef\u884c\u8def\u5f84\uff0c\u7cfb\u7edf\u9a8c\u8bc1\u8def\u7ebf\u56fe\u5728\u6bcf\u4e2a\u5206\u533a\u5185\u7684\u53ef\u884c\u6027\uff0c\u4fdd\u8bc1\u5728\u5df2\u9a8c\u8bc1\u533a\u57df\u5185\u5b9e\u73b0\u56fa\u5b9a\u65f6\u95f4\u8fd0\u52a8\u89c4\u5212\u67e5\u8be2\u3002", "result": "\u57287\u81ea\u7531\u5ea6\u6a21\u62dfPanda\u673a\u5668\u4eba\u6267\u884c\u684c\u9762\u548c\u8d27\u67b6\u4efb\u52a1\u7684\u9a8c\u8bc1\u4e2d\uff0cCOVER\u6bd4\u5148\u524d\u5de5\u4f5c\u5b9e\u73b0\u4e86\u66f4\u5e7f\u6cdb\u7684\u8986\u76d6\u8303\u56f4\u548c\u66f4\u9ad8\u7684\u67e5\u8be2\u6210\u529f\u7387\u3002", "conclusion": "COVER\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u51c6\u9759\u6001\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u4f9b\u5f62\u5f0f\u5316\u4fdd\u8bc1\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u90e8\u7f72\u573a\u666f\u3002"}}
{"id": "2510.03885", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03885", "abs": "https://arxiv.org/abs/2510.03885", "authors": ["Sunghwan Kim", "Woojeh Chung", "Zhirui Dai", "Dwait Bhatt", "Arth Shukla", "Hao Su", "Yulun Tian", "Nikolay Atanasov"], "title": "Seeing the Bigger Picture: 3D Latent Mapping for Mobile Manipulation Policy Learning", "comment": "Project website can be found at\n  https://existentialrobotics.org/sbp_page/", "summary": "In this paper, we demonstrate that mobile manipulation policies utilizing a\n3D latent map achieve stronger spatial and temporal reasoning than policies\nrelying solely on images. We introduce Seeing the Bigger Picture (SBP), an\nend-to-end policy learning approach that operates directly on a 3D map of\nlatent features. In SBP, the map extends perception beyond the robot's current\nfield of view and aggregates observations over long horizons. Our mapping\napproach incrementally fuses multiview observations into a grid of\nscene-specific latent features. A pre-trained, scene-agnostic decoder\nreconstructs target embeddings from these features and enables online\noptimization of the map features during task execution. A policy, trainable\nwith behavior cloning or reinforcement learning, treats the latent map as a\nstate variable and uses global context from the map obtained via a 3D feature\naggregator. We evaluate SBP on scene-level mobile manipulation and sequential\ntabletop manipulation tasks. Our experiments demonstrate that SBP (i) reasons\nglobally over the scene, (ii) leverages the map as long-horizon memory, and\n(iii) outperforms image-based policies in both in-distribution and novel\nscenes, e.g., improving the success rate by 25% for the sequential manipulation\ntask.", "AI": {"tldr": "SBP\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e3D\u6f5c\u5728\u5730\u56fe\u7684\u79fb\u52a8\u64cd\u4f5c\u7b56\u7565\uff0c\u76f8\u6bd4\u4ec5\u4f9d\u8d56\u56fe\u50cf\u7684\u7b56\u7565\u5177\u6709\u66f4\u5f3a\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5728\u573a\u666f\u7ea7\u79fb\u52a8\u64cd\u4f5c\u548c\u987a\u5e8f\u684c\u9762\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u56fe\u50cf\u7684\u7b56\u7565\u7f3a\u4e4f\u5168\u5c40\u573a\u666f\u7406\u89e3\u548c\u957f\u671f\u8bb0\u5fc6\u80fd\u529b\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u9700\u8981\u7a7a\u95f4\u63a8\u7406\u548c\u957f\u671f\u89c2\u5bdf\u7684\u4efb\u52a1\u3002", "method": "SBP\u6784\u5efa\u7aef\u5230\u7aef\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u89c2\u5bdf\u589e\u91cf\u878d\u5408\u5230\u573a\u666f\u7279\u5b9a\u7684\u6f5c\u5728\u7279\u5f81\u7f51\u683c\u4e2d\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u91cd\u5efa\u76ee\u6807\u5d4c\u5165\uff0c\u5e76\u5229\u75283D\u7279\u5f81\u805a\u5408\u5668\u83b7\u53d6\u5168\u5c40\u4e0a\u4e0b\u6587\u3002", "result": "SBP\u5728\u5206\u5e03\u5185\u548c\u672a\u89c1\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u57fa\u4e8e\u56fe\u50cf\u7684\u7b56\u7565\uff0c\u5728\u987a\u5e8f\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u63d0\u9ad825%\uff0c\u80fd\u591f\u8fdb\u884c\u5168\u5c40\u573a\u666f\u63a8\u7406\u5e76\u5229\u7528\u5730\u56fe\u4f5c\u4e3a\u957f\u671f\u8bb0\u5fc6\u3002", "conclusion": "3D\u6f5c\u5728\u5730\u56fe\u4e3a\u79fb\u52a8\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u662f\u5b9e\u73b0\u590d\u6742\u573a\u666f\u7406\u89e3\u548c\u957f\u671f\u4efb\u52a1\u6267\u884c\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2510.03895", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03895", "abs": "https://arxiv.org/abs/2510.03895", "authors": ["Zheng Huang", "Mingyu Liu", "Xiaoyi Lin", "Muzhi Zhu", "Canyu Zhao", "Zongze Du", "Xiaoman Li", "Yiduo Jia", "Hao Zhong", "Hao Chen", "Chunhua Shen"], "title": "NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation", "comment": null, "summary": "Vision-Language-Action (VLA) models represent a pivotal advance in embodied\nintelligence, yet they confront critical barriers to real-world deployment,\nmost notably catastrophic forgetting. This issue stems from their overreliance\non continuous action sequences or action chunks, which inadvertently create\nisolated data silos that disrupt knowledge retention across tasks. To tackle\nthese challenges, we propose the Narrowing of Trajectory VLA (NoTVLA)\nframework: a novel approach that narrows its focus to sparse trajectories,\nthereby avoiding the catastrophic forgetting associated with dense trajectory\nfine-tuning. A key innovation of NoTVLA lies in its trajectory planning\nstrategy: instead of centering on the target object's trajectory, it leverages\ntemporal compression and spatial reasoning pruning specifically for the robot\nend effector's trajectory. Furthermore, training is conducted using these\nsparse trajectories rather than dense action trajectories, an optimization that\ndelivers remarkable practical advantages with better performance in zero-shot.\nIn multi-task evaluation scenarios, NoTVLA achieves superior performance and\ngeneralization compared to pi0 while operating under two critical constraints:\nit uses over an order of magnitude less computing power than pi0 and requires\nno wrist-mounted camera. This design ensures that NoTVLA's operational accuracy\nclosely approximates that of single-task expert models. Crucially, it also\npreserves the model's inherent language capabilities, enabling zero-shot\ngeneralization in specific scenarios, supporting unified model deployment\nacross multiple robot platforms, and fostering a degree of generalization even\nwhen perceiving tasks from novel perspectives.", "AI": {"tldr": "\u63d0\u51fa\u4e86NoTVLA\u6846\u67b6\uff0c\u901a\u8fc7\u7a00\u758f\u8f68\u8ff9\u89c4\u5212\u89e3\u51b3VLA\u6a21\u578b\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u3001\u96f6\u6837\u672c\u6cdb\u5316\u548c\u591a\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3VLA\u6a21\u578b\u5728\u73b0\u5b9e\u90e8\u7f72\u4e2d\u9762\u4e34\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u6e90\u4e8e\u5bf9\u8fde\u7eed\u52a8\u4f5c\u5e8f\u5217\u7684\u8fc7\u5ea6\u4f9d\u8d56\u5bfc\u81f4\u7684\u77e5\u8bc6\u9694\u79bb\u3002", "method": "\u91c7\u7528\u7a00\u758f\u8f68\u8ff9\u89c4\u5212\u7b56\u7565\uff0c\u805a\u7126\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\u800c\u975e\u76ee\u6807\u7269\u4f53\u8f68\u8ff9\uff0c\u901a\u8fc7\u65f6\u95f4\u538b\u7f29\u548c\u7a7a\u95f4\u63a8\u7406\u526a\u679d\uff0c\u4f7f\u7528\u7a00\u758f\u8f68\u8ff9\u800c\u975e\u5bc6\u96c6\u52a8\u4f5c\u8f68\u8ff9\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff0cNoTVLA\u6027\u80fd\u4f18\u4e8epi0\uff0c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u964d\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u65e0\u9700\u8155\u90e8\u6444\u50cf\u5934\uff0c\u64cd\u4f5c\u7cbe\u5ea6\u63a5\u8fd1\u5355\u4efb\u52a1\u4e13\u5bb6\u6a21\u578b\uff0c\u4fdd\u6301\u8bed\u8a00\u80fd\u529b\u5e76\u652f\u6301\u96f6\u6837\u672c\u6cdb\u5316\u3002", "conclusion": "NoTVLA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u8de8\u5e73\u53f0\u90e8\u7f72\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.03910", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03910", "abs": "https://arxiv.org/abs/2510.03910", "authors": ["Akhil Padmanabha", "Jessie Yuan", "Tanisha Mehta", "Rajat Kumar Jenamani", "Eric Hu", "Victoria de Le\u00f3n", "Anthony Wertz", "Janavi Gupta", "Ben Dodson", "Yunting Yan", "Carmel Majidi", "Tapomayukh Bhattacharjee", "Zackory Erickson"], "title": "WAFFLE: A Wearable Approach to Bite Timing Estimation in Robot-Assisted Feeding", "comment": null, "summary": "Millions of people around the world need assistance with feeding. Robotic\nfeeding systems offer the potential to enhance autonomy and quality of life for\nindividuals with impairments and reduce caregiver workload. However, their\nwidespread adoption has been limited by technical challenges such as estimating\nbite timing, the appropriate moment for the robot to transfer food to a user's\nmouth. In this work, we introduce WAFFLE: Wearable Approach For Feeding with\nLEarned bite timing, a system that accurately predicts bite timing by\nleveraging wearable sensor data to be highly reactive to natural user cues such\nas head movements, chewing, and talking. We train a supervised regression model\non bite timing data from 14 participants and incorporate a user-adjustable\nassertiveness threshold to convert predictions into proceed or stop commands.\nIn a study with 15 participants without motor impairments with the Obi feeding\nrobot, WAFFLE performs statistically on par with or better than baseline\nmethods across measures of feeling of control, robot understanding, and\nworkload, and is preferred by the majority of participants for both individual\nand social dining. We further demonstrate WAFFLE's generalizability in a study\nwith 2 participants with motor impairments in their home environments using a\nKinova 7DOF robot. Our findings support WAFFLE's effectiveness in enabling\nnatural, reactive bite timing that generalizes across users, robot hardware,\nrobot positioning, feeding trajectories, foods, and both individual and social\ndining contexts.", "AI": {"tldr": "WAFFLE\u662f\u4e00\u4e2a\u57fa\u4e8e\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u7684\u5582\u98df\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b66\u4e60\u7528\u6237\u81ea\u7136\u884c\u4e3a\uff08\u5982\u5934\u90e8\u79fb\u52a8\u3001\u5480\u56bc\u548c\u8bf4\u8bdd\uff09\u6765\u51c6\u786e\u9884\u6d4b\u54ac\u5408\u65f6\u673a\uff0c\u63d0\u9ad8\u5582\u98df\u673a\u5668\u4eba\u7684\u53cd\u5e94\u6027\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u89e3\u51b3\u5582\u98df\u673a\u5668\u4eba\u5e7f\u6cdb\u91c7\u7528\u7684\u6280\u672f\u6311\u6218\uff0c\u7279\u522b\u662f\u51c6\u786e\u4f30\u8ba1\u54ac\u5408\u65f6\u673a\u7684\u95ee\u9898\uff0c\u4ee5\u589e\u5f3a\u6b8b\u969c\u4eba\u58eb\u7684\u81ea\u4e3b\u6027\u548c\u751f\u6d3b\u8d28\u91cf\uff0c\u51cf\u8f7b\u62a4\u7406\u4eba\u5458\u8d1f\u62c5\u3002", "method": "\u4f7f\u7528\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u6570\u636e\u8bad\u7ec3\u76d1\u7763\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u54ac\u5408\u65f6\u673a\uff0c\u7ed3\u5408\u7528\u6237\u53ef\u8c03\u8282\u7684\u4e3b\u52a8\u6027\u9608\u503c\u5c06\u9884\u6d4b\u8f6c\u6362\u4e3a\u7ee7\u7eed\u6216\u505c\u6b62\u6307\u4ee4\u3002", "result": "\u572815\u540d\u65e0\u8fd0\u52a8\u969c\u788d\u53c2\u4e0e\u8005\u7684\u7814\u7a76\u4e2d\uff0cWAFFLE\u5728\u63a7\u5236\u611f\u3001\u673a\u5668\u4eba\u7406\u89e3\u548c\u5de5\u4f5c\u91cf\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5f53\uff0c\u5927\u591a\u6570\u53c2\u4e0e\u8005\u66f4\u559c\u6b22WAFFLE\u3002\u57282\u540d\u8fd0\u52a8\u969c\u788d\u53c2\u4e0e\u8005\u7684\u5bb6\u5ead\u73af\u5883\u4e2d\u4e5f\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "WAFFLE\u80fd\u591f\u5b9e\u73b0\u81ea\u7136\u3001\u53cd\u5e94\u7075\u654f\u7684\u54ac\u5408\u65f6\u673a\u9884\u6d4b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7528\u6237\u3001\u673a\u5668\u4eba\u786c\u4ef6\u3001\u4f4d\u7f6e\u3001\u5582\u98df\u8f68\u8ff9\u3001\u98df\u7269\u7c7b\u578b\u4ee5\u53ca\u4e2a\u4eba\u548c\u793e\u4ea4\u7528\u9910\u573a\u666f\u3002"}}
{"id": "2510.03919", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.03919", "abs": "https://arxiv.org/abs/2510.03919", "authors": ["Matthew Lisondra", "Junseo Kim", "Glenn Takashi Shimoda", "Kourosh Zareinia", "Sajad Saeedi"], "title": "TCB-VIO: Tightly-Coupled Focal-Plane Binary-Enhanced Visual Inertial Odometry", "comment": "Accepted at IEEE Robotics and Automation Letters", "summary": "Vision algorithms can be executed directly on the image sensor when\nimplemented on the next-generation sensors known as focal-plane\nsensor-processor arrays (FPSP)s, where every pixel has a processor. FPSPs\ngreatly improve latency, reducing the problems associated with the bottleneck\nof data transfer from a vision sensor to a processor. FPSPs accelerate\nvision-based algorithms such as visual-inertial odometry (VIO). However, VIO\nframeworks suffer from spatial drift due to the vision-based pose estimation,\nwhilst temporal drift arises from the inertial measurements. FPSPs circumvent\nthe spatial drift by operating at a high frame rate to match the high-frequency\noutput of the inertial measurements. In this paper, we present TCB-VIO, a\ntightly-coupled 6 degrees-of-freedom VIO by a Multi-State Constraint Kalman\nFilter (MSCKF), operating at a high frame-rate of 250 FPS and from IMU\nmeasurements obtained at 400 Hz. TCB-VIO outperforms state-of-the-art methods:\nROVIO, VINS-Mono, and ORB-SLAM3.", "AI": {"tldr": "TCB-VIO\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u72b6\u6001\u7ea6\u675f\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u7d27\u8026\u54086\u81ea\u7531\u5ea6\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u5728250FPS\u9ad8\u5e27\u7387\u548c400Hz IMU\u9891\u7387\u4e0b\u8fd0\u884c\uff0c\u5728FPSP\u4f20\u611f\u5668\u4e0a\u5b9e\u73b0\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u4e2d\u89c6\u89c9\u4f4d\u59ff\u4f30\u8ba1\u5e26\u6765\u7684\u7a7a\u95f4\u6f02\u79fb\u548c\u60ef\u6027\u6d4b\u91cf\u5e26\u6765\u7684\u65f6\u95f4\u6f02\u79fb\u95ee\u9898\uff0c\u5229\u7528FPSP\u4f20\u611f\u5668\u7684\u9ad8\u5e27\u7387\u7279\u6027\u6765\u5339\u914dIMU\u7684\u9ad8\u9891\u8f93\u51fa\u3002", "method": "\u4f7f\u7528\u591a\u72b6\u6001\u7ea6\u675f\u5361\u5c14\u66fc\u6ee4\u6ce2(MSCKF)\u5b9e\u73b0\u7d27\u8026\u54086\u81ea\u7531\u5ea6VIO\uff0c\u5728FPSP\u4f20\u611f\u5668\u4e0a\u4ee5250FPS\u9ad8\u5e27\u7387\u8fd0\u884c\uff0cIMU\u6d4b\u91cf\u9891\u7387\u4e3a400Hz\u3002", "result": "TCB-VIO\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5305\u62ecROVIO\u3001VINS-Mono\u548cORB-SLAM3\u3002", "conclusion": "\u5728FPSP\u4f20\u611f\u5668\u4e0a\u5b9e\u73b0\u9ad8\u5e27\u7387VIO\u7cfb\u7edf\u80fd\u6709\u6548\u51cf\u5c11\u7a7a\u95f4\u548c\u65f6\u95f4\u6f02\u79fb\uff0cTCB-VIO\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2510.03948", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.03948", "abs": "https://arxiv.org/abs/2510.03948", "authors": ["Otobong Jerome", "Geesara Prathap Kulathunga", "Devitt Dmitry", "Eugene Murawjow", "Alexandr Klimchik"], "title": "A Real-Time Framework for Intermediate Map Construction and Kinematically Feasible Off-Road Planning Without OSM", "comment": null, "summary": "Off-road environments present unique challenges for autonomous navigation due\nto their complex and unstructured nature. Traditional global path-planning\nmethods, which typically aim to minimize path length and travel time, perform\npoorly on large-scale maps and fail to account for critical factors such as\nreal-time performance, kinematic feasibility, and memory efficiency. This paper\nintroduces a novel global path-planning method specifically designed for\noff-road environments, addressing these essential factors. The method begins by\nconstructing an intermediate map within the pixel coordinate system,\nincorporating geographical features like off-road trails, waterways, restricted\nand passable areas, and trees. The planning problem is then divided into three\nsub-problems: graph-based path planning, kinematic feasibility checking, and\npath smoothing. This approach effectively meets real-time performance\nrequirements while ensuring kinematic feasibility and efficient memory use. The\nmethod was tested in various off-road environments with large-scale maps up to\nseveral square kilometers in size, successfully identifying feasible paths in\nan average of 1.5 seconds and utilizing approximately 1.5GB of memory under\nextreme conditions. The proposed framework is versatile and applicable to a\nwide range of off-road autonomous navigation tasks, including search and rescue\nmissions and agricultural operations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e13\u95e8\u9488\u5bf9\u8d8a\u91ce\u73af\u5883\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u4e2d\u95f4\u5730\u56fe\u3001\u56fe\u8def\u5f84\u89c4\u5212\u3001\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u68c0\u67e5\u548c\u8def\u5f84\u5e73\u6ed1\u4e09\u4e2a\u5b50\u95ee\u9898\uff0c\u5728\u4fdd\u8bc1\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\u786e\u4fdd\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u548c\u5185\u5b58\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5168\u5c40\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u5728\u8d8a\u91ce\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u5904\u7406\u5927\u89c4\u6a21\u5730\u56fe\uff0c\u4e14\u5ffd\u7565\u4e86\u5b9e\u65f6\u6027\u80fd\u3001\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u548c\u5185\u5b58\u6548\u7387\u7b49\u5173\u952e\u56e0\u7d20\u3002", "method": "\u9996\u5148\u5728\u50cf\u7d20\u5750\u6807\u7cfb\u4e2d\u6784\u5efa\u5305\u542b\u5730\u7406\u7279\u5f81\u7684\u4e2d\u95f4\u5730\u56fe\uff0c\u7136\u540e\u5c06\u89c4\u5212\u95ee\u9898\u5206\u89e3\u4e3a\u56fe\u8def\u5f84\u89c4\u5212\u3001\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u68c0\u67e5\u548c\u8def\u5f84\u5e73\u6ed1\u4e09\u4e2a\u5b50\u95ee\u9898\u3002", "result": "\u5728\u591a\u79cd\u8d8a\u91ce\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0c\u5730\u56fe\u89c4\u6a21\u8fbe\u6570\u5e73\u65b9\u516c\u91cc\uff0c\u5e73\u57471.5\u79d2\u627e\u5230\u53ef\u884c\u8def\u5f84\uff0c\u6781\u7aef\u6761\u4ef6\u4e0b\u5185\u5b58\u4f7f\u7528\u7ea61.5GB\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6846\u67b6\u901a\u7528\u6027\u5f3a\uff0c\u9002\u7528\u4e8e\u641c\u6551\u4efb\u52a1\u548c\u519c\u4e1a\u4f5c\u4e1a\u7b49\u591a\u79cd\u8d8a\u91ce\u81ea\u4e3b\u5bfc\u822a\u4efb\u52a1\u3002"}}
{"id": "2510.04041", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04041", "abs": "https://arxiv.org/abs/2510.04041", "authors": ["Ayudh Saxena", "Harsh Shah", "Sandeep Routray", "Rishi Rajesh Shah", "Esha Pahwa"], "title": "SITCOM: Scaling Inference-Time COMpute for VLAs", "comment": "Accepted at the NeurIPS 2025 Workshop on Space in Vision, Language,\n  and Embodied AI (SpaVLE). *Equal contribution", "summary": "Learning robust robotic control policies remains a major challenge due to the\nhigh cost of collecting labeled data, limited generalization to unseen\nenvironments, and difficulties in planning over long horizons. While\nVision-Language-Action (VLA) models offer a promising solution by grounding\nnatural language instructions into single-step control commands, they often\nlack mechanisms for lookahead and struggle with compounding errors in dynamic\ntasks. In this project, we introduce Scaling Inference-Time COMpute for VLAs\n(SITCOM), a framework that augments any pretrained VLA with model-based\nrollouts and reward-based trajectory selection, inspired by Model Predictive\nControl algorithm. SITCOM leverages a learned dynamics model to simulate\nmulti-step action rollouts to select the best candidate plan for real-world\nexecution, transforming one-shot VLAs into robust long-horizon planners. We\ndevelop an efficient transformer-based dynamics model trained on large-scale\nBridgeV2 data and fine-tuned on SIMPLER environments to bridge the Real2Sim\ngap, and score candidate rollouts using rewards from simulator. Through\ncomprehensive evaluation across multiple tasks and settings in the SIMPLER\nenvironment, we demonstrate that SITCOM when combined with a good reward\nfunction can significantly improve task completion rate from 48% to 72% using\ntrained dynamics model.", "AI": {"tldr": "SITCOM\u6846\u67b6\u901a\u8fc7\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u589e\u5f3a\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u5229\u7528\u5b66\u4e60\u5230\u7684\u52a8\u529b\u5b66\u6a21\u578b\u8fdb\u884c\u591a\u6b65\u52a8\u4f5c\u63a8\u6f14\u6765\u9009\u62e9\u6700\u4f73\u6267\u884c\u8ba1\u5212\uff0c\u663e\u8457\u63d0\u5347\u957f\u65f6\u57df\u4efb\u52a1\u7684\u5b8c\u6210\u7387\u3002", "motivation": "\u89e3\u51b3VLA\u6a21\u578b\u5728\u52a8\u6001\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u524d\u77bb\u6027\u673a\u5236\u548c\u7d2f\u79ef\u8bef\u5dee\u7684\u95ee\u9898\uff0c\u63d0\u5347\u673a\u5668\u4eba\u63a7\u5236\u7684\u9c81\u68d2\u6027\u548c\u957f\u65f6\u57df\u89c4\u5212\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u601d\u60f3\uff0c\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u52a8\u529b\u5b66\u6a21\u578b\u8fdb\u884c\u591a\u6b65\u52a8\u4f5c\u63a8\u6f14\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5668\u5956\u52b1\u5bf9\u5019\u9009\u8f68\u8ff9\u8fdb\u884c\u8bc4\u5206\u9009\u62e9\u3002", "result": "\u5728SIMPLER\u73af\u5883\u4e2d\uff0cSITCOM\u7ed3\u5408\u826f\u597d\u5956\u52b1\u51fd\u6570\u53ef\u5c06\u4efb\u52a1\u5b8c\u6210\u7387\u4ece48%\u63d0\u5347\u81f372%\u3002", "conclusion": "SITCOM\u6210\u529f\u5c06\u4e00\u6b21\u6027VLA\u6a21\u578b\u8f6c\u53d8\u4e3a\u9c81\u68d2\u7684\u957f\u65f6\u57df\u89c4\u5212\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u63a7\u5236\u6027\u80fd\u3002"}}
{"id": "2510.04074", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04074", "abs": "https://arxiv.org/abs/2510.04074", "authors": ["Chung-Pang Wang", "Changwei Chen", "Xiao Liang", "Soofiyan Atar", "Florian Richter", "Michael Yip"], "title": "Feedback Matters: Augmenting Autonomous Dissection with Visual and Topological Feedback", "comment": null, "summary": "Autonomous surgical systems must adapt to highly dynamic environments where\ntissue properties and visual cues evolve rapidly. Central to such adaptability\nis feedback: the ability to sense, interpret, and respond to changes during\nexecution. While feedback mechanisms have been explored in surgical robotics,\nranging from tool and tissue tracking to error detection, existing methods\nremain limited in handling the topological and perceptual challenges of tissue\ndissection. In this work, we propose a feedback-enabled framework for\nautonomous tissue dissection that explicitly reasons about topological changes\nfrom endoscopic images after each dissection action. This structured feedback\nguides subsequent actions, enabling the system to localize dissection progress\nand adapt policies online. To improve the reliability of such feedback, we\nintroduce visibility metrics that quantify tissue exposure and formulate\noptimal controller designs that actively manipulate tissue to maximize\nvisibility. Finally, we integrate these feedback mechanisms with both\nplanning-based and learning-based dissection methods, and demonstrate\nexperimentally that they significantly enhance autonomy, reduce errors, and\nimprove robustness in complex surgical scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u7ec4\u7ec7\u89e3\u5256\u7684\u53cd\u9988\u9a71\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u7aa5\u955c\u56fe\u50cf\u63a8\u7406\u62d3\u6251\u53d8\u5316\uff0c\u7ed3\u5408\u53ef\u89c1\u6027\u5ea6\u91cf\u548c\u6700\u4f18\u63a7\u5236\u5668\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u624b\u672f\u81ea\u4e3b\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u624b\u672f\u673a\u5668\u4eba\u53cd\u9988\u673a\u5236\u5728\u5904\u7406\u7ec4\u7ec7\u89e3\u5256\u7684\u62d3\u6251\u548c\u611f\u77e5\u6311\u6218\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u80fd\u591f\u9002\u5e94\u52a8\u6001\u73af\u5883\u53d8\u5316\u7684\u53cd\u9988\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u53cd\u9988\u9a71\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u6790\u62d3\u6251\u53d8\u5316\uff0c\u5f15\u5165\u53ef\u89c1\u6027\u5ea6\u91cf\u91cf\u5316\u7ec4\u7ec7\u66b4\u9732\u7a0b\u5ea6\uff0c\u8bbe\u8ba1\u6700\u4f18\u63a7\u5236\u5668\u4e3b\u52a8\u64cd\u7eb5\u7ec4\u7ec7\u4ee5\u6700\u5927\u5316\u53ef\u89c1\u6027\uff0c\u5e76\u4e0e\u89c4\u5212\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u96c6\u6210\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u53cd\u9988\u673a\u5236\u663e\u8457\u589e\u5f3a\u4e86\u81ea\u4e3b\u6027\uff0c\u51cf\u5c11\u4e86\u9519\u8bef\uff0c\u5728\u590d\u6742\u624b\u672f\u573a\u666f\u4e2d\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3002", "conclusion": "\u53cd\u9988\u673a\u5236\u5bf9\u4e8e\u81ea\u4e3b\u624b\u672f\u7cfb\u7edf\u5728\u52a8\u6001\u73af\u5883\u4e2d\u9002\u5e94\u7ec4\u7ec7\u7279\u6027\u548c\u89c6\u89c9\u7ebf\u7d22\u53d8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2510.04076", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04076", "abs": "https://arxiv.org/abs/2510.04076", "authors": ["Amin Vahidi-Moghaddam", "Sayed Pedram Haeri Boroujeni", "Iman Jebellat", "Ehsan Jebellat", "Niloufar Mehrabi", "Zhaojian Li"], "title": "From Shadow to Light: Toward Safe and Efficient Policy Learning Across MPC, DeePC, RL, and LLM Agents", "comment": null, "summary": "One of the main challenges in modern control applications, particularly in\nrobot and vehicle motion control, is achieving accurate, fast, and safe\nmovement. To address this, optimal control policies have been developed to\nenforce safety while ensuring high performance. Since basic first-principles\nmodels of real systems are often available, model-based controllers are widely\nused. Model predictive control (MPC) is a leading approach that optimizes\nperformance while explicitly handling safety constraints. However, obtaining\naccurate models for complex systems is difficult, which motivates data-driven\nalternatives. ML-based MPC leverages learned models to reduce reliance on\nhand-crafted dynamics, while reinforcement learning (RL) can learn near-optimal\npolicies directly from interaction data. Data-enabled predictive control\n(DeePC) goes further by bypassing modeling altogether, directly learning safe\npolicies from raw input-output data. Recently, large language model (LLM)\nagents have also emerged, translating natural language instructions into\nstructured formulations of optimal control problems. Despite these advances,\ndata-driven policies face significant limitations. They often suffer from slow\nresponse times, high computational demands, and large memory needs, making them\nless practical for real-world systems with fast dynamics, limited onboard\ncomputing, or strict memory constraints. To address this, various technique,\nsuch as reduced-order modeling, function-approximated policy learning, and\nconvex relaxations, have been proposed to reduce computational complexity. In\nthis paper, we present eight such approaches and demonstrate their\neffectiveness across real-world applications, including robotic arms, soft\nrobots, and vehicle motion control.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u73b0\u4ee3\u63a7\u5236\u5e94\u7528\u4e2d\u6570\u636e\u9a71\u52a8\u63a7\u5236\u7b56\u7565\u9762\u4e34\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u54cd\u5e94\u901f\u5ea6\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u516b\u79cd\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u6280\u672f\u65b9\u6cd5\u3002", "motivation": "\u73b0\u4ee3\u63a7\u5236\u5e94\u7528\uff08\u5982\u673a\u5668\u4eba\u548c\u8f66\u8f86\u8fd0\u52a8\u63a7\u5236\uff09\u9700\u8981\u51c6\u786e\u3001\u5feb\u901f\u4e14\u5b89\u5168\u7684\u8fd0\u52a8\u63a7\u5236\u3002\u867d\u7136\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff08\u5982MPC\u3001RL\u3001DeePC\u548cLLM\u4ee3\u7406\uff09\u80fd\u51cf\u5c11\u5bf9\u7cbe\u786e\u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u5b58\u5728\u54cd\u5e94\u6162\u3001\u8ba1\u7b97\u9700\u6c42\u9ad8\u548c\u5185\u5b58\u5360\u7528\u5927\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5728\u5b9e\u65f6\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u516b\u79cd\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u6280\u672f\uff1a\u964d\u9636\u5efa\u6a21\u3001\u51fd\u6570\u903c\u8fd1\u7b56\u7565\u5b66\u4e60\u548c\u51f8\u677e\u5f1b\u7b49\u65b9\u6cd5\uff0c\u5e76\u5728\u771f\u5b9e\u5e94\u7528\u573a\u666f\uff08\u673a\u68b0\u81c2\u3001\u8f6f\u4f53\u673a\u5668\u4eba\u3001\u8f66\u8f86\u8fd0\u52a8\u63a7\u5236\uff09\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4f7f\u6570\u636e\u9a71\u52a8\u63a7\u5236\u7b56\u7565\u66f4\u9002\u5408\u5177\u6709\u5feb\u901f\u52a8\u6001\u3001\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u6216\u4e25\u683c\u5185\u5b58\u7ea6\u675f\u7684\u5b9e\u65f6\u7cfb\u7edf\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u7684\u516b\u79cd\u6280\u672f\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u6570\u636e\u9a71\u52a8\u63a7\u5236\u7b56\u7565\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u7cfb\u7edf\u4e2d\u90e8\u7f72\u9ad8\u6027\u80fd\u63a7\u5236\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2510.04161", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04161", "abs": "https://arxiv.org/abs/2510.04161", "authors": ["Longrui Yang", "Yiyu Wang", "Jingfan Tang", "Yunpeng Lv", "Shizhe Zhao", "Chao Cao", "Zhongqiang Ren"], "title": "HEHA: Hierarchical Planning for Heterogeneous Multi-Robot Exploration of Unknown Environments", "comment": "5 Figures", "summary": "This paper considers the path planning problem for autonomous exploration of\nan unknown environment using multiple heterogeneous robots such as drones,\nwheeled, and legged robots, which have different capabilities to traverse\ncomplex terrains. A key challenge there is to intelligently allocate the robots\nto the unknown areas to be explored and determine the visiting order of those\nspaces subject to traversablity constraints, which leads to a large scale\nconstrained optimization problem that needs to be quickly and iteratively\nsolved every time when new space are explored. To address the challenge, we\npropose HEHA (Hierarchical Exploration with Heterogeneous Agents) by leveraging\na recent hierarchical method that decompose the exploration into global\nplanning and local planning. The major contribution in HEHA is its global\nplanning, where we propose a new routing algorithm PEAF (Partial Anytime Focal\nsearch) that can quickly find bounded sub-optimal solutions to minimize the\nmaximum path length among the agents subject to traversability constraints.\nAdditionally, the local planner in HEHA also considers heterogeneity to avoid\nrepeated and duplicated exploration among the robots. The experimental results\nshow that, our HEHA can reduce up to 30% of the exploration time than the\nbaselines.", "AI": {"tldr": "\u63d0\u51faHEHA\u6846\u67b6\u89e3\u51b3\u5f02\u6784\u673a\u5668\u4eba\u81ea\u4e3b\u63a2\u7d22\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5c42\u89c4\u5212\u548cPEAF\u8def\u7531\u7b97\u6cd5\u6700\u5c0f\u5316\u6700\u5927\u8def\u5f84\u957f\u5ea6\uff0c\u5b9e\u9a8c\u663e\u793a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c1130%\u63a2\u7d22\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u5f02\u6784\u673a\u5668\u4eba\uff08\u65e0\u4eba\u673a\u3001\u8f6e\u5f0f\u3001\u817f\u5f0f\u673a\u5668\u4eba\uff09\u5728\u672a\u77e5\u73af\u5883\u4e2d\u63a2\u7d22\u65f6\u7684\u667a\u80fd\u5206\u914d\u95ee\u9898\uff0c\u9700\u8981\u8003\u8651\u4e0d\u540c\u673a\u5668\u4eba\u7684\u5730\u5f62\u901a\u8fc7\u80fd\u529b\u7ea6\u675f\uff0c\u8fd9\u662f\u4e00\u4e2a\u9700\u8981\u5feb\u901f\u8fed\u4ee3\u6c42\u89e3\u7684\u5927\u89c4\u6a21\u7ea6\u675f\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faHEHA\u5206\u5c42\u63a2\u7d22\u6846\u67b6\uff1a\u5168\u5c40\u89c4\u5212\u4f7f\u7528PEAF\uff08\u90e8\u5206\u968f\u65f6\u7126\u70b9\u641c\u7d22\uff09\u8def\u7531\u7b97\u6cd5\u5feb\u901f\u627e\u5230\u6709\u754c\u6b21\u4f18\u89e3\uff1b\u5c40\u90e8\u89c4\u5212\u8003\u8651\u5f02\u6784\u6027\u907f\u514d\u91cd\u590d\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHEHA\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u80fd\u591f\u51cf\u5c11\u9ad8\u8fbe30%\u7684\u63a2\u7d22\u65f6\u95f4\u3002", "conclusion": "HEHA\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u89c4\u5212\u548cPEAF\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u673a\u5668\u4eba\u63a2\u7d22\u4e2d\u7684\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a2\u7d22\u6548\u7387\u3002"}}
{"id": "2510.04168", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04168", "abs": "https://arxiv.org/abs/2510.04168", "authors": ["Amirmasoud Molaei", "Reza Ghabcheloo"], "title": "Learning to Capture Rocks using an Excavator: A Reinforcement Learning Approach with Guiding Reward Formulation", "comment": null, "summary": "Rock capturing with standard excavator buckets is a challenging task\ntypically requiring the expertise of skilled operators. Unlike soil digging, it\ninvolves manipulating large, irregular rocks in unstructured environments where\ncomplex contact interactions with granular material make model-based control\nimpractical. Existing autonomous excavation methods focus mainly on continuous\nmedia or rely on specialized grippers, limiting their applicability to\nreal-world construction sites. This paper introduces a fully data-driven\ncontrol framework for rock capturing that eliminates the need for explicit\nmodeling of rock or soil properties. A model-free reinforcement learning agent\nis trained in the AGX Dynamics simulator using the Proximal Policy Optimization\n(PPO) algorithm and a guiding reward formulation. The learned policy outputs\njoint velocity commands directly to the boom, arm, and bucket of a CAT365\nexcavator model. Robustness is enhanced through extensive domain randomization\nof rock geometry, density, and mass, as well as the initial configurations of\nthe bucket, rock, and goal position. To the best of our knowledge, this is the\nfirst study to develop and evaluate an RL-based controller for the rock\ncapturing task. Experimental results show that the policy generalizes well to\nunseen rocks and varying soil conditions, achieving high success rates\ncomparable to those of human participants while maintaining machine stability.\nThese findings demonstrate the feasibility of learning-based excavation\nstrategies for discrete object manipulation without requiring specialized\nhardware or detailed material models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u7684\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u6316\u6398\u673a\u81ea\u4e3b\u6293\u53d6\u5ca9\u77f3\uff0c\u65e0\u9700\u5bf9\u5ca9\u77f3\u6216\u571f\u58e4\u5c5e\u6027\u8fdb\u884c\u663e\u5f0f\u5efa\u6a21\u3002", "motivation": "\u4f20\u7edf\u6316\u6398\u673a\u6293\u53d6\u5ca9\u77f3\u9700\u8981\u719f\u7ec3\u64cd\u4f5c\u5458\uff0c\u73b0\u6709\u81ea\u4e3b\u6316\u6398\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u8fde\u7eed\u4ecb\u8d28\u6216\u4f9d\u8d56\u4e13\u7528\u5939\u5177\uff0c\u96be\u4ee5\u5e94\u7528\u4e8e\u771f\u5b9e\u5efa\u7b51\u5de5\u5730\u7684\u975e\u7ed3\u6784\u5316\u73af\u5883\u3002", "method": "\u5728AGX Dynamics\u6a21\u62df\u5668\u4e2d\u4f7f\u7528PPO\u7b97\u6cd5\u8bad\u7ec3\u6a21\u578b\u65e0\u5173\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5e7f\u6cdb\u7684\u9886\u57df\u968f\u673a\u5316\u589e\u5f3a\u9c81\u68d2\u6027\uff0c\u76f4\u63a5\u8f93\u51fa\u5173\u8282\u901f\u5ea6\u547d\u4ee4\u63a7\u5236\u6316\u6398\u673a\u3002", "result": "\u5b66\u4e60\u5230\u7684\u7b56\u7565\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u5ca9\u77f3\u548c\u4e0d\u540c\u571f\u58e4\u6761\u4ef6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4eba\u7c7b\u53c2\u4e0e\u8005\u76f8\u5f53\u7684\u9ad8\u6210\u529f\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u673a\u5668\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8bc1\u660e\u4e86\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u53ef\u4ee5\u5728\u4e0d\u9700\u8981\u4e13\u7528\u786c\u4ef6\u6216\u8be6\u7ec6\u6750\u6599\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u79bb\u6563\u7269\u4f53\u64cd\u4f5c\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2510.04171", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04171", "abs": "https://arxiv.org/abs/2510.04171", "authors": ["Lakshadeep Naik", "Adam Fischer", "Daniel Duberg", "Danica Kragic"], "title": "VBM-NET: Visual Base Pose Learning for Mobile Manipulation using Equivariant TransporterNet and GNNs", "comment": null, "summary": "In Mobile Manipulation, selecting an optimal mobile base pose is essential\nfor successful object grasping. Previous works have addressed this problem\neither through classical planning methods or by learning state-based policies.\nThey assume access to reliable state information, such as the precise object\nposes and environment models. In this work, we study base pose planning\ndirectly from top-down orthographic projections of the scene, which provide a\nglobal overview of the scene while preserving spatial structure. We propose\nVBM-NET, a learning-based method for base pose selection using such top-down\northographic projections. We use equivariant TransporterNet to exploit spatial\nsymmetries and efficiently learn candidate base poses for grasping. Further, we\nuse graph neural networks to represent a varying number of candidate base poses\nand use Reinforcement Learning to determine the optimal base pose among them.\nWe show that VBM-NET can produce comparable solutions to the classical methods\nin significantly less computation time. Furthermore, we validate sim-to-real\ntransfer by successfully deploying a policy trained in simulation to real-world\nmobile manipulation.", "AI": {"tldr": "VBM-NET\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u79fb\u52a8\u673a\u5668\u4eba\u57fa\u5ea7\u59ff\u6001\u9009\u62e9\u65b9\u6cd5\uff0c\u4f7f\u7528\u4fef\u89c6\u6b63\u4ea4\u6295\u5f71\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u4f18\u5316\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u57fa\u5ea7\u4f4d\u7f6e\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u7684\u72b6\u6001\u4fe1\u606f\uff08\u5982\u7269\u4f53\u4f4d\u59ff\u548c\u73af\u5883\u6a21\u578b\uff09\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8fd9\u4e9b\u4fe1\u606f\u5f80\u5f80\u4e0d\u53ef\u9760\u3002\u672c\u6587\u7814\u7a76\u76f4\u63a5\u4ece\u573a\u666f\u7684\u4fef\u89c6\u6b63\u4ea4\u6295\u5f71\u8fdb\u884c\u57fa\u5ea7\u59ff\u6001\u89c4\u5212\uff0c\u63d0\u4f9b\u5168\u5c40\u89c6\u89d2\u5e76\u4fdd\u6301\u7a7a\u95f4\u7ed3\u6784\u3002", "method": "\u4f7f\u7528\u7b49\u53d8TransporterNet\u5229\u7528\u7a7a\u95f4\u5bf9\u79f0\u6027\u9ad8\u6548\u5b66\u4e60\u5019\u9009\u57fa\u5ea7\u59ff\u6001\uff0c\u91c7\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u53ef\u53d8\u6570\u91cf\u7684\u5019\u9009\u59ff\u6001\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4ece\u4e2d\u9009\u62e9\u6700\u4f18\u57fa\u5ea7\u59ff\u6001\u3002", "result": "VBM-NET\u80fd\u5728\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u65f6\u95f4\u7684\u60c5\u51b5\u4e0b\u4ea7\u751f\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5728\u4eff\u771f\u5230\u5b9e\u7269\u7684\u8fc1\u79fb\u4e2d\u6210\u529f\u90e8\u7f72\u5230\u771f\u5b9e\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u76f4\u63a5\u4ece\u89c6\u89c9\u8f93\u5165\u8fdb\u884c\u57fa\u5ea7\u59ff\u6001\u89c4\u5212\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u6027\u80fd\u548c\u6210\u529f\u7684\u4eff\u771f\u5230\u5b9e\u7269\u8fc1\u79fb\u3002"}}
{"id": "2510.04178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04178", "abs": "https://arxiv.org/abs/2510.04178", "authors": ["L\u00e9a Pistorius", "Namrata U. Nayar", "Phillip Tran", "Sammy Elmariah", "Pierre E. Dupont"], "title": "Using Robotics to Improve Transcatheter Edge-to-Edge Repair of the Mitral Valve", "comment": "7 pages, 9 figures", "summary": "Transcatheter valve repair presents significant challenges due to the\nmechanical limitations and steep learning curve associated with manual catheter\nsystems. This paper investigates the use of robotics to facilitate\ntranscatheter procedures in the context of mitral valve edge-to-edge repair.\nThe complex handle-based control of a clinical repair device is replaced by\nintuitive robotic joint-based control via a game controller. Manual versus\nrobotic performance is analyzed by decomposing the overall device delivery task\ninto motion-specific steps and comparing capabilities on a step-by-step basis\nin a phantom model of the heart and vasculature. Metrics include procedure\nduration and clip placement accuracy. Results demonstrate that the robotic\nsystem can reduce procedural time and motion errors while also improving\naccuracy of clip placement. These findings suggest that robotic assistance can\naddress key limitations of manual systems, offering a more reliable and\nuser-friendly platform for complex transcatheter procedures.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4f7f\u7528\u673a\u5668\u4eba\u7cfb\u7edf\u8f85\u52a9\u7ecf\u5bfc\u7ba1\u4e8c\u5c16\u74e3\u4fee\u590d\u624b\u672f\uff0c\u901a\u8fc7\u6e38\u620f\u63a7\u5236\u5668\u5b9e\u73b0\u76f4\u89c2\u7684\u5173\u8282\u63a7\u5236\uff0c\u76f8\u6bd4\u624b\u52a8\u64cd\u4f5c\u80fd\u51cf\u5c11\u624b\u672f\u65f6\u95f4\u548c\u8fd0\u52a8\u8bef\u5dee\uff0c\u63d0\u9ad8\u5939\u5b50\u653e\u7f6e\u7cbe\u5ea6\u3002", "motivation": "\u7ecf\u5bfc\u7ba1\u74e3\u819c\u4fee\u590d\u624b\u672f\u9762\u4e34\u673a\u68b0\u9650\u5236\u548c\u9661\u5ced\u7684\u5b66\u4e60\u66f2\u7ebf\u6311\u6218\uff0c\u9700\u8981\u66f4\u53ef\u9760\u548c\u7528\u6237\u53cb\u597d\u7684\u5e73\u53f0\u6765\u6539\u5584\u624b\u672f\u6548\u679c\u3002", "method": "\u5c06\u4e34\u5e8a\u4fee\u590d\u8bbe\u5907\u7684\u590d\u6742\u624b\u67c4\u63a7\u5236\u66ff\u6362\u4e3a\u901a\u8fc7\u6e38\u620f\u63a7\u5236\u5668\u5b9e\u73b0\u7684\u76f4\u89c2\u673a\u5668\u4eba\u5173\u8282\u63a7\u5236\uff0c\u5728\u5fc3\u810f\u548c\u8840\u7ba1\u7cfb\u7edf\u7684\u4f53\u6a21\u6a21\u578b\u4e2d\u5206\u6790\u624b\u52a8\u4e0e\u673a\u5668\u4eba\u6027\u80fd\uff0c\u6bd4\u8f83\u624b\u672f\u65f6\u95f4\u548c\u5939\u5b50\u653e\u7f6e\u7cbe\u5ea6\u7b49\u6307\u6807\u3002", "result": "\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u51cf\u5c11\u624b\u672f\u65f6\u95f4\u548c\u8fd0\u52a8\u8bef\u5dee\uff0c\u540c\u65f6\u63d0\u9ad8\u5939\u5b50\u653e\u7f6e\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u673a\u5668\u4eba\u8f85\u52a9\u53ef\u4ee5\u89e3\u51b3\u624b\u52a8\u7cfb\u7edf\u7684\u5173\u952e\u9650\u5236\uff0c\u4e3a\u590d\u6742\u7684\u7ecf\u5bfc\u7ba1\u624b\u672f\u63d0\u4f9b\u66f4\u53ef\u9760\u548c\u7528\u6237\u53cb\u597d\u7684\u5e73\u53f0\u3002"}}
{"id": "2510.04190", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04190", "abs": "https://arxiv.org/abs/2510.04190", "authors": ["Jian-jie Zheng", "Chih-kai Yang", "Po-han Chen", "Lyn Chao-ling Chen"], "title": "Zenbo Patrol: A Social Assistive Robot Based on Multimodal Deep Learning for Real-time Illegal Parking Recognition and Notification", "comment": null, "summary": "In the study, the social robot act as a patrol to recognize and notify\nillegal parking in real-time. Dual-model pipeline method and large multimodal\nmodel were compared, and the GPT-4o multimodal model was adopted in license\nplate recognition without preprocessing. For moving smoothly on a flat ground,\nthe robot navigated in a simulated parking lot in the experiments. The robot\nchanges angle view of the camera automatically to capture the images around\nwith the format of license plate number. From the captured images of the robot,\nthe numbers on the plate are recognized through the GPT-4o model, and\nidentifies legality of the numbers. When an illegal parking is detected, the\nrobot sends Line messages to the system manager immediately. The contribution\nof the work is that a novel multimodal deep learning method has validated with\nhigh accuracy in license plate recognition, and a social assistive robot is\nalso provided for solving problems in a real scenario, and can be applied in an\nindoor parking lot.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u4f7f\u7528GPT-4o\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u5b9e\u65f6\u8f66\u724c\u8bc6\u522b\u548c\u975e\u6cd5\u505c\u8f66\u68c0\u6d4b\u7684\u793e\u4ea4\u5de1\u903b\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u5ba4\u5185\u505c\u8f66\u573a\u81ea\u52a8\u5bfc\u822a\u5e76\u53d1\u9001Line\u6d88\u606f\u901a\u77e5\u7ba1\u7406\u5458\u3002", "motivation": "\u89e3\u51b3\u505c\u8f66\u573a\u975e\u6cd5\u505c\u8f66\u95ee\u9898\uff0c\u901a\u8fc7\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u4f9b\u5b9e\u65f6\u76d1\u63a7\u548c\u901a\u77e5\u670d\u52a1\uff0c\u9a8c\u8bc1\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u8f66\u724c\u8bc6\u522b\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528GPT-4o\u591a\u6a21\u6001\u6a21\u578b\u8fdb\u884c\u8f66\u724c\u8bc6\u522b\uff0c\u65e0\u9700\u9884\u5904\u7406\uff1b\u673a\u5668\u4eba\u81ea\u52a8\u8c03\u6574\u6444\u50cf\u5934\u89d2\u5ea6\u6355\u6349\u8f66\u724c\u56fe\u50cf\uff0c\u5728\u6a21\u62df\u505c\u8f66\u573a\u73af\u5883\u4e2d\u8fdb\u884c\u5bfc\u822a\u5b9e\u9a8c\u3002", "result": "\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u8f66\u724c\u8bc6\u522b\uff0c\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u975e\u6cd5\u505c\u8f66\u5e76\u7acb\u5373\u901a\u8fc7Line\u6d88\u606f\u901a\u77e5\u7cfb\u7edf\u7ba1\u7406\u5458\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u9a8c\u8bc1\u4e86\u65b0\u578b\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u8f66\u724c\u8bc6\u522b\u4e2d\u7684\u9ad8\u51c6\u786e\u6027\uff0c\u63d0\u4f9b\u4e86\u53ef\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u89e3\u51b3\u95ee\u9898\u7684\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\uff0c\u9002\u7528\u4e8e\u5ba4\u5185\u505c\u8f66\u573a\u5e94\u7528\u3002"}}
{"id": "2510.04234", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04234", "abs": "https://arxiv.org/abs/2510.04234", "authors": ["Runhan Huang", "Haldun Balim", "Heng Yang", "Yilun Du"], "title": "Flexible Locomotion Learning with Diffusion Model Predictive Control", "comment": "9 pages, 8 figures", "summary": "Legged locomotion demands controllers that are both robust and adaptable,\nwhile remaining compatible with task and safety considerations. However,\nmodel-free reinforcement learning (RL) methods often yield a fixed policy that\ncan be difficult to adapt to new behaviors at test time. In contrast, Model\nPredictive Control (MPC) provides a natural approach to flexible behavior\nsynthesis by incorporating different objectives and constraints directly into\nits optimization process. However, classical MPC relies on accurate dynamics\nmodels, which are often difficult to obtain in complex environments and\ntypically require simplifying assumptions. We present Diffusion-MPC, which\nleverages a learned generative diffusion model as an approximate dynamics prior\nfor planning, enabling flexible test-time adaptation through reward and\nconstraint based optimization. Diffusion-MPC jointly predicts future states and\nactions; at each reverse step, we incorporate reward planning and impose\nconstraint projection, yielding trajectories that satisfy task objectives while\nremaining within physical limits. To obtain a planning model that adapts beyond\nimitation pretraining, we introduce an interactive training algorithm for\ndiffusion based planner: we execute our reward-and-constraint planner in\nenvironment, then filter and reweight the collected trajectories by their\nrealized returns before updating the denoiser. Our design enables strong\ntest-time adaptability, allowing the planner to adjust to new reward\nspecifications without retraining. We validate Diffusion-MPC on real world,\ndemonstrating strong locomotion and flexible adaptation.", "AI": {"tldr": "\u63d0\u51faDiffusion-MPC\u65b9\u6cd5\uff0c\u5229\u7528\u5b66\u4e60\u7684\u751f\u6210\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u89c4\u5212\u4e2d\u7684\u8fd1\u4f3c\u52a8\u529b\u5b66\u5148\u9a8c\uff0c\u901a\u8fc7\u5956\u52b1\u548c\u7ea6\u675f\u4f18\u5316\u5b9e\u73b0\u7075\u6d3b\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94", "motivation": "\u817f\u5f0f\u8fd0\u52a8\u9700\u8981\u65e2\u9c81\u68d2\u53c8\u9002\u5e94\u6027\u5f3a\u7684\u63a7\u5236\u5668\uff0c\u4f46\u6a21\u578b\u81ea\u7531\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u65b0\u884c\u4e3a\uff0c\u800c\u7ecf\u5178MPC\u4f9d\u8d56\u7cbe\u786e\u52a8\u529b\u5b66\u6a21\u578b", "method": "\u4f7f\u7528\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u52a8\u529b\u5b66\u5148\u9a8c\u8fdb\u884c\u89c4\u5212\uff0c\u5728\u53cd\u5411\u6b65\u9aa4\u4e2d\u7ed3\u5408\u5956\u52b1\u89c4\u5212\u548c\u7ea6\u675f\u6295\u5f71\uff0c\u5e76\u901a\u8fc7\u4ea4\u4e92\u5f0f\u8bad\u7ec3\u7b97\u6cd5\u66f4\u65b0\u53bb\u566a\u5668", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u9a8c\u8bc1\u4e86Diffusion-MPC\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8fd0\u52a8\u80fd\u529b\u548c\u7075\u6d3b\u7684\u9002\u5e94\u6027", "conclusion": "Diffusion-MPC\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u6027\uff0c\u5141\u8bb8\u89c4\u5212\u5668\u5728\u4e0d\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u65b0\u7684\u5956\u52b1\u89c4\u8303"}}
{"id": "2510.04246", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04246", "abs": "https://arxiv.org/abs/2510.04246", "authors": ["Huiwon Jang", "Sihyun Yu", "Heeseung Kwon", "Hojin Jeon", "Younggyo Seo", "Jinwoo Shin"], "title": "ContextVLA: Vision-Language-Action Model with Amortized Multi-Frame Context", "comment": "Project page: https://huiwon-jang.github.io/contextvla", "summary": "Leveraging temporal context is crucial for success in partially observable\nrobotic tasks. However, prior work in behavior cloning has demonstrated\ninconsistent performance gains when using multi-frame observations. In this\npaper, we introduce ContextVLA, a policy model that robustly improves robotic\ntask performance by effectively leveraging multi-frame observations. Our\napproach is motivated by the key observation that Vision-Language-Action models\n(VLA), i.e., policy models built upon a Vision-Language Model (VLM), more\neffectively utilize multi-frame observations for action generation. This\nsuggests that VLMs' inherent temporal understanding capability enables them to\nextract more meaningful context from multi-frame observations. However, the\nhigh dimensionality of video inputs introduces significant computational\noverhead, making VLA training and inference inefficient. To address this,\nContextVLA compresses past observations into a single context token, allowing\nthe policy to efficiently leverage temporal context for action generation. Our\nexperiments show that ContextVLA consistently improves over single-frame VLAs\nand achieves the benefits of full multi-frame training but with reduced\ntraining and inference times.", "AI": {"tldr": "ContextVLA\u662f\u4e00\u79cd\u901a\u8fc7\u6709\u6548\u5229\u7528\u591a\u5e27\u89c2\u6d4b\u6765\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u6027\u80fd\u7684\u7b56\u7565\u6a21\u578b\uff0c\u5b83\u5c06\u8fc7\u53bb\u89c2\u6d4b\u538b\u7f29\u4e3a\u5355\u4e2a\u4e0a\u4e0b\u6587token\uff0c\u5728\u4fdd\u6301\u591a\u5e27\u8bad\u7ec3\u4f18\u52bf\u7684\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u73b0\u6709\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u5728\u4f7f\u7528\u591a\u5e27\u89c2\u6d4b\u65f6\u6027\u80fd\u63d0\u5347\u4e0d\u4e00\u81f4\uff0c\u800c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b(VLA)\u80fd\u66f4\u6709\u6548\u5730\u5229\u7528\u591a\u5e27\u89c2\u6d4b\u751f\u6210\u52a8\u4f5c\uff0c\u4f46\u89c6\u9891\u8f93\u5165\u7684\u9ad8\u7ef4\u5ea6\u5e26\u6765\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51faContextVLA\u6a21\u578b\uff0c\u5c06\u8fc7\u53bb\u89c2\u6d4b\u538b\u7f29\u4e3a\u5355\u4e2a\u4e0a\u4e0b\u6587token\uff0c\u4f7f\u7b56\u7565\u80fd\u591f\u9ad8\u6548\u5229\u7528\u65f6\u95f4\u4e0a\u4e0b\u6587\u751f\u6210\u52a8\u4f5c\uff0c\u540c\u65f6\u51cf\u5c11\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660eContextVLA\u76f8\u6bd4\u5355\u5e27VLA\u6301\u7eed\u6539\u8fdb\uff0c\u5b9e\u73b0\u4e86\u5b8c\u6574\u591a\u5e27\u8bad\u7ec3\u7684\u4f18\u52bf\uff0c\u4f46\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u66f4\u77ed\u3002", "conclusion": "ContextVLA\u901a\u8fc7\u538b\u7f29\u591a\u5e27\u89c2\u6d4b\u4e3a\u4e0a\u4e0b\u6587token\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u4f18\u52bf\u7684\u540c\u65f6\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2510.04278", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04278", "abs": "https://arxiv.org/abs/2510.04278", "authors": ["Peiwen Yang", "Weisong Wen", "Runqiu Yang", "Yuanyuan Zhang", "Jiahao Hu", "Yingming Chen", "Naigui Xiao", "Jiaqi Zhao"], "title": "Integrated Planning and Control on Manifolds: Factor Graph Representation and Toolkit", "comment": null, "summary": "Model predictive control (MPC) faces significant limitations when applied to\nsystems evolving on nonlinear manifolds, such as robotic attitude dynamics and\nconstrained motion planning, where traditional Euclidean formulations struggle\nwith singularities, over-parameterization, and poor convergence. To overcome\nthese challenges, this paper introduces FactorMPC, a factor-graph based MPC\ntoolkit that unifies system dynamics, constraints, and objectives into a\nmodular, user-friendly, and efficient optimization structure. Our approach\nnatively supports manifold-valued states with Gaussian uncertainties modeled in\ntangent spaces. By exploiting the sparsity and probabilistic structure of\nfactor graphs, the toolkit achieves real-time performance even for\nhigh-dimensional systems with complex constraints. The velocity-extended\non-manifold control barrier function (CBF)-based obstacle avoidance factors are\ndesigned for safety-critical applications. By bridging graphical models with\nsafety-critical MPC, our work offers a scalable and geometrically consistent\nframework for integrated planning and control. The simulations and experimental\nresults on the quadrotor demonstrate superior trajectory tracking and obstacle\navoidance performance compared to baseline methods. To foster research\nreproducibility, we have provided open-source implementation offering\nplug-and-play factors.", "AI": {"tldr": "FactorMPC\u662f\u4e00\u4e2a\u57fa\u4e8e\u56e0\u5b50\u56fe\u7684MPC\u5de5\u5177\u5305\uff0c\u4e13\u95e8\u5904\u7406\u975e\u7ebf\u6027\u6d41\u5f62\u4e0a\u7684\u7cfb\u7edf\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u7edf\u4e00\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u7ea6\u675f\u548c\u76ee\u6807\u5230\u6a21\u5757\u5316\u4f18\u5316\u7ed3\u6784\u4e2d\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\u548c\u5b89\u5168\u5173\u952e\u5e94\u7528\u3002", "motivation": "\u4f20\u7edfMPC\u5728\u975e\u7ebf\u6027\u6d41\u5f62\u7cfb\u7edf\uff08\u5982\u673a\u5668\u4eba\u59ff\u6001\u52a8\u529b\u5b66\uff09\u4e2d\u5b58\u5728\u5947\u70b9\u3001\u8fc7\u53c2\u6570\u5316\u548c\u6536\u655b\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u9700\u8981\u51e0\u4f55\u4e00\u81f4\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u56e0\u5b50\u56fe\u65b9\u6cd5\uff0c\u652f\u6301\u6d41\u5f62\u503c\u72b6\u6001\u548c\u5207\u7a7a\u95f4\u9ad8\u65af\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u901f\u5ea6\u6269\u5c55\u6d41\u5f62\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7684\u907f\u969c\u56e0\u5b50\uff0c\u5b9e\u73b0\u6a21\u5757\u5316\u4f18\u5316\u7ed3\u6784\u3002", "result": "\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u7684\u4eff\u771f\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5177\u6709\u66f4\u4f18\u7684\u8f68\u8ff9\u8ddf\u8e2a\u548c\u907f\u969c\u6027\u80fd\uff0c\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u6027\u80fd\u63a7\u5236\u3002", "conclusion": "FactorMPC\u901a\u8fc7\u8fde\u63a5\u56fe\u6a21\u578b\u4e0e\u5b89\u5168\u5173\u952eMPC\uff0c\u4e3a\u96c6\u6210\u89c4\u5212\u4e0e\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u51e0\u4f55\u4e00\u81f4\u7684\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u5b9e\u73b0\u3002"}}
{"id": "2510.04353", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04353", "abs": "https://arxiv.org/abs/2510.04353", "authors": ["Stephen McCrory", "Romeo Orsolino", "Dhruv Thanki", "Luigi Penco", "Robert Griffin"], "title": "Stability-Aware Retargeting for Humanoid Multi-Contact Teleoperation", "comment": null, "summary": "Teleoperation is a powerful method to generate reference motions and enable\nhumanoid robots to perform a broad range of tasks. However, teleoperation\nbecomes challenging when using hand contacts and non-coplanar surfaces, often\nleading to motor torque saturation or loss of stability through slipping. We\npropose a centroidal stability-based retargeting method that dynamically\nadjusts contact points and posture during teleoperation to enhance stability in\nthese difficult scenarios. Central to our approach is an efficient analytical\ncalculation of the stability margin gradient. This gradient is used to identify\nscenarios for which stability is highly sensitive to teleoperation setpoints\nand inform the local adjustment of these setpoints. We validate the framework\nin simulation and hardware by teleoperating manipulation tasks on a humanoid,\ndemonstrating increased stability margins. We also demonstrate empirically that\nhigher stability margins correlate with improved impulse resilience and joint\ntorque margin.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d28\u5fc3\u7a33\u5b9a\u6027\u7684\u91cd\u5b9a\u5411\u65b9\u6cd5\uff0c\u5728\u9065\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u52a8\u6001\u8c03\u6574\u63a5\u89e6\u70b9\u548c\u59ff\u6001\uff0c\u4ee5\u589e\u5f3a\u5728\u590d\u6742\u63a5\u89e6\u573a\u666f\u4e0b\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u9065\u64cd\u4f5c\u5728\u6d89\u53ca\u624b\u90e8\u63a5\u89e6\u548c\u975e\u5171\u9762\u8868\u9762\u7684\u590d\u6742\u573a\u666f\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u5bb9\u6613\u5bfc\u81f4\u7535\u673a\u626d\u77e9\u9971\u548c\u6216\u901a\u8fc7\u6ed1\u52a8\u5931\u53bb\u7a33\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u9ad8\u6548\u7684\u8d28\u5fc3\u7a33\u5b9a\u6027\u68af\u5ea6\u89e3\u6790\u8ba1\u7b97\u65b9\u6cd5\uff0c\u8bc6\u522b\u5bf9\u7a33\u5b9a\u6027\u654f\u611f\u7684\u9065\u64cd\u4f5c\u8bbe\u5b9a\u70b9\uff0c\u5e76\u5c40\u90e8\u8c03\u6574\u8fd9\u4e9b\u8bbe\u5b9a\u70b9\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u7a33\u5b9a\u6027\u88d5\u5ea6\u7684\u63d0\u5347\uff0c\u5e76\u4e14\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\u88d5\u5ea6\u4e0e\u66f4\u597d\u7684\u8109\u51b2\u6297\u6027\u548c\u5173\u8282\u626d\u77e9\u88d5\u5ea6\u76f8\u5173\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7a33\u5b9a\u6027\u91cd\u5b9a\u5411\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5728\u590d\u6742\u63a5\u89e6\u573a\u666f\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.04354", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04354", "abs": "https://arxiv.org/abs/2510.04354", "authors": ["Apurva Badithela", "David Snyder", "Lihan Zha", "Joseph Mikhail", "Matthew O'Kelly", "Anushri Dixit", "Anirudha Majumdar"], "title": "Reliable and Scalable Robot Policy Evaluation with Imperfect Simulators", "comment": null, "summary": "Rapid progress in imitation learning, foundation models, and large-scale\ndatasets has led to robot manipulation policies that generalize to a wide-range\nof tasks and environments. However, rigorous evaluation of these policies\nremains a challenge. Typically in practice, robot policies are often evaluated\non a small number of hardware trials without any statistical assurances. We\npresent SureSim, a framework to augment large-scale simulation with relatively\nsmall-scale real-world testing to provide reliable inferences on the real-world\nperformance of a policy. Our key idea is to formalize the problem of combining\nreal and simulation evaluations as a prediction-powered inference problem, in\nwhich a small number of paired real and simulation evaluations are used to\nrectify bias in large-scale simulation. We then leverage non-asymptotic mean\nestimation algorithms to provide confidence intervals on mean policy\nperformance. Using physics-based simulation, we evaluate both diffusion policy\nand multi-task fine-tuned \\(\\pi_0\\) on a joint distribution of objects and\ninitial conditions, and find that our approach saves over \\(20-25\\%\\) of\nhardware evaluation effort to achieve similar bounds on policy performance.", "AI": {"tldr": "SureSim\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5927\u89c4\u6a21\u4eff\u771f\u548c\u5c0f\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\uff0c\u4e3a\u673a\u5668\u4eba\u7b56\u7565\u6027\u80fd\u63d0\u4f9b\u53ef\u9760\u7684\u7edf\u8ba1\u63a8\u65ad\uff0c\u53ef\u8282\u770120-25%\u7684\u786c\u4ef6\u8bc4\u4f30\u6210\u672c\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u7b56\u7565\u8bc4\u4f30\u901a\u5e38\u4f9d\u8d56\u5c11\u91cf\u786c\u4ef6\u8bd5\u9a8c\uff0c\u7f3a\u4e4f\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5c06\u771f\u5b9e\u4e0e\u4eff\u771f\u8bc4\u4f30\u7ed3\u5408\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u9884\u6d4b\u9a71\u52a8\u7684\u63a8\u7406\u95ee\u9898\uff0c\u4f7f\u7528\u5c11\u91cf\u914d\u5bf9\u771f\u5b9e\u548c\u4eff\u771f\u8bc4\u4f30\u6765\u7ea0\u6b63\u5927\u89c4\u6a21\u4eff\u771f\u7684\u504f\u5dee\uff0c\u5e76\u5229\u7528\u975e\u6e10\u8fd1\u5747\u503c\u4f30\u8ba1\u7b97\u6cd5\u63d0\u4f9b\u7b56\u7565\u6027\u80fd\u7684\u7f6e\u4fe1\u533a\u95f4\u3002", "result": "\u5728\u7269\u7406\u4eff\u771f\u4e2d\u8bc4\u4f30\u6269\u6563\u7b56\u7565\u548c\u591a\u4efb\u52a1\u5fae\u8c03\u7b56\u7565\uff0c\u8be5\u65b9\u6cd5\u53ef\u8282\u770120-25%\u7684\u786c\u4ef6\u8bc4\u4f30\u5de5\u4f5c\u91cf\uff0c\u540c\u65f6\u83b7\u5f97\u76f8\u4f3c\u7684\u6027\u80fd\u8fb9\u754c\u3002", "conclusion": "SureSim\u6846\u67b6\u80fd\u591f\u6709\u6548\u7ed3\u5408\u4eff\u771f\u548c\u771f\u5b9e\u6d4b\u8bd5\uff0c\u4e3a\u673a\u5668\u4eba\u7b56\u7565\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u7edf\u8ba1\u53ef\u9760\u7684\u63a8\u65ad\uff0c\u663e\u8457\u964d\u4f4e\u786c\u4ef6\u6d4b\u8bd5\u6210\u672c\u3002"}}
{"id": "2510.04436", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04436", "abs": "https://arxiv.org/abs/2510.04436", "authors": ["Jushan Chen", "Santiago Paternain"], "title": "PAD-TRO: Projection-Augmented Diffusion for Direct Trajectory Optimization", "comment": null, "summary": "Recently, diffusion models have gained popularity and attention in trajectory\noptimization due to their capability of modeling multi-modal probability\ndistributions. However, addressing nonlinear equality constraints, i.e, dynamic\nfeasi- bility, remains a great challenge in diffusion-based trajectory\noptimization. Recent diffusion-based trajectory optimization frameworks rely on\na single-shooting style approach where the denoised control sequence is applied\nto forward propagate the dynamical system, which cannot explicitly enforce\nconstraints on the states and frequently leads to sub-optimal solutions. In\nthis work, we propose a novel direct trajectory optimization approach via\nmodel-based diffusion, which directly generates a sequence of states. To ensure\ndynamic feasibility, we propose a gradient-free projection mechanism that is\nincorporated into the reverse diffusion process. Our results show that,\ncompared to a recent state-of-the-art baseline, our approach leads to zero\ndynamic feasibility error and approximately 4x higher success rate in a\nquadrotor waypoint navigation scenario involving dense static obstacles.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u6269\u6563\u7684\u76f4\u63a5\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u68af\u5ea6\u6295\u5f71\u673a\u5236\u786e\u4fdd\u52a8\u6001\u53ef\u884c\u6027\uff0c\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5bfc\u822a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u96f6\u52a8\u6001\u53ef\u884c\u6027\u8bef\u5dee\u548c4\u500d\u6210\u529f\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u91c7\u7528\u5355\u6b21\u5c04\u51fb\u65b9\u5f0f\uff0c\u65e0\u6cd5\u663e\u5f0f\u7ea6\u675f\u72b6\u6001\u5e76\u7ecf\u5e38\u5bfc\u81f4\u6b21\u4f18\u89e3\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u975e\u7ebf\u6027\u7b49\u5f0f\u7ea6\u675f\uff08\u52a8\u6001\u53ef\u884c\u6027\uff09\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u76f4\u63a5\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u6269\u6563\u76f4\u63a5\u751f\u6210\u72b6\u6001\u5e8f\u5217\uff0c\u5728\u53cd\u5411\u6269\u6563\u8fc7\u7a0b\u4e2d\u878d\u5165\u65e0\u68af\u5ea6\u6295\u5f71\u673a\u5236\u6765\u786e\u4fdd\u52a8\u6001\u53ef\u884c\u6027\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u822a\u70b9\u5bfc\u822a\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u96f6\u52a8\u6001\u53ef\u884c\u6027\u8bef\u5dee\u548c\u7ea64\u500d\u7684\u6210\u529f\u7387\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u6a21\u578b\u6269\u6563\u7684\u76f4\u63a5\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u975e\u7ebf\u6027\u7b49\u5f0f\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u9ad8\u8f68\u8ff9\u4f18\u5316\u7684\u52a8\u6001\u53ef\u884c\u6027\u548c\u6210\u529f\u7387\u3002"}}
{"id": "2510.04509", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.04509", "abs": "https://arxiv.org/abs/2510.04509", "authors": ["Huanqing Wang", "Kaixiang Zhang", "Kyungjoon Lee", "Yu Mei", "Vaibhav Srivastava", "Jun Sheng", "Ziyou Song", "Zhaojian Li"], "title": "Velocity-Form Data-Enabled Predictive Control of Soft Robots under Unknown External Payloads", "comment": null, "summary": "Data-driven control methods such as data-enabled predictive control (DeePC)\nhave shown strong potential in efficient control of soft robots without\nexplicit parametric models. However, in object manipulation tasks, unknown\nexternal payloads and disturbances can significantly alter the system dynamics\nand behavior, leading to offset error and degraded control performance. In this\npaper, we present a novel velocity-form DeePC framework that achieves robust\nand optimal control of soft robots under unknown payloads. The proposed\nframework leverages input-output data in an incremental representation to\nmitigate performance degradation induced by unknown payloads, eliminating the\nneed for weighted datasets or disturbance estimators. We validate the method\nexperimentally on a planar soft robot and demonstrate its superior performance\ncompared to standard DeePC in scenarios involving unknown payloads.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u901f\u5ea6\u5f62\u5f0f\u6570\u636e\u9a71\u52a8\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u672a\u77e5\u8d1f\u8f7d\u4e0b\u5b9e\u73b0\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u9c81\u68d2\u548c\u6700\u4f18\u63a7\u5236\u3002", "motivation": "\u5728\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0c\u672a\u77e5\u7684\u5916\u90e8\u8d1f\u8f7d\u548c\u5e72\u6270\u4f1a\u663e\u8457\u6539\u53d8\u7cfb\u7edf\u52a8\u529b\u5b66\u548c\u884c\u4e3a\uff0c\u5bfc\u81f4\u504f\u79fb\u8bef\u5dee\u548c\u63a7\u5236\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5229\u7528\u589e\u91cf\u8868\u793a\u7684\u8f93\u5165\u8f93\u51fa\u6570\u636e\u6765\u51cf\u8f7b\u672a\u77e5\u8d1f\u8f7d\u5f15\u8d77\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u65e0\u9700\u52a0\u6743\u6570\u636e\u96c6\u6216\u5e72\u6270\u4f30\u8ba1\u5668\u3002", "result": "\u5728\u5e73\u9762\u8f6f\u4f53\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u5176\u5728\u6d89\u53ca\u672a\u77e5\u8d1f\u8f7d\u7684\u573a\u666f\u4e2d\u76f8\u6bd4\u6807\u51c6DeePC\u5177\u6709\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u901f\u5ea6\u5f62\u5f0fDeePC\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u672a\u77e5\u8d1f\u8f7d\uff0c\u63d0\u9ad8\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u63a7\u5236\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.04585", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04585", "abs": "https://arxiv.org/abs/2510.04585", "authors": ["Jianshu Zhou", "Jing Shu", "Tianle Pan", "Puchen Zhu", "Jiajun An", "Huayu Zhang", "Junda Huang", "Upinder Kaur", "Xin Ma", "Masayoshi Tomizuka"], "title": "Everything-Grasping (EG) Gripper: A Universal Gripper with Synergistic Suction-Grasping Capabilities for Cross-Scale and Cross-State Manipulation", "comment": "19 pages, 10 figures, journal", "summary": "Grasping objects across vastly different sizes and physical states-including\nboth solids and liquids-with a single robotic gripper remains a fundamental\nchallenge in soft robotics. We present the Everything-Grasping (EG) Gripper, a\nsoft end-effector that synergistically integrates distributed surface suction\nwith internal granular jamming, enabling cross-scale and cross-state\nmanipulation without requiring airtight sealing at the contact interface with\ntarget objects. The EG Gripper can handle objects with surface areas ranging\nfrom sub-millimeter scale 0.2 mm2 (glass bead) to over 62,000 mm2 (A4 sized\npaper and woven bag), enabling manipulation of objects nearly 3,500X smaller\nand 88X larger than its own contact area (approximated at 707 mm2 for a 30\nmm-diameter base). We further introduce a tactile sensing framework that\ncombines liquid detection and pressure-based suction feedback, enabling\nreal-time differentiation between solid and liquid targets. Guided by the\nactile-Inferred Grasping Mode Selection (TIGMS) algorithm, the gripper\nautonomously selects grasping modes based on distributed pressure and voltage\nsignals. Experiments across diverse tasks-including underwater grasping,\nfragile object handling, and liquid capture-demonstrate robust and repeatable\nperformance. To our knowledge, this is the first soft gripper to reliably grasp\nboth solid and liquid objects across scales using a unified compliant\narchitecture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5206\u5e03\u5f0f\u8868\u9762\u5438\u9644\u548c\u5185\u90e8\u9897\u7c92\u5835\u585e\u7684\u8f6f\u4f53\u6293\u53d6\u5668\uff0c\u80fd\u591f\u8de8\u5c3a\u5ea6\u548c\u8de8\u72b6\u6001\uff08\u56fa\u4f53\u548c\u6db2\u4f53\uff09\u6293\u53d6\u7269\u4f53\uff0c\u65e0\u9700\u6c14\u5bc6\u5bc6\u5c01\u3002", "motivation": "\u89e3\u51b3\u8f6f\u4f53\u673a\u5668\u4eba\u4e2d\u5355\u4e00\u6293\u53d6\u5668\u5904\u7406\u4e0d\u540c\u5c3a\u5bf8\u548c\u7269\u7406\u72b6\u6001\uff08\u56fa\u4f53\u548c\u6db2\u4f53\uff09\u7269\u4f53\u7684\u57fa\u672c\u6311\u6218\u3002", "method": "\u96c6\u6210\u5206\u5e03\u5f0f\u8868\u9762\u5438\u9644\u4e0e\u5185\u90e8\u9897\u7c92\u5835\u585e\uff0c\u7ed3\u5408\u6db2\u4f53\u68c0\u6d4b\u548c\u538b\u529b\u53cd\u9988\u7684\u89e6\u89c9\u611f\u77e5\u6846\u67b6\uff0c\u4ee5\u53ca\u57fa\u4e8e\u89e6\u89c9\u63a8\u65ad\u7684\u6293\u53d6\u6a21\u5f0f\u9009\u62e9\u7b97\u6cd5\u3002", "result": "\u80fd\u591f\u6293\u53d6\u4ece0.2 mm\u00b2\u523062,000 mm\u00b2\u7684\u7269\u4f53\uff0c\u6bd4\u81ea\u8eab\u63a5\u89e6\u9762\u79ef\u5c0f3500\u500d\u548c\u592788\u500d\uff0c\u5728\u6c34\u4e0b\u6293\u53d6\u3001\u6613\u788e\u7269\u4f53\u5904\u7406\u548c\u6db2\u4f53\u6355\u83b7\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5065\u53ef\u91cd\u590d\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u4f7f\u7528\u7edf\u4e00\u67d4\u6027\u67b6\u6784\u53ef\u9760\u6293\u53d6\u8de8\u5c3a\u5ea6\u56fa\u4f53\u548c\u6db2\u4f53\u7269\u4f53\u7684\u8f6f\u4f53\u6293\u53d6\u5668\u3002"}}
{"id": "2510.04592", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04592", "abs": "https://arxiv.org/abs/2510.04592", "authors": ["Yilin Mei", "Peng Qiu", "Wei Zhang", "WenChao Zhang", "Wenjie Song"], "title": "MobRT: A Digital Twin-Based Framework for Scalable Learning in Mobile Manipulation", "comment": null, "summary": "Recent advances in robotics have been largely driven by imitation learning,\nwhich depends critically on large-scale, high-quality demonstration data.\nHowever, collecting such data remains a significant challenge-particularly for\nmobile manipulators, which must coordinate base locomotion and arm manipulation\nin high-dimensional, dynamic, and partially observable environments.\nConsequently, most existing research remains focused on simpler tabletop\nscenarios, leaving mobile manipulation relatively underexplored. To bridge this\ngap, we present \\textit{MobRT}, a digital twin-based framework designed to\nsimulate two primary categories of complex, whole-body tasks: interaction with\narticulated objects (e.g., opening doors and drawers) and mobile-base\npick-and-place operations. \\textit{MobRT} autonomously generates diverse and\nrealistic demonstrations through the integration of virtual kinematic control\nand whole-body motion planning, enabling coherent and physically consistent\nexecution. We evaluate the quality of \\textit{MobRT}-generated data across\nmultiple baseline algorithms, establishing a comprehensive benchmark and\ndemonstrating a strong correlation between task success and the number of\ngenerated trajectories. Experiments integrating both simulated and real-world\ndemonstrations confirm that our approach markedly improves policy\ngeneralization and performance, achieving robust results in both simulated and\nreal-world environments.", "AI": {"tldr": "MobRT\u662f\u4e00\u4e2a\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u79fb\u52a8\u673a\u68b0\u81c2\u590d\u6742\u5168\u8eab\u4efb\u52a1\u7684\u591a\u6837\u5316\u6f14\u793a\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "motivation": "\u79fb\u52a8\u673a\u68b0\u81c2\u9700\u8981\u5728\u52a8\u6001\u3001\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\u9ad8\u7ef4\u73af\u5883\u4e2d\u534f\u8c03\u57fa\u5ea7\u79fb\u52a8\u548c\u624b\u81c2\u64cd\u4f5c\uff0c\u4f46\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\u6536\u96c6\u56f0\u96be\uff0c\u5bfc\u81f4\u73b0\u6709\u7814\u7a76\u591a\u5c40\u9650\u4e8e\u7b80\u5355\u684c\u9762\u573a\u666f\u3002", "method": "\u901a\u8fc7\u865a\u62df\u8fd0\u52a8\u5b66\u63a7\u5236\u548c\u5168\u8eab\u8fd0\u52a8\u89c4\u5212\u96c6\u6210\uff0c\u81ea\u4e3b\u751f\u6210\u591a\u6837\u5316\u548c\u771f\u5b9e\u7684\u6f14\u793a\uff0c\u6a21\u62df\u4e0e\u94f0\u63a5\u7269\u4f53\u4ea4\u4e92\u548c\u79fb\u52a8\u57fa\u5ea7\u62fe\u653e\u64cd\u4f5c\u4e24\u7c7b\u590d\u6742\u4efb\u52a1\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u7ebf\u7b97\u6cd5\u4e0a\u8bc4\u4f30\u751f\u6210\u6570\u636e\u8d28\u91cf\uff0c\u5efa\u7acb\u7efc\u5408\u57fa\u51c6\uff0c\u5b9e\u9a8c\u8868\u660e\u4efb\u52a1\u6210\u529f\u7387\u4e0e\u751f\u6210\u8f68\u8ff9\u6570\u91cf\u5f3a\u76f8\u5173\uff0c\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u6f14\u793a\u663e\u8457\u63d0\u5347\u7b56\u7565\u6027\u80fd\u3002", "conclusion": "MobRT\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8\u673a\u68b0\u81c2\u6f14\u793a\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u80fd\uff0c\u63a8\u52a8\u4e86\u79fb\u52a8\u64cd\u4f5c\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2510.04612", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04612", "abs": "https://arxiv.org/abs/2510.04612", "authors": ["Simon Boche", "Jaehyung Jung", "Sebasti\u00e1n Barbas Laina", "Stefan Leutenegger"], "title": "OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS", "comment": "IEEE Transactions on Robotics (T-RO) - Special Issue: Visual SLAM", "summary": "To empower mobile robots with usable maps as well as highest state estimation\naccuracy and robustness, we present OKVIS2-X: a state-of-the-art multi-sensor\nSimultaneous Localization and Mapping (SLAM) system building dense volumetric\noccupancy maps, while scalable to large environments and operating in realtime.\nOur unified SLAM framework seamlessly integrates different sensor modalities:\nvisual, inertial, measured or learned depth, LiDAR and Global Navigation\nSatellite System (GNSS) measurements. Unlike most state-of-the-art SLAM\nsystems, we advocate using dense volumetric map representations when leveraging\ndepth or range-sensing capabilities. We employ an efficient submapping strategy\nthat allows our system to scale to large environments, showcased in sequences\nof up to 9 kilometers. OKVIS2-X enhances its accuracy and robustness by\ntightly-coupling the estimator and submaps through map alignment factors. Our\nsystem provides globally consistent maps, directly usable for autonomous\nnavigation. To further improve the accuracy of OKVIS2-X, we also incorporate\nthe option of performing online calibration of camera extrinsics. Our system\nachieves the highest trajectory accuracy in EuRoC against state-of-the-art\nalternatives, outperforms all competitors in the Hilti22 VI-only benchmark,\nwhile also proving competitive in the LiDAR version, and showcases state of the\nart accuracy in the diverse and large-scale sequences from the VBR dataset.", "AI": {"tldr": "OKVIS2-X\u662f\u4e00\u4e2a\u5148\u8fdb\u7684\u591a\u4f20\u611f\u5668SLAM\u7cfb\u7edf\uff0c\u80fd\u591f\u6784\u5efa\u5bc6\u96c6\u4f53\u7d20\u5360\u7528\u5730\u56fe\uff0c\u5728\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u5b9e\u65f6\u8fd0\u884c\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u63d0\u4f9b\u53ef\u7528\u7684\u5730\u56fe\u4ee5\u53ca\u6700\u9ad8\u7684\u72b6\u6001\u4f30\u8ba1\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u96c6\u6210\u591a\u79cd\u4f20\u611f\u5668\u6a21\u6001\uff0c\u5e76\u91c7\u7528\u5bc6\u96c6\u4f53\u7d20\u5730\u56fe\u8868\u793a\u6765\u5145\u5206\u5229\u7528\u6df1\u5ea6\u6216\u8ddd\u79bb\u611f\u77e5\u80fd\u529b\u3002", "method": "\u91c7\u7528\u9ad8\u6548\u5b50\u5730\u56fe\u7b56\u7565\u5b9e\u73b0\u5927\u89c4\u6a21\u73af\u5883\u6269\u5c55\uff0c\u901a\u8fc7\u5730\u56fe\u5bf9\u9f50\u56e0\u5b50\u7d27\u5bc6\u8026\u5408\u4f30\u8ba1\u5668\u548c\u5b50\u5730\u56fe\uff0c\u652f\u6301\u5728\u7ebf\u76f8\u673a\u5916\u53c2\u6807\u5b9a\uff0c\u96c6\u6210\u89c6\u89c9\u3001\u60ef\u6027\u3001\u6df1\u5ea6\u3001LiDAR\u548cGNSS\u7b49\u591a\u79cd\u4f20\u611f\u5668\u3002", "result": "\u5728EuRoC\u6570\u636e\u96c6\u4e2d\u8fbe\u5230\u6700\u9ad8\u8f68\u8ff9\u7cbe\u5ea6\uff0c\u5728Hilti22 VI-only\u57fa\u51c6\u4e2d\u8d85\u8d8a\u6240\u6709\u7ade\u4e89\u5bf9\u624b\uff0c\u5728LiDAR\u7248\u672c\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5728VBR\u6570\u636e\u96c6\u7684\u5927\u89c4\u6a21\u5e8f\u5217\u4e2d\u5c55\u793a\u51fa\u6700\u5148\u8fdb\u7cbe\u5ea6\u3002", "conclusion": "OKVIS2-X\u662f\u4e00\u4e2a\u529f\u80fd\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u591a\u4f20\u611f\u5668SLAM\u7cfb\u7edf\uff0c\u80fd\u591f\u751f\u6210\u5168\u5c40\u4e00\u81f4\u7684\u5730\u56fe\uff0c\u76f4\u63a5\u9002\u7528\u4e8e\u81ea\u4e3b\u5bfc\u822a\u4efb\u52a1\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2510.04692", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.04692", "abs": "https://arxiv.org/abs/2510.04692", "authors": ["Lyes Saad Saoud", "Irfan Hussain"], "title": "Bio-Inspired Robotic Houbara: From Development to Field Deployment for Behavioral Studies", "comment": null, "summary": "Biomimetic intelligence and robotics are transforming field ecology by\nenabling lifelike robotic surrogates that interact naturally with animals under\nreal world conditions. Studying avian behavior in the wild remains challenging\ndue to the need for highly realistic morphology, durable outdoor operation, and\nintelligent perception that can adapt to uncontrolled environments. We present\na next generation bio inspired robotic platform that replicates the morphology\nand visual appearance of the female Houbara bustard to support controlled\nethological studies and conservation oriented field research. The system\nintroduces a fully digitally replicable fabrication workflow that combines high\nresolution structured light 3D scanning, parametric CAD modelling, articulated\n3D printing, and photorealistic UV textured vinyl finishing to achieve\nanatomically accurate and durable robotic surrogates. A six wheeled rocker\nbogie chassis ensures stable mobility on sand and irregular terrain, while an\nembedded NVIDIA Jetson module enables real time RGB and thermal perception,\nlightweight YOLO based detection, and an autonomous visual servoing loop that\naligns the robot's head toward detected targets without human intervention. A\nlightweight thermal visible fusion module enhances perception in low light\nconditions. Field trials in desert aviaries demonstrated reliable real time\noperation at 15 to 22 FPS with latency under 100 ms and confirmed that the\nplatform elicits natural recognition and interactive responses from live\nHoubara bustards under harsh outdoor conditions. This integrated framework\nadvances biomimetic field robotics by uniting reproducible digital fabrication,\nembodied visual intelligence, and ecological validation, providing a\ntransferable blueprint for animal robot interaction research, conservation\nrobotics, and public engagement.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4eff\u751f\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u6a21\u62df\u96cc\u6027\u6ce2\u6591\u9e28\u7684\u5f62\u6001\u548c\u5916\u89c2\uff0c\u7528\u4e8e\u91ce\u5916\u751f\u6001\u7814\u7a76\u548c\u52a8\u7269\u4fdd\u62a4\u3002", "motivation": "\u7814\u7a76\u91ce\u751f\u9e1f\u7c7b\u884c\u4e3a\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u9ad8\u5ea6\u903c\u771f\u7684\u5f62\u6001\u3001\u8010\u7528\u7684\u6237\u5916\u64cd\u4f5c\u548c\u9002\u5e94\u975e\u53d7\u63a7\u73af\u5883\u7684\u667a\u80fd\u611f\u77e5\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5168\u6570\u5b57\u5316\u53ef\u590d\u5236\u5236\u9020\u6d41\u7a0b\uff0c\u5305\u62ec\u9ad8\u5206\u8fa8\u7387\u7ed3\u6784\u51493D\u626b\u63cf\u3001\u53c2\u6570\u5316CAD\u5efa\u6a21\u3001\u5173\u8282\u5f0f3D\u6253\u5370\u548c\u903c\u771fUV\u7eb9\u7406\u4e59\u70ef\u57fa\u9970\u9762\uff0c\u7ed3\u5408\u516d\u8f6e\u6447\u81c2\u5e95\u76d8\u548c\u5d4c\u5165\u5f0fNVIDIA Jetson\u6a21\u5757\u5b9e\u73b0\u5b9e\u65f6\u611f\u77e5\u548c\u81ea\u4e3b\u89c6\u89c9\u4f3a\u670d\u3002", "result": "\u6c99\u6f20\u9e1f\u820d\u73b0\u573a\u8bd5\u9a8c\u663e\u793a\uff0c\u7cfb\u7edf\u80fd\u4ee515-22 FPS\u5b9e\u65f6\u8fd0\u884c\uff0c\u5ef6\u8fdf\u4f4e\u4e8e100\u6beb\u79d2\uff0c\u5e76\u80fd\u5f15\u53d1\u6d3b\u4f53\u6ce2\u6591\u9e28\u7684\u81ea\u7136\u8bc6\u522b\u548c\u4e92\u52a8\u53cd\u5e94\u3002", "conclusion": "\u8be5\u96c6\u6210\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u53ef\u590d\u5236\u7684\u6570\u5b57\u5236\u9020\u3001\u5177\u8eab\u89c6\u89c9\u667a\u80fd\u548c\u751f\u6001\u9a8c\u8bc1\uff0c\u63a8\u8fdb\u4e86\u4eff\u751f\u91ce\u5916\u673a\u5668\u4eba\u6280\u672f\uff0c\u4e3a\u52a8\u7269-\u673a\u5668\u4eba\u4ea4\u4e92\u7814\u7a76\u3001\u4fdd\u62a4\u673a\u5668\u4eba\u548c\u516c\u4f17\u53c2\u4e0e\u63d0\u4f9b\u4e86\u53ef\u8f6c\u79fb\u7684\u84dd\u56fe\u3002"}}
{"id": "2510.04696", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04696", "abs": "https://arxiv.org/abs/2510.04696", "authors": ["Alexander L. Mitchell", "Joe Watson", "Ingmar Posner"], "title": "Building Gradient by Gradient: Decentralised Energy Functions for Bimanual Robot Assembly", "comment": "8 pages, 6 figures, 1 table", "summary": "There are many challenges in bimanual assembly, including high-level\nsequencing, multi-robot coordination, and low-level, contact-rich operations\nsuch as component mating. Task and motion planning (TAMP) methods, while\neffective in this domain, may be prohibitively slow to converge when adapting\nto disturbances that require new task sequencing and optimisation. These events\nare common during tight-tolerance assembly, where difficult-to-model dynamics\nsuch as friction or deformation require rapid replanning and reattempts.\nMoreover, defining explicit task sequences for assembly can be cumbersome,\nlimiting flexibility when task replanning is required. To simplify this\nplanning, we introduce a decentralised gradient-based framework that uses a\npiecewise continuous energy function through the automatic composition of\nadaptive potential functions. This approach generates sub-goals using only\nmyopic optimisation, rather than long-horizon planning. It demonstrates\neffectiveness at solving long-horizon tasks due to the structure and adaptivity\nof the energy function. We show that our approach scales to physical bimanual\nassembly tasks for constructing tight-tolerance assemblies. In these\nexperiments, we discover that our gradient-based rapid replanning framework\ngenerates automatic retries, coordinated motions and autonomous handovers in an\nemergent fashion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u5206\u6563\u5f0f\u6846\u67b6\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u52bf\u51fd\u6570\u7684\u81ea\u52a8\u7ec4\u5408\u6765\u751f\u6210\u5206\u6bb5\u8fde\u7eed\u80fd\u91cf\u51fd\u6570\uff0c\u7528\u4e8e\u89e3\u51b3\u53cc\u81c2\u88c5\u914d\u4e2d\u7684\u5feb\u901f\u91cd\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u53cc\u81c2\u88c5\u914d\u9762\u4e34\u9ad8\u5c42\u5e8f\u5217\u89c4\u5212\u3001\u591a\u673a\u5668\u4eba\u534f\u8c03\u548c\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u7b49\u6311\u6218\u3002\u4f20\u7edf\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5728\u9700\u8981\u65b0\u4efb\u52a1\u5e8f\u5217\u65f6\u6536\u655b\u7f13\u6162\uff0c\u4e14\u663e\u5f0f\u5b9a\u4e49\u4efb\u52a1\u5e8f\u5217\u9650\u5236\u4e86\u91cd\u89c4\u5212\u65f6\u7684\u7075\u6d3b\u6027\u3002", "method": "\u4f7f\u7528\u81ea\u9002\u5e94\u52bf\u51fd\u6570\u81ea\u52a8\u7ec4\u5408\u6784\u5efa\u5206\u6bb5\u8fde\u7eed\u80fd\u91cf\u51fd\u6570\uff0c\u901a\u8fc7\u8fd1\u89c6\u4f18\u5316\u800c\u975e\u957f\u89c6\u8ddd\u89c4\u5212\u751f\u6210\u5b50\u76ee\u6807\uff0c\u5b9e\u73b0\u5feb\u901f\u91cd\u89c4\u5212\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6269\u5c55\u5230\u7269\u7406\u53cc\u81c2\u88c5\u914d\u4efb\u52a1\uff0c\u6784\u5efa\u7d27\u5bc6\u516c\u5dee\u88c5\u914d\u4ef6\uff0c\u5e76\u81ea\u53d1\u4ea7\u751f\u81ea\u52a8\u91cd\u8bd5\u3001\u534f\u8c03\u8fd0\u52a8\u548c\u81ea\u4e3b\u4ea4\u63a5\u884c\u4e3a\u3002", "conclusion": "\u57fa\u4e8e\u68af\u5ea6\u7684\u5feb\u901f\u91cd\u89c4\u5212\u6846\u67b6\u901a\u8fc7\u80fd\u91cf\u51fd\u6570\u7684\u7ed3\u6784\u548c\u81ea\u9002\u5e94\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u8ddd\u4efb\u52a1\uff0c\u5728\u7d27\u5bc6\u516c\u5dee\u88c5\u914d\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2510.04724", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04724", "abs": "https://arxiv.org/abs/2510.04724", "authors": ["Etor Arza", "Welf Rehberg", "Philipp Weiss", "Mihir Kulkarni", "Kostas Alexis"], "title": "Performance-guided Task-specific Optimization for Multirotor Design", "comment": null, "summary": "This paper introduces a methodology for task-specific design optimization of\nmultirotor Micro Aerial Vehicles. By leveraging reinforcement learning,\nBayesian optimization, and covariance matrix adaptation evolution strategy, we\noptimize aerial robot designs guided exclusively by their closed-loop\nperformance in a considered task. Our approach systematically explores the\ndesign space of motor pose configurations while ensuring manufacturability\nconstraints and minimal aerodynamic interference. Results demonstrate that\noptimized designs achieve superior performance compared to conventional\nmultirotor configurations in agile waypoint navigation tasks, including against\nfully actuated designs from the literature. We build and test one of the\noptimized designs in the real world to validate the sim2real transferability of\nour approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u3001\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u534f\u65b9\u5dee\u77e9\u9635\u81ea\u9002\u5e94\u8fdb\u5316\u7b56\u7565\u7684\u591a\u65cb\u7ffc\u5fae\u578b\u98de\u884c\u5668\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ed\u73af\u6027\u80fd\u4f18\u5316\u7535\u673a\u59ff\u6001\u914d\u7f6e\uff0c\u5728\u654f\u6377\u822a\u70b9\u5bfc\u822a\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edf\u591a\u65cb\u7ffc\u8bbe\u8ba1\u901a\u5e38\u91c7\u7528\u6807\u51c6\u914d\u7f6e\uff0c\u7f3a\u4e4f\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u4f18\u5316\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u4ec5\u57fa\u4e8e\u95ed\u73af\u4efb\u52a1\u6027\u80fd\u6765\u4f18\u5316\u98de\u884c\u5668\u8bbe\u8ba1\u3002", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u3001\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u534f\u65b9\u5dee\u77e9\u9635\u81ea\u9002\u5e94\u8fdb\u5316\u7b56\u7565\uff0c\u5728\u8003\u8651\u53ef\u5236\u9020\u6027\u7ea6\u675f\u548c\u6700\u5c0f\u7a7a\u6c14\u52a8\u529b\u5b66\u5e72\u6270\u7684\u524d\u63d0\u4e0b\uff0c\u7cfb\u7edf\u63a2\u7d22\u7535\u673a\u59ff\u6001\u914d\u7f6e\u7684\u8bbe\u8ba1\u7a7a\u95f4\u3002", "result": "\u4f18\u5316\u8bbe\u8ba1\u5728\u654f\u6377\u822a\u70b9\u5bfc\u822a\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u591a\u65cb\u7ffc\u914d\u7f6e\u7684\u6027\u80fd\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u6587\u732e\u4e2d\u7684\u5168\u9a71\u52a8\u8bbe\u8ba1\u3002\u901a\u8fc7\u5b9e\u9645\u5efa\u9020\u548c\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f18\u5316\u591a\u65cb\u7ffc\u98de\u884c\u5668\u7684\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\uff0c\u901a\u8fc7\u95ed\u73af\u6027\u80fd\u6307\u5bfc\u7684\u8bbe\u8ba1\u4f18\u5316\u53ef\u4ee5\u4ea7\u751f\u8d85\u8d8a\u4f20\u7edf\u914d\u7f6e\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e14\u5177\u6709\u826f\u597d\u7684\u73b0\u5b9e\u5e94\u7528\u53ef\u884c\u6027\u3002"}}
{"id": "2510.04774", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.04774", "abs": "https://arxiv.org/abs/2510.04774", "authors": ["Weixu Zhu", "Marco Dorigo", "Mary Katherine Heinrich"], "title": "Online automatic code generation for robot swarms: LLMs and self-organizing hierarchy", "comment": null, "summary": "Our recently introduced self-organizing nervous system (SoNS) provides robot\nswarms with 1) ease of behavior design and 2) global estimation of the swarm\nconfiguration and its collective environment, facilitating the implementation\nof online automatic code generation for robot swarms. In a demonstration with 6\nreal robots and simulation trials with >30 robots, we show that when a\nSoNS-enhanced robot swarm gets stuck, it can automatically solicit and run code\ngenerated by an external LLM on the fly, completing its mission with an 85%\nsuccess rate.", "AI": {"tldr": "SoNS\u4e3a\u673a\u5668\u4eba\u7fa4\u4f53\u63d0\u4f9b\u4e86\u884c\u4e3a\u8bbe\u8ba1\u7b80\u4fbf\u6027\u548c\u5168\u5c40\u914d\u7f6e\u4f30\u8ba1\uff0c\u80fd\u591f\u81ea\u52a8\u8bf7\u6c42\u5916\u90e8LLM\u751f\u6210\u4ee3\u7801\u4ee5\u89e3\u51b3\u4efb\u52a1\u5361\u987f\u95ee\u9898\uff0c\u57286\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u548c30+\u6a21\u62df\u673a\u5668\u4eba\u4e2d\u8fbe\u523085%\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u4efb\u52a1\u6267\u884c\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u5361\u987f\u7684\u95ee\u9898\uff0c\u540c\u65f6\u7b80\u5316\u7fa4\u4f53\u884c\u4e3a\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u5b9e\u73b0\u7fa4\u4f53\u914d\u7f6e\u548c\u73af\u5883\u7684\u5168\u5c40\u611f\u77e5\u3002", "method": "\u91c7\u7528\u81ea\u7ec4\u7ec7\u795e\u7ecf\u7cfb\u7edf(SoNS)\u67b6\u6784\uff0c\u5f53\u7fa4\u4f53\u5361\u987f\u65f6\u81ea\u52a8\u5411\u5916\u90e8\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u8bf7\u6c42\u751f\u6210\u4ee3\u7801\u5e76\u5373\u65f6\u6267\u884c\u3002", "result": "\u57286\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u548c\u8d85\u8fc730\u4e2a\u6a21\u62df\u673a\u5668\u4eba\u7684\u5b9e\u9a8c\u4e2d\uff0c\u7cfb\u7edf\u80fd\u591f\u81ea\u52a8\u751f\u6210\u4ee3\u7801\u89e3\u51b3\u5361\u987f\u95ee\u9898\uff0c\u4efb\u52a1\u5b8c\u6210\u6210\u529f\u7387\u8fbe\u523085%\u3002", "conclusion": "SoNS\u7ed3\u5408LLM\u4ee3\u7801\u751f\u6210\u80fd\u591f\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u6027\u548c\u4efb\u52a1\u5b8c\u6210\u80fd\u529b\u3002"}}
{"id": "2510.04839", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04839", "abs": "https://arxiv.org/abs/2510.04839", "authors": ["Shuo Sha", "Anupam Bhakta", "Zhenyuan Jiang", "Kevin Qiu", "Ishaan Mahajan", "Gabriel Bravo", "Brian Plancher"], "title": "TAG-K: Tail-Averaged Greedy Kaczmarz for Computationally Efficient and Performant Online Inertial Parameter Estimation", "comment": null, "summary": "Accurate online inertial parameter estimation is essential for adaptive\nrobotic control, enabling real-time adjustment to payload changes,\nenvironmental interactions, and system wear. Traditional methods such as\nRecursive Least Squares (RLS) and the Kalman Filter (KF) often struggle to\ntrack abrupt parameter shifts or incur high computational costs, limiting their\neffectiveness in dynamic environments and for computationally constrained\nrobotic systems. As such, we introduce TAG-K, a lightweight extension of the\nKaczmarz method that combines greedy randomized row selection for rapid\nconvergence with tail averaging for robustness under noise and inconsistency.\nThis design enables fast, stable parameter adaptation while retaining the low\nper-iteration complexity inherent to the Kaczmarz framework. We evaluate TAG-K\nin synthetic benchmarks and quadrotor tracking tasks against RLS, KF, and other\nKaczmarz variants. TAG-K achieves 1.5x-1.9x faster solve times on laptop-class\nCPUs and 4.8x-20.7x faster solve times on embedded microcontrollers. More\nimportantly, these speedups are paired with improved resilience to measurement\nnoise and a 25% reduction in estimation error, leading to nearly 2x better\nend-to-end tracking performance.", "AI": {"tldr": "TAG-K\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684Kaczmarz\u65b9\u6cd5\u6269\u5c55\uff0c\u7ed3\u5408\u8d2a\u5fc3\u968f\u673a\u884c\u9009\u62e9\u548c\u5c3e\u90e8\u5e73\u5747\uff0c\u7528\u4e8e\u5728\u7ebf\u60ef\u6027\u53c2\u6570\u4f30\u8ba1\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u73b0\u5feb\u901f\u7a33\u5b9a\u7684\u53c2\u6570\u9002\u5e94\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982\u9012\u5f52\u6700\u5c0f\u4e8c\u4e58\u6cd5\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5728\u8ddf\u8e2a\u7a81\u53d8\u53c2\u6570\u53d8\u5316\u65f6\u8868\u73b0\u4e0d\u4f73\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u5728\u52a8\u6001\u73af\u5883\u548c\u8ba1\u7b97\u53d7\u9650\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "TAG-K\u7ed3\u5408\u8d2a\u5fc3\u968f\u673a\u884c\u9009\u62e9\u5b9e\u73b0\u5feb\u901f\u6536\u655b\uff0c\u4f7f\u7528\u5c3e\u90e8\u5e73\u5747\u5728\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u6027\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301Kaczmarz\u6846\u67b6\u7684\u4f4e\u6bcf\u8fed\u4ee3\u590d\u6742\u5ea6\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u548c\u56db\u65cb\u7ffc\u8ddf\u8e2a\u4efb\u52a1\u4e2d\uff0cTAG-K\u5728\u7b14\u8bb0\u672c\u7535\u8111CPU\u4e0a\u5b9e\u73b01.5x-1.9x\u66f4\u5feb\u7684\u6c42\u89e3\u65f6\u95f4\uff0c\u5728\u5d4c\u5165\u5f0f\u5fae\u63a7\u5236\u5668\u4e0a\u5b9e\u73b04.8x-20.7x\u66f4\u5feb\u7684\u6c42\u89e3\u65f6\u95f4\uff0c\u540c\u65f6\u63d0\u9ad8\u5bf9\u6d4b\u91cf\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u4f30\u8ba1\u8bef\u5dee\u51cf\u5c1125%\uff0c\u7aef\u5230\u7aef\u8ddf\u8e2a\u6027\u80fd\u63d0\u5347\u8fd12\u500d\u3002", "conclusion": "TAG-K\u4e3a\u81ea\u9002\u5e94\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u5728\u7ebf\u60ef\u6027\u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8ddf\u8e2a\u6027\u80fd\u65b9\u9762\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002"}}
{"id": "2510.04883", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04883", "abs": "https://arxiv.org/abs/2510.04883", "authors": ["Nathan Shankar", "Pawel Ladosz", "Hujun Yin"], "title": "CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery", "comment": "8 pages, 8 figures", "summary": "This paper presents a novel approach for enabling robust robotic perception\nin dark environments using infrared (IR) stream. IR stream is less susceptible\nto noise than RGB in low-light conditions. However, it is dominated by active\nemitter patterns that hinder high-level tasks such as object detection,\ntracking and localisation. To address this, a U-Net-based architecture is\nproposed that reconstructs clean IR images from emitter-populated input,\nimproving both image quality and downstream robotic performance. This approach\noutperforms existing enhancement techniques and enables reliable operation of\nvision-driven robotic systems across illumination conditions from well-lit to\nextreme low-light scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eU-Net\u67b6\u6784\u7684\u65b9\u6cd5\uff0c\u4ece\u5e26\u6709\u53d1\u5c04\u5668\u566a\u58f0\u7684\u7ea2\u5916\u56fe\u50cf\u4e2d\u91cd\u5efa\u5e72\u51c0\u56fe\u50cf\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u9ed1\u6697\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u7ea2\u5916\u6d41\u5728\u4f4e\u5149\u6761\u4ef6\u4e0b\u6bd4RGB\u6d41\u66f4\u6297\u566a\u58f0\uff0c\u4f46\u53d7\u4e3b\u52a8\u53d1\u5c04\u5668\u6a21\u5f0f\u5e72\u6270\uff0c\u5f71\u54cd\u7269\u4f53\u68c0\u6d4b\u3001\u8ddf\u8e2a\u548c\u5b9a\u4f4d\u7b49\u9ad8\u7ea7\u4efb\u52a1\u3002", "method": "\u4f7f\u7528U-Net\u67b6\u6784\u4ece\u5e26\u6709\u53d1\u5c04\u5668\u566a\u58f0\u7684\u7ea2\u5916\u8f93\u5165\u4e2d\u91cd\u5efa\u5e72\u51c0\u7684IR\u56fe\u50cf\u3002", "result": "\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u589e\u5f3a\u6280\u672f\uff0c\u80fd\u591f\u5728\u4ece\u826f\u597d\u5149\u7167\u5230\u6781\u7aef\u4f4e\u5149\u573a\u666f\u7684\u5404\u79cd\u5149\u7167\u6761\u4ef6\u4e0b\u5b9e\u73b0\u53ef\u9760\u7684\u89c6\u89c9\u9a71\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\u64cd\u4f5c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7ea2\u5916\u56fe\u50cf\u4e2d\u7684\u53d1\u5c04\u5668\u566a\u58f0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u9ed1\u6697\u73af\u5883\u4e2d\u7684\u611f\u77e5\u6027\u80fd\u3002"}}
{"id": "2510.04898", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.04898", "abs": "https://arxiv.org/abs/2510.04898", "authors": ["Zheng Xiong", "Kang Li", "Zilin Wang", "Matthew Jackson", "Jakob Foerster", "Shimon Whiteson"], "title": "HyperVLA: Efficient Inference in Vision-Language-Action Models via Hypernetworks", "comment": null, "summary": "Built upon language and vision foundation models with strong generalization\nability and trained on large-scale robotic data, Vision-Language-Action (VLA)\nmodels have recently emerged as a promising approach to learning generalist\nrobotic policies. However, a key drawback of existing VLAs is their extremely\nhigh inference costs. In this paper, we propose HyperVLA to address this\nproblem. Unlike existing monolithic VLAs that activate the whole model during\nboth training and inference, HyperVLA uses a novel hypernetwork (HN)-based\narchitecture that activates only a small task-specific policy during inference,\nwhile still retaining the high model capacity needed to accommodate diverse\nmulti-task behaviors during training. Successfully training an HN-based VLA is\nnontrivial so HyperVLA contains several key algorithm design features that\nimprove its performance, including properly utilizing the prior knowledge from\nexisting vision foundation models, HN normalization, and an action generation\nstrategy. Compared to monolithic VLAs, HyperVLA achieves a similar or even\nhigher success rate for both zero-shot generalization and few-shot adaptation,\nwhile significantly reducing inference costs. Compared to OpenVLA, a\nstate-of-the-art VLA model, HyperVLA reduces the number of activated parameters\nat test time by $90\\times$, and accelerates inference speed by $120\\times$.\nCode is publicly available at https://github.com/MasterXiong/HyperVLA", "AI": {"tldr": "HyperVLA\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u67b6\u6784\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u591a\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5b58\u5728\u6781\u9ad8\u7684\u63a8\u7406\u6210\u672c\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u6a21\u578b\u5bb9\u91cf\u53c8\u80fd\u964d\u4f4e\u63a8\u7406\u5f00\u9500\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8d85\u7f51\u7edc\u67b6\u6784\uff0c\u4ec5\u5728\u63a8\u7406\u65f6\u6fc0\u6d3b\u5c0f\u578b\u4efb\u52a1\u7279\u5b9a\u7b56\u7565\uff0c\u540c\u65f6\u4fdd\u7559\u8bad\u7ec3\u65f6\u7684\u9ad8\u6a21\u578b\u5bb9\u91cf\uff1b\u5305\u542b\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5148\u9a8c\u77e5\u8bc6\u3001\u8d85\u7f51\u7edc\u5f52\u4e00\u5316\u548c\u52a8\u4f5c\u751f\u6210\u7b56\u7565\u7b49\u5173\u952e\u6280\u672f\u3002", "result": "\u76f8\u6bd4\u5355\u4f53VLA\u6a21\u578b\uff0c\u5728\u96f6\u6837\u672c\u6cdb\u5316\u548c\u5c11\u6837\u672c\u9002\u5e94\u65b9\u9762\u8fbe\u5230\u76f8\u4f3c\u6216\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff1b\u76f8\u6bd4OpenVLA\uff0c\u6fc0\u6d3b\u53c2\u6570\u91cf\u51cf\u5c1190\u500d\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u5347120\u500d\u3002", "conclusion": "HyperVLA\u6210\u529f\u89e3\u51b3\u4e86VLA\u6a21\u578b\u7684\u9ad8\u63a8\u7406\u6210\u672c\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u3002"}}
{"id": "2510.04991", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.04991", "abs": "https://arxiv.org/abs/2510.04991", "authors": ["D. Schwartz", "K. Kondo", "J. P. How"], "title": "Efficient Navigation in Unknown Indoor Environments with Vision-Language Models", "comment": "8 pages, 4 figures", "summary": "We present a novel high-level planning framework that leverages\nvision-language models (VLMs) to improve autonomous navigation in unknown\nindoor environments with many dead ends. Traditional exploration methods often\ntake inefficient routes due to limited global reasoning and reliance on local\nheuristics. In contrast, our approach enables a VLM to reason directly about an\noccupancy map in a zero-shot manner, selecting subgoals that are likely to lead\nto more efficient paths. At each planning step, we convert a 3D occupancy grid\ninto a partial 2D map of the environment, and generate candidate subgoals. Each\nsubgoal is then evaluated and ranked against other candidates by the model. We\nintegrate this planning scheme into DYNUS \\cite{kondo2025dynus}, a\nstate-of-the-art trajectory planner, and demonstrate improved navigation\nefficiency in simulation. The VLM infers structural patterns (e.g., rooms,\ncorridors) from incomplete maps and balances the need to make progress toward a\ngoal against the risk of entering unknown space. This reduces common greedy\nfailures (e.g., detouring into small rooms) and achieves about 10\\% shorter\npaths on average.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u5c42\u89c4\u5212\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u63a8\u7406\u5360\u636e\u5730\u56fe\u6765\u9009\u62e9\u66f4\u6709\u6548\u7684\u5b50\u76ee\u6807\uff0c\u63d0\u9ad8\u672a\u77e5\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u63a2\u7d22\u65b9\u6cd5\u7531\u4e8e\u5168\u5c40\u63a8\u7406\u80fd\u529b\u6709\u9650\u548c\u4f9d\u8d56\u5c40\u90e8\u542f\u53d1\u5f0f\uff0c\u5728\u5b58\u5728\u8bb8\u591a\u6b7b\u80e1\u540c\u7684\u672a\u77e5\u5ba4\u5185\u73af\u5883\u4e2d\u5f80\u5f80\u91c7\u53d6\u4f4e\u6548\u8def\u5f84\u3002", "method": "\u5c063D\u5360\u636e\u7f51\u683c\u8f6c\u6362\u4e3a\u90e8\u52062D\u5730\u56fe\uff0c\u751f\u6210\u5019\u9009\u5b50\u76ee\u6807\uff0c\u4f7f\u7528VLM\u5bf9\u5b50\u76ee\u6807\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\u548c\u6392\u5e8f\uff0c\u5e76\u4e0eDYNUS\u8f68\u8ff9\u89c4\u5212\u5668\u96c6\u6210\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u5b9e\u73b0\u4e86\u7ea610%\u7684\u5e73\u5747\u8def\u5f84\u7f29\u77ed\uff0c\u51cf\u5c11\u4e86\u5e38\u89c1\u7684\u8d2a\u5a6a\u5931\u8d25\uff08\u5982\u7ed5\u8fdb\u5c0f\u623f\u95f4\uff09\u3002", "conclusion": "VLM\u80fd\u591f\u4ece\u4e0d\u5b8c\u6574\u5730\u56fe\u4e2d\u63a8\u65ad\u7ed3\u6784\u6a21\u5f0f\uff0c\u5e73\u8861\u5411\u76ee\u6807\u524d\u8fdb\u4e0e\u8fdb\u5165\u672a\u77e5\u7a7a\u95f4\u7684\u98ce\u9669\uff0c\u663e\u8457\u63d0\u9ad8\u5bfc\u822a\u6548\u7387\u3002"}}
{"id": "2510.05001", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05001", "abs": "https://arxiv.org/abs/2510.05001", "authors": ["Aditya Sripada", "Abhishek Warrier"], "title": "Walking, Rolling, and Beyond: First-Principles and RL Locomotion on a TARS-Inspired Robot", "comment": "6 pages, 10 figures. Presented at IEEE-RAS International Conference\n  on Humanoid Robots (Humanoids) 2025", "summary": "Robotic locomotion research typically draws from biologically inspired leg\ndesigns, yet many human-engineered settings can benefit from\nnon-anthropomorphic forms. TARS3D translates the block-shaped 'TARS' robot from\nInterstellar into a 0.25 m, 0.99 kg research platform with seven actuated\ndegrees of freedom. The film shows two primary gaits: a bipedal-like walk and a\nhigh-speed rolling mode. For TARS3D, we build reduced-order models for each,\nderive closed-form limit-cycle conditions, and validate the predictions on\nhardware. Experiments confirm that the robot respects its +/-150 degree hip\nlimits, alternates left-right contacts without interference, and maintains an\neight-step hybrid limit cycle in rolling mode. Because each telescopic leg\nprovides four contact corners, the rolling gait is modeled as an eight-spoke\ndouble rimless wheel. The robot's telescopic leg redundancy implies a far\nricher gait repertoire than the two limit cycles treated analytically. So, we\nused deep reinforcement learning (DRL) in simulation to search the unexplored\nspace. We observed that the learned policy can recover the analytic gaits under\nthe right priors and discover novel behaviors as well. Our findings show that\nTARS3D's fiction-inspired bio-transcending morphology can realize multiple\npreviously unexplored locomotion modes and that further learning-driven search\nis likely to reveal more. This combination of analytic synthesis and\nreinforcement learning opens a promising pathway for multimodal robotics.", "AI": {"tldr": "TARS3D\u5c06\u7535\u5f71\u300a\u661f\u9645\u7a7f\u8d8a\u300b\u4e2d\u7684\u65b9\u5757\u673a\u5668\u4ebaTARS\u8f6c\u5316\u4e3a\u73b0\u5b9e\u7814\u7a76\u5e73\u53f0\uff0c\u901a\u8fc7\u5206\u6790\u5efa\u6a21\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\u4e86\u5176\u4e30\u5bcc\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5305\u62ec\u53cc\u8db3\u884c\u8d70\u548c\u9ad8\u901f\u6eda\u52a8\u7b49\u751f\u7269\u8d85\u8d8a\u5f62\u6001\u7684\u591a\u79cd\u8fd0\u52a8\u65b9\u5f0f\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u8fd0\u52a8\u7814\u7a76\u591a\u53d7\u751f\u7269\u542f\u53d1\uff0c\u4f46\u8bb8\u591a\u5de5\u7a0b\u573a\u666f\u9700\u8981\u975e\u4eba\u5f62\u8bbe\u8ba1\u3002\u53d7\u7535\u5f71\u300a\u661f\u9645\u7a7f\u8d8a\u300b\u4e2dTARS\u673a\u5668\u4eba\u542f\u53d1\uff0c\u63a2\u7d22\u65b9\u5757\u5f62\u6001\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u6f5c\u529b\u3002", "method": "\u6784\u5efa\u4e86TARS3D\u673a\u5668\u4eba\u5e73\u53f0\uff080.25m\uff0c0.99kg\uff0c7\u4e2a\u9a71\u52a8\u81ea\u7531\u5ea6\uff09\uff0c\u4e3a\u4e24\u79cd\u4e3b\u8981\u6b65\u6001\u5efa\u7acb\u964d\u9636\u6a21\u578b\u5e76\u63a8\u5bfc\u95ed\u5f0f\u6781\u9650\u73af\u6761\u4ef6\uff0c\u540c\u65f6\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u4eff\u771f\u4e2d\u63a2\u7d22\u672a\u5f00\u53d1\u8fd0\u52a8\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u673a\u5668\u4eba\u9075\u5b88\u00b1150\u5ea6\u9acb\u5173\u8282\u9650\u5236\uff0c\u5de6\u53f3\u4ea4\u66ff\u63a5\u89e6\u65e0\u5e72\u6270\uff0c\u5728\u6eda\u52a8\u6a21\u5f0f\u4e2d\u7ef4\u6301\u516b\u6b65\u6df7\u5408\u6781\u9650\u73af\u3002\u5b66\u4e60\u7b56\u7565\u80fd\u6062\u590d\u5206\u6790\u6b65\u6001\u5e76\u53d1\u73b0\u65b0\u884c\u4e3a\u3002", "conclusion": "TARS3D\u7684\u865a\u6784\u751f\u7269\u8d85\u8d8a\u5f62\u6001\u80fd\u5b9e\u73b0\u591a\u79cd\u672a\u63a2\u7d22\u8fd0\u52a8\u6a21\u5f0f\uff0c\u7ed3\u5408\u5206\u6790\u5408\u6210\u548c\u5f3a\u5316\u5b66\u4e60\u4e3a\u591a\u6a21\u6001\u673a\u5668\u4eba\u5f00\u8f9f\u4e86\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2510.05057", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.05057", "abs": "https://arxiv.org/abs/2510.05057", "authors": ["Mingyu Liu", "Jiuhe Shu", "Hui Chen", "Zeju Li", "Canyu Zhao", "Jiange Yang", "Shenyuan Gao", "Hao Chen", "Chunhua Shen"], "title": "StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation", "comment": null, "summary": "A fundamental challenge in embodied intelligence is developing expressive and\ncompact state representations for efficient world modeling and decision making.\nHowever, existing methods often fail to achieve this balance, yielding\nrepresentations that are either overly redundant or lacking in task-critical\ninformation. We propose an unsupervised approach that learns a highly\ncompressed two-token state representation using a lightweight encoder and a\npre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong\ngenerative prior. Our representation is efficient, interpretable, and\nintegrates seamlessly into existing VLA-based models, improving performance by\n14.3% on LIBERO and 30% in real-world task success with minimal inference\noverhead. More importantly, we find that the difference between these tokens,\nobtained via latent interpolation, naturally serves as a highly effective\nlatent action, which can be further decoded into executable robot actions. This\nemergent capability reveals that our representation captures structured\ndynamics without explicit supervision. We name our method StaMo for its ability\nto learn generalizable robotic Motion from compact State representation, which\nis encoded from static images, challenging the prevalent dependence to learning\nlatent action on complex architectures and video data. The resulting latent\nactions also enhance policy co-training, outperforming prior methods by 10.4%\nwith improved interpretability. Moreover, our approach scales effectively\nacross diverse data sources, including real-world robot data, simulation, and\nhuman egocentric video.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u65b9\u6cd5StaMo\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u548c\u9884\u8bad\u7ec3DiT\u89e3\u7801\u5668\u5b66\u4e60\u9ad8\u5ea6\u538b\u7f29\u7684\u53cctoken\u72b6\u6001\u8868\u793a\uff0c\u8be5\u8868\u793a\u4e0d\u4ec5\u9ad8\u6548\u53ef\u89e3\u91ca\uff0c\u8fd8\u80fd\u81ea\u7136\u5730\u4ea7\u751f\u6709\u6548\u7684\u6f5c\u5728\u52a8\u4f5c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5177\u8eab\u667a\u80fd\u4e2d\u96be\u4ee5\u5e73\u8861\u72b6\u6001\u8868\u793a\u7684\u8868\u8fbe\u6027\u548c\u7d27\u51d1\u6027\uff0c\u8981\u4e48\u8fc7\u4e8e\u5197\u4f59\uff0c\u8981\u4e48\u7f3a\u4e4f\u4efb\u52a1\u5173\u952e\u4fe1\u606f\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668\u548c\u9884\u8bad\u7ec3\u6269\u6563\u53d8\u6362\u5668\u89e3\u7801\u5668\u5b66\u4e60\u538b\u7f29\u7684\u53cctoken\u72b6\u6001\u8868\u793a\uff0c\u901a\u8fc7\u6f5c\u5728\u63d2\u503c\u83b7\u5f97\u6f5c\u5728\u52a8\u4f5c\u3002", "result": "\u5728LIBERO\u4e0a\u6027\u80fd\u63d0\u534714.3%\uff0c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534730%\uff0c\u6f5c\u5728\u52a8\u4f5c\u589e\u5f3a\u7b56\u7565\u534f\u540c\u8bad\u7ec3\uff0c\u6027\u80fd\u63d0\u534710.4%\u3002", "conclusion": "StaMo\u65b9\u6cd5\u80fd\u591f\u4ece\u9759\u6001\u56fe\u50cf\u5b66\u4e60\u901a\u7528\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u6311\u6218\u4e86\u4f9d\u8d56\u590d\u6742\u67b6\u6784\u548c\u89c6\u9891\u6570\u636e\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\u7684\u4e3b\u6d41\u65b9\u6cd5\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2510.05061", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.05061", "abs": "https://arxiv.org/abs/2510.05061", "authors": ["Anastasios Manganaris", "Vittorio Giammarino", "Ahmed H. Qureshi"], "title": "Automaton Constrained Q-Learning", "comment": "9 pages, 4 figures, 39th Conference on Neural Information Processing\n  Systems (NeurIPS 2025)", "summary": "Real-world robotic tasks often require agents to achieve sequences of goals\nwhile respecting time-varying safety constraints. However, standard\nReinforcement Learning (RL) paradigms are fundamentally limited in these\nsettings. A natural approach to these problems is to combine RL with\nLinear-time Temporal Logic (LTL), a formal language for specifying complex,\ntemporally extended tasks and safety constraints. Yet, existing RL methods for\nLTL objectives exhibit poor empirical performance in complex and continuous\nenvironments. As a result, no scalable methods support both temporally ordered\ngoals and safety simultaneously, making them ill-suited for realistic robotics\nscenarios. We propose Automaton Constrained Q-Learning (ACQL), an algorithm\nthat addresses this gap by combining goal-conditioned value learning with\nautomaton-guided reinforcement. ACQL supports most LTL task specifications and\nleverages their automaton representation to explicitly encode stage-wise goal\nprogression and both stationary and non-stationary safety constraints. We show\nthat ACQL outperforms existing methods across a range of continuous control\ntasks, including cases where prior methods fail to satisfy either goal-reaching\nor safety constraints. We further validate its real-world applicability by\ndeploying ACQL on a 6-DOF robotic arm performing a goal-reaching task in a\ncluttered, cabinet-like space with safety constraints. Our results demonstrate\nthat ACQL is a robust and scalable solution for learning robotic behaviors\naccording to rich temporal specifications.", "AI": {"tldr": "\u63d0\u51faACQL\u7b97\u6cd5\uff0c\u7ed3\u5408\u76ee\u6807\u6761\u4ef6\u503c\u5b66\u4e60\u548c\u81ea\u52a8\u673a\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u65f6\u5e8f\u76ee\u6807\u8fbe\u6210\u548c\u65f6\u95f4\u53d8\u5316\u5b89\u5168\u7ea6\u675f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u9700\u8981\u8fbe\u6210\u5e8f\u5217\u76ee\u6807\u5e76\u9075\u5b88\u65f6\u53d8\u5b89\u5168\u7ea6\u675f\uff0c\u4f46\u6807\u51c6RL\u65b9\u6cd5\u5728\u6b64\u7c7b\u8bbe\u7f6e\u4e2d\u5b58\u5728\u6839\u672c\u9650\u5236\u3002\u73b0\u6709LTL\u76ee\u6807RL\u65b9\u6cd5\u5728\u590d\u6742\u8fde\u7eed\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u540c\u65f6\u652f\u6301\u65f6\u5e8f\u76ee\u6807\u548c\u5b89\u5168\u7ea6\u675f\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\u3002", "method": "ACQL\u7b97\u6cd5\u7ed3\u5408\u76ee\u6807\u6761\u4ef6\u503c\u5b66\u4e60\u548c\u81ea\u52a8\u673a\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528LTL\u4efb\u52a1\u89c4\u8303\u7684\u81ea\u52a8\u673a\u8868\u793a\uff0c\u663e\u5f0f\u7f16\u7801\u9636\u6bb5\u5316\u76ee\u6807\u8fdb\u5c55\u4ee5\u53ca\u9759\u6001\u548c\u975e\u9759\u6001\u5b89\u5168\u7ea6\u675f\u3002", "result": "ACQL\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5305\u62ec\u5148\u524d\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u76ee\u6807\u8fbe\u6210\u6216\u5b89\u5168\u7ea6\u675f\u7684\u60c5\u51b5\u3002\u57286-DOF\u673a\u68b0\u81c2\u7684\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "ACQL\u662f\u6839\u636e\u4e30\u5bcc\u65f6\u5e8f\u89c4\u8303\u5b66\u4e60\u673a\u5668\u4eba\u884c\u4e3a\u7684\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.05070", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.05070", "abs": "https://arxiv.org/abs/2510.05070", "authors": ["Siheng Zhao", "Yanjie Ze", "Yue Wang", "C. Karen Liu", "Pieter Abbeel", "Guanya Shi", "Rocky Duan"], "title": "ResMimic: From General Motion Tracking to Humanoid Whole-body Loco-Manipulation via Residual Learning", "comment": "9 pages, 8 figures", "summary": "Humanoid whole-body loco-manipulation promises transformative capabilities\nfor daily service and warehouse tasks. While recent advances in general motion\ntracking (GMT) have enabled humanoids to reproduce diverse human motions, these\npolicies lack the precision and object awareness required for\nloco-manipulation. To this end, we introduce ResMimic, a two-stage residual\nlearning framework for precise and expressive humanoid control from human\nmotion data. First, a GMT policy, trained on large-scale human-only motion,\nserves as a task-agnostic base for generating human-like whole-body movements.\nAn efficient but precise residual policy is then learned to refine the GMT\noutputs to improve locomotion and incorporate object interaction. To further\nfacilitate efficient training, we design (i) a point-cloud-based object\ntracking reward for smoother optimization, (ii) a contact reward that\nencourages accurate humanoid body-object interactions, and (iii) a\ncurriculum-based virtual object controller to stabilize early training. We\nevaluate ResMimic in both simulation and on a real Unitree G1 humanoid. Results\nshow substantial gains in task success, training efficiency, and robustness\nover strong baselines. Videos are available at https://resmimic.github.io/ .", "AI": {"tldr": "ResMimic\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6b8b\u5dee\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u4e2d\u5b9e\u73b0\u7cbe\u786e\u4e14\u5bcc\u6709\u8868\u73b0\u529b\u7684\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u3002\u5b83\u7ed3\u5408\u4e86\u901a\u7528\u8fd0\u52a8\u8ddf\u8e2a\u7b56\u7565\u548c\u7cbe\u786e\u7684\u6b8b\u5dee\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u7cbe\u5ea6\u548c\u7269\u4f53\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u867d\u7136\u901a\u7528\u8fd0\u52a8\u8ddf\u8e2a(GMT)\u6280\u672f\u5df2\u80fd\u8ba9\u4eba\u5f62\u673a\u5668\u4eba\u590d\u73b0\u591a\u6837\u7684\u4eba\u7c7b\u52a8\u4f5c\uff0c\u4f46\u8fd9\u4e9b\u7b56\u7565\u7f3a\u4e4f\u7cbe\u786e\u6027\u548c\u7269\u4f53\u611f\u77e5\u80fd\u529b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6b8b\u5dee\u5b66\u4e60\u6846\u67b6\uff1a\u9996\u5148\u8bad\u7ec3\u57fa\u4e8e\u5927\u89c4\u6a21\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u7684GMT\u7b56\u7565\u4f5c\u4e3a\u57fa\u7840\uff0c\u7136\u540e\u5b66\u4e60\u9ad8\u6548\u7684\u6b8b\u5dee\u7b56\u7565\u6765\u7cbe\u70bcGMT\u8f93\u51fa\uff0c\u63d0\u5347\u8fd0\u52a8\u80fd\u529b\u5e76\u878d\u5165\u7269\u4f53\u4ea4\u4e92\u3002\u8bbe\u8ba1\u4e86\u70b9\u4e91\u7269\u4f53\u8ddf\u8e2a\u5956\u52b1\u3001\u63a5\u89e6\u5956\u52b1\u548c\u57fa\u4e8e\u8bfe\u7a0b\u7684\u865a\u62df\u7269\u4f53\u63a7\u5236\u5668\u6765\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9eUnitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0cResMimic\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u8bad\u7ec3\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "ResMimic\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u7cbe\u786e\u63a7\u5236\u548c\u7269\u4f53\u4ea4\u4e92\u95ee\u9898\uff0c\u4e3a\u65e5\u5e38\u670d\u52a1\u548c\u4ed3\u50a8\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
