<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 54]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [HUNT: High-Speed UAV Navigation and Tracking in Unstructured Environments via Instantaneous Relative Frames](https://arxiv.org/abs/2509.19452)
*Alessandro Saviolo,Jeffrey Mao,Giuseppe Loianno*

Main category: cs.RO

TL;DR: HUNT是一个实时框架，将无人机的高速穿越、目标获取和跟踪统一在相对导航框架中，无需全局定位即可在未知环境中运行


<details>
  <summary>Details</summary>
Motivation: 解决无人机在搜索救援任务中同时需要高速穿越未知环境和跟踪目标的双重挑战，特别是在传感器退化和无全局定位的情况下

Method: 基于机载瞬时观测数据（姿态、高度、速度）定义导航目标，使用统一的感知控制管道，在搜索时实现反应式高速飞行，检测到目标后无缝切换到跟踪模式

Result: 在茂密森林、集装箱场地和搜救场景中的户外实验证明，该方法在全局方法失效的情况下仍能实现稳健的自主飞行

Conclusion: HUNT框架成功实现了无人机在未知非结构化环境中的高速导航和目标跟踪的统一解决方案，为搜索救援任务提供了有效的自主能力

Abstract: Search and rescue operations require unmanned aerial vehicles to both
traverse unknown unstructured environments at high speed and track targets once
detected. Achieving both capabilities under degraded sensing and without global
localization remains an open challenge. Recent works on relative navigation
have shown robust tracking by anchoring planning and control to a visible
detected object, but cannot address navigation when no target is in the field
of view. We present HUNT (High-speed UAV Navigation and Tracking), a real-time
framework that unifies traversal, acquisition, and tracking within a single
relative formulation. HUNT defines navigation objectives directly from onboard
instantaneous observables such as attitude, altitude, and velocity, enabling
reactive high-speed flight during search. Once a target is detected, the same
perception-control pipeline transitions seamlessly to tracking. Outdoor
experiments in dense forests, container compounds, and search-and-rescue
operations with vehicles and mannequins demonstrate robust autonomy where
global methods fail.

</details>


### [2] [ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation](https://arxiv.org/abs/2509.19454)
*Jason Chen,I-Chun Arthur Liu,Gaurav Sukhatme,Daniel Seita*

Main category: cs.RO

TL;DR: ROPA是一种离线模仿学习数据增强方法，通过微调Stable Diffusion合成第三人称RGB和RGB-D观察数据，同时生成对应的关节空间动作标签，用于双手机器人操作任务的数据增强。


<details>
  <summary>Details</summary>
Motivation: 收集真实世界多样且精确的双手机器人操作演示数据成本高昂且耗时，限制了可扩展性。现有数据增强方法主要针对手腕摄像头设置或仅生成新图像而无配对动作，对第三人称RGB-D训练的新动作标签增强研究较少。

Method: 提出ROPA方法，通过约束优化强制执行物理一致性，在双手机器人场景中施加适当的夹爪-物体接触约束，同时生成RGB/RGB-D观察数据和对应的关节空间动作。

Result: 在5个模拟任务和3个真实世界任务上进行评估，2625次模拟试验和300次真实世界试验结果表明，ROPA优于基线方法和消融实验。

Conclusion: ROPA展示了在第三人称双手机器人操作中进行RGB和RGB-D数据增强的可扩展潜力。

Abstract: Training robust bimanual manipulation policies via imitation learning
requires demonstration data with broad coverage over robot poses, contacts, and
scene contexts. However, collecting diverse and precise real-world
demonstrations is costly and time-consuming, which hinders scalability. Prior
works have addressed this with data augmentation, typically for either
eye-in-hand (wrist camera) setups with RGB inputs or for generating novel
images without paired actions, leaving augmentation for eye-to-hand
(third-person) RGB-D training with new action labels less explored. In this
paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data
Augmentation (ROPA), an offline imitation learning data augmentation method
that fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D
observations of novel robot poses. Our approach simultaneously generates
corresponding joint-space action labels while employing constrained
optimization to enforce physical consistency through appropriate
gripper-to-object contact constraints in bimanual scenarios. We evaluate our
method on 5 simulated and 3 real-world tasks. Our results across 2625
simulation trials and 300 real-world trials demonstrate that ROPA outperforms
baselines and ablations, showing its potential for scalable RGB and RGB-D data
augmentation in eye-to-hand bimanual manipulation. Our project website is
available at: https://ropaaug.github.io/.

</details>


### [3] [Self-evolved Imitation Learning in Simulated World](https://arxiv.org/abs/2509.19460)
*Yifan Ye,Jun Cen,Jing Chen,Zhihe Lu*

Main category: cs.RO

TL;DR: SEIL是一个自演化的模仿学习框架，通过模拟器交互逐步改进少样本模型，利用双级增强和轻量级选择器来减少对大规模专家演示的依赖。


<details>
  <summary>Details</summary>
Motivation: 模仿学习需要大量专家演示，但收集成本高昂。为了解决监督数据有限的问题，需要开发能够在少量演示下有效学习的框架。

Method: SEIL框架通过模拟器交互收集成功轨迹作为新演示进行迭代优化，采用模型级（EMA模型协作）和环境级（初始位置变化）双级增强，并使用轻量级选择器筛选高质量轨迹。

Result: 在LIBERO基准测试中，SEIL在少样本模仿学习场景下达到了新的最先进性能，用更少的训练样本实现了有竞争力的表现。

Conclusion: SEIL通过自演化机制有效解决了模仿学习中监督数据稀缺的问题，为少样本学习提供了有效的解决方案。

Abstract: Imitation learning has been a trend recently, yet training a generalist agent
across multiple tasks still requires large-scale expert demonstrations, which
are costly and labor-intensive to collect. To address the challenge of limited
supervision, we propose Self-Evolved Imitation Learning (SEIL), a framework
that progressively improves a few-shot model through simulator interactions.
The model first attempts tasksin the simulator, from which successful
trajectories are collected as new demonstrations for iterative refinement. To
enhance the diversity of these demonstrations, SEIL employs dual-level
augmentation: (i) Model-level, using an Exponential Moving Average (EMA) model
to collaborate with the primary model, and (ii) Environment-level, introducing
slight variations in initial object positions. We further introduce a
lightweight selector that filters complementary and informative trajectories
from the generated pool to ensure demonstration quality. These curated samples
enable the model to achieve competitive performance with far fewer training
examples. Extensive experiments on the LIBERO benchmark show that SEIL achieves
a new state-of-the-art performance in few-shot imitation learning scenarios.
Code is available at https://github.com/Jasper-aaa/SEIL.git.

</details>


### [4] [CU-Multi: A Dataset for Multi-Robot Collaborative Perception](https://arxiv.org/abs/2509.19463)
*Doncey Albin,Daniel McGann,Miles Mena,Annika Thomas,Harel Biggie,Xuefei Sun,Steve McGuire,Jonathan P. How,Christoffer Heckman*

Main category: cs.RO

TL;DR: CU-Multi是一个新的多机器人数据集，旨在解决现有数据集在轨迹重叠和闭环检测方面的不足，为多机器人协同感知任务提供标准化基准。


<details>
  <summary>Details</summary>
Motivation: 当前多机器人系统的研究缺乏专门的数据集，现有数据集通常轨迹短、机器人间重叠少、闭环稀疏，且评估方法缺乏标准化，导致结果难以比较。

Method: 在科罗拉多大学博尔德分校的两个大型户外场地收集数据，包含四个同步运行的机器人轨迹，具有对齐的起始时间和可控的轨迹重叠，提供RGB-D感知、RTK GPS、语义LiDAR和精炼的里程计真值。

Result: 构建了CU-Multi数据集，通过结合重叠变化和密集语义标注，为多机器人协同感知任务的可重复评估提供了坚实基础。

Conclusion: CU-Multi数据集克服了现有数据集的局限性，能够更好地反映真实多机器人操作场景，促进多机器人系统研究的标准化和可比性。

Abstract: A central challenge for multi-robot systems is fusing independently gathered
perception data into a unified representation. Despite progress in
Collaborative SLAM (C-SLAM), benchmarking remains hindered by the scarcity of
dedicated multi-robot datasets. Many evaluations instead partition single-robot
trajectories, a practice that may only partially reflect true multi-robot
operations and, more critically, lacks standardization, leading to results that
are difficult to interpret or compare across studies. While several multi-robot
datasets have recently been introduced, they mostly contain short trajectories
with limited inter-robot overlap and sparse intra-robot loop closures. To
overcome these limitations, we introduce CU-Multi, a dataset collected over
multiple days at two large outdoor sites on the University of Colorado Boulder
campus. CU-Multi comprises four synchronized runs with aligned start times and
controlled trajectory overlap, replicating the distinct perspectives of a robot
team. It includes RGB-D sensing, RTK GPS, semantic LiDAR, and refined
ground-truth odometry. By combining overlap variation with dense semantic
annotations, CU-Multi provides a strong foundation for reproducible evaluation
in multi-robot collaborative perception tasks.

</details>


### [5] [Crater Observing Bio-inspired Rolling Articulator (COBRA)](https://arxiv.org/abs/2509.19473)
*Adarsh Salagame,Henry Noyes,Alireza Ramezani,Eric Sihite,Arash Kalantari*

Main category: cs.RO

TL;DR: COBRA是一种多模态蛇形机器人，专为在月球永久阴影区陨石坑的恶劣地形中导航而设计，结合滑行和翻滚两种运动模式来适应不同地形条件。


<details>
  <summary>Details</summary>
Motivation: NASA计划在月球建立可持续的人类基地，但现有探测车难以在永久阴影区陨石坑等极端地形中安全移动，限制了月球水冰资源的开发利用。

Method: COBRA采用蛇形机器人设计，具有两种运动模式：蛇模式使用侧向滑行在平坦或缓坡地形移动；翻滚模式通过连接头尾形成圆筒状结构，在陡坡上实现高效滚动。机器人配备机载计算机、立体相机、惯性测量单元和关节编码器。

Result: 通过仿真和实验验证，COBRA在沙克尔顿陨石坑的崎岖环境中表现出良好的鲁棒性和移动效率。

Conclusion: COBRA的多模态移动策略为解决月球极端地形探测提供了可行的技术方案，有望支持未来月球基地建设和资源勘探任务。

Abstract: NASA aims to establish a sustainable human basecamp on the Moon as a stepping
stone for future missions to Mars and beyond. The discovery of water ice on the
Moon's craters located in permanently shadowed regions, which can provide
drinking water, oxygen, and rocket fuel, is therefore of critical importance.
However, current methods to access lunar ice deposits are limited. While rovers
have been used to explore the lunar surface for decades, they face significant
challenges in navigating harsh terrains, such as permanently shadowed craters,
due to the high risk of immobilization. This report introduces COBRA (Crater
Observing Bio-inspired Rolling Articulator), a multi-modal snake-style robot
designed to overcome mobility challenges in Shackleton Crater's rugged
environment. COBRA combines slithering and tumbling locomotion to adapt to
various crater terrains. In snake mode, it uses sidewinding to traverse flat or
low inclined surfaces, while in tumbling mode, it forms a circular barrel by
linking its head and tail, enabling rapid movement with minimal energy on steep
slopes. Equipped with an onboard computer, stereo camera, inertial measurement
unit, and joint encoders, COBRA facilitates real-time data collection and
autonomous operation. This paper highlights COBRAs robustness and efficiency in
navigating extreme terrains through both simulations and experimental
validation.

</details>


### [6] [OmniVLA: An Omni-Modal Vision-Language-Action Model for Robot Navigation](https://arxiv.org/abs/2509.19480)
*Noriaki Hirose,Catherine Glossop,Dhruv Shah,Sergey Levine*

Main category: cs.RO

TL;DR: 提出了OmniVLA训练框架，实现机器人导航的全模态目标条件化，支持语言指令、空间坐标和视觉参考等多种目标指定方式


<details>
  <summary>Details</summary>
Motivation: 现有机器人导航策略通常只针对单一模态训练，限制了在现实场景中的适应性，而人类能够灵活理解多种目标指定方式

Method: 使用高容量视觉-语言-动作(VLA)骨干网络，通过随机模态融合策略训练2D位姿、第一人称图像和自然语言三种主要目标模态及其组合

Result: OmniVLA模型在未见环境中表现出强泛化能力，对稀缺模态具有鲁棒性，并能遵循新的自然语言指令，优于各模态的专用基线

Conclusion: OmniVLA为构建广泛泛化和灵活的导航策略迈出了重要一步，为构建全模态机器人基础模型提供了可扩展路径

Abstract: Humans can flexibly interpret and compose different goal specifications, such
as language instructions, spatial coordinates, or visual references, when
navigating to a destination. In contrast, most existing robotic navigation
policies are trained on a single modality, limiting their adaptability to
real-world scenarios where different forms of goal specification are natural
and complementary. In this work, we present a training framework for robotic
foundation models that enables omni-modal goal conditioning for vision-based
navigation. Our approach leverages a high-capacity vision-language-action (VLA)
backbone and trains with three primary goal modalities: 2D poses, egocentric
images, and natural language, as well as their combinations, through a
randomized modality fusion strategy. This design not only expands the pool of
usable datasets but also encourages the policy to develop richer geometric,
semantic, and visual representations. The resulting model, OmniVLA, achieves
strong generalization to unseen environments, robustness to scarce modalities,
and the ability to follow novel natural language instructions. We demonstrate
that OmniVLA outperforms specialist baselines across modalities and offers a
flexible foundation for fine-tuning to new modalities and tasks. We believe
OmniVLA provides a step toward broadly generalizable and flexible navigation
policies, and a scalable path for building omni-modal robotic foundation
models. We present videos showcasing OmniVLA performance and will release its
checkpoints and training code on our project page.

</details>


### [7] [Supercomputing for High-speed Avoidance and Reactive Planning in Robots](https://arxiv.org/abs/2509.19486)
*Kieran S. Lachmansingh,José R. González-Estrada,Ryan E. Grant,Matthew K. X. J. Pan*

Main category: cs.RO

TL;DR: SHARP是一个概念验证研究，展示了高性能计算如何实现机器人控制的毫秒级响应。通过将轨迹规划任务卸载到HPC集群，在7自由度机械臂躲避高速泡沫弹的测试中，实现了22.9ms（本地）和30.0ms（远程）的平均规划延迟，成功率分别达到84%和88%。


<details>
  <summary>Details</summary>
Motivation: 现代机器人在人机共享工作空间中对反应性需求日益增加，但机载处理器受限于尺寸、功耗和成本。HPC卸载提供了大规模并行计算能力，但其在实时机器人应用中的可行性因网络延迟和抖动而不确定。

Method: 使用MPI在本地和远程HPC集群上实现并行化多目标A*搜索算法，在7自由度机械臂躲避高速泡沫弹的应力测试场景中进行评估。

Result: 系统实现了平均规划延迟22.9ms（本地）和30.0ms（远程，约300公里外），回避成功率分别为84%和88%。当往返延迟保持在数十毫秒范围内时，HPC侧计算不再是瓶颈。

Conclusion: SHARP研究证明HPC卸载是实现在动态环境中可靠、反应灵敏机器人的可行途径，支持混合控制架构：低级反射保持机载以确保安全，而突发性、高吞吐量的规划任务卸载到HPC以实现可扩展性。

Abstract: This paper presents SHARP (Supercomputing for High-speed Avoidance and
Reactive Planning), a proof-of-concept study demonstrating how high-performance
computing (HPC) can enable millisecond-scale responsiveness in robotic control.
While modern robots face increasing demands for reactivity in human--robot
shared workspaces, onboard processors are constrained by size, power, and cost.
Offloading to HPC offers massive parallelism for trajectory planning, but its
feasibility for real-time robotics remains uncertain due to network latency and
jitter. We evaluate SHARP in a stress-test scenario where a 7-DOF manipulator
must dodge high-speed foam projectiles. Using a parallelized multi-goal A*
search implemented with MPI on both local and remote HPC clusters, the system
achieves mean planning latencies of 22.9 ms (local) and 30.0 ms (remote, ~300
km away), with avoidance success rates of 84% and 88%, respectively. These
results show that when round-trip latency remains within the
tens-of-milliseconds regime, HPC-side computation is no longer the bottleneck,
enabling avoidance well below human reaction times. The SHARP results motivate
hybrid control architectures: low-level reflexes remain onboard for safety,
while bursty, high-throughput planning tasks are offloaded to HPC for
scalability. By reporting per-stage timing and success rates, this study
provides a reproducible template for assessing real-time feasibility of
HPC-driven robotics. Collectively, SHARP reframes HPC offloading as a viable
pathway toward dependable, reactive robots in dynamic environments.

</details>


### [8] [A Bimanual Gesture Interface for ROS-Based Mobile Manipulators Using TinyML and Sensor Fusion](https://arxiv.org/abs/2509.19521)
*Najeeb Ahmed Bhuiyan,M. Nasimul Huq,Sakib H. Chowdhury,Rahul Mangharam*

Main category: cs.RO

TL;DR: 本文提出了一种基于双手机势控制的移动机械臂系统，通过TinyML、频谱分析和传感器融合技术，实现了同时导航和操作的可靠、高效、直观的人机交互界面。


<details>
  <summary>Details</summary>
Motivation: 解决移动机械臂手势控制在可靠性、效率和直观性方面的持续挑战，为工业自动化、辅助机器人和危险环境提供更有效的人机交互方案。

Method: 采用双手机势界面，左手倾斜和手指弯曲控制移动底座导航，右手IMU信号通过频谱分析和轻量神经网络分类控制7自由度机械臂，在ROS框架下实现传感器融合和实时低功耗手势识别。

Result: 开发了一个支持同时导航和操作的双手机势控制系统，相比顺序方法提高了效率和协调性，具有实时性、低功耗和鲁棒性。

Conclusion: 该方法为人机交互在工业自动化等领域的应用提供了经济高效的开源解决方案，具有实际部署和进一步优化的强大潜力。

Abstract: Gesture-based control for mobile manipulators faces persistent challenges in
reliability, efficiency, and intuitiveness. This paper presents a dual-hand
gesture interface that integrates TinyML, spectral analysis, and sensor fusion
within a ROS framework to address these limitations. The system uses left-hand
tilt and finger flexion, captured using accelerometer and flex sensors, for
mobile base navigation, while right-hand IMU signals are processed through
spectral analysis and classified by a lightweight neural network. This pipeline
enables TinyML-based gesture recognition to control a 7-DOF Kinova Gen3
manipulator. By supporting simultaneous navigation and manipulation, the
framework improves efficiency and coordination compared to sequential methods.
Key contributions include a bimanual control architecture, real-time low-power
gesture recognition, robust multimodal sensor fusion, and a scalable ROS-based
implementation. The proposed approach advances Human-Robot Interaction (HRI)
for industrial automation, assistive robotics, and hazardous environments,
offering a cost-effective, open-source solution with strong potential for
real-world deployment and further optimization.

</details>


### [9] [Bioinspired SLAM Approach for Unmanned Surface Vehicle](https://arxiv.org/abs/2509.19522)
*Fabio Coelho,Joao Victor T. Borges,Paulo Padrao,Jose Fuentes,Ramon R. Costa,Liu Hsu,Leonardo Bobadilla*

Main category: cs.RO

TL;DR: OpenRatSLAM2是一个基于啮齿动物海马体计算模型的生物启发式SLAM框架新版本，提供低计算成本的视觉-惯性SLAM，适用于GPS拒止环境。


<details>
  <summary>Details</summary>
Motivation: 开发适用于GPS拒止环境的低成本SLAM系统，特别是在无人水面艇(USV)上的首次应用。

Method: 采用ROS2架构，结合视觉和惯性数据，基于生物启发式算法进行SLAM。

Result: 在水道数据集上的实验表明，算法能够生成半度量地图，误差范围在大多数机器人应用可接受范围内。

Conclusion: OpenRatSLAM2是第一个在USV上应用的RatSLAM系统，展示了在GPS拒止环境中的可行性和实用性。

Abstract: This paper presents OpenRatSLAM2, a new version of OpenRatSLAM - a
bioinspired SLAM framework based on computational models of the rodent
hippocampus. OpenRatSLAM2 delivers low-computation-cost visual-inertial based
SLAM, suitable for GPS-denied environments. Our contributions include a
ROS2-based architecture, experimental results on new waterway datasets, and
insights into system parameter tuning. This work represents the first known
application of RatSLAM on USVs. The estimated trajectory was compared with
ground truth data using the Hausdorff distance. The results show that the
algorithm can generate a semimetric map with an error margin acceptable for
most robotic applications.

</details>


### [10] [Real-Time Reinforcement Learning for Dynamic Tasks with a Parallel Soft Robot](https://arxiv.org/abs/2509.19525)
*James Avtges,Jake Ketchum,Millicent Schlafly,Helena Young,Taekyoung Kim,Allison Pinosky,Ryan L. Truby,Todd D. Murphey*

Main category: cs.RO

TL;DR: 本文提出了一种基于强化学习的单次部署学习方法，用于软机器人实时动态平衡控制，能够在硬件上快速学习控制策略，甚至在部分执行器损坏的情况下仍能保持性能。


<details>
  <summary>Details</summary>
Motivation: 软机器人的闭环控制面临挑战，传统方法难以处理非线性响应、滞后效应和大变形等问题，而传统的强化学习方法存在样本效率低和初始化不一致的问题。

Method: 采用基于课程学习的强化学习方法，通过扩展已知平衡点的邻域来训练控制策略，使用基于电机驱动手性剪切拉胀结构的3D打印软执行器构建变形Stewart平台。

Result: 在单次部署中，最大扩散强化学习能够在15分钟内学习动态平衡，即使一半执行器被破坏后仍能保持与完整平台几乎相同的性能。

Conclusion: 单次硬件学习使软机器人系统能够在现实世界中可靠学习，将为开发更多样化和能力更强的软机器人奠定基础。

Abstract: Closed-loop control remains an open challenge in soft robotics. The nonlinear
responses of soft actuators under dynamic loading conditions limit the use of
analytic models for soft robot control. Traditional methods of controlling soft
robots underutilize their configuration spaces to avoid nonlinearity,
hysteresis, large deformations, and the risk of actuator damage. Furthermore,
episodic data-driven control approaches such as reinforcement learning (RL) are
traditionally limited by sample efficiency and inconsistency across
initializations. In this work, we demonstrate RL for reliably learning control
policies for dynamic balancing tasks in real-time single-shot hardware
deployments. We use a deformable Stewart platform constructed using parallel,
3D-printed soft actuators based on motorized handed shearing auxetic (HSA)
structures. By introducing a curriculum learning approach based on expanding
neighborhoods of a known equilibrium, we achieve reliable single-deployment
balancing at arbitrary coordinates. In addition to benchmarking the performance
of model-based and model-free methods, we demonstrate that in a single
deployment, Maximum Diffusion RL is capable of learning dynamic balancing after
half of the actuators are effectively disabled, by inducing buckling and by
breaking actuators with bolt cutters. Training occurs with no prior data, in as
fast as 15 minutes, with performance nearly identical to the fully-intact
platform. Single-shot learning on hardware facilitates soft robotic systems
reliably learning in the real world and will enable more diverse and capable
soft robots.

</details>


### [11] [Autonomous Elemental Characterization Enabled by a Low Cost Robotic Platform Built Upon a Generalized Software Architecture](https://arxiv.org/abs/2509.19541)
*Xuan Cao,Yuxin Wu,Michael L. Whittaker*

Main category: cs.RO

TL;DR: 本文提出了一种用于科学实验室机器人系统的软件架构，通过双层的动作服务器设计（Socket.IO和ROS），实现了基于web的用户友好操作界面和ROS行为树的任务规划与执行。该架构应用于矿物和材料样品表征的自动化平台，使用低成本的三轴数控龙门系统作为主要机器人，并集成了手持LIBS分析仪进行自动化2D化学映射。


<details>
  <summary>Details</summary>
Motivation: 尽管机器人在工业中应用迅速增长，但在科学实验室中由于缺乏通用方法和硬件成本高，机器人自动化任务的应用较少。本文旨在通过自动化表征任务来降低成本并保持通用性。

Method: 采用双层的动作服务器设计（Socket.IO和ROS），构建基于web的前端界面和ROS行为树的任务规划系统。使用开源低成本的三轴数控龙门系统作为主要机器人，通过3D打印适配器集成手持LIBS分析仪，实现自动化2D化学映射。

Result: 成功构建了自动化化学映射系统，在锂辉石伟晶岩岩心样品表面进行了1071个点的密集高光谱映射，采集速率为1520比特/秒。自动化LIBS扫描实现了实验室中受控的化学定量分析。

Conclusion: 该自动化系统能够补充基于现场的手持设备测量，将锂基电池材料供应链中的资源勘探和加工步骤联系起来，展示了自动化表征在科学实验室中的实用价值。

Abstract: Despite the rapidly growing applications of robots in industry, the use of
robots to automate tasks in scientific laboratories is less prolific due to
lack of generalized methodologies and high cost of hardware. This paper focuses
on the automation of characterization tasks necessary for reducing cost while
maintaining generalization, and proposes a software architecture for building
robotic systems in scientific laboratory environment. A dual-layer (Socket.IO
and ROS) action server design is the basic building block, which facilitates
the implementation of a web-based front end for user-friendly operations and
the use of ROS Behavior Tree for convenient task planning and execution. A
robotic platform for automating mineral and material sample characterization is
built upon the architecture, with an open source, low-cost three-axis computer
numerical control gantry system serving as the main robot. A handheld laser
induced breakdown spectroscopy (LIBS) analyzer is integrated with a 3D printed
adapter, enabling automated 2D chemical mapping. We demonstrate the utility of
automated chemical mapping by scanning of the surface of a spodumene-bearing
pegmatite core sample with a 1071-point dense hyperspectral map acquired at a
rate of 1520 bits per second. Automated LIBS scanning enables controlled
chemical quantification in the laboratory that complements field-based
measurements acquired with the same handheld device, linking resource
exploration and processing steps in the supply chain for lithium-based battery
materials.

</details>


### [12] [RoMoCo: Robotic Motion Control Toolbox for Reduced-Order Model-Based Locomotion on Bipedal and Humanoid Robots](https://arxiv.org/abs/2509.19545)
*Min Dai,Aaron D. Ames*

Main category: cs.RO

TL;DR: RoMoCo是一个开源的C++工具箱，用于合成和评估基于降阶模型的足式机器人和人形机器人的规划器及全身控制器。


<details>
  <summary>Details</summary>
Motivation: 为了统一最先进的规划器和全身运动控制器，实现快速原型设计和可重复的基准测试，并支持跨不同机器人的灵活控制器设计。

Method: 采用模块化架构，利用降阶模型进行平台无关的步态生成，提供一致的API接口。

Result: 在Cassie、Unitree H1和G1机器人上进行了广泛的仿真验证，并在Cassie和G1人形机器人上进行了硬件实验验证。

Conclusion: RoMoCo展示了其多功能性和高性能，能够有效支持足式和人形机器人的规划与控制开发。

Abstract: We present RoMoCo, an open-source C++ toolbox for the synthesis and
evaluation of reduced-order model-based planners and whole-body controllers for
bipedal and humanoid robots. RoMoCo's modular architecture unifies
state-of-the-art planners and whole-body locomotion controllers under a
consistent API, enabling rapid prototyping and reproducible benchmarking. By
leveraging reduced-order models for platform-agnostic gait generation, RoMoCo
enables flexible controller design across diverse robots. We demonstrate its
versatility and performance through extensive simulations on the Cassie,
Unitree H1, and G1 robots, and validate its real-world efficacy with hardware
experiments on the Cassie and G1 humanoids.

</details>


### [13] [AnySafe: Adapting Latent Safety Filters at Runtime via Safety Constraint Parameterization in the Latent Space](https://arxiv.org/abs/2509.19555)
*Sankalp Agrawal,Junwon Seo,Kensuke Nakamura,Ran Tian,Andrea Bajcsy*

Main category: cs.RO

TL;DR: 本文提出了一种约束参数化的潜在安全过滤器，能够在运行时根据用户指定的安全约束进行自适应调整，解决了传统安全过滤器在部署过程中约束固定不变的限制。


<details>
  <summary>Details</summary>
Motivation: 现有的潜在安全过滤器方法假设安全约束是预先已知且在部署过程中保持固定的，这限制了安全过滤器在不同场景下的适应性。

Method: 通过定义基于约束图像编码的安全约束，使用潜在空间相似性度量，并通过保形校准来对齐故障相似性概念。安全过滤器完全在世界模型的想象中进行训练，将模型看到的任何图像视为潜在的测试时约束。

Result: 在基于视觉的控制任务仿真和硬件实验中，该方法能够在运行时通过条件化用户指定的约束图像编码来适应，而不会牺牲性能。

Conclusion: 该方法实现了对任意安全约束的运行时自适应，为视觉控制任务提供了更灵活的安全保障。

Abstract: Recent works have shown that foundational safe control methods, such as
Hamilton-Jacobi (HJ) reachability analysis, can be applied in the latent space
of world models. While this enables the synthesis of latent safety filters for
hard-to-model vision-based tasks, they assume that the safety constraint is
known a priori and remains fixed during deployment, limiting the safety
filter's adaptability across scenarios. To address this, we propose
constraint-parameterized latent safety filters that can adapt to user-specified
safety constraints at runtime. Our key idea is to define safety constraints by
conditioning on an encoding of an image that represents a constraint, using a
latent-space similarity measure. The notion of similarity to failure is aligned
in a principled way through conformal calibration, which controls how closely
the system may approach the constraint representation. The parameterized safety
filter is trained entirely within the world model's imagination, treating any
image seen by the model as a potential test-time constraint, thereby enabling
runtime adaptation to arbitrary safety constraints. In simulation and hardware
experiments on vision-based control tasks with a Franka manipulator, we show
that our method adapts at runtime by conditioning on the encoding of
user-specified constraint images, without sacrificing performance. Video
results can be found on https://any-safe.github.io

</details>


### [14] [Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action](https://arxiv.org/abs/2509.19571)
*Sacha Morin,Kumaraditya Gupta,Mahtab Sandhu,Charlie Gauthier,Francesco Argenziano,Kirsty Ellis,Liam Paull*

Main category: cs.RO

TL;DR: ASP是一种基于场景表示的智能框架，利用现代场景表示的语义、空间和功能查询能力来执行语言条件化的机器人策略，能够零样本处理开放词汇查询，并通过功能推理处理复杂技能。


<details>
  <summary>Details</summary>
Motivation: 解决机器人执行开放自然语言查询的问题，克服现有模仿学习和视觉-语言-动作模型在处理复杂指令和新场景时的局限性。

Method: 设计显式场景表示作为机器人与世界之间的可查询接口，利用查询结果指导下游运动规划，通过功能推理处理复杂技能。

Result: 在桌面操作问题上与VLAs进行广泛比较，展示了ASP能够通过功能引导导航处理房间级查询，并扩展场景表示。

Conclusion: ASP框架通过利用现代场景表示的高级查询能力，实现了强大的语言条件化机器人策略，能够有效处理开放词汇查询和复杂技能。

Abstract: Executing open-ended natural language queries is a core problem in robotics.
While recent advances in imitation learning and vision-language-actions models
(VLAs) have enabled promising end-to-end policies, these models struggle when
faced with complex instructions and new scenes. An alternative is to design an
explicit scene representation as a queryable interface between the robot and
the world, using query results to guide downstream motion planning. In this
work, we present Agentic Scene Policies (ASP), an agentic framework that
leverages the advanced semantic, spatial, and affordance-based querying
capabilities of modern scene representations to implement a capable
language-conditioned robot policy. ASP can execute open-vocabulary queries in a
zero-shot manner by explicitly reasoning about object affordances in the case
of more complex skills. Through extensive experiments, we compare ASP with VLAs
on tabletop manipulation problems and showcase how ASP can tackle room-level
queries through affordance-guided navigation, and a scaled-up scene
representation. (Project page:
https://montrealrobotics.ca/agentic-scene-policies.github.io/)

</details>


### [15] [Chasing Stability: Humanoid Running via Control Lyapunov Function Guided Reinforcement Learning](https://arxiv.org/abs/2509.19573)
*Zachary Olkin,Kejun Li,William D. Compton,Aaron D. Ames*

Main category: cs.RO

TL;DR: 本文提出了一种结合控制Lyapunov函数（CLFs）和强化学习（CLF-RL）的方法，用于人形机器人的动态运动控制，特别是跑步行为。该方法通过将非线性控制理论嵌入RL训练过程，消除了手动设计启发式奖励项的需求，同时保证了可证明的稳定性。


<details>
  <summary>Details</summary>
Motivation: 人形机器人实现高度动态行为（如跑步）需要既鲁棒又精确的控制器，但传统控制方法在处理非线性和混合动力学系统时面临挑战。虽然强化学习在处理复杂动力学方面表现出色，但需要精心设计的奖励函数。

Method: CLF-RL方法将控制Lyapunov函数和优化的动态参考轨迹嵌入强化学习训练过程中，用于塑造奖励函数。该方法基于动态可行的轨迹进行策略学习，扩展了机器人的动态能力。

Result: 生成的策略在跑步机和室外环境中可靠运行，对躯干和脚部施加的干扰表现出鲁棒性。仅使用机载传感器就能实现准确的全局参考跟踪。

Conclusion: 该方法为实现动态运动集成到完整自主堆栈迈出了关键一步，证明了结合控制理论和强化学习的有效性。

Abstract: Achieving highly dynamic behaviors on humanoid robots, such as running,
requires controllers that are both robust and precise, and hence difficult to
design. Classical control methods offer valuable insight into how such systems
can stabilize themselves, but synthesizing real-time controllers for nonlinear
and hybrid dynamics remains challenging. Recently, reinforcement learning (RL)
has gained popularity for locomotion control due to its ability to handle these
complex dynamics. In this work, we embed ideas from nonlinear control theory,
specifically control Lyapunov functions (CLFs), along with optimized dynamic
reference trajectories into the reinforcement learning training process to
shape the reward. This approach, CLF-RL, eliminates the need to handcraft and
tune heuristic reward terms, while simultaneously encouraging certifiable
stability and providing meaningful intermediate rewards to guide learning. By
grounding policy learning in dynamically feasible trajectories, we expand the
robot's dynamic capabilities and enable running that includes both flight and
single support phases. The resulting policy operates reliably on a treadmill
and in outdoor environments, demonstrating robustness to disturbances applied
to the torso and feet. Moreover, it achieves accurate global reference tracking
utilizing only on-board sensors, making a critical step toward integrating
these dynamic motions into a full autonomy stack.

</details>


### [16] [Terra: Hierarchical Terrain-Aware 3D Scene Graph for Task-Agnostic Outdoor Mapping](https://arxiv.org/abs/2509.19579)
*Chad R. Samuelson,Abigail Austin,Seth Knoop,Blake Romrell,Gabriel R. Slade,Timothy W. McLain,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: 提出了一种结合室内3D场景图技术与标准室外几何映射和地形感知推理的新方法，为户外环境生成地形感知的地点节点和分层组织区域，构建轻量级的任务无关度量-语义稀疏地图和3DSG。


<details>
  <summary>Details</summary>
Motivation: 户外智能自主机器人操作需要足够表达的环境地图。传统几何地图方法缺乏语义理解和组织，无法支持高级机器人推理。3D场景图通过整合几何、拓扑和语义关系来解决这一限制，但需要适应户外环境和地形信息需求。

Method: 将室内3DSG技术与标准室外几何映射和地形感知推理相结合，生成地形感知的地点节点和分层组织区域，构建任务无关的度量-语义稀疏地图，并从中构建3DSG用于下游规划任务。

Result: 评估表明该方法在物体检索方面与最先进的基于相机的3DSG方法相当，在区域分类方面超过它们，同时保持内存高效。在仿真和真实环境中验证了在物体检索和区域监控等机器人任务中的有效性。

Conclusion: 该方法成功地将3DSG技术扩展到户外环境，结合地形感知推理，为户外自主机器人操作提供了轻量级且高效的语义地图解决方案。

Abstract: Outdoor intelligent autonomous robotic operation relies on a sufficiently
expressive map of the environment. Classical geometric mapping methods retain
essential structural environment information, but lack a semantic understanding
and organization to allow high-level robotic reasoning. 3D scene graphs (3DSGs)
address this limitation by integrating geometric, topological, and semantic
relationships into a multi-level graph-based map. Outdoor autonomous operations
commonly rely on terrain information either due to task-dependence or the
traversability of the robotic platform. We propose a novel approach that
combines indoor 3DSG techniques with standard outdoor geometric mapping and
terrain-aware reasoning, producing terrain-aware place nodes and hierarchically
organized regions for outdoor environments. Our method generates a
task-agnostic metric-semantic sparse map and constructs a 3DSG from this map
for downstream planning tasks, all while remaining lightweight for autonomous
robotic operation. Our thorough evaluation demonstrates our 3DSG method
performs on par with state-of-the-art camera-based 3DSG methods in object
retrieval and surpasses them in region classification while remaining memory
efficient. We demonstrate its effectiveness in diverse robotic tasks of object
retrieval and region monitoring in both simulation and real-world environments.

</details>


### [17] [From Space to Time: Enabling Adaptive Safety with Learned Value Functions via Disturbance Recasting](https://arxiv.org/abs/2509.19597)
*Sander Tonkens,Nikhil Uday Shinde,Azra Begzadić,Michael C. Yip,Jorge Cortés,Sylvia L. Herbert*

Main category: cs.RO

TL;DR: SPACE2TIME是一种安全滤波器方法，能够在使用离线学习值函数的安全系统中处理未知的空间变化扰动，通过将空间扰动重新参数化为时间扰动来实现安全自适应部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于值函数的安全滤波器方法假设对模型失配有详细先验知识，但现实中无人机等自主系统在复杂环境中会遇到难以预知的空间变化扰动，如湍流、载荷交互等，这些信息在实际部署中通常不可得。

Method: SPACE2TIME的核心思想是将空间扰动重新参数化为时间扰动，使得在在线操作时能够利用预计算的值函数，从而实现对未知空间变化扰动的自适应处理。

Result: 在四旋翼无人机上的大量仿真和硬件实验验证表明，SPACE2TIME相比基线方法有显著改进。

Conclusion: 该方法为在未知空间变化扰动环境下安全部署离线学习的安全滤波器提供了有效解决方案，对城市空中交通等安全关键应用具有重要意义。

Abstract: The widespread deployment of autonomous systems in safety-critical
environments such as urban air mobility hinges on ensuring reliable,
performant, and safe operation under varying environmental conditions. One such
approach, value function-based safety filters, minimally modifies a nominal
controller to ensure safety. Recent advances leverage offline learned value
functions to scale these safety filters to high-dimensional systems. However,
these methods assume detailed priors on all possible sources of model mismatch,
in the form of disturbances in the environment -- information that is rarely
available in real world settings. Even in well-mapped environments like urban
canyons or industrial sites, drones encounter complex, spatially-varying
disturbances arising from payload-drone interaction, turbulent airflow, and
other environmental factors. We introduce SPACE2TIME, which enables safe and
adaptive deployment of offline-learned safety filters under unknown,
spatially-varying disturbances. The key idea is to reparameterize spatial
variations in disturbance as temporal variations, enabling the use of
precomputed value functions during online operation. We validate SPACE2TIME on
a quadcopter through extensive simulations and hardware experiments,
demonstrating significant improvement over baselines.

</details>


### [18] [Look as You Leap: Planning Simultaneous Motion and Perception for High-DOF Robots](https://arxiv.org/abs/2509.19610)
*Qingxi Meng,Emiliano Flores,Carlos Quintero-Peña,Peizhu Qian,Zachary Kingston,Shannan K. Hamlin,Vaibhav Unhelkar,Lydia E. Kavraki*

Main category: cs.RO

TL;DR: 提出了一种用于高自由度机器人在动态环境中同时完成导航和感知任务的运动规划方法PS-PRM，该方法使用GPU并行化的感知评分引导概率路线图规划器，通过神经网络代理模型估计感知质量。


<details>
  <summary>Details</summary>
Motivation: 在动态环境中，机器人需要同时完成导航和感知任务，但这两个目标往往存在冲突。现有方法要么不考虑障碍物，要么只适用于低自由度机器人，或者依赖简化的感知模型。在动态环境中，机器人需要快速重新规划，但直接评估感知质量（如目标检测置信度）在运行时往往代价高昂或不可行。

Method: 提出了PS-PRM方法，使用GPU并行化的感知评分引导概率路线图规划器，结合神经网络代理模型来近似感知评分。该方法将感知任务的质量估计显式地纳入高自由度机器人的运动规划中。

Result: 在仿真和真实机器人实验中，该方法在高自由度机器人上优于基线方法，在静态和动态环境中都表现出色。

Conclusion: PS-PRM方法能够有效解决高自由度机器人在动态环境中同时完成导航和感知任务的运动规划问题，通过GPU并行化和学习模型实现了高效的在线重新规划。

Abstract: In this work, we address the problem of planning robot motions for a
high-degree-of-freedom (DoF) robot that effectively achieves a given perception
task while the robot and the perception target move in a dynamic environment.
Achieving navigation and perception tasks simultaneously is challenging, as
these objectives often impose conflicting requirements. Existing methods that
compute motion under perception constraints fail to account for obstacles, are
designed for low-DoF robots, or rely on simplified models of perception.
Furthermore, in dynamic real-world environments, robots must replan and react
quickly to changes and directly evaluating the quality of perception (e.g.,
object detection confidence) is often expensive or infeasible at runtime. This
problem is especially important in human-centered environments such as homes
and hospitals, where effective perception is essential for safe and reliable
operation. To address these challenges, we propose a GPU-parallelized
perception-score-guided probabilistic roadmap planner with a neural surrogate
model (PS-PRM). The planner explicitly incorporates the estimated quality of a
perception task into motion planning for high-DoF robots. Our method uses a
learned model to approximate perception scores and leverages GPU parallelism to
enable efficient online replanning in dynamic settings. We demonstrate that our
planner, evaluated on high-DoF robots, outperforms baseline methods in both
static and dynamic environments in both simulation and real-robot experiments.

</details>


### [19] [EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data](https://arxiv.org/abs/2509.19626)
*Ryan Punamiya,Dhruv Patel,Patcharapong Aphiwetsa,Pranav Kuppili,Lawrence Y. Zhu,Simar Kareer,Judy Hoffman,Danfei Xu*

Main category: cs.RO

TL;DR: EgoBridge是一个统一的协同训练框架，通过领域适应方法对齐人类和机器人数据的策略潜在空间，显著提升了机器人操作的模仿学习性能。


<details>
  <summary>Details</summary>
Motivation: 利用人类自我中心体验数据扩展机器人端到端模仿学习，但人类和机器人之间存在视觉外观、传感器模态和运动学方面的显著领域差距，阻碍了知识迁移。

Method: 使用最优传输（OT）度量联合策略潜在特征和动作的差异，学习既能在人类和机器人领域间对齐又能保留策略学习关键动作相关信息的观察表示。

Result: 在三个真实世界单臂和双臂操作任务中，EgoBridge相比人类增强的跨具身基线实现了44%的绝对策略成功率提升，并能泛化到仅在人数据中出现的新物体、场景和任务。

Conclusion: EgoBridge框架通过有效的领域对齐方法成功解决了人类-机器人知识迁移的领域差距问题，为利用人类数据扩展机器人学习提供了有效途径。

Abstract: Egocentric human experience data presents a vast resource for scaling up
end-to-end imitation learning for robotic manipulation. However, significant
domain gaps in visual appearance, sensor modalities, and kinematics between
human and robot impede knowledge transfer. This paper presents EgoBridge, a
unified co-training framework that explicitly aligns the policy latent spaces
between human and robot data using domain adaptation. Through a measure of
discrepancy on the joint policy latent features and actions based on Optimal
Transport (OT), we learn observation representations that not only align
between the human and robot domain but also preserve the action-relevant
information critical for policy learning. EgoBridge achieves a significant
absolute policy success rate improvement by 44% over human-augmented
cross-embodiment baselines in three real-world single-arm and bimanual
manipulation tasks. EgoBridge also generalizes to new objects, scenes, and
tasks seen only in human data, where baselines fail entirely. Videos and
additional information can be found at https://ego-bridge.github.io

</details>


### [20] [Minimalistic Autonomous Stack for High-Speed Time-Trial Racing](https://arxiv.org/abs/2509.19636)
*Mahmoud Ali,Hassan Jardali,Youwei Yu,Durgakant Pushp,Lantao Liu*

Main category: cs.RO

TL;DR: 本文提出了一种简约的自主赛车系统，旨在高速计时赛中实现快速部署和高效系统集成，仅需极少的赛道测试时间。该系统在真实赛道上验证，在仅11小时的赛道练习中达到最高速度206公里/小时。


<details>
  <summary>Details</summary>
Motivation: 开发全尺寸自主赛车系统通常受限于专用测试赛道的有限访问，限制了实际验证机会。传统方法需要长开发周期和大量赛道时间，而本研究旨在解决这一问题。

Method: 采用简约的自主赛车堆栈设计，强调快速部署和高效系统集成。通过最小化赛道测试需求，在真实赛道上进行验证，包括跟踪精度、车辆动力学和安全考虑的系统性能分析。

Result: 在真实赛道上验证成功，总行驶325公里，在仅11小时赛道练习中达到最高速度206公里/小时。系统性能分析显示良好的跟踪精度和车辆动力学表现。

Conclusion: 该研究为受赛道访问限制的团队提供了快速开发和部署自主赛车系统的可行方案，证明了通过简约设计和高效集成可以在有限测试条件下实现高性能自主赛车。

Abstract: Autonomous racing has seen significant advancements, driven by competitions
such as the Indy Autonomous Challenge (IAC) and the Abu Dhabi Autonomous Racing
League (A2RL). However, developing an autonomous racing stack for a full-scale
car is often constrained by limited access to dedicated test tracks,
restricting opportunities for real-world validation. While previous work
typically requires extended development cycles and significant track time, this
paper introduces a minimalistic autonomous racing stack for high-speed
time-trial racing that emphasizes rapid deployment and efficient system
integration with minimal on-track testing. The proposed stack was validated on
real speedways, achieving a top speed of 206 km/h within just 11 hours'
practice run on the track with 325 km in total. Additionally, we present the
system performance analysis, including tracking accuracy, vehicle dynamics, and
safety considerations, offering insights for teams seeking to rapidly develop
and deploy an autonomous racing stack with limited track access.

</details>


### [21] [RoboSSM: Scalable In-context Imitation Learning via State-Space Models](https://arxiv.org/abs/2509.19658)
*Youngju Yoo,Jiaheng Hu,Yifeng Zhu,Bo Liu,Qiang Liu,Roberto Martín-Martín,Peter Stone*

Main category: cs.RO

TL;DR: RoboSSM提出了一种基于状态空间模型（SSM）的可扩展上下文模仿学习方法，用Longhorn替代Transformer来解决长上下文提示的性能下降问题，在LIBERO基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的上下文模仿学习方法依赖Transformer，但Transformer存在计算限制，在处理比训练时更长的提示时性能下降。需要一种更高效且能处理长上下文的方法。

Method: 用Longhorn（一种先进的SSM）替代Transformer，利用SSM的线性时间推理和强外推能力，使其更适合处理长上下文提示。

Result: 在LIBERO基准测试中，RoboSSM能有效外推到不同数量的上下文演示，在未见任务上表现优异，并在长视野场景中保持鲁棒性。

Conclusion: SSM作为ICIL的高效可扩展骨干具有巨大潜力，RoboSSM为解决Transformer在长上下文处理中的局限性提供了有效方案。

Abstract: In-context imitation learning (ICIL) enables robots to learn tasks from
prompts consisting of just a handful of demonstrations. By eliminating the need
for parameter updates at deployment time, this paradigm supports few-shot
adaptation to novel tasks. However, recent ICIL methods rely on Transformers,
which have computational limitations and tend to underperform when handling
longer prompts than those seen during training. In this work, we introduce
RoboSSM, a scalable recipe for in-context imitation learning based on
state-space models (SSM). Specifically, RoboSSM replaces Transformers with
Longhorn -- a state-of-the-art SSM that provides linear-time inference and
strong extrapolation capabilities, making it well-suited for long-context
prompts. We evaluate our approach on the LIBERO benchmark and compare it
against strong Transformer-based ICIL baselines. Experiments show that RoboSSM
extrapolates effectively to varying numbers of in-context demonstrations,
yields high performance on unseen tasks, and remains robust in long-horizon
scenarios. These results highlight the potential of SSMs as an efficient and
scalable backbone for ICIL. Our code is available at
https://github.com/youngjuY/RoboSSM.

</details>


### [22] [Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains](https://arxiv.org/abs/2509.19672)
*Dongzhe Zheng,Wenjie Mei*

Main category: cs.RO

TL;DR: 本文提出了记忆增强势场理论，通过整合历史经验来改进随机最优控制方法，解决在复杂非凸环境中容易陷入局部最优的问题。


<details>
  <summary>Details</summary>
Motivation: 传统随机最优控制方法在复杂非凸环境中容易陷入局部最优，因为它们无法从历史轨迹数据中学习经验。

Method: 开发了记忆增强势场理论框架，动态构建基于记忆的势场来识别和编码状态空间的关键拓扑特征，使控制器能够从过去经验中学习并调整优化策略。

Result: 理论分析表明记忆增强势场具有非凸逃逸特性、渐近收敛性和计算效率。在记忆增强模型预测路径积分控制器中的实现显示在挑战性非凸环境中性能显著提升。

Conclusion: 该框架为控制系统（特别是机器人动力学）提供了一种通用的基于经验学习的方法，增强了在复杂状态空间中导航的能力，无需专门领域知识或大量离线训练。

Abstract: Stochastic optimal control methods often struggle in complex non-convex
landscapes, frequently becoming trapped in local optima due to their inability
to learn from historical trajectory data. This paper introduces
Memory-Augmented Potential Field Theory, a unified mathematical framework that
integrates historical experience into stochastic optimal control. Our approach
dynamically constructs memory-based potential fields that identify and encode
key topological features of the state space, enabling controllers to
automatically learn from past experiences and adapt their optimization
strategy. We provide a theoretical analysis showing that memory-augmented
potential fields possess non-convex escape properties, asymptotic convergence
characteristics, and computational efficiency. We implement this theoretical
framework in a Memory-Augmented Model Predictive Path Integral (MPPI)
controller that demonstrates significantly improved performance in challenging
non-convex environments. The framework represents a generalizable approach to
experience-based learning within control systems (especially robotic dynamics),
enhancing their ability to navigate complex state spaces without requiring
specialized domain knowledge or extensive offline training.

</details>


### [23] [Formal Safety Verification and Refinement for Generative Motion Planners via Certified Local Stabilization](https://arxiv.org/abs/2509.19688)
*Devesh Nath,Haoran Yin,Glen Chou*

Main category: cs.RO

TL;DR: 提出一种基于学习生成运动规划器的形式化安全验证方法，通过小型神经跟踪控制器稳定GMP生成的参考轨迹，然后对闭环动力学应用神经网络验证来保证安全性。


<details>
  <summary>Details</summary>
Motivation: 生成运动规划器(GMPs)比传统规划器有优势，但由于神经网络验证工具只能处理几百个神经元，而GMPs通常包含数百万参数，因此验证其输出的安全性和动态可行性很困难。

Method: 关键思路是通过小型神经跟踪控制器模仿GMP并稳定其采样的参考轨迹，然后对闭环动力学应用神经网络验证，获得可验证的安全可达集。构建已验证的GMP参考库，在线部署时尽可能模仿原始GMP分布。

Result: 在多种规划器（扩散模型、流匹配、视觉语言模型）上评估，在仿真（地面机器人和四旋翼）和硬件（差速驱动机器人）上均提高了安全性。

Conclusion: 该方法在保持GMP表达能力的同时实现了可验证的安全性，无需重新训练即可提高安全性，适用于各种类型的生成运动规划器。

Abstract: We present a method for formal safety verification of learning-based
generative motion planners. Generative motion planners (GMPs) offer advantages
over traditional planners, but verifying the safety and dynamic feasibility of
their outputs is difficult since neural network verification (NNV) tools scale
only to a few hundred neurons, while GMPs often contain millions. To preserve
GMP expressiveness while enabling verification, our key insight is to imitate
the GMP by stabilizing references sampled from the GMP with a small neural
tracking controller and then applying NNV to the closed-loop dynamics. This
yields reachable sets that rigorously certify closed-loop safety, while the
controller enforces dynamic feasibility. Building on this, we construct a
library of verified GMP references and deploy them online in a way that
imitates the original GMP distribution whenever it is safe to do so, improving
safety without retraining. We evaluate across diverse planners, including
diffusion, flow matching, and vision-language models, improving safety in
simulation (on ground robots and quadcopters) and on hardware
(differential-drive robot).

</details>


### [24] [Diffusion-Based Impedance Learning for Contact-Rich Manipulation Tasks](https://arxiv.org/abs/2509.19696)
*Noah Geiger,Tamim Asfour,Neville Hogan,Johannes Lachner*

Main category: cs.RO

TL;DR: 提出了Diffusion-Based Impedance Learning框架，结合学习方法和阻抗控制，通过Transformer-based Diffusion Model重建模拟零力轨迹，实现实时扭矩控制和自主刚度适应。


<details>
  <summary>Details</summary>
Motivation: 学习方法擅长运动生成但不适合物理交互，阻抗控制需要任务感知调参。本文旨在结合两个领域的优势。

Method: 使用基于Transformer的扩散模型重建模拟零力轨迹，引入SLERP-based四元数噪声调度器保证几何一致性，通过能量基估计器更新刚度和阻尼参数。

Result: 在KUKA LBR iiwa机器人上实现亚毫米位置精度和亚度旋转精度，成功完成圆柱、方形和星形孔插入任务，30/30成功率。

Conclusion: 这是实现物理AI的重要一步，融合了基于模型的物理交互控制和基于学习的轨迹生成方法。

Abstract: Learning methods excel at motion generation in the information domain but are
not primarily designed for physical interaction in the energy domain. Impedance
Control shapes physical interaction but requires task-aware tuning by selecting
feasible impedance parameters. We present Diffusion-Based Impedance Learning, a
framework that combines both domains. A Transformer-based Diffusion Model with
cross-attention to external wrenches reconstructs a simulated Zero-Force
Trajectory (sZFT). This captures both translational and rotational task-space
behavior. For rotations, we introduce a novel SLERP-based quaternion noise
scheduler that ensures geometric consistency. The reconstructed sZFT is then
passed to an energy-based estimator that updates stiffness and damping
parameters. A directional rule is applied that reduces impedance along non task
axes while preserving rigidity along task directions. Training data were
collected for a parkour scenario and robotic-assisted therapy tasks using
teleoperation with Apple Vision Pro. With only tens of thousands of samples,
the model achieved sub-millimeter positional accuracy and sub-degree rotational
accuracy. Its compact model size enabled real-time torque control and
autonomous stiffness adaptation on a KUKA LBR iiwa robot. The controller
achieved smooth parkour traversal within force and velocity limits and 30/30
success rates for cylindrical, square, and star peg insertions without any
peg-specific demonstrations in the training data set. All code for the
Transformer-based Diffusion Model, the robot controller, and the Apple Vision
Pro telemanipulation framework is publicly available. These results mark an
important step towards Physical AI, fusing model-based control for physical
interaction with learning-based methods for trajectory generation.

</details>


### [25] [TopoCut: Learning Multi-Step Cutting with Spectral Rewards and Discrete Diffusion Policies](https://arxiv.org/abs/2509.19712)
*Liquan Wang,Jiangjie Bian,Eric Heiden,Animesh Garg*

Main category: cs.RO

TL;DR: TopoCut是一个用于多步骤机器人切割任务的综合基准，通过高保真仿真环境、拓扑感知奖励设计和集成策略学习来解决可变形物体切割的挑战。


<details>
  <summary>Details</summary>
Motivation: 可变形物体切割任务面临复杂拓扑行为、密集状态感知困难和切割结果评估方法缺乏等挑战，需要开发一个全面的基准来支持机器人切割研究。

Method: 基于粒子弹性塑性求解器构建高保真仿真环境，引入损伤驱动的拓扑发现机制；设计集成了拓扑发现和拉普拉斯-贝尔特拉米特征分析的谱奖励模型；开发了动态感知的感知模块和基于粒子分数熵离散扩散策略的集成策略学习管道。

Result: 实验证明TopoCut支持轨迹生成、可扩展学习、精确评估，并在不同物体几何、尺度、姿态和切割目标上表现出强大的泛化能力。

Conclusion: TopoCut为解决机器人切割可变形物体的挑战提供了一个有效的综合基准，为相关研究提供了重要的工具和框架。

Abstract: Robotic manipulation tasks involving cutting deformable objects remain
challenging due to complex topological behaviors, difficulties in perceiving
dense object states, and the lack of efficient evaluation methods for cutting
outcomes. In this paper, we introduce TopoCut, a comprehensive benchmark for
multi-step robotic cutting tasks that integrates a cutting environment and
generalized policy learning. TopoCut is built upon three core components: (1)
We introduce a high-fidelity simulation environment based on a particle-based
elastoplastic solver with compliant von Mises constitutive models, augmented by
a novel damage-driven topology discovery mechanism that enables accurate
tracking of multiple cutting pieces. (2) We develop a comprehensive reward
design that integrates the topology discovery with a pose-invariant spectral
reward model based on Laplace-Beltrami eigenanalysis, facilitating consistent
and robust assessment of cutting quality. (3) We propose an integrated policy
learning pipeline, where a dynamics-informed perception module predicts
topological evolution and produces particle-wise, topology-aware embeddings to
support PDDP (Particle-based Score-Entropy Discrete Diffusion Policy) for
goal-conditioned policy learning. Extensive experiments demonstrate that
TopoCut supports trajectory generation, scalable learning, precise evaluation,
and strong generalization across diverse object geometries, scales, poses, and
cutting goals.

</details>


### [26] [Towards Autonomous Robotic Electrosurgery via Thermal Imaging](https://arxiv.org/abs/2509.19725)
*Naveed D. Riaziat,Joseph Chen,Axel Krieger,Jeremy D. Brown*

Main category: cs.RO

TL;DR: ThERMO系统通过热成像反馈智能控制电外科手术工具速度，在减少热损伤的同时平衡切割力，相比恒定速度方法显著提高切割成功率并降低峰值切割力。


<details>
  <summary>Details</summary>
Motivation: 电外科手术虽然能减少切割力和出血，但存在热损伤风险。专家依赖经验估计切割速度，缺乏量化参考。现有自主电外科系统使用恒定速度，无法适应组织特性、功率设置或工具类型的变化。

Method: 提出ThERMO（基于优化的热成像电外科速率调制）系统，利用热成像反馈信息智能控制工具速度，在减少热损伤的同时平衡切割力。

Result: 在组织模型实验中，ThERMO将切割成功率提高三倍，峰值切割力降低两倍。系统能响应环境变化，减少组织损伤，完成恒定速度方法会失败的任务。

Conclusion: ThERMO系统通过热成像反馈的智能速度控制，显著提升了电外科手术的安全性和有效性，为自主电外科手术提供了更可靠的解决方案。

Abstract: Electrosurgery is a surgical technique that can improve tissue cutting by
reducing cutting force and bleeding. However, electrosurgery adds a risk of
thermal injury to surrounding tissue. Expert surgeons estimate desirable
cutting velocities based on experience but have no quantifiable reference to
indicate if a particular velocity is optimal. Furthermore, prior demonstrations
of autonomous electrosurgery have primarily used constant tool velocity, which
is not robust to changes in electrosurgical tissue characteristics, power
settings, or tool type. Thermal imaging feedback provides information that can
be used to reduce thermal injury while balancing cutting force by controlling
tool velocity. We introduce Thermography for Electrosurgical Rate Modulation
via Optimization (ThERMO) to autonomously reduce thermal injury while balancing
cutting force by intelligently controlling tool velocity. We demonstrate ThERMO
in tissue phantoms and compare its performance to the constant velocity
approach. Overall, ThERMO improves cut success rate by a factor of three and
can reduce peak cutting force by a factor of two. ThERMO responds to varying
environmental disturbances, reduces damage to tissue, and completes cutting
tasks that would otherwise result in catastrophic failure for the constant
velocity approach.

</details>


### [27] [Simultaneous estimation of contact position and tool shape with high-dimensional parameters using force measurements and particle filtering](https://arxiv.org/abs/2509.19732)
*Kyo Kutsuzawa,Mitsuhiro Hayashibe*

Main category: cs.RO

TL;DR: 提出了一种同时估计接触位置和工具形状的方法，使用网格表示工具形状，通过粒子滤波器避免直接处理高维参数空间


<details>
  <summary>Details</summary>
Motivation: 现有方法需要已知工具几何形状或估计时间过长，限制了在工具形状未知或复杂情况下的应用

Method: 使用粒子滤波器，每个粒子具有独立的工具形状参数，通过力/扭矩信号同时估计接触位置和工具形状

Result: 通过仿真和实验验证，该方法能够同时准确估计工具形状和接触位置，提高接触位置估计的准确性

Conclusion: 所提方法能够有效处理高维工具形状参数，实现接触位置和工具形状的同步估计，为接触任务提供了更准确的工具状态感知

Abstract: Estimating the contact state between a grasped tool and the environment is
essential for performing contact tasks such as assembly and object
manipulation. Force signals are valuable for estimating the contact state, as
they can be utilized even when the contact location is obscured by the tool.
Previous studies proposed methods for estimating contact positions using
force/torque signals; however, most methods require the geometry of the tool
surface to be known. Although several studies have proposed methods that do not
require the tool shape, these methods require considerable time for estimation
or are limited to tools with low-dimensional shape parameters. Here, we propose
a method for simultaneously estimating the contact position and tool shape,
where the tool shape is represented by a grid, which is high-dimensional (more
than 1000 dimensional). The proposed method uses a particle filter in which
each particle has individual tool shape parameters, thereby to avoid directly
handling a high-dimensional parameter space. The proposed method is evaluated
through simulations and experiments using tools with curved shapes on a plane.
Consequently, the proposed method can estimate the shape of the tool
simultaneously with the contact positions, making the contact-position
estimation more accurate.

</details>


### [28] [Trajectory Planning Using Safe Ellipsoidal Corridors as Projections of Orthogonal Trust Regions](https://arxiv.org/abs/2509.19734)
*Akshay Jaitly,Jon Arrizabalaga,Guanrui Li*

Main category: cs.RO

TL;DR: 提出了一种新的轨迹参数化方法，将非凸碰撞自由走廊表示为球的笛卡尔积的凸集，从而解耦问题规模与几何复杂度，并避免显式时间分配。


<details>
  <summary>Details</summary>
Motivation: 现有基于走廊的规划器在复杂环境中扩展性差，需要显式分配时间窗口，限制了在高度复杂环境中的性能。

Method: 引入正交信任区域问题（Orth-TRP），这是一个具有可分离块约束的专门凸规划问题，并开发了利用并行结构和子问题独特结构的高效求解器。

Result: 在四旋翼轨迹规划基准测试中，该方法比最先进的基于走廊的规划器产生更平滑的轨迹和更低的运行时间，特别是在高度复杂的环境中。

Conclusion: 所提出的轨迹参数化和求解框架能够有效处理复杂环境中的轨迹规划问题，显著优于现有方法。

Abstract: Planning collision free trajectories in complex environments remains a core
challenge in robotics. Existing corridor based planners which rely on
decomposition of the free space into collision free subsets scale poorly with
environmental complexity and require explicit allocations of time windows to
trajectory segments. We introduce a new trajectory parameterization that
represents trajectories in a nonconvex collision free corridor as being in a
convex cartesian product of balls. This parameterization allows us to decouple
problem size from geometric complexity of the solution and naturally avoids
explicit time allocation by allowing trajectories to evolve continuously inside
ellipsoidal corridors. Building on this representation, we formulate the
Orthogonal Trust Region Problem (Orth-TRP), a specialized convex program with
separable block constraints, and develop a solver that exploits this parallel
structure and the unique structure of each parallel subproblem for efficient
optimization. Experiments on a quadrotor trajectory planning benchmark show
that our approach produces smoother trajectories and lower runtimes than
state-of-the-art corridor based planners, especially in highly complicated
environments.

</details>


### [29] [Beyond Human Demonstrations: Diffusion-Based Reinforcement Learning to Generate Data for VLA Training](https://arxiv.org/abs/2509.19752)
*Rushuai Yang,Hangxing Wei,Ran Zhang,Zhiyuan Feng,Xiaoyu Chen,Tong Li,Chuheng Zhang,Li Zhao,Jiang Bian,Xiu Su,Yi Chen*

Main category: cs.RO

TL;DR: 本文提出了一种改进的扩散策略优化算法，用于生成高质量、低方差的轨迹，构建了扩散强化学习驱动的VLA训练管道。该方法在LIBERO基准测试中表现出色，生成的轨迹比人类演示和标准高斯RL策略更平滑、更一致。


<details>
  <summary>Details</summary>
Motivation: VLA模型在任务和实现方式上表现出强大的泛化能力，但依赖大规模人类演示限制了其可扩展性。强化学习可以自主生成演示，但传统RL算法在稀疏奖励的长时程操作任务上表现不佳。

Method: 提出改进的扩散策略优化算法，利用扩散模型的高表达能力探索复杂多样行为，同时通过迭代去噪过程的隐式正则化产生平滑一致的演示。构建扩散RL驱动的VLA训练管道。

Result: 在包含130个长时程操作任务的LIBERO基准测试中，扩散RL生成的轨迹比人类演示和标准高斯RL策略更平滑、更一致。仅使用扩散RL生成数据训练的VLA模型平均成功率达到81.9%，比人类数据训练模型高5.3%，比高斯RL生成数据训练模型高12.6%。

Conclusion: 扩散RL是生成丰富、高质量、低方差VLA模型演示的有效替代方案，解决了人类数据收集成本高和传统RL在长时程任务中表现不佳的问题。

Abstract: Vision-language-action (VLA) models have shown strong generalization across
tasks and embodiments; however, their reliance on large-scale human
demonstrations limits their scalability owing to the cost and effort of manual
data collection. Reinforcement learning (RL) offers a potential alternative to
generate demonstrations autonomously, yet conventional RL algorithms often
struggle on long-horizon manipulation tasks with sparse rewards. In this paper,
we propose a modified diffusion policy optimization algorithm to generate
high-quality and low-variance trajectories, which contributes to a diffusion
RL-powered VLA training pipeline. Our algorithm benefits from not only the high
expressiveness of diffusion models to explore complex and diverse behaviors but
also the implicit regularization of the iterative denoising process to yield
smooth and consistent demonstrations. We evaluate our approach on the LIBERO
benchmark, which includes 130 long-horizon manipulation tasks, and show that
the generated trajectories are smoother and more consistent than both human
demonstrations and those from standard Gaussian RL policies. Further, training
a VLA model exclusively on the diffusion RL-generated data achieves an average
success rate of 81.9%, which outperforms the model trained on human data by
+5.3% and that on Gaussian RL-generated data by +12.6%. The results highlight
our diffusion RL as an effective alternative for generating abundant,
high-quality, and low-variance demonstrations for VLA models.

</details>


### [30] [DynaFlow: Dynamics-embedded Flow Matching for Physically Consistent Motion Generation from State-only Demonstrations](https://arxiv.org/abs/2509.19804)
*Sowoo Lee,Dongyun Kang,Jaehyun Park,Hae-Won Park*

Main category: cs.RO

TL;DR: DynaFlow是一个将可微分模拟器嵌入流匹配模型的新框架，通过生成动作空间轨迹并映射到动态可行的状态轨迹，确保所有输出在物理上一致。该框架支持仅使用状态演示进行训练，并能生成可部署到真实机器人的动作序列。


<details>
  <summary>Details</summary>
Motivation: 解决从仅状态演示中学习动态可行轨迹的问题，弥合运动学数据与实际执行之间的差距，使机器人能够生成物理一致且可部署的运动。

Method: 将可微分模拟器嵌入流匹配模型，在动作空间生成轨迹并通过模拟器映射到状态轨迹，实现端到端可微分训练，支持仅状态演示学习。

Result: 在Go1四足机器人上成功部署，复现了数据集中的多种步态，执行了长期开环控制，并将不可行的运动学演示转化为动态可执行的风格化行为。

Conclusion: DynaFlow能够从仅状态演示中生成可部署的高效运动，有效连接了运动学数据与实际硬件执行，在真实机器人上验证了其有效性。

Abstract: This paper introduces DynaFlow, a novel framework that embeds a
differentiable simulator directly into a flow matching model. By generating
trajectories in the action space and mapping them to dynamically feasible state
trajectories via the simulator, DynaFlow ensures all outputs are physically
consistent by construction. This end-to-end differentiable architecture enables
training on state-only demonstrations, allowing the model to simultaneously
generate physically consistent state trajectories while inferring the
underlying action sequences required to produce them. We demonstrate the
effectiveness of our approach through quantitative evaluations and showcase its
real-world applicability by deploying the generated actions onto a physical Go1
quadruped robot. The robot successfully reproduces diverse gait present in the
dataset, executes long-horizon motions in open-loop control and translates
infeasible kinematic demonstrations into dynamically executable, stylistic
behaviors. These hardware experiments validate that DynaFlow produces
deployable, highly effective motions on real-world hardware from state-only
demonstrations, effectively bridging the gap between kinematic data and
real-world execution.

</details>


### [31] [Where Did I Leave My Glasses? Open-Vocabulary Semantic Exploration in Real-World Semi-Static Environments](https://arxiv.org/abs/2509.19851)
*Benjamin Bogenberger,Oliver Harrison,Orrin Dahanaggamaarachchi,Lukas Brunke,Jingxing Qian,Siqi Zhou,Angela P. Schoellig*

Main category: cs.RO

TL;DR: 提出了一种用于半静态环境的开放词汇语义探索系统，通过概率模型跟踪对象实例的静态性，主动维护一致性地图，并利用LLM推理实现开放词汇目标导航


<details>
  <summary>Details</summary>
Motivation: 现有语义探索研究主要关注静态场景，缺乏持续的对象级实例跟踪。在现实机器人应用中，环境中的对象会随时间被移除、重新引入或移动，因此需要构建和维护准确反映环境当前状态的语义地图

Method: 系统通过构建对象实例静态性的概率模型，系统跟踪半静态变化，并主动探索长时间未访问的区域来维护一致性地图。同时利用LLM推理进行开放词汇目标导航，通过优先考虑上下文相关区域来提高搜索效率

Result: 在多个真实半静态环境中评估，系统平均检测到95%的地图变化，效率比随机和巡逻基线提高29%以上。地图精度与完全重建地图相差在2%以内，但需要更少的探索，目标导航任务完成速度比次优策略快约14%

Conclusion: 该方法能够有效处理半静态环境中的地图维护和对象导航问题，为现实世界机器人应用提供了实用的语义探索解决方案

Abstract: Robots deployed in real-world environments, such as homes, must not only
navigate safely but also understand their surroundings and adapt to environment
changes. To perform tasks efficiently, they must build and maintain a semantic
map that accurately reflects the current state of the environment. Existing
research on semantic exploration largely focuses on static scenes without
persistent object-level instance tracking. A consistent map is, however,
crucial for real-world robotic applications where objects in the environment
can be removed, reintroduced, or shifted over time. In this work, to close this
gap, we propose an open-vocabulary, semantic exploration system for semi-static
environments. Our system maintains a consistent map by building a probabilistic
model of object instance stationarity, systematically tracking semi-static
changes, and actively exploring areas that have not been visited for a
prolonged period of time. In addition to active map maintenance, our approach
leverages the map's semantic richness with LLM-based reasoning for
open-vocabulary object-goal navigation. This enables the robot to search more
efficiently by prioritizing contextually relevant areas. We evaluate our
approach across multiple real-world semi-static environments. Our system
detects 95% of map changes on average, improving efficiency by more than 29% as
compared to random and patrol baselines. Overall, our approach achieves a
mapping precision within 2% of a fully rebuilt map while requiring
substantially less exploration and further completes object goal navigation
tasks about 14% faster than the next-best tested strategy (coverage
patrolling). A video of our work can be found at
http://tiny.cc/sem-explor-semi-static .

</details>


### [32] [SAGE:State-Aware Guided End-to-End Policy for Multi-Stage Sequential Tasks via Hidden Markov Decision Process](https://arxiv.org/abs/2509.19853)
*BinXu Wu,TengFei Zhang,Chen Yang,JiaHao Wen,HaoCheng Li,JingTian Ma,Zhen Chen,JingYuan Wang*

Main category: cs.RO

TL;DR: SAGE是一个状态感知引导模仿学习框架，通过隐马尔可夫决策过程建模多阶段顺序机器人操作任务，解决状态模糊性问题，在真实世界实验中达到100%任务成功率


<details>
  <summary>Details</summary>
Motivation: 多阶段顺序机器人操作任务普遍存在状态模糊性问题，即视觉相似的观察对应不同的动作，需要明确捕捉潜在任务阶段来解决模糊性

Method: 将任务建模为隐马尔可夫决策过程，包含状态转移网络推断隐藏状态，以及状态感知动作策略基于观察和隐藏状态生成动作；提出半自动标注管道结合主动学习和软标签插值

Result: 在多个具有状态模糊性的复杂多阶段顺序任务中，SAGE在标准评估协议下达到100%任务成功率，显著超越基线方法；消融研究显示仅需手动标注约13%的状态即可维持性能

Conclusion: SAGE框架通过显式建模隐藏状态有效解决机器人操作中的状态模糊性问题，且通过半自动标注大大减少了人工标注工作量，具有强有效性

Abstract: Multi-stage sequential (MSS) robotic manipulation tasks are prevalent and
crucial in robotics. They often involve state ambiguity, where visually similar
observations correspond to different actions. We present SAGE, a state-aware
guided imitation learning framework that models tasks as a Hidden Markov
Decision Process (HMDP) to explicitly capture latent task stages and resolve
ambiguity. We instantiate the HMDP with a state transition network that infers
hidden states, and a state-aware action policy that conditions on both
observations and hidden states to produce actions, thereby enabling
disambiguation across task stages. To reduce manual annotation effort, we
propose a semi-automatic labeling pipeline combining active learning and soft
label interpolation. In real-world experiments across multiple complex MSS
tasks with state ambiguity, SAGE achieved 100% task success under the standard
evaluation protocol, markedly surpassing the baselines. Ablation studies
further show that such performance can be maintained with manual labeling for
only about 13% of the states, indicating its strong effectiveness.

</details>


### [33] [D3Grasp: Diverse and Deformable Dexterous Grasping for General Objects](https://arxiv.org/abs/2509.19892)
*Keyu Wang,Bingcong Lu,Zhengxue Cheng,Hengdi Zhang,Li Song*

Main category: cs.RO

TL;DR: D3Grasp是一个多模态感知引导的强化学习框架，用于实现多样化和稳定的灵巧抓取，特别针对可变形物体，在真实世界试验中达到95.1%的平均成功率。


<details>
  <summary>Details</summary>
Motivation: 解决机器人灵巧抓取中高维动作空间和感知不确定性的挑战，特别是针对一般物体和可变形物体的多样化稳定抓取问题。

Method: 1）引入统一的多模态表示整合视觉和触觉感知；2）提出非对称强化学习架构利用特权信息；3）设计训练策略合成无穿透、运动学可行的抓取动作。

Result: 在大规模多样化物体类别上表现出高度鲁棒性，在可变形和柔顺物体灵巧抓取方面显著推进了技术前沿，真实世界试验平均成功率95.1%。

Conclusion: D3Grasp框架在感知不确定性和真实世界干扰下，对刚性和可变形物体基准测试均优于现有方法，为灵巧抓取提供了有效的解决方案。

Abstract: Achieving diverse and stable dexterous grasping for general and deformable
objects remains a fundamental challenge in robotics, due to high-dimensional
action spaces and uncertainty in perception. In this paper, we present D3Grasp,
a multimodal perception-guided reinforcement learning framework designed to
enable Diverse and Deformable Dexterous Grasping. We firstly introduce a
unified multimodal representation that integrates visual and tactile perception
to robustly grasp common objects with diverse properties. Second, we propose an
asymmetric reinforcement learning architecture that exploits privileged
information during training while preserving deployment realism, enhancing both
generalization and sample efficiency. Third, we meticulously design a training
strategy to synthesize contact-rich, penetration-free, and kinematically
feasible grasps with enhanced adaptability to deformable and contact-sensitive
objects. Extensive evaluations confirm that D3Grasp delivers highly robust
performance across large-scale and diverse object categories, and substantially
advances the state of the art in dexterous grasping for deformable and
compliant objects, even under perceptual uncertainty and real-world
disturbances. D3Grasp achieves an average success rate of 95.1% in real-world
trials,outperforming prior methods on both rigid and deformable objects
benchmarks.

</details>


### [34] [GUIDE: A Diffusion-Based Autonomous Robot Exploration Framework Using Global Graph Inference](https://arxiv.org/abs/2509.19916)
*Zijun Che,Yinghong Zhang,Shengyi Liang,Boyu Zhou,Jun Ma,Jinni Zhou*

Main category: cs.RO

TL;DR: GUIDE是一个新颖的自主探索框架，通过结合全局图推理和扩散决策，在结构化室内环境中实现高效探索。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模未观察空间和规划全局高效路径方面存在困难，特别是在复杂室内环境中。

Method: 提出区域评估全局图表示，整合观测数据和未探索区域预测；使用扩散策略网络生成稳定的前瞻性动作序列。

Result: 在仿真和实际部署中，GUIDE比最先进方法快18.3%完成覆盖，减少34.9%冗余运动。

Conclusion: GUIDE框架通过全局推理和扩散决策的协同作用，显著提升了自主探索的效率和性能。

Abstract: Autonomous exploration in structured and complex indoor environments remains
a challenging task, as existing methods often struggle to appropriately model
unobserved space and plan globally efficient paths. To address these
limitations, we propose GUIDE, a novel exploration framework that
synergistically combines global graph inference with diffusion-based
decision-making. We introduce a region-evaluation global graph representation
that integrates both observed environmental data and predictions of unexplored
areas, enhanced by a region-level evaluation mechanism to prioritize reliable
structural inferences while discounting uncertain predictions. Building upon
this enriched representation, a diffusion policy network generates stable,
foresighted action sequences with significantly reduced denoising steps.
Extensive simulations and real-world deployments demonstrate that GUIDE
consistently outperforms state-of-the-art methods, achieving up to 18.3% faster
coverage completion and a 34.9% reduction in redundant movements.

</details>


### [35] [Robot Trajectron V2: A Probabilistic Shared Control Framework for Navigation](https://arxiv.org/abs/2509.19954)
*Pinhao Song,Yurui Du,Ophelie Saussus,Sofie De Schrijver,Irene Caprara,Peter Janssen,Renaud Detry*

Main category: cs.RO

TL;DR: RT-V2是一个概率共享控制导航解决方案，通过结合先验意图模型和后验更新，实现准确意图预测和安全有效的人机交互辅助


<details>
  <summary>Details</summary>
Motivation: 解决人机交互中意图预测不准确、辅助不安全的问题，平衡用户自主性和辅助干预

Method: 结合循环神经网络和条件变分自编码器建模多模态历史依赖的用户意图先验，通过后验更新整合实时用户输入和环境上下文

Result: 在合成基准测试、人机交互实验和脑机接口实验中，RT-V2在意图估计、安全导航支持和平衡用户自主性方面优于现有技术

Conclusion: RT-V2通过统一概率建模、强化学习和安全优化，为多样化辅助技术提供了原则性和可推广的共享控制方法

Abstract: We propose a probabilistic shared-control solution for navigation, called
Robot Trajectron V2 (RT-V2), that enables accurate intent prediction and safe,
effective assistance in human-robot interaction. RT-V2 jointly models a user's
long-term behavioral patterns and their noisy, low-dimensional control signals
by combining a prior intent model with a posterior update that accounts for
real-time user input and environmental context. The prior captures the
multimodal and history-dependent nature of user intent using recurrent neural
networks and conditional variational autoencoders, while the posterior
integrates this with uncertain user commands to infer desired actions. We
conduct extensive experiments to validate RT-V2 across synthetic benchmarks,
human-computer interaction studies with keyboard input, and brain-machine
interface experiments with non-human primates. Results show that RT-V2
outperforms the state of the art in intent estimation, provides safe and
efficient navigation support, and adequately balances user autonomy with
assistive intervention. By unifying probabilistic modeling, reinforcement
learning, and safe optimization, RT-V2 offers a principled and generalizable
approach to shared control for diverse assistive technologies.

</details>


### [36] [Generalist Robot Manipulation beyond Action Labeled Data](https://arxiv.org/abs/2509.19958)
*Alexander Spiridonov,Jan-Nico Zaech,Nikolay Nikolov,Luc Van Gool,Danda Pani Paudel*

Main category: cs.RO

TL;DR: 提出一种利用无动作标签视频（包含人类和/或机器人动作）的方法，通过提取手部或夹爪位置的密集动态3D点云，并使用3D动态预测器进行自监督学习，从而提高开放词汇性能并实现数据高效的新任务学习。


<details>
  <summary>Details</summary>
Motivation: 解决高质量动作标记机器人演示数据难以扩展的问题，现有方法依赖此类数据来实现鲁棒性和泛化能力。

Method: 提取手部/夹爪位置的密集动态3D点云，使用提出的3D动态预测器进行自监督学习，然后使用较小的标记数据集将预测器调整为动作预测器以实现动作对齐。

Result: 该方法不仅能从无标记的人类和机器人演示中学习，改进下游通用机器人策略，还能使机器人在真实世界和模拟环境中无需动作标签即可学习新任务（即动作外泛化）。

Conclusion: 提出的方法有效解决了机器人演示数据标注难的问题，实现了从无标签视频中学习并提升机器人通用能力的目标。

Abstract: Recent advances in generalist robot manipulation leverage pre-trained
Vision-Language Models (VLMs) and large-scale robot demonstrations to tackle
diverse tasks in a zero-shot manner. A key challenge remains: scaling
high-quality, action-labeled robot demonstration data, which existing methods
rely on for robustness and generalization. To address this, we propose a method
that benefits from videos without action labels - featuring humans and/or
robots in action - enhancing open-vocabulary performance and enabling
data-efficient learning of new tasks. Our method extracts dense, dynamic 3D
point clouds at the hand or gripper location and uses a proposed 3D dynamics
predictor for self-supervision. This predictor is then tuned to an action
predictor using a smaller labeled dataset for action alignment. We show that
our method not only learns from unlabeled human and robot demonstrations -
improving downstream generalist robot policies - but also enables robots to
learn new tasks without action labels (i.e., out-of-action generalization) in
both real-world and simulated settings.

</details>


### [37] [An effective control of large systems of active particles: An application to evacuation problem](https://arxiv.org/abs/2509.19972)
*Albina Klepach,Egor E. Nuzhin,Alexey A. Tsukanov,Nikolay V. Brilliantov*

Main category: cs.RO

TL;DR: 本文提出了一种结合强化学习和人工力的领导者控制策略，用于有效引导活性粒子系统（如人群）从危险区域疏散。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景下缺乏可扩展性和鲁棒性，特别是需要对每个个体进行单独控制。通过领导者控制整个系统可以解决这一问题。

Method: 引入了广义Vicsek模型描述领导者引导活性粒子的行为，结合强化学习与人工力开发了有效的领导者控制策略。

Result: 该方法在机器人救援人员引导人群疏散的应用中表现出色，相比直接应用强化学习的方法提供了更鲁棒和高效的疏散策略。

Conclusion: 提出的领导者控制策略为解决大规模活性粒子系统的操控问题提供了有效的解决方案，在人群疏散等实际应用中具有重要价值。

Abstract: Manipulation of large systems of active particles is a serious challenge
across diverse domains, including crowd management, control of robotic swarms,
and coordinated material transport. The development of advanced control
strategies for complex scenarios is hindered, however, by the lack of
scalability and robustness of the existing methods, in particular, due to the
need of an individual control for each agent. One possible solution involves
controlling a system through a leader or a group of leaders, which other agents
tend to follow. Using such an approach we develop an effective control strategy
for a leader, combining reinforcement learning (RL) with artificial forces
acting on the system. To describe the guidance of active particles by a leader
we introduce the generalized Vicsek model. This novel method is then applied to
the problem of the effective evacuation by a robot-rescuer (leader) of large
groups of people from hazardous places. We demonstrate, that while a
straightforward application of RL yields suboptimal results, even for advanced
architectures, our approach provides a robust and efficient evacuation
strategy. The source code supporting this study is publicly available at:
https://github.com/cinemere/evacuation.

</details>


### [38] [Lidar-based Tracking of Traffic Participants with Sensor Nodes in Existing Urban Infrastructure](https://arxiv.org/abs/2509.20009)
*Simon Schäfer,Bassam Alrifaee,Ehsan Hashemi*

Main category: cs.RO

TL;DR: 提出了一种仅使用激光雷达的路边状态估计和跟踪框架，结合边缘计算单元，实现实时、可扩展的城市基础设施集成方案


<details>
  <summary>Details</summary>
Motivation: 传统远程感知方案成本高、计算量大，特别是在感知条件恶劣时表现不佳，需要开发更高效的路边跟踪解决方案

Method: 使用扩展卡尔曼滤波器进行状态更新，1D网格图/贝叶斯更新进行尺寸估计，查找表驱动分类更新，基于跟踪年龄和边界框一致性进行存在概率估计

Result: 在动态城市场景中实现实时性能，端到端流水线99.88%的消息处理时间小于100毫秒，检测率高，在模拟风力和传感器振动下保持鲁棒性

Conclusion: 该框架证明在仅使用CPU的边缘硬件上可实现可靠的路边实时跟踪，能够与现有城市基础设施集成，降低部署成本，支持大规模城市推广

Abstract: This paper presents a lidar-only state estimation and tracking framework,
along with a roadside sensing unit for integration with existing urban
infrastructure. Urban deployments demand scalable, real-time tracking
solutions, yet traditional remote sensing remains costly and computationally
intensive, especially under perceptually degraded conditions. Our sensor node
couples a single lidar with an edge computing unit and runs a computationally
efficient, GPU-free observer that simultaneously estimates object state, class,
dimensions, and existence probability. The pipeline performs: (i) state updates
via an extended Kalman filter, (ii) dimension estimation using a 1D
grid-map/Bayesian update, (iii) class updates via a lookup table driven by the
most probable footprint, and (iv) existence estimation from track age and
bounding-box consistency. Experiments in dynamic urban-like scenes with diverse
traffic participants demonstrate real-time performance and high precision: The
complete end-to-end pipeline finishes within \SI{100}{\milli\second} for
\SI{99.88}{\%} of messages, with an excellent detection rate. Robustness is
further confirmed under simulated wind and sensor vibration. These results
indicate that reliable, real-time roadside tracking is feasible on CPU-only
edge hardware, enabling scalable, privacy-friendly deployments within existing
city infrastructure. The framework integrates with existing poles, traffic
lights, and buildings, reducing deployment costs and simplifying large-scale
urban rollouts and maintenance efforts.

</details>


### [39] [MARG: MAstering Risky Gap Terrains for Legged Robots with Elevation Mapping](https://arxiv.org/abs/2509.20036)
*Yinzhao Dong,Ji Ma,Liu Zhao,Wanyue Li,Peng Lu*

Main category: cs.RO

TL;DR: 提出了一种名为MARG的深度强化学习控制器，用于四足机器人在危险间隙地形上的稳定运动，通过结合地形图和本体感知来动态调整动作，并设计了地形图生成模型来减少映射漂移。


<details>
  <summary>Details</summary>
Motivation: 现有的盲运动控制器在危险间隙地形上难以确保安全性和高效穿越，而基于感知的控制器存在多传感器部署复杂和计算资源需求高的问题。

Method: MARG控制器整合地形图和本体感知，在训练阶段选择性加入特权信息加速策略优化，设计了三个脚部相关奖励来鼓励探索安全落脚点，并提出地形图生成模型仅使用一个LiDAR提供准确地形图。

Result: 实验结果表明MARG在各种危险地形任务中保持稳定性，实现了学习策略的零样本迁移。

Conclusion: MARG控制器能够有效解决四足机器人在危险间隙地形上的运动挑战，通过简化传感器部署和减少计算需求，实现了从仿真到真实世界的有效迁移。

Abstract: Deep Reinforcement Learning (DRL) controllers for quadrupedal locomotion have
demonstrated impressive performance on challenging terrains, allowing robots to
execute complex skills such as climbing, running, and jumping. However,
existing blind locomotion controllers often struggle to ensure safety and
efficient traversal through risky gap terrains, which are typically highly
complex, requiring robots to perceive terrain information and select
appropriate footholds during locomotion accurately. Meanwhile, existing
perception-based controllers still present several practical limitations,
including a complex multi-sensor deployment system and expensive computing
resource requirements. This paper proposes a DRL controller named MAstering
Risky Gap Terrains (MARG), which integrates terrain maps and proprioception to
dynamically adjust the action and enhance the robot's stability in these tasks.
During the training phase, our controller accelerates policy optimization by
selectively incorporating privileged information (e.g., center of mass,
friction coefficients) that are available in simulation but unmeasurable
directly in real-world deployments due to sensor limitations. We also designed
three foot-related rewards to encourage the robot to explore safe footholds.
More importantly, a terrain map generation (TMG) model is proposed to reduce
the drift existing in mapping and provide accurate terrain maps using only one
LiDAR, providing a foundation for zero-shot transfer of the learned policy. The
experimental results indicate that MARG maintains stability in various risky
terrain tasks.

</details>


### [40] [LLM Trainer: Automated Robotic Data Generating via Demonstration Augmentation using LLMs](https://arxiv.org/abs/2509.20070)
*Abraham George,Amir Barati Farimani*

Main category: cs.RO

TL;DR: LLM Trainer是一个自动化流水线，利用大型语言模型的世界知识将少量人类演示转化为大规模机器人数据集用于模仿学习。该方法通过离线演示标注和在线关键姿态重定向，能够从一个演示生成多个新场景的轨迹。


<details>
  <summary>Details</summary>
Motivation: 传统的模仿学习需要大量人类演示数据，这在实际应用中成本高昂且难以获取。LLM Trainer旨在解决数据稀缺问题，通过自动化生成演示数据来降低模仿学习的数据需求。

Method: 方法分为两步：离线演示标注（提取关键帧、显著物体和姿态-物体关系）和在线关键姿态重定向（将关键帧适配到新场景）。使用Thompson采样优化标注过程，提高生成成功率。

Result: 在多个任务上的评估表明，该方法的数据标注方法持续优于专家设计的基线方法。还展示了结合优化的LLM前馈规划和学习的反馈模仿学习控制器的集成策略，并在Franka Emika Panda机器人上验证了硬件可行性。

Conclusion: LLM Trainer提供了一种高效的数据生成方法，显著减少了模仿学习对大量人类演示的依赖，为机器人学习提供了可扩展的解决方案。

Abstract: We present LLM Trainer, a fully automated pipeline that leverages the world
knowledge of Large Language Models (LLMs) to transform a small number of human
demonstrations (as few as one) into a large robot dataset for imitation
learning. Our approach decomposes demonstration generation into two steps: (1)
offline demonstration annotation that extracts keyframes, salient objects, and
pose-object relations; and (2) online keypose retargeting that adapts those
keyframes to a new scene, given an initial observation. Using these modified
keypoints, our system warps the original demonstration to generate a new
trajectory, which is then executed, and the resulting demo, if successful, is
saved. Because the annotation is reusable across scenes, we use Thompson
sampling to optimize the annotation, significantly improving generation success
rate. We evaluate our method on a range of tasks, and find that our data
annotation method consistently outperforms expert-engineered baselines. We
further show an ensemble policy that combines the optimized LLM feed-forward
plan with a learned feedback imitation learning controller. Finally, we
demonstrate hardware feasibility on a Franka Emika Panda robot. For additional
materials and demonstration videos, please see the project website:
https://sites.google.com/andrew.cmu.edu/llm-trainer

</details>


### [41] [Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning](https://arxiv.org/abs/2509.20077)
*Xun Li,Rodrigo Santa Cruz,Mingze Xi,Hu Zhang,Madhawa Perera,Ziwei Wang,Ahalya Ravendran,Brandon J. Matthews,Feng Xu,Matt Adcock,Dadong Wang,Jiajun Liu*

Main category: cs.RO

TL;DR: 本文提出了一种名为3D可查询场景表示（3D QSR）的新框架，旨在通过融合几何结构、视觉信息和语义理解，使机器人能够理解高级人类指令并在复杂3D环境中执行任务。


<details>
  <summary>Details</summary>
Motivation: 为了让机器人理解高级人类指令并执行复杂任务，关键在于实现全面的场景理解：以有意义的方式解释和交互3D环境。这需要一个智能地图，将精确的几何结构与丰富的人类可理解语义相融合。

Method: 3D QSR框架基于多媒体数据，统一了三种互补的3D表示：(1) 全景重建的3D一致新视角渲染和分割，(2) 3D点云的精确几何结构，(3) 通过3D场景图实现的结构化、可扩展组织。该框架采用以对象为中心的设计，与大型视觉语言模型集成，通过链接多模态对象嵌入实现语义可查询性，并支持几何、视觉和语义信息的对象级检索。

Result: 通过在Unity中的模拟机器人任务规划场景进行评估，使用室内公共数据集Replica，并在真实湿实验室环境的数字副本中测试QSR支持的机器人应急响应任务规划。结果表明，该框架能够促进场景理解，整合空间和语义推理，有效将高级人类指令转化为复杂3D环境中的精确机器人任务规划。

Conclusion: 3D QSR框架成功实现了场景理解和语义推理的整合，为机器人执行复杂任务提供了有效的解决方案，特别是在需要精确空间和语义理解的应急响应等场景中表现出色。

Abstract: To enable robots to comprehend high-level human instructions and perform
complex tasks, a key challenge lies in achieving comprehensive scene
understanding: interpreting and interacting with the 3D environment in a
meaningful way. This requires a smart map that fuses accurate geometric
structure with rich, human-understandable semantics. To address this, we
introduce the 3D Queryable Scene Representation (3D QSR), a novel framework
built on multimedia data that unifies three complementary 3D representations:
(1) 3D-consistent novel view rendering and segmentation from panoptic
reconstruction, (2) precise geometry from 3D point clouds, and (3) structured,
scalable organization via 3D scene graphs. Built on an object-centric design,
the framework integrates with large vision-language models to enable semantic
queryability by linking multimodal object embeddings, and supporting
object-level retrieval of geometric, visual, and semantic information. The
retrieved data are then loaded into a robotic task planner for downstream
execution. We evaluate our approach through simulated robotic task planning
scenarios in Unity, guided by abstract language instructions and using the
indoor public dataset Replica. Furthermore, we apply it in a digital duplicate
of a real wet lab environment to test QSR-supported robotic task planning for
emergency response. The results demonstrate the framework's ability to
facilitate scene understanding and integrate spatial and semantic reasoning,
effectively translating high-level human instructions into precise robotic task
planning in complex 3D environments.

</details>


### [42] [DB-TSDF: Directional Bitmask-based Truncated Signed Distance Fields for Efficient Volumetric Mapping](https://arxiv.org/abs/2509.20081)
*Jose E. Maese,Luis Merino,Fernando Caballero*

Main category: cs.RO

TL;DR: 提出了一种基于TSDF的高效CPU专用体素映射框架，通过方向性位掩码集成方案实现实时3D重建，处理时间与体素网格分辨率无关


<details>
  <summary>Details</summary>
Motivation: 现有TSDF/ESDF方法大多依赖GPU加速，需要开发能够在CPU上实现竞争性性能的高分辨率实时3D重建方法

Method: 使用截断符号距离场(TSDF)和方向性位掩码集成方案，将原始LiDAR点云数据增量融合到体素网格中

Result: 在真实世界开放数据集上的实验表明，生成的地图精度与当代映射技术相当，处理时间保持恒定

Conclusion: 该方法证明了在CPU上实现高分辨率实时3D重建的可行性，为不依赖GPU的实时映射应用提供了有效解决方案

Abstract: This paper presents a high-efficiency, CPU-only volumetric mapping framework
based on a Truncated Signed Distance Field (TSDF). The system incrementally
fuses raw LiDAR point-cloud data into a voxel grid using a directional
bitmask-based integration scheme, producing dense and consistent TSDF
representations suitable for real-time 3D reconstruction. A key feature of the
approach is that the processing time per point-cloud remains constant,
regardless of the voxel grid resolution, enabling high resolution mapping
without sacrificing runtime performance. In contrast to most recent TSDF/ESDF
methods that rely on GPU acceleration, our method operates entirely on CPU,
achieving competitive results in speed. Experiments on real-world open datasets
demonstrate that the generated maps attain accuracy on par with contemporary
mapping techniques.

</details>


### [43] [Orbital Stabilization and Time Synchronization of Unstable Periodic Motions in Underactuated Robots](https://arxiv.org/abs/2509.20082)
*Surov Maksim*

Main category: cs.RO

TL;DR: 提出了一种控制方法，用于实现欠驱动机器人系统的轨道稳定性和时间同步，扩展了经典横向线性化框架以包含时间失同步动态，并采用时变LQR和滑模控制进行稳定。


<details>
  <summary>Details</summary>
Motivation: 解决欠驱动机器人系统在周期性轨迹跟踪中的时间同步问题，传统方法未充分考虑时间失同步动态。

Method: 扩展经典横向线性化框架以显式包含时间失同步动态，使用时变LQR和滑模控制组合来稳定扩展后的横向动态。

Result: 通过六台Butterfly机器人的集中式和分散式控制策略实验验证了理论结果的有效性。

Conclusion: 所提出的方法能够有效实现欠驱动机器人系统的轨道稳定性和时间同步，为多机器人协同控制提供了可行方案。

Abstract: This paper presents a control methodology for achieving orbital stabilization
with simultaneous time synchronization of periodic trajectories in
underactuated robotic systems. The proposed approach extends the classical
transverse linearization framework to explicitly incorporate
time-desynchronization dynamics. To stabilize the resulting extended transverse
dynamics, we employ a combination of time-varying LQR and sliding-mode control.
The theoretical results are validated experimentally through the implementation
of both centralized and decentralized control strategies on a group of six
Butterfly robots.

</details>


### [44] [C-3TO: Continuous 3D Trajectory Optimization on Neural Euclidean Signed Distance Fields](https://arxiv.org/abs/2509.20084)
*Guillermo Gil,Jose Antonio Cobano,Luis Merino,Fernando Caballero*

Main category: cs.RO

TL;DR: 本文提出了一种在杂乱环境中进行连续3D轨迹优化的新框架C-3TO，利用在线神经欧几里得符号距离场（ESDF）直接优化平滑的五次多项式轨迹，确保整个轨迹的精确梯度信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖离散化ESDF网格和插值，无法提供连续的梯度信息，限制了轨迹优化的精度和安全性。需要一种能够直接优化连续轨迹的方法来应对杂乱环境中的导航挑战。

Method: 采用两阶段非线性优化流程，结合连续神经ESDF和五次多项式轨迹表示，通过定义局部窗口大小和优化参数来平衡效率、安全性和平滑性。

Result: 实验结果表明C-3TO能够生成碰撞感知且动态可行的轨迹，其灵活性允许根据用户需求轻松调整参数而不影响性能。

Conclusion: 通过将连续轨迹参数化与持续更新的神经ESDF相结合，C-3TO为空中机器人的安全高效局部重规划建立了稳健且可推广的基础。

Abstract: This paper introduces a novel framework for continuous 3D trajectory
optimization in cluttered environments, leveraging online neural Euclidean
Signed Distance Fields (ESDFs). Unlike prior approaches that rely on
discretized ESDF grids with interpolation, our method directly optimizes smooth
trajectories represented by fifth-order polynomials over a continuous neural
ESDF, ensuring precise gradient information throughout the entire trajectory.
The framework integrates a two-stage nonlinear optimization pipeline that
balances efficiency, safety and smoothness. Experimental results demonstrate
that C-3TO produces collision-aware and dynamically feasible trajectories.
Moreover, its flexibility in defining local window sizes and optimization
parameters enables straightforward adaptation to diverse user's needs without
compromising performance. By combining continuous trajectory parameterization
with a continuously updated neural ESDF, C-3TO establishes a robust and
generalizable foundation for safe and efficient local replanning in aerial
robotics.

</details>


### [45] [Hybrid Safety Verification of Multi-Agent Systems using $ψ$-Weighted CBFs and PAC Guarantees](https://arxiv.org/abs/2509.20093)
*Venkat Margapuri,Garik Kazanjian,Naren Kosaraju*

Main category: cs.RO

TL;DR: 提出一种用于有界随机扰动下闭环多智能体系统的混合安全验证框架，结合确定性可容许性与蒙特卡洛经验验证，提供概率安全证书


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统在有界随机扰动下的安全验证问题，传统方法难以处理复杂的方向性控制对齐和概率性安全保证

Method: 将控制屏障函数与新颖的ψ加权公式相结合，将智能体间的方向性控制对齐编码到安全约束中，结合确定性分析和蒙特卡洛滚动验证

Result: 在不同有界随机扰动下的实验验证了所提方法的可行性

Conclusion: 该混合框架为多智能体系统提供了有效的概率安全验证方法，能够处理复杂的方向性控制约束和随机扰动

Abstract: This study proposes a hybrid safety verification framework for closed-loop
multi-agent systems under bounded stochastic disturbances. The proposed
approach augments control barrier functions with a novel $\psi$-weighted
formulation that encodes directional control alignment between agents into the
safety constraints. Deterministic admissibility is combined with empirical
validation via Monte Carlo rollouts, and a PAC-style guarantee is derived based
on margin-aware safety violations to provide a probabilistic safety
certificate. The results from the experiments conducted under different bounded
stochastic disturbances validate the feasibility of the proposed approach.

</details>


### [46] [Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving](https://arxiv.org/abs/2509.20109)
*Pengxiang Li,Yinan Zheng,Yue Wang,Huimin Wang,Hang Zhao,Jingjing Liu,Xianyuan Zhan,Kun Zhan,Xianpeng Lang*

Main category: cs.RO

TL;DR: ReflectDrive是一个基于学习的框架，通过离散扩散和反射机制生成安全轨迹，解决了现有VLA模型在自动驾驶中难以编码物理规则的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端自动驾驶方法受限于模仿学习，无法有效编码物理规则，通常需要复杂的后处理规则或计算昂贵的强化学习/扩散引导。

Method: 首先将二维驾驶空间离散化构建动作码本，利用预训练扩散语言模型进行规划；核心是安全感知的反射机制，通过局部搜索识别不安全标记并基于安全锚点进行修复再生。

Result: 在NAVSIM基准测试中，ReflectDrive在安全关键轨迹生成方面表现出显著优势。

Conclusion: 该方法为自动驾驶系统提供了可扩展且可靠的解决方案，通过无梯度计算的迭代自校正实现了安全轨迹生成。

Abstract: End-to-End (E2E) solutions have emerged as a mainstream approach for
autonomous driving systems, with Vision-Language-Action (VLA) models
representing a new paradigm that leverages pre-trained multimodal knowledge
from Vision-Language Models (VLMs) to interpret and interact with complex
real-world environments. However, these methods remain constrained by the
limitations of imitation learning, which struggles to inherently encode
physical rules during training. Existing approaches often rely on complex
rule-based post-refinement, employ reinforcement learning that remains largely
limited to simulation, or utilize diffusion guidance that requires
computationally expensive gradient calculations. To address these challenges,
we introduce ReflectDrive, a novel learning-based framework that integrates a
reflection mechanism for safe trajectory generation via discrete diffusion. We
first discretize the two-dimensional driving space to construct an action
codebook, enabling the use of pre-trained Diffusion Language Models for
planning tasks through fine-tuning. Central to our approach is a safety-aware
reflection mechanism that performs iterative self-correction without gradient
computation. Our method begins with goal-conditioned trajectory generation to
model multi-modal driving behaviors. Based on this, we apply local search
methods to identify unsafe tokens and determine feasible solutions, which then
serve as safe anchors for inpainting-based regeneration. Evaluated on the
NAVSIM benchmark, ReflectDrive demonstrates significant advantages in
safety-critical trajectory generation, offering a scalable and reliable
solution for autonomous driving systems.

</details>


### [47] [A Biomimetic Vertebraic Soft Robotic Tail for High-Speed, High-Force Dynamic Maneuvering](https://arxiv.org/abs/2509.20219)
*Sicong Liu,Jianhui Liu,Fang Chen,Wenjian Yang,Juan Yi,Yu Zheng,Zheng Wang,Wanchao Chi,Chaoyang Song*

Main category: cs.RO

TL;DR: 本文提出了一种仿生椎体软体机器人尾巴（BVSR），通过结合刚性椎体和软体气动驱动，解决了传统刚性尾巴安全性差和软体尾巴动力不足的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前机器人尾巴设计面临刚性系统动力强但安全性差与软体系统安全但动力不足的权衡问题，需要一种既能产生强大惯性效应又能在非结构化环境中安全运行的解决方案。

Method: 采用仿生椎体结构设计，将刚性椎体与软体气动驱动相结合，实现负载承载与驱动的解耦。开发了包含椎体约束的专用运动学和动力学模型，并通过实验验证。

Result: BVSR尾巴实现了超过670度/秒的角速度，产生高达5.58N的惯性力和1.21Nm的扭矩，相比非椎体设计性能提升超过200%。在快速推车稳定、障碍物穿越、高速转向和四足机器人集成等应用中表现出优异性能。

Conclusion: BVSR尾巴通过仿生椎体设计成功解决了刚性-软体尾巴的权衡问题，为敏捷机器人平台提供了兼具高动态性能和安全性保障的实用解决方案。

Abstract: Robotic tails can enhance the stability and maneuverability of mobile robots,
but current designs face a trade-off between the power of rigid systems and the
safety of soft ones. Rigid tails generate large inertial effects but pose risks
in unstructured environments, while soft tails lack sufficient speed and force.
We present a Biomimetic Vertebraic Soft Robotic (BVSR) tail that resolves this
challenge through a compliant pneumatic body reinforced by a passively jointed
vertebral column inspired by musculoskeletal structures. This hybrid design
decouples load-bearing and actuation, enabling high-pressure actuation (up to 6
bar) for superior dynamics while preserving compliance. A dedicated kinematic
and dynamic model incorporating vertebral constraints is developed and
validated experimentally. The BVSR tail achieves angular velocities above
670{\deg}/s and generates inertial forces and torques up to 5.58 N and 1.21 Nm,
indicating over 200% improvement compared to non-vertebraic designs.
Demonstrations on rapid cart stabilization, obstacle negotiation, high-speed
steering, and quadruped integration confirm its versatility and practical
utility for agile robotic platforms.

</details>


### [48] [Techno-Economic analysis for Smart Hangar inspection operations through Sensing and Localisation at scale](https://arxiv.org/abs/2509.20229)
*Angelos Plastropoulos,Nicolas P. Avdelidis,Argyrios Zolotas*

Main category: cs.RO

TL;DR: 本文提出了首个技术经济路线图，在40x50米的机库环境中对运动捕捉系统、超宽带技术和天花板摄像头网络进行基准测试，并引入了摄像头选择和定位的双层优化框架。


<details>
  <summary>Details</summary>
Motivation: 飞机维护机库作为GPS信号缺失环境，存在严重的多径效应和严格的操作限制，需要针对特定领域的比较研究来指导可靠且可扩展的定位系统部署。

Method: 采用双层优化框架，结合基于市场的摄像头-镜头选择和优化求解器，生成最小化硬件成本同时满足精度目标的摄像头布局方案。

Result: 优化的视觉架构有潜力为下一代智能机库提供稳健且经济高效的传感解决方案。

Conclusion: 该路线图为MRO规划者提供了平衡精度、覆盖范围和预算的可操作方法，展示了优化视觉架构在智能机库中的应用价值。

Abstract: The accuracy, resilience, and affordability of localisation are fundamental
to autonomous robotic inspection within aircraft maintenance and overhaul (MRO)
hangars. Hangars typically feature tall ceilings and are often made of
materials such as metal. Due to its nature, it is considered a GPS-denied
environment, with extensive multipath effects and stringent operational
constraints that collectively create a uniquely challenging environment. This
persistent gap highlights the need for domain-specific comparative studies,
including rigorous cost, accuracy, and integration assessments, to inform a
reliable and scalable deployment of a localisation system in the Smart Hangar.
This paper presents the first techno-economic roadmap that benchmarks motion
capture (MoCap), ultra-wideband (UWB), and a ceiling-mounted camera network
across three operational scenarios: robot localisation, asset tracking, and
surface defect detection within a 40x50 m hangar bay. A dual-layer optimisation
for camera selection and positioning framework is introduced, which couples
market-based camera-lens selection with an optimisation solver, producing
camera layouts that minimise hardware while meeting accuracy targets. The
roadmap equips MRO planners with an actionable method to balance accuracy,
coverage, and budget, demonstrating that an optimised vision architecture has
the potential to unlock robust and cost-effective sensing for next-generation
Smart Hangars.

</details>


### [49] [AnchDrive: Bootstrapping Diffusion Policies with Hybrid Trajectory Anchors for End-to-End Driving](https://arxiv.org/abs/2509.20253)
*Jinhao Chai,Anqing Jiang,Hao Jiang,Shiyi Mu,Zichong Gu,Shugong Xu*

Main category: cs.RO

TL;DR: AnchDrive是一个端到端自动驾驶框架，通过使用混合轨迹锚点来引导扩散策略，有效降低了传统生成模型的高计算成本，实现了高效生成多样化高质量轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决端到端多模态规划中行为多模态性和长尾场景泛化挑战，同时降低传统生成模型的计算开销。

Method: 提出基于锚点的扩散策略：使用静态通用驾驶先验和动态上下文感知轨迹组成的混合轨迹锚点初始化规划器，扩散模型学习预测轨迹偏移分布进行细粒度优化。

Result: 在NAVSIM基准测试中达到新的最先进水平，展现出强大的泛化能力。

Conclusion: AnchDrive框架通过锚点引导的扩散策略，在保持高质量轨迹生成的同时显著提升了计算效率，为端到端自动驾驶规划提供了有效解决方案。

Abstract: End-to-end multi-modal planning has become a transformative paradigm in
autonomous driving, effectively addressing behavioral multi-modality and the
generalization challenge in long-tail scenarios. We propose AnchDrive, a
framework for end-to-end driving that effectively bootstraps a diffusion policy
to mitigate the high computational cost of traditional generative models.
Rather than denoising from pure noise, AnchDrive initializes its planner with a
rich set of hybrid trajectory anchors. These anchors are derived from two
complementary sources: a static vocabulary of general driving priors and a set
of dynamic, context-aware trajectories. The dynamic trajectories are decoded in
real-time by a Transformer that processes dense and sparse perceptual features.
The diffusion model then learns to refine these anchors by predicting a
distribution of trajectory offsets, enabling fine-grained refinement. This
anchor-based bootstrapping design allows for efficient generation of diverse,
high-quality trajectories. Experiments on the NAVSIM benchmark confirm that
AnchDrive sets a new state-of-the-art and shows strong gen?eralizability

</details>


### [50] [HL-IK: A Lightweight Implementation of Human-Like Inverse Kinematics in Humanoid Arms](https://arxiv.org/abs/2509.20263)
*Bingjie Chen,Zihan Wang,Zhe Han,Guoping Pan,Yi Cheng,Houde Liu*

Main category: cs.RO

TL;DR: HL-IK是一个轻量级逆运动学框架，通过学习的肘部先验来生成类人手臂配置，在保持末端执行器跟踪的同时提高运动的人性化程度。


<details>
  <summary>Details</summary>
Motivation: 传统逆运动学方法虽然能实现机械有效的配置，但往往缺乏人类般的自然性。本文旨在解决人形机器人运动不够人性化的问题。

Method: 使用大规模人体运动数据训练FiSTA网络预测肘部姿态，并将其作为残差项整合到Levenberg-Marquardt优化器中，形成可即插即用的数值IK解决方案。

Result: 在18.3万次仿真步骤中，HL-IK将手臂相似性位置和方向误差分别降低了30.6%和35.4%，在最具挑战性的轨迹上分别降低了42.2%和47.4%。硬件遥操作进一步验证了人性化程度的提升。

Conclusion: HL-IK简单易集成，通过提出的流程可跨平台适配，计算开销小，能够为人形机器人实现类人运动。

Abstract: Traditional IK methods for redundant humanoid manipulators emphasize
end-effector (EE) tracking, frequently producing configurations that are valid
mechanically but not human-like. We present Human-Like Inverse Kinematics
(HL-IK), a lightweight IK framework that preserves EE tracking while shaping
whole-arm configurations to appear human-like, without full-body sensing at
runtime. The key idea is a learned elbow prior: using large-scale human motion
data retargeted to the robot, we train a FiLM-modulated spatio-temporal
attention network (FiSTA) to predict the next-step elbow pose from the EE
target and a short history of EE-elbow states.This prediction is incorporated
as a small residual alongside EE and smoothness terms in a standard
Levenberg-Marquardt optimizer, making HL-IK a drop-in addition to numerical IK
stacks. Over 183k simulation steps, HL-IK reduces arm-similarity position and
direction error by 30.6% and 35.4% on average, and by 42.2% and 47.4% on the
most challenging trajectories. Hardware teleoperation on a robot distinct from
simulation further confirms the gains in anthropomorphism. HL-IK is simple to
integrate, adaptable across platforms via our pipeline, and adds minimal
computation, enabling human-like motions for humanoid robots. Project page:
https://hl-ik.github.io/

</details>


### [51] [Parse-Augment-Distill: Learning Generalizable Bimanual Visuomotor Policies from Single Human Video](https://arxiv.org/abs/2509.20286)
*Georgios Tziafas,Jiayun Zhang,Hamidreza Kasaei*

Main category: cs.RO

TL;DR: PAD框架通过解析人类视频为机器人可执行的关键点轨迹，利用任务和运动规划进行无模拟器的演示增强，并蒸馏为关键点条件策略，实现了从单个人类视频学习通用双手机器人策略。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需要大量遥操作数据收集且难以泛化到分布外场景的问题，同时避免模拟器引入的仿真到现实差距。

Method: 三步框架：(a)解析人类视频为机器人可执行的关键点-动作轨迹；(b)使用双手机器人任务和运动规划进行无模拟器的演示增强；(c)将增强轨迹蒸馏为关键点条件策略。

Result: PAD在六种真实世界双手机器人任务中优于依赖图像策略和模拟器的最先进方法，在成功率和样本/成本效率方面表现更优。

Conclusion: PAD框架能够从单个人类视频生成一次性策略，在未见过的空间布置、物体实例和背景干扰下具有良好的泛化能力。

Abstract: Learning visuomotor policies from expert demonstrations is an important
frontier in modern robotics research, however, most popular methods require
copious efforts for collecting teleoperation data and struggle to generalize
out-ofdistribution. Scaling data collection has been explored through
leveraging human videos, as well as demonstration augmentation techniques. The
latter approach typically requires expensive simulation rollouts and trains
policies with synthetic image data, therefore introducing a sim-to-real gap. In
parallel, alternative state representations such as keypoints have shown great
promise for category-level generalization. In this work, we bring these avenues
together in a unified framework: PAD (Parse-AugmentDistill), for learning
generalizable bimanual policies from a single human video. Our method relies on
three steps: (a) parsing a human video demo into a robot-executable
keypoint-action trajectory, (b) employing bimanual task-and-motion-planning to
augment the demonstration at scale without simulators, and (c) distilling the
augmented trajectories into a keypoint-conditioned policy. Empirically, we
showcase that PAD outperforms state-ofthe-art bimanual demonstration
augmentation works relying on image policies with simulation rollouts, both in
terms of success rate and sample/cost efficiency. We deploy our framework in
six diverse real-world bimanual tasks such as pouring drinks, cleaning trash
and opening containers, producing one-shot policies that generalize in unseen
spatial arrangements, object instances and background distractors.
Supplementary material can be found in the project webpage
https://gtziafas.github.io/PAD_project/.

</details>


### [52] [mindmap: Spatial Memory in Deep Feature Maps for 3D Action Policies](https://arxiv.org/abs/2509.20297)
*Remo Steiner,Alexander Millane,David Tingdahl,Clemens Volk,Vikram Ramasamy,Xinjie Yao,Peter Du,Soha Pouya,Shiwei Sheng*

Main category: cs.RO

TL;DR: 本文提出了mindmap方法，一种基于语义3D重建的3D扩散策略，用于解决机器人操作任务中的空间记忆问题。


<details>
  <summary>Details</summary>
Motivation: 在机器人操作任务中，相关物体需要进出机器人的视野范围，因此空间记忆能力对于完成任务至关重要。然而，在机器人学习系统中构建这种机制仍然是一个开放的研究问题。

Method: 提出了mindmap方法，这是一种3D扩散策略，基于环境的语义3D重建来生成机器人轨迹。该方法通过深度特征图实现空间记忆功能。

Result: 在仿真实验中，该方法能够有效解决那些缺乏记忆机制的最先进方法难以完成的任务。

Conclusion: 该方法为解决机器人学习系统中的空间记忆问题提供了有效方案，并发布了重建系统、训练代码和评估任务以促进该方向的研究。

Abstract: End-to-end learning of robot control policies, structured as neural networks,
has emerged as a promising approach to robotic manipulation. To complete many
common tasks, relevant objects are required to pass in and out of a robot's
field of view. In these settings, spatial memory - the ability to remember the
spatial composition of the scene - is an important competency. However,
building such mechanisms into robot learning systems remains an open research
problem. We introduce mindmap (Spatial Memory in Deep Feature Maps for 3D
Action Policies), a 3D diffusion policy that generates robot trajectories based
on a semantic 3D reconstruction of the environment. We show in simulation
experiments that our approach is effective at solving tasks where
state-of-the-art approaches without memory mechanisms struggle. We release our
reconstruction system, training code, and evaluation tasks to spur research in
this direction.

</details>


### [53] [VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation](https://arxiv.org/abs/2509.20322)
*Shaofeng Yin,Yanjie Ze,Hong-Xing Yu,C. Karen Liu,Jiajun Wu*

Main category: cs.RO

TL;DR: VisualMimic是一个视觉模拟到真实世界的框架，将自我中心视觉与分层全身控制相结合，实现人形机器人在非结构化环境中的零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖外部运动捕捉系统，要么无法在不同任务间泛化。需要一种能够整合自我中心感知和全身控制的方法来解决人形机器人在非结构化环境中的定位操作问题。

Method: 结合任务无关的低级关键点跟踪器（通过师生方案从人类运动数据训练）和任务特定的高级策略（从视觉和本体感觉输入生成关键点命令）。通过向低级策略注入噪声和使用人类运动统计数据裁剪高级动作来确保训练稳定性。

Result: 实现了从模拟到真实人形机器人的零样本迁移，完成了多种定位操作任务（如箱子举升、推动、足球运球和踢球），并在室外环境中表现出强大的泛化能力。

Conclusion: VisualMimic框架成功地将视觉感知与分层控制相结合，为人形机器人在复杂环境中的自主操作提供了有效的解决方案。

Abstract: Humanoid loco-manipulation in unstructured environments demands tight
integration of egocentric perception and whole-body control. However, existing
approaches either depend on external motion capture systems or fail to
generalize across diverse tasks. We introduce VisualMimic, a visual sim-to-real
framework that unifies egocentric vision with hierarchical whole-body control
for humanoid robots. VisualMimic combines a task-agnostic low-level keypoint
tracker -- trained from human motion data via a teacher-student scheme -- with
a task-specific high-level policy that generates keypoint commands from visual
and proprioceptive input. To ensure stable training, we inject noise into the
low-level policy and clip high-level actions using human motion statistics.
VisualMimic enables zero-shot transfer of visuomotor policies trained in
simulation to real humanoid robots, accomplishing a wide range of
loco-manipulation tasks such as box lifting, pushing, football dribbling, and
kicking. Beyond controlled laboratory settings, our policies also generalize
robustly to outdoor environments. Videos are available at:
https://visualmimic.github.io .

</details>


### [54] [BBoE: Leveraging Bundle of Edges for Kinodynamic Bidirectional Motion Planning](https://arxiv.org/abs/2509.20333)
*Srikrishna Bangalore Raghu,Alessandro Roncone*

Main category: cs.RO

TL;DR: BBoE是一种双向、动力学感知的采样运动规划器，能在不同障碍物密度的环境中快速找到低成本解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂障碍物环境中快速生成可行且低成本运动轨迹的问题，提高规划效率和成功率。

Method: 结合探索和利用策略，使用预计算的机器人状态遍历，通过排序和序列化预处理的前向传播来导航障碍物密集空间。

Result: 相比现有方法，BBoE减少了规划时间，降低了解决方案成本，并提高了成功率。

Conclusion: BBoE是一个鲁棒的双向动力学规划器，能够高效生成快速可行的运动解决方案。

Abstract: In this work, we introduce BBoE, a bidirectional, kinodynamic, sampling-based
motion planner that consistently and quickly finds low-cost solutions in
environments with varying obstacle clutter. The algorithm combines exploration
and exploitation while relying on precomputed robot state traversals, resulting
in efficient convergence towards the goal. Our key contributions include: i) a
strategy to navigate through obstacle-rich spaces by sorting and sequencing
preprocessed forward propagations; and ii) BBoE, a robust bidirectional
kinodynamic planner that utilizes this strategy to produce fast and feasible
solutions. The proposed framework reduces planning time, diminishes solution
cost and increases success rate in comparison to previous approaches.

</details>
