{"id": "2512.11802", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11802", "abs": "https://arxiv.org/abs/2512.11802", "authors": ["Zheng Li", "Peng Zhang", "Shixiao Liang", "Hang Zhou", "Chengyuan Ma", "Handong Yao", "Qianwen Li", "Xiaopeng Li"], "title": "Benchmarking Tesla's Traffic Light and Stop Sign Control: Field Dataset and Behavior Insights", "comment": null, "summary": "Understanding how Advanced Driver-Assistance Systems (ADAS) interact with Traffic Control Devices (TCDs) is critical for assessing their influence on traffic operations, yet this interaction has received little focused empirical study. This paper presents a field dataset and behavioral analysis of Tesla's Traffic Light and Stop Sign Control (TLSSC), a mature ADAS that perceives traffic lights and stop signs. We design and execute experiments across varied speed limits and TCD types, collecting synchronized high-resolution vehicle trajectory data and driver-perspective video. From these data, we develop a taxonomy of TLSSC-TCD interaction behaviors (i.e., stopping, accelerating, and car following) and calibrate the Full Velocity Difference Model (FVDM) to quantitatively characterize each behavior mode. A novel empirical insight is the identification of a car-following threshold (~90 m). Calibration results reveal that stopping behavior is driven by strong responsiveness to both desired speed deviation and relative speed, whereas accelerating behavior is more conservative. Intersection car-following behavior exhibits smoother dynamics and tighter headways compared to standard car-following behaviors. The established dataset, behavior definitions, and model characterizations together provide a foundation for future simulation, safety evaluation, and design of ADAS-TCD interaction logic. Our dataset is available at GitHub.", "AI": {"tldr": "\u7814\u7a76\u7279\u65af\u62c9\u4ea4\u901a\u706f\u548c\u505c\u8f66\u6807\u5fd7\u63a7\u5236\u7cfb\u7edf(TLSSC)\u4e0e\u4ea4\u901a\u63a7\u5236\u8bbe\u5907(TCD)\u7684\u4ea4\u4e92\u884c\u4e3a\uff0c\u901a\u8fc7\u5b9e\u5730\u5b9e\u9a8c\u6536\u96c6\u6570\u636e\uff0c\u5efa\u7acb\u884c\u4e3a\u5206\u7c7b\u5e76\u6821\u51c6FVDM\u6a21\u578b\uff0c\u53d1\u73b0\u8ddf\u8f66\u9608\u503c\u7ea690\u7c73\u7b49\u65b0\u89c1\u89e3\u3002", "motivation": "\u9ad8\u7ea7\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf(ADAS)\u4e0e\u4ea4\u901a\u63a7\u5236\u8bbe\u5907(TCD)\u7684\u4ea4\u4e92\u5bf9\u4ea4\u901a\u8fd0\u884c\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8fd9\u4e00\u9886\u57df\u7f3a\u4e4f\u6df1\u5165\u7684\u5b9e\u8bc1\u7814\u7a76\u3002\u7279\u65af\u62c9\u7684TLSSC\u7cfb\u7edf\u4f5c\u4e3a\u6210\u719f\u7684ADAS\uff0c\u5176\u4e0eTCD\u7684\u4ea4\u4e92\u884c\u4e3a\u9700\u8981\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u8bbe\u8ba1\u5e76\u6267\u884c\u4e0d\u540c\u9650\u901f\u548cTCD\u7c7b\u578b\u7684\u5b9e\u9a8c\uff0c\u6536\u96c6\u540c\u6b65\u7684\u9ad8\u5206\u8fa8\u7387\u8f66\u8f86\u8f68\u8ff9\u6570\u636e\u548c\u9a7e\u9a76\u5458\u89c6\u89d2\u89c6\u9891\u3002\u57fa\u4e8e\u6570\u636e\u5efa\u7acbTLSSC-TCD\u4ea4\u4e92\u884c\u4e3a\u5206\u7c7b\uff08\u505c\u8f66\u3001\u52a0\u901f\u3001\u8ddf\u8f66\uff09\uff0c\u5e76\u6821\u51c6\u5168\u901f\u5ea6\u5dee\u6a21\u578b(FVDM)\u6765\u5b9a\u91cf\u8868\u5f81\u6bcf\u79cd\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0\u8ddf\u8f66\u9608\u503c\u7ea690\u7c73\u7684\u65b0\u7ecf\u9a8c\u89c1\u89e3\u3002\u6821\u51c6\u7ed3\u679c\u663e\u793a\uff1a\u505c\u8f66\u884c\u4e3a\u5bf9\u671f\u671b\u901f\u5ea6\u504f\u5dee\u548c\u76f8\u5bf9\u901f\u5ea6\u90fd\u6709\u5f3a\u70c8\u54cd\u5e94\uff1b\u52a0\u901f\u884c\u4e3a\u66f4\u4e3a\u4fdd\u5b88\uff1b\u4ea4\u53c9\u53e3\u8ddf\u8f66\u884c\u4e3a\u6bd4\u6807\u51c6\u8ddf\u8f66\u884c\u4e3a\u5177\u6709\u66f4\u5e73\u6ed1\u7684\u52a8\u6001\u7279\u6027\u548c\u66f4\u5c0f\u7684\u8f66\u5934\u65f6\u8ddd\u3002", "conclusion": "\u5efa\u7acb\u7684\u6570\u636e\u96c6\u3001\u884c\u4e3a\u5b9a\u4e49\u548c\u6a21\u578b\u8868\u5f81\u4e3a\u672a\u6765ADAS-TCD\u4ea4\u4e92\u903b\u8f91\u7684\u4eff\u771f\u3001\u5b89\u5168\u8bc4\u4f30\u548c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002\u6570\u636e\u96c6\u5df2\u5728GitHub\u4e0a\u516c\u5f00\u3002"}}
{"id": "2512.11824", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11824", "abs": "https://arxiv.org/abs/2512.11824", "authors": ["Rosh Ho", "Jian Zhang"], "title": "ReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision", "comment": null, "summary": "This paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves \\SI{96.73}{\\percent} grasp classification accuracy with sub-\\SI{40.00}{\\milli\\second} end-to-end latency. Physical validation using standardized benchmarks shows \\SI{82.71}{\\percent} success on YCB object manipulation and reliable performance across \\SI{27.00}{} Activities of Daily Living (ADL) tasks. With a total cost under \\$\\SI{250.00}{} and exclusively commercial components, ReGlove provides a technical foundation for accessible, vision-based upper-limb assistance that could benefit populations excluded from traditional EMG-controlled devices.", "AI": {"tldr": "ReGlove\u5c06\u4f4e\u6210\u672c\u5546\u4e1a\u6c14\u52a8\u5eb7\u590d\u624b\u5957\u6539\u9020\u4e3a\u89c6\u89c9\u5f15\u5bfc\u8f85\u52a9\u77eb\u5f62\u5668\uff0c\u901a\u8fc7\u624b\u8155\u6444\u50cf\u5934\u548c\u8fb9\u7f18\u8ba1\u7b97\u5b9e\u73b0\u65e0\u9700\u53ef\u9760\u808c\u8089\u4fe1\u53f7\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u6293\u63e1\uff0c\u6210\u672c\u4f4e\u4e8e250\u7f8e\u5143\u3002", "motivation": "\u6162\u6027\u4e0a\u80a2\u969c\u788d\u5f71\u54cd\u5168\u7403\u6570\u767e\u4e07\u4eba\uff0c\u73b0\u6709\u8f85\u52a9\u6280\u672f\u8981\u4e48\u8fc7\u4e8e\u6602\u8d35\uff0c\u8981\u4e48\u4f9d\u8d56\u4e0d\u53ef\u9760\u7684\u751f\u7269\u4fe1\u53f7\uff0c\u9700\u8981\u66f4\u7ecf\u6d4e\u3001\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6574\u5408\u624b\u8155\u6444\u50cf\u5934\u4e0e\u8fb9\u7f18\u8ba1\u7b97\u63a8\u7406\u5f15\u64ce\uff08Raspberry Pi 5\uff09\uff0c\u91c7\u7528\u5b9e\u65f6YOLO\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\uff0c\u5c06\u4f4e\u6210\u672c\u5546\u4e1a\u6c14\u52a8\u5eb7\u590d\u624b\u5957\u6539\u9020\u4e3a\u89c6\u89c9\u5f15\u5bfc\u8f85\u52a9\u7cfb\u7edf\u3002", "result": "\u5b9e\u73b096.73%\u7684\u6293\u63e1\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u4f4e\u4e8e40\u6beb\u79d2\uff1b\u5728YCB\u7269\u4f53\u64cd\u4f5c\u6d4b\u8bd5\u4e2d\u8fbe\u523082.71%\u6210\u529f\u7387\uff0c\u572827\u9879\u65e5\u5e38\u751f\u6d3b\u6d3b\u52a8\u4efb\u52a1\u4e2d\u8868\u73b0\u53ef\u9760\u3002", "conclusion": "ReGlove\u4e3a\u57fa\u4e8e\u89c6\u89c9\u7684\u4e0a\u80a2\u8f85\u52a9\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\uff0c\u6210\u672c\u4f4e\u4e8e250\u7f8e\u5143\uff0c\u4f7f\u7528\u5168\u5546\u4e1a\u7ec4\u4ef6\uff0c\u53ef\u60e0\u53ca\u88ab\u4f20\u7edfEMG\u63a7\u5236\u8bbe\u5907\u6392\u9664\u5728\u5916\u7684\u4eba\u7fa4\u3002"}}
{"id": "2512.11872", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11872", "abs": "https://arxiv.org/abs/2512.11872", "authors": ["Mingwang Xu", "Jiahao Cui", "Feipeng Cai", "Hanlin Shang", "Zhihao Zhu", "Shan Luan", "Yifang Xu", "Neng Zhang", "Yaoyi Li", "Jia Cai", "Siyu Zhu"], "title": "WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff", "AI": {"tldr": "WAM-Diff\u662f\u4e00\u4e2a\u57fa\u4e8e\u63a9\u7801\u6269\u6563\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u91c7\u7528\u79bb\u6563\u5e8f\u5217\u8fed\u4ee3\u4f18\u5316\u672a\u6765\u8f68\u8ff9\uff0c\u5728NAVSIM\u57fa\u51c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e3b\u8981\u4f7f\u7528\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8fde\u7eed\u6269\u6563\u7b56\u7565\uff0c\u800c\u79bb\u6563\u63a9\u7801\u6269\u6563\u5728\u8f68\u8ff9\u751f\u6210\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u63a9\u7801\u6269\u6563\u4f5c\u4e3a\u81ea\u56de\u5f52\u548c\u6269\u6563\u7b56\u7565\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faWAM-Diff\u6846\u67b6\uff1a1\uff09\u7cfb\u7edf\u5730\u5c06\u63a9\u7801\u6269\u6563\u9002\u914d\u5230\u81ea\u52a8\u9a7e\u9a76\uff0c\u652f\u6301\u7075\u6d3b\u7684\u975e\u56e0\u679c\u89e3\u7801\u987a\u5e8f\uff1b2\uff09\u901a\u8fc7\u7a00\u758fMoE\u67b6\u6784\u8054\u5408\u8bad\u7ec3\u8fd0\u52a8\u9884\u6d4b\u548c\u9a7e\u9a76\u5bfc\u5411\u7684\u89c6\u89c9\u95ee\u7b54\uff1b3\uff09\u4f7f\u7528GSPO\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5e8f\u5217\u7ea7\u9a7e\u9a76\u5956\u52b1\u3002", "result": "\u5728NAVSIM-v1\u4e0a\u8fbe\u523091.0 PDMS\uff0c\u5728NAVSIM-v2\u4e0a\u8fbe\u523089.7 EPDMS\uff0c\u8bc1\u660e\u4e86\u63a9\u7801\u6269\u6563\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63a9\u7801\u6269\u6563\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f68\u8ff9\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u652f\u6301\u573a\u666f\u611f\u77e5\u7684\u89e3\u7801\u7b56\u7565\uff0c\u662f\u81ea\u56de\u5f52\u548c\u6269\u6563\u7b56\u7565\u7684\u6709\u6548\u8865\u5145\u3002"}}
{"id": "2512.11873", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11873", "abs": "https://arxiv.org/abs/2512.11873", "authors": ["Antonia Yepes", "Marie Charbonneau"], "title": "Audio-Based Tactile Human-Robot Interaction Recognition", "comment": "1 page, 1 figure, 1 table", "summary": "This study explores the use of microphones placed on a robot's body to detect tactile interactions via sounds produced when the hard shell of the robot is touched. This approach is proposed as an alternative to traditional methods using joint torque sensors or 6-axis force/torque sensors. Two Adafruit I2S MEMS microphones integrated with a Raspberry Pi 4 were positioned on the torso of a Pollen Robotics Reachy robot to capture audio signals from various touch types on the robot arms (tapping, knocking, rubbing, stroking, scratching, and pressing). A convolutional neural network was trained for touch classification on a dataset of 336 pre-processed samples (48 samples per touch type). The model shows high classification accuracy between touch types with distinct acoustic dominant frequencies.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u4eba\u8eab\u4f53\u4e0a\u7684\u9ea6\u514b\u98ce\u901a\u8fc7\u89e6\u6478\u786c\u58f3\u4ea7\u751f\u7684\u58f0\u97f3\u6765\u68c0\u6d4b\u89e6\u89c9\u4ea4\u4e92\uff0c\u4f5c\u4e3a\u4f20\u7edf\u5173\u8282\u626d\u77e9\u62166\u8f74\u529b/\u626d\u77e9\u4f20\u611f\u5668\u7684\u66ff\u4ee3\u65b9\u6848", "motivation": "\u4f20\u7edf\u89e6\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff08\u5982\u5173\u8282\u626d\u77e9\u4f20\u611f\u5668\u62166\u8f74\u529b/\u626d\u77e9\u4f20\u611f\u5668\uff09\u6210\u672c\u9ad8\u4e14\u590d\u6742\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u7b80\u5355\u3001\u6210\u672c\u66f4\u4f4e\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u5728\u673a\u5668\u4eba\u8eaf\u5e72\u4e0a\u5b89\u88c5\u4e24\u4e2aAdafruit I2S MEMS\u9ea6\u514b\u98ce\uff0c\u901a\u8fc7Raspberry Pi 4\u91c7\u96c6\u673a\u5668\u4eba\u624b\u81c2\u4e0a\u4e0d\u540c\u89e6\u6478\u7c7b\u578b\uff08\u6572\u51fb\u3001\u8f7b\u6572\u3001\u6469\u64e6\u3001\u629a\u6478\u3001\u6293\u6320\u3001\u6309\u538b\uff09\u4ea7\u751f\u7684\u58f0\u97f3\u4fe1\u53f7\uff0c\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5bf9336\u4e2a\u9884\u5904\u7406\u6837\u672c\u8fdb\u884c\u5206\u7c7b", "result": "\u6a21\u578b\u5728\u5177\u6709\u660e\u663e\u58f0\u5b66\u4e3b\u9891\u7684\u89e6\u6478\u7c7b\u578b\u4e4b\u95f4\u663e\u793a\u51fa\u9ad8\u5206\u7c7b\u51c6\u786e\u7387", "conclusion": "\u57fa\u4e8e\u58f0\u97f3\u7684\u89e6\u89c9\u68c0\u6d4b\u662f\u4f20\u7edf\u89e6\u89c9\u4f20\u611f\u5668\u7684\u53ef\u884c\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u660e\u663e\u58f0\u5b66\u7279\u5f81\u7684\u89e6\u6478\u7c7b\u578b\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u826f\u597d"}}
{"id": "2512.11811", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11811", "abs": "https://arxiv.org/abs/2512.11811", "authors": ["Fengyi Xu", "Jun Ma", "Waishan Qiu", "Cui Guo"], "title": "Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention", "comment": null, "summary": "Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.", "AI": {"tldr": "VPR-AttLLM\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u6846\u67b6\uff0c\u901a\u8fc7LLM\u7684\u8bed\u4e49\u63a8\u7406\u548c\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u589e\u5f3a\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\uff0c\u63d0\u5347\u793e\u4ea4\u5a92\u4f53\u6d2a\u6c34\u56fe\u50cf\u68c0\u7d22\u6027\u80fd\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u8857\u666f\u56fe\u50cf\u63d0\u4f9b\u5b9e\u65f6\u57ce\u5e02\u6d2a\u6c34\u7b49\u5371\u673a\u4e8b\u4ef6\u7684\u89c6\u89c9\u8bc1\u636e\uff0c\u4f46\u7f3a\u4e4f\u53ef\u9760\u7684\u5730\u7406\u5143\u6570\u636e\u3002\u73b0\u6709\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u6a21\u578b\u5728\u8de8\u6e90\u573a\u666f\u4e2d\u56e0\u89c6\u89c9\u626d\u66f2\u548c\u9886\u57df\u504f\u79fb\u800c\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u63d0\u51faVPR-AttLLM\u6846\u67b6\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u63a8\u7406\u548c\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u63cf\u8ff0\u7b26\u589e\u5f3a\u96c6\u6210\u5230\u73b0\u6709VPR\u6d41\u7a0b\u4e2d\uff0c\u5229\u7528LLM\u8bc6\u522b\u57ce\u5e02\u80cc\u666f\u4e2d\u7684\u4f4d\u7f6e\u4fe1\u606f\u533a\u57df\u5e76\u6291\u5236\u77ac\u6001\u89c6\u89c9\u566a\u58f0\u3002", "result": "\u5728\u6269\u5c55\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5c06VPR-AttLLM\u4e0e\u4e09\u79cdSOTA VPR\u6a21\u578b\u96c6\u6210\uff0c\u53ec\u56de\u6027\u80fd\u4e00\u81f4\u63d0\u5347\uff0c\u76f8\u5bf9\u589e\u76ca\u901a\u5e38\u57281-3%\uff0c\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u771f\u5b9e\u6d2a\u6c34\u56fe\u50cf\u4e0a\u53ef\u8fbe8%\u3002", "conclusion": "VPR-AttLLM\u4e3aLLM\u5f15\u5bfc\u7684\u591a\u6a21\u6001\u878d\u5408\u5728\u89c6\u89c9\u68c0\u7d22\u7cfb\u7edf\u4e2d\u5efa\u7acb\u4e86\u53ef\u63a8\u5e7f\u8303\u5f0f\uff0c\u5c06\u57ce\u5e02\u611f\u77e5\u7406\u8bba\u5d4c\u5165\u6ce8\u610f\u529b\u673a\u5236\uff0c\u8fde\u63a5\u4eba\u7c7b\u7a7a\u95f4\u63a8\u7406\u4e0e\u73b0\u4ee3VPR\u67b6\u6784\uff0c\u5177\u6709\u5373\u63d2\u5373\u7528\u8bbe\u8ba1\u3001\u5f3a\u8de8\u6e90\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2512.11876", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11876", "abs": "https://arxiv.org/abs/2512.11876", "authors": ["Hrigved Mahesh Suryawanshi"], "title": "Traversability Aware Autonomous Navigation for Multi-Modal Mobility Morphobot (M4)", "comment": null, "summary": "Autonomous navigation in unstructured environments requires robots to assess terrain difficulty in real-time and plan paths that balance efficiency with safety. This thesis presents a traversability-aware navigation framework for the M4 robot platform that uses learned terrain analysis to generate energy-efficient paths avoiding difficult terrain.Our approach uses FAST-LIO for real-time localization, generating 2.5D elevation maps from LiDAR point clouds. A CNN-based model processes these elevation maps to estimate traversability scores, which are converted into navigation costs for path planning. A custom A* planner incorporates these costs alongside geometric distance and energy consumption to find paths that trade modest distance increases for substantial terrain quality improvements. Before system development, a platform-agnostic study compared LiDAR-based and camera-based SLAM using OptiTrack ground truth. Point cloud comparison through ICP alignment and cloud-to-mesh distance analysis demonstrated that LiDAR-based mapping achieves centimeter-level precision essential for elevation mapping, while camera-based approaches exhibited significantly higher geometric error. These findings directly resulted in the selection of LiDAR as the primary sensor to generate elevation maps. The complete pipeline integrates FAST-LIO localization, GPU-accelerated elevation mapping, CNN-based traversability estimation, and Nav2 navigation with a custom traversability-aware planner. Experimental results demonstrate that the system successfully avoids low traversability regions and accepts a few longer paths to achieve a reduction in terrain cost. This work establishes a foundation for intelligent terrain-aware navigation applicable to multi-modal robotic platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u53ef\u901a\u884c\u6027\u611f\u77e5\u5bfc\u822a\u6846\u67b6\uff0c\u7528\u4e8eM4\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u901a\u8fc7LiDAR\u751f\u62102.5D\u9ad8\u7a0b\u56fe\uff0cCNN\u8bc4\u4f30\u5730\u5f62\u96be\u5ea6\uff0cA*\u89c4\u5212\u5668\u7ed3\u5408\u5730\u5f62\u6210\u672c\u548c\u80fd\u8017\u89c4\u5212\u8def\u5f84\uff0c\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5e73\u8861\u6548\u7387\u4e0e\u5b89\u5168\u3002", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u5b9e\u65f6\u8bc4\u4f30\u5730\u5f62\u96be\u5ea6\uff0c\u89c4\u5212\u65e2\u80fd\u4fdd\u8bc1\u5b89\u5168\u53c8\u9ad8\u6548\u7684\u8def\u5f84\u3002\u73b0\u6709\u5bfc\u822a\u65b9\u6cd5\u5f80\u5f80\u53ea\u8003\u8651\u51e0\u4f55\u8ddd\u79bb\uff0c\u5ffd\u7565\u4e86\u5730\u5f62\u53ef\u901a\u884c\u6027\u548c\u80fd\u8017\u56e0\u7d20\uff0c\u5bfc\u81f4\u5728\u5b9e\u9645\u590d\u6742\u5730\u5f62\u4e2d\u6027\u80fd\u4e0d\u4f73\u3002", "method": "1) \u4f7f\u7528FAST-LIO\u8fdb\u884c\u5b9e\u65f6\u5b9a\u4f4d\uff1b2) \u4eceLiDAR\u70b9\u4e91\u751f\u62102.5D\u9ad8\u7a0b\u56fe\uff1b3) \u57fa\u4e8eCNN\u7684\u6a21\u578b\u5904\u7406\u9ad8\u7a0b\u56fe\u4f30\u8ba1\u53ef\u901a\u884c\u6027\u5206\u6570\uff1b4) \u5c06\u53ef\u901a\u884c\u6027\u5206\u6570\u8f6c\u6362\u4e3a\u5bfc\u822a\u6210\u672c\uff1b5) \u5b9a\u5236A*\u89c4\u5212\u5668\u7ed3\u5408\u51e0\u4f55\u8ddd\u79bb\u3001\u80fd\u8017\u548c\u5730\u5f62\u6210\u672c\u89c4\u5212\u8def\u5f84\uff1b6) \u524d\u671f\u8fdb\u884c\u4e86LiDAR\u4e0e\u76f8\u673aSLAM\u7684\u5bf9\u6bd4\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86LiDAR\u7684\u5398\u7c73\u7ea7\u7cbe\u5ea6\u4f18\u52bf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7cfb\u7edf\u6210\u529f\u907f\u5f00\u4e86\u4f4e\u53ef\u901a\u884c\u6027\u533a\u57df\uff0c\u867d\u7136\u8def\u5f84\u957f\u5ea6\u7565\u6709\u589e\u52a0\uff0c\u4f46\u663e\u8457\u964d\u4f4e\u4e86\u5730\u5f62\u6210\u672c\u3002LiDAR-based mapping\u8fbe\u5230\u5398\u7c73\u7ea7\u7cbe\u5ea6\uff0c\u800c\u76f8\u673a\u65b9\u6cd5\u8bef\u5dee\u8f83\u5927\u3002\u5b8c\u6574\u7cfb\u7edf\u96c6\u6210\u4e86FAST-LIO\u5b9a\u4f4d\u3001GPU\u52a0\u901f\u9ad8\u7a0b\u6620\u5c04\u3001CNN\u53ef\u901a\u884c\u6027\u4f30\u8ba1\u548cNav2\u5bfc\u822a\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u667a\u80fd\u5730\u5f62\u611f\u77e5\u5bfc\u822a\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u80fd\u591f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u81ea\u4e3b\u5bfc\u822a\uff0c\u901a\u8fc7\u63a5\u53d7\u9002\u5ea6\u8def\u5f84\u957f\u5ea6\u589e\u52a0\u6765\u663e\u8457\u6539\u5584\u5730\u5f62\u8d28\u91cf\u3002"}}
{"id": "2512.11816", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11816", "abs": "https://arxiv.org/abs/2512.11816", "authors": ["Enes \u00d6zeren", "Matthias A\u00dfenmacher"], "title": "Reinforcement Learning for Latent-Space Thinking in LLMs", "comment": "16 pages, 16 figures, 7 tables", "summary": "Chain-of-Thought (CoT) reasoning typically utilizes the discrete language space for thinking, which is inherently inefficient, as many generated tokens only enforce linguistic rules that are not required for reasoning. To bypass this, latent-space thinking allows models to think using the continuous embedding space. While existing methods for training those models show domain-specific gains, they fail to maintain performance in complex tasks, such as mathematical reasoning. We experimentally demonstrate that the Coconut approach, a form of supervised fine-tuning for latent-space thinking, is highly sensitive to design choices and exhibits several inherent limitations. To address these issues, we investigate reinforcement learning (RL) techniques -- an underexplored direction in latent-space thinking -- including GRPO and design a novel Latent RL method for directly optimizing the latent thinking steps. Our experimental results reveal that these RL-trained models still lag behind traditional language-space CoT models in the mathematical reasoning domain. We make our codebase publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6f5c\u5728\u7a7a\u95f4\u601d\u7ef4\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u53d1\u73b0\u5373\u4f7f\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u4ecd\u65e0\u6cd5\u8d85\u8d8a\u4f20\u7edf\u7684\u8bed\u8a00\u7a7a\u95f4\u601d\u7ef4\u94fe\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u601d\u7ef4\u94fe\u65b9\u6cd5\u4f7f\u7528\u79bb\u6563\u8bed\u8a00\u7a7a\u95f4\u8fdb\u884c\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u4e3a\u8bb8\u591a\u751f\u6210\u7684token\u53ea\u662f\u6267\u884c\u8bed\u8a00\u89c4\u5219\u800c\u975e\u63a8\u7406\u6240\u9700\u3002\u6f5c\u5728\u7a7a\u95f4\u601d\u7ef4\u4f7f\u7528\u8fde\u7eed\u5d4c\u5165\u7a7a\u95f4\u8fdb\u884c\u601d\u8003\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u4efb\u52a1\uff08\u5982\u6570\u5b66\u63a8\u7406\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u7814\u7a76\u4e86\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5Coconut\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u7d22\u4e86\u5f3a\u5316\u5b66\u4e60\u6280\u672f\uff08\u5305\u62ecGRPO\u548c\u8bbe\u8ba1\u7684\u65b0\u578bLatent RL\u65b9\u6cd5\uff09\u6765\u76f4\u63a5\u4f18\u5316\u6f5c\u5728\u601d\u7ef4\u6b65\u9aa4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5728\u6570\u5b66\u63a8\u7406\u9886\u57df\u4ecd\u7136\u843d\u540e\u4e8e\u4f20\u7edf\u7684\u8bed\u8a00\u7a7a\u95f4\u601d\u7ef4\u94fe\u6a21\u578b\u3002", "conclusion": "\u6f5c\u5728\u7a7a\u95f4\u601d\u7ef4\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u4ecd\u5b58\u5728\u6839\u672c\u6027\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002"}}
{"id": "2512.11886", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11886", "abs": "https://arxiv.org/abs/2512.11886", "authors": ["Mohammed Irfan Ali"], "title": "Enabling Autonomous Navigation in a Snake Robot through Visual-Inertial Odometry and Closed-Loop Trajectory Tracking Control", "comment": null, "summary": "Snake robots offer exceptional mobility across extreme terrain inaccessible to conventional rovers, yet their highly articulated bodies present fundamental challenges for autonomous navigation in environments lacking external tracking infrastructure. This thesis develops a complete autonomy pipeline for COBRA, an 11 degree-of-freedom modular snake robot designed for planetary exploration. While the robot's biologically inspired serpentine gaits achieve impressive mobility, prior work has relied entirely on open-loop teleoperation. This approach integrates onboard visual-inertial SLAM, reduced-order state estimation, and closed-loop trajectory tracking to enable autonomous waypoint navigation. A depth camera paired with edge computing performs real-time localization during dynamic locomotion, validated against motion-capture ground truth to characterize drift behavior and failure modes unique to snake robot platforms. A reduced-order framework estimates Center-of-Mass pose, driving a closed-loop controller that modulates CPG gait parameters through distance-dependent yaw error blending. Physical experiments validate the complete system, demonstrating accurate multi-waypoint tracking and establishing foundations for autonomous snake robot navigation.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u7528\u4e8e\u86c7\u5f62\u673a\u5668\u4ebaCOBRA\u7684\u5b8c\u6574\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\uff0c\u7ed3\u5408\u89c6\u89c9\u60ef\u6027SLAM\u3001\u964d\u9636\u72b6\u6001\u4f30\u8ba1\u548c\u95ed\u73af\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u5b9e\u73b0\u81ea\u4e3b\u822a\u70b9\u5bfc\u822a", "motivation": "\u86c7\u5f62\u673a\u5668\u4eba\u5728\u6781\u7aef\u5730\u5f62\u4e2d\u5177\u6709\u5353\u8d8a\u7684\u673a\u52a8\u6027\uff0c\u4f46\u9ad8\u5ea6\u5173\u8282\u5316\u7684\u8eab\u4f53\u5728\u6ca1\u6709\u5916\u90e8\u8ddf\u8e2a\u57fa\u7840\u8bbe\u65bd\u7684\u73af\u5883\u4e2d\u9762\u4e34\u81ea\u4e3b\u5bfc\u822a\u7684\u57fa\u672c\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u5b8c\u5168\u4f9d\u8d56\u5f00\u73af\u9065\u64cd\u4f5c\uff0c\u9700\u8981\u5f00\u53d1\u5b8c\u6574\u7684\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf", "method": "\u96c6\u6210\u673a\u8f7d\u89c6\u89c9\u60ef\u6027SLAM\u3001\u964d\u9636\u72b6\u6001\u4f30\u8ba1\u548c\u95ed\u73af\u8f68\u8ff9\u8ddf\u8e2a\u3002\u4f7f\u7528\u6df1\u5ea6\u76f8\u673a\u548c\u8fb9\u7f18\u8ba1\u7b97\u8fdb\u884c\u5b9e\u65f6\u5b9a\u4f4d\uff0c\u901a\u8fc7\u964d\u9636\u6846\u67b6\u4f30\u8ba1\u8d28\u5fc3\u4f4d\u59ff\uff0c\u91c7\u7528\u95ed\u73af\u63a7\u5236\u5668\u901a\u8fc7\u8ddd\u79bb\u76f8\u5173\u7684\u504f\u822a\u8bef\u5dee\u6df7\u5408\u8c03\u5236CPG\u6b65\u6001\u53c2\u6570", "result": "\u7269\u7406\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b8c\u6574\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u7cbe\u786e\u7684\u591a\u822a\u70b9\u8ddf\u8e2a\u80fd\u529b\uff0c\u4e3a\u86c7\u5f62\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u7cfb\u7edf\u901a\u8fc7\u8fd0\u52a8\u6355\u6349\u5730\u9762\u771f\u5b9e\u9a8c\u8bc1\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u6f02\u79fb\u884c\u4e3a", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u86c7\u5f62\u673a\u5668\u4ebaCOBRA\u7684\u5b8c\u6574\u81ea\u4e3b\u5bfc\u822a\u7ba1\u9053\uff0c\u96c6\u6210\u4e86SLAM\u3001\u72b6\u6001\u4f30\u8ba1\u548c\u95ed\u73af\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u81ea\u4e3b\u822a\u70b9\u5bfc\u822a\uff0c\u4e3a\u884c\u661f\u63a2\u7d22\u7b49\u5e94\u7528\u4e2d\u7684\u86c7\u5f62\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2512.11849", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11849", "abs": "https://arxiv.org/abs/2512.11849", "authors": ["Nimol Thuon", "Jun Du"], "title": "KH-FUNSD: A Hierarchical and Fine-Grained Layout Analysis Dataset for Low-Resource Khmer Business Document", "comment": null, "summary": "Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present \\textbf{KH-FUNSD}, the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.", "AI": {"tldr": "KH-FUNSD\u662f\u9996\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u9ad8\u68c9\u8bed\u8868\u5355\u6587\u6863\u7406\u89e3\u6570\u636e\u96c6\uff0c\u5305\u542b\u6536\u636e\u3001\u53d1\u7968\u548c\u62a5\u4ef7\u5355\uff0c\u91c7\u7528\u4e09\u7ea7\u6807\u6ce8\u6846\u67b6\u652f\u6301\u5e03\u5c40\u5206\u6790\u548c\u4fe1\u606f\u63d0\u53d6\u3002", "motivation": "\u9ad8\u68c9\u8bed\u4f5c\u4e3a\u67ec\u57d4\u5be8\u8d85\u8fc71700\u4e07\u4eba\u4f7f\u7528\u7684\u8bed\u8a00\uff0c\u5728\u6587\u6863AI\u5de5\u5177\u5f00\u53d1\u4e2d\u53d7\u5230\u8f83\u5c11\u5173\u6ce8\uff0c\u7279\u522b\u662f\u5546\u4e1a\u6587\u6863\u8d44\u6e90\u4e25\u91cd\u7f3a\u4e4f\uff0c\u800c\u8fd9\u7c7b\u6587\u6863\u5bf9\u516c\u5171\u7ba1\u7406\u548c\u79c1\u8425\u4f01\u4e1a\u90fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e09\u7ea7\u6807\u6ce8\u6846\u67b6\uff1a1)\u533a\u57df\u68c0\u6d4b\uff08\u5212\u5206\u6587\u6863\u6838\u5fc3\u533a\u57df\u5982\u9875\u7709\u3001\u8868\u5355\u5b57\u6bb5\u3001\u9875\u811a\uff09\uff1b2)FUNSD\u98ce\u683c\u6807\u6ce8\uff08\u533a\u5206\u95ee\u9898\u3001\u7b54\u6848\u3001\u6807\u9898\u7b49\u5b9e\u4f53\u53ca\u5176\u5173\u7cfb\uff09\uff1b3)\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff08\u5206\u914d\u5177\u4f53\u8bed\u4e49\u89d2\u8272\u5982\u5b57\u6bb5\u6807\u7b7e\u3001\u503c\u3001\u9875\u7709\u3001\u9875\u811a\u3001\u7b26\u53f7\uff09\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2a\u516c\u5f00\u53ef\u7528\u7684\u9ad8\u68c9\u8bed\u5546\u4e1a\u6587\u6863\u6570\u636e\u96c6KH-FUNSD\uff0c\u4e3a\u591a\u4e2a\u9886\u5148\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff0c\u9996\u6b21\u4e3a\u9ad8\u68c9\u8bed\u5546\u4e1a\u6587\u6863\u5efa\u7acb\u4e86\u57fa\u7ebf\u6027\u80fd\u3002", "conclusion": "KH-FUNSD\u586b\u8865\u4e86\u4f4e\u8d44\u6e90\u975e\u62c9\u4e01\u6587\u5b57\u6587\u6863AI\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u652f\u6301\u5168\u9762\u7684\u5e03\u5c40\u5206\u6790\u548c\u7cbe\u786e\u7684\u4fe1\u606f\u63d0\u53d6\uff0c\u4e3a\u9ad8\u68c9\u8bed\u5546\u4e1a\u6587\u6863\u5904\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2512.11891", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.11891", "abs": "https://arxiv.org/abs/2512.11891", "authors": ["Songqiao Hu", "Zeyi Liu", "Shuang Liu", "Jun Cen", "Zihan Meng", "Xiao He"], "title": "VLSA: Vision-Language-Action Models with Plug-and-Play Safety Constraint Layer", "comment": "20 pages, 14 figures", "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in generalizing across diverse robotic manipulation tasks. However, deploying these models in unstructured environments remains challenging due to the critical need for simultaneous task compliance and safety assurance, particularly in preventing potential collisions during physical interactions. In this work, we introduce a Vision-Language-Safe Action (VLSA) architecture, named AEGIS, which contains a plug-and-play safety constraint (SC) layer formulated via control barrier functions. AEGIS integrates directly with existing VLA models to improve safety with theoretical guarantees, while maintaining their original instruction-following performance. To evaluate the efficacy of our architecture, we construct a comprehensive safety-critical benchmark SafeLIBERO, spanning distinct manipulation scenarios characterized by varying degrees of spatial complexity and obstacle intervention. Extensive experiments demonstrate the superiority of our method over state-of-the-art baselines. Notably, AEGIS achieves a 59.16% improvement in obstacle avoidance rate while substantially increasing the task execution success rate by 17.25%. To facilitate reproducibility and future research, we make our code, models, and the benchmark datasets publicly available at https://vlsa-aegis.github.io/.", "AI": {"tldr": "\u63d0\u51faAEGIS\u67b6\u6784\uff0c\u5728VLA\u6a21\u578b\u4e2d\u96c6\u6210\u57fa\u4e8e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7684\u5b89\u5168\u7ea6\u675f\u5c42\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u5b89\u5168\u6027\u548c\u4efb\u52a1\u6210\u529f\u7387", "motivation": "VLA\u6a21\u578b\u5728\u591a\u6837\u5316\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u90e8\u7f72\u65f6\u9762\u4e34\u5b89\u5168\u6311\u6218\uff0c\u7279\u522b\u662f\u9700\u8981\u540c\u65f6\u4fdd\u8bc1\u4efb\u52a1\u6267\u884c\u548c\u9632\u6b62\u78b0\u649e", "method": "\u63d0\u51faVLSA\u67b6\u6784AEGIS\uff0c\u5305\u542b\u57fa\u4e8e\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u7684\u5373\u63d2\u5373\u7528\u5b89\u5168\u7ea6\u675f\u5c42\uff0c\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u73b0\u6709VLA\u6a21\u578b\u4e2d\uff0c\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u7684\u5b89\u5168\u6539\u8fdb", "result": "\u5728\u6784\u5efa\u7684SafeLIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAEGIS\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u969c\u788d\u7269\u907f\u514d\u7387\u4e0a\u63d0\u534759.16%\uff0c\u4efb\u52a1\u6267\u884c\u6210\u529f\u7387\u63d0\u534717.25%", "conclusion": "AEGIS\u67b6\u6784\u6709\u6548\u63d0\u5347\u4e86VLA\u6a21\u578b\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u6709\u7684\u6307\u4ee4\u8ddf\u968f\u6027\u80fd\uff0c\u4e3a\u5b89\u5168\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.11998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11998", "abs": "https://arxiv.org/abs/2512.11998", "authors": ["Glenn Zhang", "Treasure Mayowa", "Jason Fan", "Yicheng Fu", "Aaron Sandoval", "Sean O'Brien", "Kevin Zhu"], "title": "Direct Confidence Alignment: Aligning Verbalized Confidence with Internal Confidence In Large Language Models", "comment": "Accepted at ACL 2025 SRW, 5 pages body, 14 pages total", "summary": "Producing trustworthy and reliable Large Language Models (LLMs) has become increasingly important as their usage becomes more widespread. Calibration seeks to achieve this by improving the alignment between the model's confidence and the actual likelihood of its responses being correct or desirable. However, it has been observed that the internal confidence of a model, derived from token probabilities, is not well aligned with its verbalized confidence, leading to misleading results with different calibration methods. In this paper, we propose Direct Confidence Alignment (DCA), a method using Direct Preference Optimization to align an LLM's verbalized confidence with its internal confidence rather than ground-truth accuracy, enhancing model transparency and reliability by ensuring closer alignment between the two confidence measures. We evaluate DCA across multiple open-weight LLMs on a wide range of datasets. To further assess this alignment, we also introduce three new calibration error-based metrics. Our results show that DCA improves alignment metrics on certain model architectures, reducing inconsistencies in a model's confidence expression. However, we also show that it can be ineffective on others, highlighting the need for more model-aware approaches in the pursuit of more interpretable and trustworthy LLMs.", "AI": {"tldr": "\u63d0\u51faDirect Confidence Alignment (DCA)\u65b9\u6cd5\uff0c\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\u5bf9\u9f50\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8a00\u8bed\u5316\u7f6e\u4fe1\u5ea6\u4e0e\u5185\u90e8\u7f6e\u4fe1\u5ea6\uff0c\u800c\u975e\u771f\u5b9e\u51c6\u786e\u7387\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u6784\u5efa\u53ef\u4fe1\u53ef\u9760\u7684\u6a21\u578b\u53d8\u5f97\u6108\u53d1\u91cd\u8981\u3002\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u5185\u90e8\u7f6e\u4fe1\u5ea6\uff08\u6765\u81eatoken\u6982\u7387\uff09\u4e0e\u8a00\u8bed\u5316\u7f6e\u4fe1\u5ea6\u4e0d\u4e00\u81f4\uff0c\u5bfc\u81f4\u4e0d\u540c\u6821\u51c6\u65b9\u6cd5\u4ea7\u751f\u8bef\u5bfc\u6027\u7ed3\u679c\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u79cd\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faDirect Confidence Alignment (DCA)\u65b9\u6cd5\uff0c\u4f7f\u7528Direct Preference Optimization\u6280\u672f\u6765\u5bf9\u9f50LLM\u7684\u8a00\u8bed\u5316\u7f6e\u4fe1\u5ea6\u4e0e\u5185\u90e8\u7f6e\u4fe1\u5ea6\uff0c\u800c\u975e\u4e0e\u771f\u5b9e\u51c6\u786e\u7387\u5bf9\u9f50\u3002\u540c\u65f6\u5f15\u5165\u4e86\u4e09\u79cd\u65b0\u7684\u57fa\u4e8e\u6821\u51c6\u8bef\u5dee\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "DCA\u5728\u67d0\u4e9b\u6a21\u578b\u67b6\u6784\u4e0a\u6539\u5584\u4e86\u7f6e\u4fe1\u5ea6\u5bf9\u9f50\u6307\u6807\uff0c\u51cf\u5c11\u4e86\u6a21\u578b\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u4f46\u5728\u5176\u4ed6\u6a21\u578b\u4e0a\u6548\u679c\u6709\u9650\uff0c\u8868\u660e\u9700\u8981\u66f4\u9488\u5bf9\u5177\u4f53\u6a21\u578b\u7684\u6821\u51c6\u65b9\u6cd5\u3002", "conclusion": "DCA\u65b9\u6cd5\u80fd\u6709\u6548\u6539\u5584\u67d0\u4e9bLLM\u7684\u7f6e\u4fe1\u5ea6\u5bf9\u9f50\uff0c\u589e\u5f3a\u6a21\u578b\u900f\u660e\u5ea6\u548c\u53ef\u9760\u6027\u3002\u4f46\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u6548\u679c\u5b58\u5728\u5dee\u5f02\uff0c\u5f3a\u8c03\u4e86\u5728\u8ffd\u6c42\u66f4\u53ef\u89e3\u91ca\u548c\u53ef\u4fe1\u7684LLM\u65f6\u9700\u8981\u66f4\u5173\u6ce8\u6a21\u578b\u7279\u6027\u7684\u65b9\u6cd5\u3002"}}
{"id": "2512.11900", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11900", "abs": "https://arxiv.org/abs/2512.11900", "authors": ["Christopher E. Mower", "Rui Zong", "Haitham Bou-Ammar"], "title": "Data-driven Interpretable Hybrid Robot Dynamics", "comment": null, "summary": "We study data-driven identification of interpretable hybrid robot dynamics, where an analytical rigid-body dynamics model is complemented by a learned residual torque term. Using symbolic regression and sparse identification of nonlinear dynamics (SINDy), we recover compact closed-form expressions for this residual from joint-space data. In simulation on a 7-DoF Franka arm with known dynamics, these interpretable models accurately recover inertial, Coriolis, gravity, and viscous effects with very small relative error and outperform neural-network baselines in both accuracy and generalization. On real data from a 7-DoF WAM arm, symbolic-regression residuals generalize substantially better than SINDy and neural networks, which tend to overfit, and suggest candidate new closed-form formulations that extend the nominal dynamics model for this robot. Overall, the results indicate that interpretable residual dynamics models provide compact, accurate, and physically meaningful alternatives to black-box function approximators for torque prediction.", "AI": {"tldr": "\u4f7f\u7528\u7b26\u53f7\u56de\u5f52\u548c\u7a00\u758f\u8bc6\u522b\u65b9\u6cd5\u4ece\u673a\u5668\u4eba\u5173\u8282\u6570\u636e\u4e2d\u5b66\u4e60\u53ef\u89e3\u91ca\u7684\u6b8b\u4f59\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u66ff\u4ee3\u9ed1\u76d2\u795e\u7ecf\u7f51\u7edc\uff0c\u83b7\u5f97\u66f4\u51c6\u786e\u3001\u6cdb\u5316\u6027\u66f4\u597d\u7684\u7d27\u51d1\u89e3\u6790\u8868\u8fbe\u5f0f\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u8bc6\u522b\u53ef\u89e3\u91ca\u7684\u6df7\u5408\u673a\u5668\u4eba\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5728\u89e3\u6790\u521a\u4f53\u52a8\u529b\u5b66\u6a21\u578b\u57fa\u7840\u4e0a\u5b66\u4e60\u6b8b\u4f59\u626d\u77e9\u9879\uff0c\u4ee5\u63d0\u4f9b\u6bd4\u9ed1\u76d2\u51fd\u6570\u903c\u8fd1\u5668\u66f4\u7d27\u51d1\u3001\u51c6\u786e\u4e14\u5177\u6709\u7269\u7406\u610f\u4e49\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u7b26\u53f7\u56de\u5f52\u548c\u7a00\u758f\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u8bc6\u522b(SINDy)\u65b9\u6cd5\uff0c\u4ece\u5173\u8282\u7a7a\u95f4\u6570\u636e\u4e2d\u6062\u590d\u6b8b\u4f59\u626d\u77e9\u7684\u7d27\u51d1\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5c06\u89e3\u6790\u521a\u4f53\u52a8\u529b\u5b66\u6a21\u578b\u4e0e\u5b66\u4e60\u5230\u7684\u6b8b\u4f59\u9879\u76f8\u7ed3\u5408\u3002", "result": "\u57287\u81ea\u7531\u5ea6Franka\u673a\u68b0\u81c2\u4eff\u771f\u4e2d\uff0c\u53ef\u89e3\u91ca\u6a21\u578b\u51c6\u786e\u6062\u590d\u4e86\u60ef\u6027\u3001\u79d1\u91cc\u5965\u5229\u3001\u91cd\u529b\u548c\u7c98\u6027\u6548\u5e94\uff0c\u76f8\u5bf9\u8bef\u5dee\u5f88\u5c0f\uff0c\u5728\u51c6\u786e\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u5747\u4f18\u4e8e\u795e\u7ecf\u7f51\u7edc\u57fa\u7ebf\u3002\u5728\u771f\u5b9e7\u81ea\u7531\u5ea6WAM\u673a\u68b0\u81c2\u6570\u636e\u4e0a\uff0c\u7b26\u53f7\u56de\u5f52\u6b8b\u4f59\u6a21\u578b\u6bd4SINDy\u548c\u795e\u7ecf\u7f51\u7edc\u6cdb\u5316\u80fd\u529b\u663e\u8457\u66f4\u597d\uff0c\u540e\u8005\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u5e76\u63d0\u51fa\u4e86\u6269\u5c55\u540d\u4e49\u52a8\u529b\u5b66\u6a21\u578b\u7684\u5019\u9009\u95ed\u5f0f\u516c\u5f0f\u3002", "conclusion": "\u53ef\u89e3\u91ca\u7684\u6b8b\u4f59\u52a8\u529b\u5b66\u6a21\u578b\u4e3a\u626d\u77e9\u9884\u6d4b\u63d0\u4f9b\u4e86\u7d27\u51d1\u3001\u51c6\u786e\u4e14\u5177\u6709\u7269\u7406\u610f\u4e49\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f18\u4e8e\u9ed1\u76d2\u51fd\u6570\u903c\u8fd1\u5668\uff0c\u80fd\u591f\u4ece\u6570\u636e\u4e2d\u53d1\u73b0\u6269\u5c55\u540d\u4e49\u6a21\u578b\u7684\u65b0\u95ed\u5f0f\u516c\u5f0f\u3002"}}
{"id": "2512.12008", "categories": ["cs.CL", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.12008", "abs": "https://arxiv.org/abs/2512.12008", "authors": ["Minghui Liu", "Aadi Palnitkar", "Tahseen Rabbani", "Hyunwoo Jae", "Kyle Rui Sang", "Dixi Yao", "Shayan Shabihi", "Fuheng Zhao", "Tian Li", "Ce Zhang", "Furong Huang", "Kunpeng Zhang"], "title": "Hold Onto That Thought: Assessing KV Cache Compression On Reasoning", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.", "AI": {"tldr": "\u8bc4\u4f30\u591a\u79cdKV\u7f13\u5b58\u538b\u7f29\u7b56\u7565\u5728\u957f\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5bf9\u4e8e\u63a8\u7406\u6a21\u578b\uff0cH2O\u548cSnapKV\u7684\u53d8\u4f53\u662f\u6700\u4f73\u7b56\u7565\uff0c\u63ed\u793a\u4e86\u7f13\u5b58\u5927\u5c0f\u4e0e\u63a8\u7406\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53d7\u9650\u4e8eKV\u7f13\u5b58\u7ebf\u6027\u589e\u957f\u7684\u5b58\u50a8\u74f6\u9888\u3002\u73b0\u6709\u538b\u7f29\u7b56\u7565\u4e3b\u8981\u9488\u5bf9\u9884\u586b\u5145\u9636\u6bb5\uff0c\u5f88\u5c11\u8bc4\u4f30\u5728\u9700\u8981\u957f\u89e3\u7801\u7684\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u50cfGSM8K\u548cMATH500\u8fd9\u6837\u7684\u77ed\u4f46\u590d\u6742\u7684\u63d0\u793a\uff0c\u4f1a\u4ea7\u751f\u6570\u5343token\u7684\u63a8\u7406\u5e8f\u5217\u3002", "method": "\u5728\u957f\u63a8\u7406\u4efb\u52a1\u4e0a\u5bf9\u591a\u79cd\u6d41\u884c\u7684\u538b\u7f29\u7b56\u7565\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u62ecH2O\u548cSnapKV\u7684\u89e3\u7801\u542f\u7528\u53d8\u4f53\uff0c\u5206\u6790\u4e0d\u540c\u7b56\u7565\u5728\u4e0d\u540c\u6570\u636e\u96c6\u7c7b\u578b\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u7814\u7a76\u4f4e\u9884\u7b97\u4e0b\u7684\u9a71\u9010\u7b56\u7565\u5bf9\u63a8\u7406\u8f68\u8ff9\u957f\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u5bf9\u4e8e\u975e\u63a8\u7406\u6a21\u578bLlama-3.1-8B-Instruct\uff0c\u6ca1\u6709\u5355\u4e00\u7b56\u7565\u9002\u5408\u6240\u6709\u60c5\u51b5\uff0c\u6027\u80fd\u53d7\u6570\u636e\u96c6\u7c7b\u578b\u5f71\u54cd\u5f88\u5927\u3002\u5bf9\u4e8e\u63a8\u7406\u6a21\u578b\uff0cH2O\u548cSnapKV\u7684\u89e3\u7801\u542f\u7528\u53d8\u4f53\u662f\u4e3b\u5bfc\u7b56\u7565\uff0c\u8868\u660e\u91cd\u51fb\u8005\u8ddf\u8e2a\u5bf9\u63a8\u7406\u8f68\u8ff9\u5f88\u6709\u7528\u3002\u4f4e\u9884\u7b97\u4e0b\u7684\u9a71\u9010\u7b56\u7565\u53ef\u4ee5\u4ea7\u751f\u66f4\u957f\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u63ed\u793a\u4e86\u7f13\u5b58\u5927\u5c0f\u4e0e\u63a8\u7406\u6210\u672c\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "KV\u7f13\u5b58\u538b\u7f29\u7b56\u7565\u7684\u9009\u62e9\u9700\u8981\u6839\u636e\u6a21\u578b\u7c7b\u578b\u548c\u4efb\u52a1\u7279\u6027\u8fdb\u884c\u8c03\u6574\uff0c\u5bf9\u4e8e\u63a8\u7406\u4efb\u52a1\uff0c\u57fa\u4e8e\u91cd\u51fb\u8005\u8ddf\u8e2a\u7684\u7b56\u7565\uff08\u5982H2O\u548cSnapKV\u53d8\u4f53\uff09\u8868\u73b0\u6700\u4f73\uff0c\u540c\u65f6\u9700\u8981\u5728\u7f13\u5b58\u5927\u5c0f\u548c\u63a8\u7406\u6210\u672c\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u70b9\u3002"}}
{"id": "2512.11903", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11903", "abs": "https://arxiv.org/abs/2512.11903", "authors": ["Iacopo Catalano", "Eduardo Montijano", "Javier Civera", "Julio A. Placed", "Jorge Pena-Queralta"], "title": "Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics", "comment": null, "summary": "Autonomous navigation in dynamic environments requires spatial representations that capture both semantic structure and temporal evolution. 3D Scene Graphs (3DSGs) provide hierarchical multi-resolution abstractions that encode geometry and semantics, but existing extensions toward dynamics largely focus on individual objects or agents. In parallel, Maps of Dynamics (MoDs) model typical motion patterns and temporal regularities, yet are usually tied to grid-based discretizations that lack semantic awareness and do not scale well to large environments. In this paper we introduce Aion, a framework that embeds temporal flow dynamics directly within a hierarchical 3DSG, effectively incorporating the temporal dimension. Aion employs a graph-based sparse MoD representation to capture motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph, yielding more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments.", "AI": {"tldr": "Aion\u6846\u67b6\u5c06\u65f6\u95f4\u6d41\u52a8\u6001\u5d4c\u5165\u5230\u5206\u5c423D\u573a\u666f\u56fe\u4e2d\uff0c\u7ed3\u5408\u4e86\u8bed\u4e49\u7ed3\u6784\u548c\u8fd0\u52a8\u6a21\u5f0f\uff0c\u7528\u4e8e\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u3002", "motivation": "\u73b0\u67093D\u573a\u666f\u56fe\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u8bed\u4e49\u7ed3\u6784\uff0c\u800c\u52a8\u6001\u5730\u56fe\u901a\u5e38\u57fa\u4e8e\u7f51\u683c\u79bb\u6563\u5316\u4e14\u7f3a\u4e4f\u8bed\u4e49\u610f\u8bc6\uff0c\u4e24\u8005\u90fd\u96be\u4ee5\u540c\u65f6\u6355\u6349\u8bed\u4e49\u7ed3\u6784\u548c\u65f6\u95f4\u6f14\u5316\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "method": "Aion\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u7a00\u758f\u52a8\u6001\u5730\u56fe\u8868\u793a\uff0c\u6355\u6349\u4efb\u610f\u65f6\u95f4\u95f4\u9694\u5185\u7684\u8fd0\u52a8\u6d41\uff0c\u5e76\u5c06\u8fd9\u4e9b\u52a8\u6001\u4fe1\u606f\u9644\u52a0\u5230\u573a\u666f\u56fe\u7684\u5bfc\u822a\u8282\u70b9\u4e0a\uff0c\u5f62\u6210\u5c42\u6b21\u5316\u7684\u65f6\u7a7a\u8868\u793a\u3002", "result": "\u8be5\u65b9\u6cd5\u4ea7\u751f\u4e86\u66f4\u53ef\u89e3\u91ca\u548c\u53ef\u6269\u5c55\u7684\u9884\u6d4b\uff0c\u80fd\u591f\u6539\u5584\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u89c4\u5212\u548c\u4ea4\u4e92\u80fd\u529b\u3002", "conclusion": "Aion\u6846\u67b6\u6210\u529f\u5730\u5c06\u65f6\u95f4\u7ef4\u5ea6\u6574\u5408\u5230\u5c42\u6b21\u53163D\u573a\u666f\u56fe\u4e2d\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u65f6\u7a7a\u8868\u793a\u3002"}}
{"id": "2512.12042", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12042", "abs": "https://arxiv.org/abs/2512.12042", "authors": ["Philipp Habicht", "Lev Sorokin", "Abdullah Saydemir", "Ken E. Friedl", "Andrea Stocco"], "title": "Benchmarking Contextual Understanding for In-Car Conversational Systems", "comment": null, "summary": "In-Car Conversational Question Answering (ConvQA) systems significantly enhance user experience by enabling seamless voice interactions. However, assessing their accuracy and reliability remains a challenge. This paper explores the use of Large Language Models (LLMs) alongside advanced prompting techniques and agent-based methods to evaluate the extent to which ConvQA system responses adhere to user utterances. The focus lies on contextual understanding and the ability to provide accurate venue recommendations considering user constraints and situational context. To evaluate utterance-response coherence using an LLM, we synthetically generate user utterances accompanied by correct and modified failure-containing system responses. We use input-output, chain-of-thought, self-consistency prompting, and multi-agent prompting techniques with 13 reasoning and non-reasoning LLMs of varying sizes and providers, including OpenAI, DeepSeek, Mistral AI, and Meta. We evaluate our approach on a case study involving restaurant recommendations. The most substantial improvements occur for small non-reasoning models when applying advanced prompting techniques, particularly multi-agent prompting. However, reasoning models consistently outperform non-reasoning models, with the best performance achieved using single-agent prompting with self-consistency. Notably, DeepSeek-R1 reaches an F1-score of 0.99 at a cost of 0.002 USD per request. Overall, the best balance between effectiveness and cost-time efficiency is reached with the non-reasoning model DeepSeek-V3. Our findings show that LLM-based evaluation offers a scalable and accurate alternative to traditional human evaluation for benchmarking contextual understanding in ConvQA systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4f7f\u7528LLM\u8bc4\u4f30\u8f66\u8f7d\u5bf9\u8bdd\u95ee\u7b54\u7cfb\u7edf\u7684\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u591a\u79cd\u63d0\u793a\u6280\u672f\u548c\u4ee3\u7406\u65b9\u6cd5\u6d4b\u8bd513\u4e2a\u4e0d\u540c\u89c4\u6a21\u7684LLM\uff0c\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0cDeepSeek-R1\u8fbe\u5230F1\u5206\u65700.99\uff0c\u800c\u975e\u63a8\u7406\u6a21\u578bDeepSeek-V3\u5728\u6548\u679c\u4e0e\u6210\u672c\u6548\u7387\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002", "motivation": "\u8f66\u8f7d\u5bf9\u8bdd\u95ee\u7b54\u7cfb\u7edf\u80fd\u63d0\u5347\u7528\u6237\u4f53\u9a8c\uff0c\u4f46\u8bc4\u4f30\u5176\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u4ecd\u5177\u6311\u6218\u3002\u4f20\u7edf\u4eba\u5de5\u8bc4\u4f30\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u6d4b\u8bd5\u7cfb\u7edf\u54cd\u5e94\u662f\u5426\u7b26\u5408\u7528\u6237\u610f\u56fe\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u3002", "method": "\u4f7f\u7528LLM\u7ed3\u5408\u591a\u79cd\u63d0\u793a\u6280\u672f\uff08\u8f93\u5165\u8f93\u51fa\u3001\u601d\u7ef4\u94fe\u3001\u81ea\u6d3d\u6027\u63d0\u793a\uff09\u548c\u4ee3\u7406\u65b9\u6cd5\uff08\u5355\u4ee3\u7406\u3001\u591a\u4ee3\u7406\uff09\u8bc4\u4f30\u8f66\u8f7dConvQA\u7cfb\u7edf\u3002\u901a\u8fc7\u5408\u6210\u751f\u6210\u7528\u6237\u8bdd\u8bed\u53ca\u6b63\u786e/\u5305\u542b\u9519\u8bef\u7684\u7cfb\u7edf\u54cd\u5e94\uff0c\u5728\u9910\u5385\u63a8\u8350\u6848\u4f8b\u4e2d\u6d4b\u8bd513\u4e2a\u4e0d\u540c\u89c4\u6a21\u548c\u63d0\u4f9b\u5546\u7684LLM\u3002", "result": "\u63a8\u7406\u6a21\u578b\u59cb\u7ec8\u4f18\u4e8e\u975e\u63a8\u7406\u6a21\u578b\uff0c\u6700\u4f73\u6027\u80fd\u6765\u81ea\u5355\u4ee3\u7406\u81ea\u6d3d\u6027\u63d0\u793a\uff0cDeepSeek-R1\u8fbe\u5230F1\u5206\u65700.99\uff08\u6bcf\u6b21\u8bf7\u6c42\u6210\u672c0.002\u7f8e\u5143\uff09\u3002\u975e\u63a8\u7406\u6a21\u578b\u4e2d\uff0cDeepSeek-V3\u5728\u6548\u679c\u4e0e\u6210\u672c\u65f6\u95f4\u6548\u7387\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\u3002\u591a\u4ee3\u7406\u63d0\u793a\u5bf9\u5c0f\u89c4\u6a21\u975e\u63a8\u7406\u6a21\u578b\u63d0\u5347\u6700\u5927\u3002", "conclusion": "LLM\u8bc4\u4f30\u4e3a\u8f66\u8f7dConvQA\u7cfb\u7edf\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u51c6\u786e\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u4eba\u5de5\u8bc4\u4f30\u3002\u63a8\u7406\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u975e\u63a8\u7406\u6a21\u578b\u5728\u6210\u672c\u6548\u7387\u65b9\u9762\u66f4\u5177\u4f18\u52bf\uff0cDeepSeek-V3\u662f\u5e73\u8861\u6548\u679c\u4e0e\u6548\u7387\u7684\u6700\u4f73\u9009\u62e9\u3002"}}
{"id": "2512.11908", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11908", "abs": "https://arxiv.org/abs/2512.11908", "authors": ["Heng Zhang", "Rui Dai", "Gokhan Solak", "Pokuang Zhou", "Yu She", "Arash Ajoudani"], "title": "Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models", "comment": null, "summary": "Contact-rich tasks pose significant challenges for robotic systems due to inherent uncertainty, complex dynamics, and the high risk of damage during interaction. Recent advances in learning-based control have shown great potential in enabling robots to acquire and generalize complex manipulation skills in such environments, but ensuring safety, both during exploration and execution, remains a critical bottleneck for reliable real-world deployment. This survey provides a comprehensive overview of safe learning-based methods for robot contact-rich tasks. We categorize existing approaches into two main domains: safe exploration and safe execution. We review key techniques, including constrained reinforcement learning, risk-sensitive optimization, uncertainty-aware modeling, control barrier functions, and model predictive safety shields, and highlight how these methods incorporate prior knowledge, task structure, and online adaptation to balance safety and efficiency. A particular emphasis of this survey is on how these safe learning principles extend to and interact with emerging robotic foundation models, especially vision-language models (VLMs) and vision-language-action models (VLAs), which unify perception, language, and control for contact-rich manipulation. We discuss both the new safety opportunities enabled by VLM/VLA-based methods, such as language-level specification of constraints and multimodal grounding of safety signals, and the amplified risks and evaluation challenges they introduce. Finally, we outline current limitations and promising future directions toward deploying reliable, safety-aligned, and foundation-model-enabled robots in complex contact-rich environments. More details and materials are available at our \\href{ https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks}{Project GitHub Repository}.", "AI": {"tldr": "\u8be5\u7efc\u8ff0\u5168\u9762\u56de\u987e\u4e86\u673a\u5668\u4eba\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\uff0c\u6db5\u76d6\u5b89\u5168\u63a2\u7d22\u4e0e\u5b89\u5168\u6267\u884c\u4e24\u5927\u9886\u57df\uff0c\u5e76\u7279\u522b\u5173\u6ce8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7b49\u57fa\u7840\u6a21\u578b\u5e26\u6765\u7684\u65b0\u5b89\u5168\u673a\u9047\u4e0e\u6311\u6218\u3002", "motivation": "\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u3001\u590d\u6742\u52a8\u529b\u5b66\u548c\u9ad8\u635f\u4f24\u98ce\u9669\u7b49\u6311\u6218\uff0c\u5b66\u4e60\u63a7\u5236\u65b9\u6cd5\u867d\u80fd\u5e2e\u52a9\u673a\u5668\u4eba\u638c\u63e1\u590d\u6742\u64cd\u4f5c\u6280\u80fd\uff0c\u4f46\u786e\u4fdd\u63a2\u7d22\u548c\u6267\u884c\u8fc7\u7a0b\u4e2d\u7684\u5b89\u5168\u6027\u4ecd\u662f\u5b9e\u9645\u90e8\u7f72\u7684\u5173\u952e\u74f6\u9888\u3002", "method": "\u5c06\u73b0\u6709\u65b9\u6cd5\u5206\u4e3a\u5b89\u5168\u63a2\u7d22\u548c\u5b89\u5168\u6267\u884c\u4e24\u5927\u9886\u57df\uff0c\u7efc\u8ff0\u4e86\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u3001\u98ce\u9669\u654f\u611f\u4f18\u5316\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5efa\u6a21\u3001\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u3001\u6a21\u578b\u9884\u6d4b\u5b89\u5168\u5c4f\u853d\u7b49\u5173\u952e\u6280\u672f\uff0c\u5e76\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u4e0e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7b49\u57fa\u7840\u6a21\u578b\u7684\u7ed3\u5408\u3002", "result": "\u7cfb\u7edf\u68b3\u7406\u4e86\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\u5b89\u5168\u5b66\u4e60\u63a7\u5236\u7684\u6280\u672f\u4f53\u7cfb\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\uff0c\u7279\u522b\u6307\u51fa\u4e86\u57fa\u7840\u6a21\u578b\u5e26\u6765\u7684\u8bed\u8a00\u7ea7\u7ea6\u675f\u89c4\u8303\u3001\u591a\u6a21\u6001\u5b89\u5168\u4fe1\u53f7\u63a5\u5730\u7b49\u65b0\u673a\u9047\uff0c\u4ee5\u53ca\u76f8\u5e94\u7684\u98ce\u9669\u4e0e\u8bc4\u4f30\u6311\u6218\u3002", "conclusion": "\u8be5\u9886\u57df\u9700\u8981\u5728\u73b0\u6709\u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u53d1\u5c55\uff0c\u4ee5\u5b9e\u73b0\u53ef\u9760\u3001\u5b89\u5168\u5bf9\u9f50\u4e14\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u673a\u5668\u4eba\u5728\u590d\u6742\u63a5\u89e6\u73af\u5883\u4e2d\u7684\u90e8\u7f72\uff0c\u672a\u6765\u65b9\u5411\u5305\u62ec\u6539\u8fdb\u5b89\u5168\u8bc4\u4f30\u3001\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u548c\u63a2\u7d22\u65b0\u7684\u5b89\u5168\u89c4\u8303\u65b9\u6cd5\u3002"}}
{"id": "2512.12072", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12072", "abs": "https://arxiv.org/abs/2512.12072", "authors": ["Avinash Amballa", "Yashas Malur Saidutta", "Chi-Heng Lin", "Vivek Kulkarni", "Srinivas Chappidi"], "title": "VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs", "comment": "Arxiv Submission", "summary": "Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.", "AI": {"tldr": "Voyager\u662f\u4e00\u79cd\u57fa\u4e8e\u884c\u5217\u5f0f\u70b9\u8fc7\u7a0b\u7684\u8fed\u4ee3\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u591a\u6837\u5316\u7684LLM\u5408\u6210\u6570\u636e\u96c6\uff0c\u65e0\u9700\u8bad\u7ec3\u4e14\u9002\u7528\u4e8e\u95ed\u6e90\u6a21\u578b\uff0c\u80fd\u63d0\u53471.5-3\u500d\u7684\u591a\u6837\u6027\u3002", "motivation": "\u5f53\u524d\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\u7684\u65b9\u6cd5\u5b58\u5728\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u8fd9\u9650\u5236\u4e86\u6570\u636e\u96c6\u5728\u4e0b\u6e38\u6a21\u578b\u8bc4\u4f30\u548c\u8bad\u7ec3\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faVoyager\u65b9\u6cd5\uff1a\u57fa\u4e8e\u884c\u5217\u5f0f\u70b9\u8fc7\u7a0b\u7684\u8fed\u4ee3\u4f18\u5316\u6846\u67b6\uff0c\u76f4\u63a5\u4f18\u5316\u6570\u636e\u96c6\u7684\u6570\u5b66\u591a\u6837\u6027\u5ea6\u91cf\uff0c\u65e0\u9700\u8bad\u7ec3\u4e14\u9002\u7528\u4e8e\u95ed\u6e90\u6a21\u578b\u3002", "result": "Voyager\u5728\u591a\u6837\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u4f9b1.5-3\u500d\u7684\u591a\u6837\u6027\u63d0\u5347\uff0c\u5e76\u901a\u8fc7\u7efc\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "Voyager\u4e3a\u751f\u6210\u591a\u6837\u5316\u7684LLM\u5408\u6210\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002"}}
{"id": "2512.11921", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11921", "abs": "https://arxiv.org/abs/2512.11921", "authors": ["Abdullah Yahya Abdullah Omaisan", "Ibrahim Sheikh Mohamed"], "title": "Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control", "comment": null, "summary": "Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u4f7f\u5927\u578b\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u80fd\u5728\u4f4e\u6210\u672c\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u90e8\u7f72\uff0c\u901a\u8fc7LoRA\u548c\u91cf\u5316\u6280\u672f\u8ba931\u4ebf\u53c2\u6570\u6a21\u578b\u57288GB\u663e\u5b58\u7684\u6d88\u8d39\u7ea7GPU\u4e0a\u8fd0\u884c\u3002", "motivation": "\u5927\u89c4\u6a21VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u90e8\u7f72\u5230\u4f4e\u6210\u672c\u673a\u5668\u4eba\u5e73\u53f0\u9762\u4e34\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u548c\u65b0\u673a\u5668\u4eba\u672c\u4f53\u9002\u914d\u7684\u6311\u6218\u3002\u9700\u8981\u89e3\u51b3\u5982\u4f55\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u548c\u5c11\u91cf\u6f14\u793a\u6570\u636e\u4e0b\u6709\u6548\u9002\u914d\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u548c\u91cf\u5316\u6280\u672f\u7684\u9ad8\u6548\u5fae\u8c03\u7b56\u7565\uff0c\u91cd\u70b9\u7814\u7a76\u51bb\u7ed3\u4e0e\u89e3\u51bb\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6743\u8861\u3002\u5728SO101\u673a\u68b0\u81c2\u4e0a\u8fdb\u884c\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u6d4b\u8bd5\uff0c\u4f7f\u7528200\u4e2a\u6f14\u793a\u7247\u6bb5\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u65b9\u6cd5\u6210\u529f\u5c0631\u4ebf\u53c2\u6570VLA\u6a21\u578b\u90e8\u7f72\u52308GB\u663e\u5b58\u7684\u6d88\u8d39\u7ea7GPU\u4e0a\uff0c\u5728\u6309\u94ae\u6309\u538b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6709\u6548\u64cd\u4f5c\u6027\u80fd\u3002\u5206\u6790\u4e86\u90e8\u7f72\u6311\u6218\u3001\u5931\u8d25\u6a21\u5f0f\u4ee5\u53ca\u8bad\u7ec3\u6570\u636e\u91cf\u4e0e\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u7684\u5173\u7cfb\u3002", "conclusion": "\u901a\u8fc7\u9002\u5f53\u7684\u5fae\u8c03\u65b9\u6cd5\uff0cVLA\u6a21\u578b\u53ef\u4ee5\u6210\u529f\u90e8\u7f72\u5230\u7ecf\u6d4e\u5b9e\u60e0\u7684\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u4f7f\u5148\u8fdb\u7684\u64cd\u63a7\u80fd\u529b\u8d85\u8d8a\u6602\u8d35\u7684\u7814\u7a76\u673a\u5668\u4eba\uff0c\u53d8\u5f97\u66f4\u52a0\u666e\u53ca\u3002"}}
{"id": "2512.12087", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12087", "abs": "https://arxiv.org/abs/2512.12087", "authors": ["Jiayi Yuan", "Cameron Shinn", "Kai Xu", "Jingze Cui", "George Klimiashvili", "Guangxuan Xiao", "Perkz Zheng", "Bo Li", "Yuxin Zhou", "Zhouhai Ye", "Weijie You", "Tian Zheng", "Dominic Brown", "Pengbo Wang", "Richard Cai", "Julien Demouth", "John D. Owens", "Xia Hu", "Song Han", "Timmy Liu", "Huizi Mao"], "title": "BLASST: Dynamic BLocked Attention Sparsity via Softmax Thresholding", "comment": null, "summary": "The growing demand for long-context inference capabilities in Large Language Models (LLMs) has intensified the computational and memory bottlenecks inherent to the standard attention mechanism. To address this challenge, we introduce BLASST, a drop-in sparse attention method that dynamically prunes the attention matrix without any pre-computation or proxy scores. Our method uses a fixed threshold and existing information from online softmax to identify negligible attention scores, skipping softmax computation, Value block loading, and the subsequent matrix multiplication. This fits seamlessly into existing FlashAttention kernel designs with negligible latency overhead. The approach is applicable to both prefill and decode stages across all attention variants (MHA, GQA, MQA, and MLA), providing a unified solution for accelerating long-context inference. We develop an automated calibration procedure that reveals a simple inverse relationship between optimal threshold and context length, enabling robust deployment across diverse scenarios. Maintaining high accuracy, we demonstrate a 1.62x speedup for prefill at 74.7% sparsity and a 1.48x speedup for decode at 73.2% sparsity on modern GPUs. Furthermore, we explore sparsity-aware training as a natural extension, showing that models can be trained to be inherently more robust to sparse attention patterns, pushing the accuracy-sparsity frontier even further.", "AI": {"tldr": "BLASST\u662f\u4e00\u79cd\u65e0\u9700\u9884\u8ba1\u7b97\u7684\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fa\u5b9a\u9608\u503c\u548c\u5728\u7ebfsoftmax\u4fe1\u606f\u8bc6\u522b\u53ef\u5ffd\u7565\u7684\u6ce8\u610f\u529b\u5206\u6570\uff0c\u8df3\u8fc7\u76f8\u5173\u8ba1\u7b97\uff0c\u5b9e\u73b0\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u9700\u6c42\u7684\u589e\u957f\uff0c\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u74f6\u9888\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u9ad8\u6548\u7684\u7a00\u758f\u6ce8\u610f\u529b\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u56fa\u5b9a\u9608\u503c\u548c\u5728\u7ebfsoftmax\u7684\u73b0\u6709\u4fe1\u606f\u52a8\u6001\u526a\u679d\u6ce8\u610f\u529b\u77e9\u9635\uff0c\u8df3\u8fc7\u53ef\u5ffd\u7565\u6ce8\u610f\u529b\u5206\u6570\u7684softmax\u8ba1\u7b97\u3001Value\u5757\u52a0\u8f7d\u548c\u77e9\u9635\u4e58\u6cd5\uff0c\u65e0\u7f1d\u96c6\u6210\u5230FlashAttention\u5185\u6838\u8bbe\u8ba1\u4e2d\u3002", "result": "\u5728\u73b0\u4ee3GPU\u4e0a\uff0c\u9884\u586b\u5145\u9636\u6bb5\u8fbe\u523074.7%\u7a00\u758f\u5ea6\u65f6\u5b9e\u73b01.62\u500d\u52a0\u901f\uff0c\u89e3\u7801\u9636\u6bb5\u8fbe\u523073.2%\u7a00\u758f\u5ea6\u65f6\u5b9e\u73b01.48\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "BLASST\u4e3a\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u6240\u6709\u6ce8\u610f\u529b\u53d8\u4f53\uff0c\u5e76\u901a\u8fc7\u7a00\u758f\u611f\u77e5\u8bad\u7ec3\u8fdb\u4e00\u6b65\u63d0\u5347\u7cbe\u5ea6-\u7a00\u758f\u5ea6\u8fb9\u754c\u3002"}}
{"id": "2512.11944", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11944", "abs": "https://arxiv.org/abs/2512.11944", "authors": ["Jia Hu", "Yang Chang", "Haoran Wang"], "title": "A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach", "comment": "34 pages, 11 figures", "summary": "Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, \"black-box\" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: \"Human-Centric\" customization, \"Platform-Adaptive\" dynamics adaptation, and \"System Self-Optimization\" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u7684\u6700\u4f18\u63a7\u5236\u8303\u5f0f\u4f5c\u4e3a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u7ecf\u5178\u63a7\u5236\u7684\u53ef\u9a8c\u8bc1\u7ed3\u6784\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u4ee5\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u4e2d\u900f\u660e\u6027\u4e0e\u9002\u5e94\u6027\u4e4b\u95f4\u7684\u6839\u672c\u77db\u76fe\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u9762\u4e34\u6839\u672c\u6027\u77db\u76fe\uff1a\u4f20\u7edf\u6d41\u6c34\u7ebf\u65b9\u6cd5\u900f\u660e\u4f46\u8106\u5f31\uff0c\u800c\u73b0\u4ee3\u5b66\u4e60\u7cfb\u7edf\u9002\u5e94\u6027\u5f3a\u4f46\u7f3a\u4e4f\u900f\u660e\u5ea6\uff08\"\u9ed1\u7bb1\"\u7279\u6027\uff09\u3002\u8fd9\u79cd\u6301\u4e45\u56f0\u5883\u963b\u788d\u4e86\u771f\u6b63\u53ef\u4fe1\u8d56\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5168\u9762\u56de\u987e\u57fa\u4e8e\u5b66\u4e60\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u7684\u6700\u4f18\u63a7\u5236\u8303\u5f0f\u4f5c\u4e3a\u7edf\u4e00\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5c06\u7ecf\u5178\u63a7\u5236\u7684\u53ef\u9a8c\u8bc1\u7ed3\u6784\u4e0e\u673a\u5668\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u5229\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u6301\u7eed\u4f18\u5316\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u6210\u672c\u51fd\u6570\u548c\u5b89\u5168\u7ea6\u675f\u7b49\u5173\u952e\u7ec4\u4ef6\u3002", "result": "\u8be5\u6846\u67b6\u6709\u671b\u5b9e\u73b0\u4e09\u4e2a\u5173\u952e\u4e0b\u4e00\u4ee3\u80fd\u529b\uff1a1)\"\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\"\u7684\u5b9a\u5236\u5316\uff1b2)\"\u5e73\u53f0\u81ea\u9002\u5e94\"\u7684\u52a8\u6001\u9002\u5e94\uff1b3)\u901a\u8fc7\u81ea\u8c03\u4f18\u5b9e\u73b0\"\u7cfb\u7edf\u81ea\u4f18\u5316\"\u3002\u4e3a\u5f00\u53d1\u540c\u65f6\u5177\u5907\u5b89\u5168\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7c7b\u4eba\u81ea\u4e3b\u6027\u7684\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u7684\u6700\u4f18\u63a7\u5236\u8303\u5f0f\u4e3a\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u900f\u660e\u6027\u4e0e\u9002\u5e94\u6027\u77db\u76fe\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u6846\u67b6\u878d\u5408\u4e86\u7ecf\u5178\u63a7\u5236\u7684\u7406\u8bba\u4e25\u8c28\u6027\u548c\u673a\u5668\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u80fd\u529b\uff0c\u4e3a\u5b9e\u73b0\u5b89\u5168\u3001\u53ef\u89e3\u91ca\u4e14\u5177\u5907\u7c7b\u4eba\u81ea\u4e3b\u6027\u7684\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u6307\u660e\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2512.12167", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12167", "abs": "https://arxiv.org/abs/2512.12167", "authors": ["Yoav Gelberg", "Koshi Eguchi", "Takuya Akiba", "Edoardo Cetin"], "title": "Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings", "comment": null, "summary": "So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.", "AI": {"tldr": "DroPE\u65b9\u6cd5\u901a\u8fc7\u8bad\u7ec3\u540e\u4e22\u5f03\u4f4d\u7f6e\u5d4c\u5165\uff0c\u5b9e\u73b0\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u5fae\u8c03\u96f6\u6837\u672c\u4e0a\u4e0b\u6587\u6269\u5c55\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u9700\u8981\u6602\u8d35\u957f\u4e0a\u4e0b\u6587\u5fae\u8c03\u7684\u9650\u5236\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u901a\u8fc7\u6602\u8d35\u7684\u5fae\u8c03\u6765\u6269\u5c55\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u8fd9\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u74f6\u9888\u3002\u4f5c\u8005\u53d1\u73b0\u4f4d\u7f6e\u5d4c\u5165\u5728\u8bad\u7ec3\u65f6\u5f88\u91cd\u8981\uff0c\u4f46\u8fc7\u5ea6\u4f9d\u8d56\u5b83\u963b\u788d\u4e86\u6a21\u578b\u5bf9\u672a\u89c1\u957f\u5ea6\u5e8f\u5217\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faDroPE\u65b9\u6cd5\uff1a\u5728\u9884\u8bad\u7ec3\u540e\u4e22\u5f03\u4f4d\u7f6e\u5d4c\u5165\uff0c\u7ecf\u8fc7\u77ed\u6682\u7684\u91cd\u65b0\u6821\u51c6\u9636\u6bb5\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u9002\u5e94\u65e0\u4f4d\u7f6e\u5d4c\u5165\u7684\u63a8\u7406\u3002\u8be5\u65b9\u6cd5\u7b80\u5355\u6709\u6548\uff0c\u65e0\u9700\u957f\u4e0a\u4e0b\u6587\u5fae\u8c03\u3002", "result": "DroPE\u5b9e\u73b0\u4e86\u65e0\u7f1d\u7684\u96f6\u6837\u672c\u4e0a\u4e0b\u6587\u6269\u5c55\uff0c\u4e0d\u635f\u5bb3\u6a21\u578b\u5728\u539f\u59cb\u8bad\u7ec3\u4e0a\u4e0b\u6587\u4e2d\u7684\u80fd\u529b\u3002\u5728\u4e0d\u540c\u6a21\u578b\u548c\u6570\u636e\u96c6\u89c4\u6a21\u4e0a\u90fd\u8868\u73b0\u4f18\u5f02\uff0c\u8fdc\u8d85\u4e4b\u524d\u7684\u4e13\u95e8\u67b6\u6784\u548c\u65cb\u8f6c\u4f4d\u7f6e\u5d4c\u5165\u7f29\u653e\u65b9\u6cd5\u3002", "conclusion": "\u4f4d\u7f6e\u5d4c\u5165\u4e0d\u662f\u8bed\u8a00\u5efa\u6a21\u7684\u5185\u5728\u8981\u6c42\uff0c\u53ef\u4ee5\u5728\u9884\u8bad\u7ec3\u540e\u5b89\u5168\u79fb\u9664\u3002DroPE\u65b9\u6cd5\u7a81\u7834\u4e86\u4e0a\u4e0b\u6587\u6269\u5c55\u7684\u5173\u952e\u74f6\u9888\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u5e94\u7528\u63d0\u4f9b\u4e86\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12021", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12021", "abs": "https://arxiv.org/abs/2512.12021", "authors": ["Xincheng Cao", "Haochong Chen", "Bilin Aksun-Guvenc", "Levent Guvenc"], "title": "Modified Hybrid A* Collision-Free Path-Planning for Automated Reverse Parking", "comment": null, "summary": "Parking a vehicle in tight spaces is a challenging task to perform due to the scarcity of feasible paths that are also collision-free. This paper presents a strategy to tackle this kind of maneuver with a modified Hybrid-A* path-planning algorithm that combines the feasibility guarantee inherent in the standard Hybrid A* algorithm with the addition of static obstacle collision avoidance. A kinematic single-track model is derived to describe the low-speed motion of the vehicle, which is subsequently used as the motion model in the Hybrid A* path-planning algorithm to generate feasible motion primitive branches. The model states are also used to reconstruct the vehicle centerline, which, in conjunction with an inflated binary occupancy map, facilitates static obstacle collision avoidance functions. Simulation study and animation are set up to test the efficacy of the approach, and the proposed algorithm proves to consistently provide kinematically feasible trajectories that are also collision-free.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684Hybrid-A*\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u72ed\u7a84\u7a7a\u95f4\u4e2d\u7684\u8f66\u8f86\u6cca\u8f66\uff0c\u7ed3\u5408\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u548c\u9759\u6001\u969c\u788d\u7269\u907f\u969c", "motivation": "\u5728\u72ed\u7a84\u7a7a\u95f4\u6cca\u8f66\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u65e2\u9700\u8981\u8def\u5f84\u53ef\u884c\u53c8\u9700\u8981\u907f\u514d\u78b0\u649e\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u8fd9\u4e24\u70b9\u3002", "method": "1. \u63a8\u5bfc\u8fd0\u52a8\u5b66\u5355\u8f68\u6a21\u578b\u63cf\u8ff0\u8f66\u8f86\u4f4e\u901f\u8fd0\u52a8\uff1b2. \u5c06\u6a21\u578b\u96c6\u6210\u5230\u6539\u8fdb\u7684Hybrid-A*\u7b97\u6cd5\u4e2d\u751f\u6210\u53ef\u884c\u8fd0\u52a8\u57fa\u5143\uff1b3. \u5229\u7528\u8f66\u8f86\u4e2d\u5fc3\u7ebf\u91cd\u5efa\u548c\u81a8\u80c0\u4e8c\u503c\u5360\u636e\u5730\u56fe\u5b9e\u73b0\u9759\u6001\u969c\u788d\u7269\u907f\u969c", "result": "\u901a\u8fc7\u4eff\u771f\u7814\u7a76\u548c\u52a8\u753b\u6d4b\u8bd5\uff0c\u8bc1\u660e\u8be5\u7b97\u6cd5\u80fd\u6301\u7eed\u63d0\u4f9b\u8fd0\u52a8\u5b66\u53ef\u884c\u4e14\u65e0\u78b0\u649e\u7684\u8f68\u8ff9", "conclusion": "\u63d0\u51fa\u7684\u6539\u8fdbHybrid-A*\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u72ed\u7a84\u7a7a\u95f4\u6cca\u8f66\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u8bc1\u8def\u5f84\u53ef\u884c\u6027\u548c\u78b0\u649e\u907f\u514d"}}
{"id": "2512.12168", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12168", "abs": "https://arxiv.org/abs/2512.12168", "authors": ["Zheng Huang", "Kiran Ramnath", "Yueyan Chen", "Aosong Feng", "Sangmin Woo", "Balasubramaniam Srinivasan", "Zhichao Xu", "Kang Zhou", "Shuai Wang", "Haibo Ding", "Lin Lee Cheong"], "title": "Diffusion Language Model Inference with Monte Carlo Tree Search", "comment": null, "summary": "Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.", "AI": {"tldr": "MEDAL\u6846\u67b6\u5c06\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u96c6\u6210\u5230\u6269\u6563\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u521d\u59cb\u5316\u9636\u6bb5\uff0c\u901a\u8fc7\u63a2\u7d22\u6709\u524d\u666f\u7684\u89e3\u7801\u8def\u5f84\u6765\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u8fbe22.0%\u3002", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u6709\u5e76\u884c\u751f\u6210\u548c\u5168\u5c40\u8fde\u8d2f\u6027\u7684\u4f18\u52bf\uff0c\u4f46\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u786e\u5b9a\u54ea\u4e9b\u4f4d\u7f6e\u53bb\u63a9\u7801\u4ee5\u53ca\u9009\u62e9\u54ea\u4e9b\u6807\u8bb0\u662f\u4e00\u4e2a\u5de8\u5927\u7684\u7ec4\u5408\u641c\u7d22\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f7f\u7528\u542f\u53d1\u5f0f\u8fd1\u4f3c\uff08\u6548\u679c\u6b20\u4f73\uff09\uff0c\u8981\u4e48\u4f9d\u8d56\u989d\u5916\u8bad\u7ec3\u6765\u6307\u5bfc\u6807\u8bb0\u9009\u62e9\uff0c\u7f3a\u4e4f\u539f\u5219\u6027\u7684\u641c\u7d22\u673a\u5236\u3002", "method": "MEDAL\u6846\u67b6\u5728\u6269\u6563\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u521d\u59cb\u5316\u9636\u6bb5\u96c6\u6210\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u901a\u8fc7\u9650\u5236\u641c\u7d22\u7a7a\u95f4\u5230\u9ad8\u7f6e\u4fe1\u5ea6\u52a8\u4f5c\uff0c\u5e76\u4f18\u5148\u9009\u62e9\u80fd\u63d0\u9ad8\u5269\u4f59\u63a9\u7801\u4f4d\u7f6e\u6a21\u578b\u7f6e\u4fe1\u5ea6\u7684\u6807\u8bb0\uff0c\u63a2\u7d22\u6709\u524d\u666f\u7684\u53bb\u63a9\u7801\u8f68\u8ff9\uff0c\u4e3a\u540e\u7eed\u7ec6\u5316\u63d0\u4f9b\u7a33\u5065\u7684\u8d77\u70b9\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMEDAL\u76f8\u6bd4\u73b0\u6709\u7684\u63a8\u7406\u7b56\u7565\u5b9e\u73b0\u4e86\u9ad8\u8fbe22.0%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u4e86\u57fa\u4e8e\u641c\u7d22\u63a8\u7406\u7684\u65b0\u8303\u5f0f\u3002", "conclusion": "MEDAL\u901a\u8fc7\u5c06\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u96c6\u6210\u5230\u6269\u6563\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u7684\u641c\u7d22\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u4f18\u5316\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2512.12058", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12058", "abs": "https://arxiv.org/abs/2512.12058", "authors": ["Anja Sheppard", "Chris Reale", "Katherine A. Skinner"], "title": "A Stochastic Approach to Terrain Maps for Safe Lunar Landing", "comment": "Accepted to IEEE Aerospace 2026", "summary": "Safely landing on the lunar surface is a challenging task, especially in the heavily-shadowed South Pole region where traditional vision-based hazard detection methods are not reliable. The potential existence of valuable resources at the lunar South Pole has made landing in that region a high priority for many space agencies and commercial companies. However, relying on a LiDAR for hazard detection during descent is risky, as this technology is fairly untested in the lunar environment.\n  There exists a rich log of lunar surface data from the Lunar Reconnaissance Orbiter (LRO), which could be used to create informative prior maps of the surface before descent. In this work, we propose a method for generating stochastic elevation maps from LRO data using Gaussian processes (GPs), which are a powerful Bayesian framework for non-parametric modeling that produce accompanying uncertainty estimates. In high-risk environments such as autonomous spaceflight, interpretable estimates of terrain uncertainty are critical. However, no previous approaches to stochastic elevation mapping have taken LRO Digital Elevation Model (DEM) confidence maps into account, despite this data containing key information about the quality of the DEM in different areas.\n  To address this gap, we introduce a two-stage GP model in which a secondary GP learns spatially varying noise characteristics from DEM confidence data. This heteroscedastic information is then used to inform the noise parameters for the primary GP, which models the lunar terrain. Additionally, we use stochastic variational GPs to enable scalable training. By leveraging GPs, we are able to more accurately model the impact of heteroscedastic sensor noise on the resulting elevation map. As a result, our method produces more informative terrain uncertainty, which can be used for downstream tasks such as hazard detection and safe landing site selection.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5229\u7528LRO\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\u7f6e\u4fe1\u5ea6\u6570\u636e\u751f\u6210\u6708\u7403\u5357\u6781\u533a\u57df\u968f\u673a\u9ad8\u7a0b\u5730\u56fe\uff0c\u4e3a\u5b89\u5168\u7740\u9646\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u5f02\u65b9\u5dee\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u6708\u7403\u5357\u6781\u533a\u57df\u5b58\u5728\u9634\u5f71\uff0c\u4f20\u7edf\u89c6\u89c9\u5371\u9669\u68c0\u6d4b\u4e0d\u53ef\u9760\uff0c\u800cLiDAR\u6280\u672f\u672a\u7ecf\u9a8c\u8bc1\u3002\u8be5\u533a\u57df\u8d44\u6e90\u4ef7\u503c\u9ad8\uff0c\u9700\u8981\u5b89\u5168\u7740\u9646\u65b9\u6848\u3002LRO\u6570\u636e\u5305\u542b\u4e30\u5bcc\u8868\u9762\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528DEM\u7f6e\u4fe1\u5ea6\u6570\u636e\u4e2d\u7684\u5f02\u65b9\u5dee\u566a\u58f0\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u6b21\u7ea7GP\u4eceDEM\u7f6e\u4fe1\u5ea6\u6570\u636e\u5b66\u4e60\u7a7a\u95f4\u53d8\u5316\u7684\u566a\u58f0\u7279\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u4e3bGP\u5efa\u6a21\u6708\u7403\u5730\u5f62\uff0c\u5e76\u5c06\u5f02\u65b9\u5dee\u566a\u58f0\u4fe1\u606f\u4f5c\u4e3a\u566a\u58f0\u53c2\u6570\u3002\u91c7\u7528\u968f\u673a\u53d8\u5206GP\u5b9e\u73b0\u53ef\u6269\u5c55\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u5f02\u65b9\u5dee\u4f20\u611f\u5668\u566a\u58f0\u5bf9\u9ad8\u7a0b\u5730\u56fe\u7684\u5f71\u54cd\uff0c\u751f\u6210\u66f4\u5177\u4fe1\u606f\u91cf\u7684\u5730\u5f62\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u53ef\u7528\u4e8e\u5371\u9669\u68c0\u6d4b\u548c\u5b89\u5168\u7740\u9646\u70b9\u9009\u62e9\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e24\u9636\u6bb5GP\u65b9\u6cd5\u5145\u5206\u5229\u7528LRO DEM\u7f6e\u4fe1\u5ea6\u6570\u636e\uff0c\u4e3a\u6708\u7403\u5357\u6781\u7740\u9646\u4efb\u52a1\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u5730\u5f62\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u63d0\u9ad8\u4e86\u81ea\u4e3b\u822a\u5929\u4efb\u52a1\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2512.12238", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12238", "abs": "https://arxiv.org/abs/2512.12238", "authors": ["Yinzhu Cheng", "Haihua Xie", "Yaqing Wang", "Miao He", "Mingming Sun"], "title": "Semantic Distance Measurement based on Multi-Kernel Gaussian Processes", "comment": null, "summary": "Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Mat\u00e9rn and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u6838\u9ad8\u65af\u8fc7\u7a0b\u7684\u8bed\u4e49\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u6838\u53c2\u6570\u5b66\u4e60\u66ff\u4ee3\u4f20\u7edf\u56fa\u5b9a\u65b9\u6cd5\uff0c\u5728\u7ec6\u7c92\u5ea6\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u8bed\u4e49\u8ddd\u79bb\u65b9\u6cd5\u901a\u5e38\u662f\u56fa\u5b9a\u7684\uff0c\u96be\u4ee5\u9002\u5e94\u7279\u5b9a\u6570\u636e\u5206\u5e03\u548c\u4efb\u52a1\u9700\u6c42\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u3001\u53ef\u81ea\u9002\u5e94\u5b66\u4e60\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u591a\u6838\u9ad8\u65af\u8fc7\u7a0b\u7684\u8bed\u4e49\u8ddd\u79bb\u5ea6\u91cf\uff1a\u5c06\u6587\u672c\u7684\u6f5c\u5728\u8bed\u4e49\u51fd\u6570\u5efa\u6a21\u4e3a\u9ad8\u65af\u8fc7\u7a0b\uff0c\u4f7f\u7528\u7ed3\u5408Mat\u00e9rn\u6838\u548c\u591a\u9879\u5f0f\u6838\u7684\u7ec4\u5408\u6838\u51fd\u6570\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u81ea\u52a8\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u6838\u53c2\u6570\u3002", "result": "\u5728\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u7684\u7ec6\u7c92\u5ea6\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u8bed\u4e49\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\u6709\u6548\u3002", "conclusion": "\u591a\u6838\u9ad8\u65af\u8fc7\u7a0b\u65b9\u6cd5\u80fd\u591f\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u65b9\u5f0f\u5b66\u4e60\u8bed\u4e49\u8ddd\u79bb\uff0c\u76f8\u6bd4\u4f20\u7edf\u56fa\u5b9a\u65b9\u6cd5\u66f4\u5177\u9002\u5e94\u6027\u548c\u7075\u6d3b\u6027\uff0c\u5728\u6587\u672c\u8bed\u4e49\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u6027\u80fd\u3002"}}
{"id": "2512.12194", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12194", "abs": "https://arxiv.org/abs/2512.12194", "authors": ["Min-Won Seo", "Aamodh Suresh", "Carlos Nieto-Granda", "Solmaz S. Kia"], "title": "B-ActiveSEAL: Scalable Uncertainty-Aware Active Exploration with Tightly Coupled Localization-Mapping", "comment": "18 pages, 17 figures", "summary": "Active robot exploration requires decision-making processes that integrate localization and mapping under tightly coupled uncertainty. However, managing these interdependent uncertainties over long-term operations in large-scale environments rapidly becomes computationally intractable. To address this challenge, we propose B-ActiveSEAL, a scalable information-theoretic active exploration framework that explicitly accounts for coupled uncertainties-from perception through mapping-into the decision-making process. Our framework (i) adaptively balances map uncertainty (exploration) and localization uncertainty (exploitation), (ii) accommodates a broad class of generalized entropy measures, enabling flexible and uncertainty-aware active exploration, and (iii) establishes Behavioral entropy (BE) as an effective information measure for active exploration by enabling intuitive and adaptive decision-making under coupled uncertainties. We establish a theoretical foundation for propagating coupled uncertainties and integrating them into general entropy formulations, enabling uncertainty-aware active exploration under tightly coupled localization-mapping. The effectiveness of the proposed approach is validated through rigorous theoretical analysis and extensive experiments on open-source maps and ROS-Unity simulations across diverse and complex environments. The results demonstrate that B-ActiveSEAL achieves a well-balanced exploration-exploitation trade-off and produces diverse, adaptive exploration behaviors across environments, highlighting clear advantages over representative baselines.", "AI": {"tldr": "B-ActiveSEAL\uff1a\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u4fe1\u606f\u8bba\u4e3b\u52a8\u63a2\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u884c\u4e3a\u71b5\uff08BE\uff09\u5728\u7d27\u5bc6\u8026\u5408\u7684\u5b9a\u4f4d-\u5efa\u56fe\u4e0d\u786e\u5b9a\u6027\u4e0b\u5b9e\u73b0\u81ea\u9002\u5e94\u63a2\u7d22-\u5229\u7528\u5e73\u8861\u3002", "motivation": "\u5728\u957f\u671f\u5927\u89c4\u6a21\u73af\u5883\u4e2d\uff0c\u4e3b\u52a8\u673a\u5668\u4eba\u63a2\u7d22\u9700\u8981\u5904\u7406\u5b9a\u4f4d\u548c\u5efa\u56fe\u4e4b\u95f4\u7d27\u5bc6\u8026\u5408\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u8ba1\u7b97\u4e0a\u96be\u4ee5\u5904\u7406\u8fd9\u4e9b\u76f8\u4e92\u4f9d\u8d56\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51faB-ActiveSEAL\u6846\u67b6\uff0c\u4f7f\u7528\u884c\u4e3a\u71b5\uff08BE\uff09\u4f5c\u4e3a\u4fe1\u606f\u5ea6\u91cf\uff0c\u81ea\u9002\u5e94\u5e73\u8861\u5730\u56fe\u4e0d\u786e\u5b9a\u6027\uff08\u63a2\u7d22\uff09\u548c\u5b9a\u4f4d\u4e0d\u786e\u5b9a\u6027\uff08\u5229\u7528\uff09\uff0c\u652f\u6301\u5e7f\u4e49\u71b5\u5ea6\u91cf\uff0c\u5efa\u7acb\u8026\u5408\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u7684\u7406\u8bba\u57fa\u7840\u3002", "result": "\u5728\u5f00\u6e90\u5730\u56fe\u548cROS-Unity\u4eff\u771f\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u826f\u597d\u7684\u63a2\u7d22-\u5229\u7528\u5e73\u8861\uff0c\u5728\u4e0d\u540c\u590d\u6742\u73af\u5883\u4e2d\u4ea7\u751f\u591a\u6837\u5316\u7684\u81ea\u9002\u5e94\u63a2\u7d22\u884c\u4e3a\uff0c\u4f18\u4e8e\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "B-ActiveSEAL\u4e3a\u7d27\u5bc6\u8026\u5408\u7684\u5b9a\u4f4d-\u5efa\u56fe\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u4e3b\u52a8\u63a2\u7d22\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u884c\u4e3a\u71b5\u4f5c\u4e3a\u6709\u6548\u7684\u4fe1\u606f\u5ea6\u91cf\u5b9e\u73b0\u4e86\u76f4\u89c2\u7684\u81ea\u9002\u5e94\u51b3\u7b56\u3002"}}
{"id": "2512.12245", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12245", "abs": "https://arxiv.org/abs/2512.12245", "authors": ["Anika Sharma", "Tianyi Niu", "Emma Wrenn", "Shashank Srivastava"], "title": "Adversarially Probing Cross-Family Sound Symbolism in 27 Languages", "comment": null, "summary": "The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.", "AI": {"tldr": "\u9996\u6b21\u5927\u89c4\u6a21\u8de8\u8bed\u8a00\u8ba1\u7b97\u5206\u6790\u58f0\u97f3\u8c61\u5f81\u6027\u5728\u5c3a\u5bf8\u8bed\u4e49\u57df\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8bed\u97f3\u5f62\u5f0f\u80fd\u9884\u6d4b\u5c3a\u5bf8\u542b\u4e49\uff0c\u4e14\u5b58\u5728\u8de8\u8bed\u7cfb\u7684\u58f0\u97f3\u8c61\u5f81\u6027\u504f\u5dee", "motivation": "\u58f0\u97f3\u8c61\u5f81\u6027\uff08\u8bed\u97f3\u4e0e\u610f\u4e49\u7684\u975e\u4efb\u610f\u6620\u5c04\uff09\u957f\u671f\u901a\u8fc7Bouba Kiki\u7b49\u8f76\u4e8b\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u4f46\u7f3a\u4e4f\u5927\u89c4\u6a21\u7cfb\u7edf\u6027\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9996\u6b21\u5728\u5c3a\u5bf8\u8bed\u4e49\u57df\u8fdb\u884c\u8de8\u8bed\u8a00\u8ba1\u7b97\u5206\u6790", "method": "\u6536\u96c627\u79cd\u8bed\u8a00\u5171810\u4e2a\u5f62\u5bb9\u8bcd\uff08\u6bcf\u79cd\u8bed\u8a0030\u8bcd\uff09\uff0c\u8fdb\u884c\u97f3\u4f4d\u8f6c\u5199\u5e76\u7528\u6bcd\u8bed\u8005\u97f3\u9891\u9a8c\u8bc1\u3002\u4f7f\u7528\u57fa\u4e8e\u97f3\u6bb5\u888b\u7279\u5f81\u7684\u53ef\u89e3\u91ca\u5206\u7c7b\u5668\uff0c\u5e76\u8bad\u7ec3\u5bf9\u6297\u6027\u64e6\u9664\u5668\u6765\u6291\u5236\u8bed\u8a00\u8eab\u4efd\u540c\u65f6\u4fdd\u7559\u5c3a\u5bf8\u4fe1\u53f7", "result": "\u97f3\u4f4d\u5f62\u5f0f\u80fd\u663e\u8457\u9884\u6d4b\u5c3a\u5bf8\u8bed\u4e49\uff0c\u5143\u97f3\u548c\u8f85\u97f3\u5747\u6709\u8d21\u732e\u3002\u5bf9\u6297\u6027\u64e6\u9664\u540e\uff0c\u8bed\u8a00\u9884\u6d4b\u964d\u81f3\u673a\u4f1a\u6c34\u5e73\u4ee5\u4e0b\uff0c\u800c\u5c3a\u5bf8\u9884\u6d4b\u4ecd\u663e\u8457\u9ad8\u4e8e\u673a\u4f1a\u6c34\u5e73\uff0c\u8868\u660e\u5b58\u5728\u8de8\u8bed\u7cfb\u7684\u58f0\u97f3\u8c61\u5f81\u6027\u504f\u5dee", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u4e86\u8de8\u8bed\u8a00\u7684\u58f0\u97f3\u8c61\u5f81\u6027\u6a21\u5f0f\uff0c\u7279\u522b\u662f\u5728\u5c3a\u5bf8\u8bed\u4e49\u57df\u3002\u63d0\u4f9b\u4e86\u6570\u636e\u3001\u4ee3\u7801\u548c\u8bca\u65ad\u5de5\u5177\uff0c\u652f\u6301\u672a\u6765\u5927\u89c4\u6a21\u8c61\u4f3c\u6027\u7814\u7a76"}}
{"id": "2512.12203", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12203", "abs": "https://arxiv.org/abs/2512.12203", "authors": ["Eric J. Elias", "Michael Esswein", "Jonathan P. How", "David W. Miller"], "title": "Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion", "comment": "18 pages, 11 figures. To be published in proceedings of AIAA SCITECH 2026 Forum", "summary": "As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids. The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera. However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive. Thermal-infrared cameras can track the target RSO throughout difficult illumination conditions without these limitations. While useful, thermal-infrared imagery lacks the resolution and feature-richness of visible cameras. In this work, images of a target satellite in low Earth orbit are photo-realistically simulated in both visible and thermal-infrared bands. Pixel-level fusion methods are used to create visible/thermal-infrared composites that leverage the best aspects of each camera. Navigation errors from a monocular SLAM algorithm are compared between visible, thermal-infrared, and fused imagery in various lighting and trajectories. Fused imagery yields substantially improved navigation performance over visible-only and thermal-only methods.", "AI": {"tldr": "\u70ed\u7ea2\u5916\u4e0e\u53ef\u89c1\u5149\u56fe\u50cf\u878d\u5408\u63d0\u5347\u592a\u7a7a\u76ee\u6807\u5bfc\u822a\u6027\u80fd", "motivation": "\u5728\u8f68\u64cd\u4f5c\u9700\u6c42\u589e\u957f\uff0c\u9700\u8981\u7cbe\u786e\u5bfc\u822a\u672a\u77e5\u7a7a\u95f4\u76ee\u6807\u3002\u4f20\u7edf\u76f8\u673a\u5728\u9634\u5f71\u671f\u6027\u80fd\u53d7\u9650\uff0c\u6fc0\u5149\u96f7\u8fbe\u7b28\u91cd\u8017\u7535\uff0c\u70ed\u7ea2\u5916\u76f8\u673a\u867d\u80fd\u5728\u6076\u52a3\u5149\u7167\u4e0b\u5de5\u4f5c\u4f46\u5206\u8fa8\u7387\u4f4e\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u5bf9\u4f4e\u5730\u7403\u8f68\u9053\u76ee\u6807\u536b\u661f\u8fdb\u884c\u53ef\u89c1\u5149\u548c\u70ed\u7ea2\u5916\u6ce2\u6bb5\u7684\u5149\u771f\u5b9e\u6a21\u62df\uff0c\u4f7f\u7528\u50cf\u7d20\u7ea7\u878d\u5408\u65b9\u6cd5\u521b\u5efa\u53ef\u89c1\u5149/\u70ed\u7ea2\u5916\u590d\u5408\u56fe\u50cf\uff0c\u5728\u4e0d\u540c\u5149\u7167\u548c\u8f68\u8ff9\u6761\u4ef6\u4e0b\u6bd4\u8f83\u5355\u76eeSLAM\u7b97\u6cd5\u7684\u5bfc\u822a\u8bef\u5dee\u3002", "result": "\u878d\u5408\u56fe\u50cf\u5728\u5bfc\u822a\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u53ef\u89c1\u5149\u6216\u4ec5\u4f7f\u7528\u70ed\u7ea2\u5916\u7684\u65b9\u6cd5\u3002", "conclusion": "\u53ef\u89c1\u5149\u4e0e\u70ed\u7ea2\u5916\u56fe\u50cf\u878d\u5408\u80fd\u6709\u6548\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\uff0c\u4e3a\u592a\u7a7a\u76ee\u6807\u5bfc\u822a\u63d0\u4f9b\u66f4\u4f18\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12264", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12264", "abs": "https://arxiv.org/abs/2512.12264", "authors": ["Abhay Srivastava", "Sam Jung", "Spencer Mateega"], "title": "Market-Bench: Evaluating Large Language Models on Introductory Quantitative Trading and Market Dynamics", "comment": null, "summary": "We introduce MARKET-BENCH, a benchmark that evaluates large language models (LLMs) on introductory quantitative trading tasks by asking them to construct executable backtesters from natural-language strategy descriptions and market assumptions. Each instance specifies one of three canonical strategies -- scheduled trading on Microsoft (NASDAQ: MSFT), pairs trading on Coca-Cola (NASDAQ: KO) and Pepsi (NASDAQ: PEP), or delta hedging on MSFT -- and models must produce code whose P\\&L, drawdown, and position paths match a verifiable reference implementation. We assess twelve state-of-the-art models using a multi-round pass@k metric that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics). While most models reliably execute the simplest strategy (average pass@3 of 0.80), errors vary by orders of magnitude across models and tasks: Gemini 3 Pro and Claude 4.5 Sonnet combine strong reliability with low error on simpler strategies, GPT-5.1 Codex-Max achieves perfect pass@1 on the first two strategies and the lowest best-run error on the easiest task, and Qwen3 Max attains perfect pass@3 yet sometimes produces inaccurate P\\&L paths. These results show that current LLMs can scaffold basic trading infrastructure but still struggle to reason robustly about prices, inventory, and risk; we release MARKET-BENCH and a public leaderboard at https://marketbench.ai.", "AI": {"tldr": "MARKET-BENCH\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91cf\u5316\u4ea4\u6613\u4efb\u52a1\u4e0a\u8868\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u6a21\u578b\u6839\u636e\u81ea\u7136\u8bed\u8a00\u7b56\u7565\u63cf\u8ff0\u6784\u5efa\u53ef\u6267\u884c\u7684\u56de\u6d4b\u5668\uff0c\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\u5f53\u524dLLM\u80fd\u591f\u642d\u5efa\u57fa\u672c\u4ea4\u6613\u6846\u67b6\u4f46\u5728\u4ef7\u683c\u3001\u5e93\u5b58\u548c\u98ce\u9669\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u9886\u57df\u7684\u5e94\u7528\u8bc4\u4f30\u7f3a\u4e4f\u7cfb\u7edf\u6027\uff0c\u7279\u522b\u662f\u5728\u91cf\u5316\u4ea4\u6613\u8fd9\u79cd\u9700\u8981\u7cbe\u786e\u4ee3\u7801\u751f\u6210\u548c\u91d1\u878d\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u5728\u5c06\u81ea\u7136\u8bed\u8a00\u4ea4\u6613\u7b56\u7565\u8f6c\u5316\u4e3a\u53ef\u6267\u884c\u56de\u6d4b\u4ee3\u7801\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u521b\u5efaMARKET-BENCH\u57fa\u51c6\uff0c\u5305\u542b\u4e09\u79cd\u7ecf\u5178\u4ea4\u6613\u7b56\u7565\uff1a\u5fae\u8f6f\u7684\u5b9a\u65f6\u4ea4\u6613\u3001\u53ef\u53e3\u53ef\u4e50\u548c\u767e\u4e8b\u7684\u914d\u5bf9\u4ea4\u6613\u3001\u5fae\u8f6f\u7684Delta\u5bf9\u51b2\u3002\u6a21\u578b\u9700\u8981\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u5176\u76c8\u4e8f\u3001\u56de\u64a4\u548c\u6301\u4ed3\u8def\u5f84\u5fc5\u987b\u4e0e\u53c2\u8003\u5b9e\u73b0\u5339\u914d\u3002\u4f7f\u7528\u591a\u8f6epass@k\u6307\u6807\u8bc4\u4f30\uff0c\u533a\u5206\u7ed3\u6784\u53ef\u9760\u6027\uff08\u4ee3\u7801\u80fd\u5426\u8fd0\u884c\uff09\u548c\u6570\u503c\u51c6\u786e\u6027\uff08\u56de\u6d4b\u6307\u6807\u7684\u5747\u65b9\u8bef\u5dee\uff09\u3002", "result": "\u8bc4\u4f30\u4e8612\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff1a\u5927\u591a\u6570\u6a21\u578b\u80fd\u53ef\u9760\u6267\u884c\u6700\u7b80\u5355\u7684\u7b56\u7565\uff08\u5e73\u5747pass@3\u4e3a0.80\uff09\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u548c\u4efb\u52a1\u95f4\u7684\u8bef\u5dee\u5dee\u5f02\u5de8\u5927\u3002Gemini 3 Pro\u548cClaude 4.5 Sonnet\u5728\u7b80\u5355\u7b56\u7565\u4e0a\u7ed3\u5408\u4e86\u5f3a\u53ef\u9760\u6027\u548c\u4f4e\u8bef\u5dee\uff1bGPT-5.1 Codex-Max\u5728\u524d\u4e24\u4e2a\u7b56\u7565\u4e0a\u8fbe\u5230\u5b8c\u7f8epass@1\uff0c\u5728\u6700\u7b80\u5355\u4efb\u52a1\u4e0a\u83b7\u5f97\u6700\u4f4e\u6700\u4f73\u8fd0\u884c\u8bef\u5dee\uff1bQwen3 Max\u8fbe\u5230\u5b8c\u7f8epass@3\u4f46\u6709\u65f6\u4ea7\u751f\u4e0d\u51c6\u786e\u7684\u76c8\u4e8f\u8def\u5f84\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u642d\u5efa\u57fa\u672c\u7684\u4ea4\u6613\u57fa\u7840\u8bbe\u65bd\u6846\u67b6\uff0c\u4f46\u5728\u4ef7\u683c\u3001\u5e93\u5b58\u548c\u98ce\u9669\u65b9\u9762\u7684\u7a33\u5065\u63a8\u7406\u80fd\u529b\u4ecd\u7136\u4e0d\u8db3\u3002\u7814\u7a76\u56e2\u961f\u53d1\u5e03\u4e86MARKET-BENCH\u57fa\u51c6\u548c\u516c\u5f00\u6392\u884c\u699c\uff0c\u4ee5\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.12211", "categories": ["cs.RO", "cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12211", "abs": "https://arxiv.org/abs/2512.12211", "authors": ["Longchao Da", "David Isele", "Hua Wei", "Manish Saroya"], "title": "Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving", "comment": "9 Pages, 8 Figures", "summary": "Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u81ea\u9002\u5e94\u8bc4\u4f30\u8f68\u8ff9\u9884\u6d4b\u5668\u6027\u80fd\u7684\u7ba1\u9053\uff0c\u4ece\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u4e24\u4e2a\u7ef4\u5ea6\u52a8\u6001\u8bc4\u4f30\uff0c\u6bd4\u4f20\u7edf\u8bef\u5dee\u6307\u6807\u66f4\u80fd\u53cd\u6620\u9884\u6d4b\u5668\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5b9e\u9645\u9a7e\u9a76\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u8f68\u8ff9\u9884\u6d4b\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56ADE\u3001FDE\u7b49\u8bef\u5dee\u6307\u6807\uff0c\u8fd9\u4e9b\u6307\u6807\u53ea\u5173\u6ce8\u4e8b\u540e\u51c6\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u9884\u6d4b\u5668\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u5f71\u54cd\u3002\u9ad8\u8d28\u91cf\u9884\u6d4b\u5668\u4e0d\u4ec5\u9700\u8981\u51c6\u786e\u6027\uff0c\u8fd8\u5e94\u6355\u6349\u90bb\u5c45\u8f66\u8f86\u6240\u6709\u53ef\u80fd\u7684\u8fd0\u52a8\u65b9\u5411\uff0c\u4ee5\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8c28\u614e\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7efc\u5408\u8bc4\u4f30\u7ba1\u9053\uff0c\u4ece\u4e24\u4e2a\u7ef4\u5ea6\u81ea\u9002\u5e94\u8bc4\u4f30\u9884\u6d4b\u5668\u6027\u80fd\uff1a\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\u3002\u57fa\u4e8e\u9a7e\u9a76\u573a\u666f\u7684\u5173\u952e\u6027\uff0c\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u88ab\u52a8\u6001\u7ec4\u5408\uff0c\u6700\u7ec8\u5f97\u51fa\u9884\u6d4b\u5668\u6027\u80fd\u8bc4\u5206\u3002\u5728\u95ed\u73af\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u8bc4\u4f30\u7ba1\u9053\u6bd4\u4f20\u7edf\u6307\u6807\u4ea7\u751f\u66f4\u5408\u7406\u7684\u8bc4\u4f30\u7ed3\u679c\uff0c\u80fd\u66f4\u597d\u5730\u53cd\u6620\u9884\u6d4b\u5668\u8bc4\u4f30\u4e0e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9a7e\u9a76\u6027\u80fd\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u8be5\u7ba1\u9053\u63d0\u4f9b\u4e86\u4e00\u79cd\u7a33\u5065\u7684\u65b9\u6cd5\u6765\u9009\u62e9\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9a7e\u9a76\u6027\u80fd\u8d21\u732e\u6700\u5927\u7684\u9884\u6d4b\u5668\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bc4\u4f30\u7ba1\u9053\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u8f68\u8ff9\u9884\u6d4b\u5668\u6027\u80fd\uff0c\u4e0d\u4ec5\u8003\u8651\u51c6\u786e\u6027\uff0c\u8fd8\u8003\u8651\u591a\u6837\u6027\uff0c\u4ece\u800c\u66f4\u597d\u5730\u652f\u6301\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u51b3\u7b56\u5236\u5b9a\uff0c\u4e3a\u9009\u62e9\u6700\u4f18\u9884\u6d4b\u5668\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2512.12297", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12297", "abs": "https://arxiv.org/abs/2512.12297", "authors": ["Radu-Gabriel Chivereanu", "Tiberiu Boros"], "title": "F5-TTS-RO: Extending F5-TTS to Romanian TTS via Lightweight Input Adaptation", "comment": "Accepted at The 20th International Conference on Linguistic Resources and Tools for Natural Language Processing", "summary": "This work introduces a lightweight input-level adapter for the F5-TTS model that enables Romanian Language support. To preserve the existing capabilities of the model (voice cloning, English and Chinese support), we keep the original weights frozen, append a sub-network to the model and train it as an extension for the textual embedding matrix of the text encoder. For simplicity, we rely on ConvNeXt module implemented in F5-TTS to also model the co-dependencies between the new character-level embeddings. The module serves as a ``soft`` letter-to-sound layer, converting Romanian text into a continuous representation that the F5-TTS model uses to produce naturally sounding Romanian utterances. We evaluate the model with a pool of 20 human listeners across three tasks: (a) audio similarity between reference and generated speech, (b) pronunciation and naturalness and (c) Romanian-English code-switching. The results indicate that our approach maintains voice cloning capabilities and enables, to a certain extent, code-switching within the same utterance; however, residual English accent characteristics remain. We open-source our code and provide example audio samples at https://github.com/racai-ro/Ro-F5TTS.", "AI": {"tldr": "\u4e3aF5-TTS\u6a21\u578b\u5f15\u5165\u8f7b\u91cf\u7ea7\u8f93\u5165\u9002\u914d\u5668\uff0c\u652f\u6301\u7f57\u9a6c\u5c3c\u4e9a\u8bed\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u6709\u8bed\u97f3\u514b\u9686\u3001\u82f1\u8bed\u548c\u4e2d\u6587\u80fd\u529b\u3002", "motivation": "\u6269\u5c55F5-TTS\u6a21\u578b\u4ee5\u652f\u6301\u7f57\u9a6c\u5c3c\u4e9a\u8bed\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u539f\u6709\u7684\u8bed\u97f3\u514b\u9686\u3001\u82f1\u8bed\u548c\u4e2d\u6587\u80fd\u529b\uff0c\u907f\u514d\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\u3002", "method": "\u51bb\u7ed3\u539f\u59cb\u6a21\u578b\u6743\u91cd\uff0c\u5728\u6587\u672c\u7f16\u7801\u5668\u7684\u6587\u672c\u5d4c\u5165\u77e9\u9635\u540e\u6dfb\u52a0\u5b50\u7f51\u7edc\uff0c\u4f7f\u7528ConvNeXt\u6a21\u5757\u5efa\u6a21\u65b0\u5b57\u7b26\u7ea7\u5d4c\u5165\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f5c\u4e3a\"\u8f6f\"\u5b57\u6bcd\u5230\u58f0\u97f3\u8f6c\u6362\u5c42\u3002", "result": "\u6a21\u578b\u4fdd\u6301\u4e86\u8bed\u97f3\u514b\u9686\u80fd\u529b\uff0c\u80fd\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u5b9e\u73b0\u7f57\u9a6c\u5c3c\u4e9a\u8bed-\u82f1\u8bed\u4ee3\u7801\u5207\u6362\uff0c\u4f46\u4ecd\u6709\u6b8b\u7559\u7684\u82f1\u8bed\u53e3\u97f3\u7279\u5f81\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u8f93\u5165\u9002\u914d\u5668\u6709\u6548\u6269\u5c55\u4e86F5-TTS\u5bf9\u7f57\u9a6c\u5c3c\u4e9a\u8bed\u7684\u652f\u6301\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u539f\u6709\u529f\u80fd\uff0c\u4e3a\u591a\u8bed\u8a00TTS\u6269\u5c55\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.12228", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12228", "abs": "https://arxiv.org/abs/2512.12228", "authors": ["Huichang Yun", "Seungho Yoo"], "title": "Semantic Zone based 3D Map Management for Mobile Robot", "comment": "12 pages, 11 figures", "summary": "Mobile robots in large-scale indoor environments, such as hospitals and logistics centers, require accurate 3D spatial representations. However, 3D maps consume substantial memory, making it difficult to maintain complete map data within limited computational resources. Existing SLAM frameworks typically rely on geometric distance or temporal metrics for memory management, often resulting in inefficient data retrieval in spatially compartmentalized environments. To address this, we propose a semantic zone-based 3D map management method that shifts the paradigm from geometry-centric to semantics-centric control. Our approach partitions the environment into meaningful spatial units (e.g., lobbies, hallways) and designates these zones as the primary unit for memory management. By dynamically loading only task-relevant zones into Working Memory (WM) and offloading inactive zones to Long-Term Memory (LTM), the system strictly enforces user-defined memory thresholds. Implemented within the RTAB-Map framework, our method demonstrates substantial reductions in unnecessary signature load/unload cycles and cumulative memory utilization compared to standard approaches. The results confirm that semantic zone-based management ensures stable, predictable memory usage while preserving map availability for navigation. Code is available at: https://github.com/huichangs/rtabmap/tree/segment", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u4e49\u5206\u533a\u76843D\u5730\u56fe\u7ba1\u7406\u65b9\u6cd5\uff0c\u5c06\u73af\u5883\u5212\u5206\u4e3a\u6709\u610f\u4e49\u7684\u7a7a\u95f4\u5355\u5143\uff08\u5982\u5927\u5385\u3001\u8d70\u5eca\uff09\uff0c\u4f5c\u4e3a\u5185\u5b58\u7ba1\u7406\u7684\u57fa\u672c\u5355\u4f4d\uff0c\u52a8\u6001\u52a0\u8f7d\u4efb\u52a1\u76f8\u5173\u533a\u57df\u5230\u5de5\u4f5c\u5185\u5b58\uff0c\u5378\u8f7d\u975e\u6d3b\u8dc3\u533a\u57df\u5230\u957f\u671f\u5185\u5b58\uff0c\u5b9e\u73b0\u7a33\u5b9a\u7684\u5185\u5b58\u4f7f\u7528\u3002", "motivation": "\u5927\u89c4\u6a21\u5ba4\u5185\u73af\u5883\uff08\u5982\u533b\u9662\u3001\u7269\u6d41\u4e2d\u5fc3\uff09\u4e2d\u7684\u79fb\u52a8\u673a\u5668\u4eba\u9700\u8981\u7cbe\u786e\u76843D\u7a7a\u95f4\u8868\u793a\uff0c\u4f463D\u5730\u56fe\u5360\u7528\u5927\u91cf\u5185\u5b58\uff0c\u96be\u4ee5\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e2d\u7ef4\u62a4\u5b8c\u6574\u5730\u56fe\u6570\u636e\u3002\u73b0\u6709SLAM\u6846\u67b6\u901a\u5e38\u4f9d\u8d56\u51e0\u4f55\u8ddd\u79bb\u6216\u65f6\u95f4\u6307\u6807\u8fdb\u884c\u5185\u5b58\u7ba1\u7406\uff0c\u5728\u7a7a\u95f4\u5206\u9694\u73af\u5883\u4e2d\u5bfc\u81f4\u6570\u636e\u68c0\u7d22\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u5206\u533a\u57fa\u7840\u76843D\u5730\u56fe\u7ba1\u7406\u65b9\u6cd5\uff0c\u5c06\u73af\u5883\u5212\u5206\u4e3a\u6709\u610f\u4e49\u7684\u7a7a\u95f4\u5355\u5143\uff08\u8bed\u4e49\u533a\u57df\uff09\uff0c\u5c06\u8fd9\u4e9b\u533a\u57df\u4f5c\u4e3a\u5185\u5b58\u7ba1\u7406\u7684\u4e3b\u8981\u5355\u4f4d\u3002\u7cfb\u7edf\u52a8\u6001\u5730\u5c06\u4efb\u52a1\u76f8\u5173\u533a\u57df\u52a0\u8f7d\u5230\u5de5\u4f5c\u5185\u5b58\uff08WM\uff09\uff0c\u5c06\u975e\u6d3b\u8dc3\u533a\u57df\u5378\u8f7d\u5230\u957f\u671f\u5185\u5b58\uff08LTM\uff09\uff0c\u4e25\u683c\u5f3a\u5236\u6267\u884c\u7528\u6237\u5b9a\u4e49\u7684\u5185\u5b58\u9608\u503c\u3002\u8be5\u65b9\u6cd5\u5728RTAB-Map\u6846\u67b6\u4e2d\u5b9e\u73b0\u3002", "result": "\u4e0e\u6807\u51c6\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u7b7e\u540d\u52a0\u8f7d/\u5378\u8f7d\u5468\u671f\u548c\u7d2f\u79ef\u5185\u5b58\u4f7f\u7528\u3002\u7ed3\u679c\u8bc1\u5b9e\u57fa\u4e8e\u8bed\u4e49\u5206\u533a\u7684\u7ba1\u7406\u786e\u4fdd\u4e86\u7a33\u5b9a\u3001\u53ef\u9884\u6d4b\u7684\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bfc\u822a\u6240\u9700\u7684\u5730\u56fe\u53ef\u7528\u6027\u3002", "conclusion": "\u8bed\u4e49\u5206\u533a\u57fa\u7840\u76843D\u5730\u56fe\u7ba1\u7406\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u5ba4\u5185\u73af\u5883\u4e2d3D\u5730\u56fe\u5185\u5b58\u6d88\u8017\u5927\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4ece\u51e0\u4f55\u4e2d\u5fc3\u8f6c\u5411\u8bed\u4e49\u4e2d\u5fc3\u7684\u63a7\u5236\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5185\u5b58\u7ba1\u7406\u548c\u6570\u636e\u68c0\u7d22\u3002"}}
{"id": "2512.12337", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12337", "abs": "https://arxiv.org/abs/2512.12337", "authors": ["Yushen Fang", "Jianjun Li", "Mingqian Ding", "Chang Liu", "Xinchi Zou", "Wenqi Yang"], "title": "SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema", "comment": null, "summary": "Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.", "AI": {"tldr": "\u63d0\u51faSCIR\u6846\u67b6\u548cMBSC\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u81ea\u6821\u6b63\u8fed\u4ee3\u4f18\u5316\u964d\u4f4eLLM\u4fe1\u606f\u62bd\u53d6\u7684\u8bad\u7ec3\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u4fe1\u606f\u62bd\u53d6\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u8bad\u7ec3\u6210\u672c\u9ad8\uff1b2\uff09\u96be\u4ee5\u4e0eLLM\u504f\u597d\u5bf9\u9f50\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSCIR\uff08\u81ea\u6821\u6b63\u8fed\u4ee3\u4f18\u5316\uff09\u6846\u67b6\uff0c\u5305\u542b\u53cc\u8def\u5f84\u81ea\u6821\u6b63\u6a21\u5757\u548c\u53cd\u9988\u9a71\u52a8\u4f18\u5316\uff0c\u5b9e\u73b0\u4e0e\u73b0\u6709LLM\u548cIE\u7cfb\u7edf\u7684\u5373\u63d2\u5373\u7528\u517c\u5bb9\u3002\u540c\u65f6\u6784\u5efaMBSC\uff08\u591a\u4efb\u52a1\u53cc\u8bed\u81ea\u6821\u6b63\uff09\u6570\u636e\u96c6\uff0c\u5305\u542b10\u4e07+\u6761\u76ee\uff0c\u901a\u8fc7\u95f4\u63a5\u84b8\u998fGPT-4\u80fd\u529b\u6765\u5bf9\u9f50\u504f\u597d\u3002", "result": "SCIR\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u5173\u7cfb\u62bd\u53d6\u548c\u4e8b\u4ef6\u62bd\u53d6\u4e09\u4e2a\u5173\u952e\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff0cspan-based Micro-F1\u5e73\u5747\u63d0\u53475.27%\uff0c\u540c\u65f6\u8bad\u7ec3\u6210\u672c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u964d\u4f4e87%\u3002", "conclusion": "SCIR\u6846\u67b6\u548cMBSC\u6570\u636e\u96c6\u4e0d\u4ec5\u63d0\u5347\u4e86IE\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\uff0c\u8fd8\u4e3a\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684IE\u8303\u5f0f\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2512.12230", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12230", "abs": "https://arxiv.org/abs/2512.12230", "authors": ["Jonathan Spraggett"], "title": "Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy", "comment": "Accepted at 28th RoboCup International Symposium", "summary": "Fall recovery is a critical skill for humanoid robots in dynamic environments such as RoboCup, where prolonged downtime often decides the match. Recent techniques using deep reinforcement learning (DRL) have produced robust get-up behaviors, yet existing methods require training of separate policies for each robot morphology. This paper presents a single DRL policy capable of recovering from falls across seven humanoid robots with diverse heights (0.48 - 0.81 m), weights (2.8 - 7.9 kg), and dynamics. Trained with CrossQ, the unified policy transfers zero-shot up to 86 +/- 7% (95% CI [81, 89]) on unseen morphologies, eliminating the need for robot-specific training. Comprehensive leave-one-out experiments, morph scaling analysis, and diversity ablations show that targeted morphological coverage improves zero-shot generalization. In some cases, the shared policy even surpasses the specialist baselines. These findings illustrate the practicality of morphology-agnostic control for fall recovery, laying the foundation for generalist humanoid control. The software is open-source and available at: https://github.com/utra-robosoccer/unified-humanoid-getup", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u80fd\u8ba9\u4e03\u79cd\u4e0d\u540c\u5f62\u6001\u7684\u4eba\u5f62\u673a\u5668\u4eba\u4ece\u8dcc\u5012\u4e2d\u6062\u590d\uff0c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u673a\u5668\u4eba\u5355\u72ec\u8bad\u7ec3\u3002", "motivation": "\u5728RoboCup\u7b49\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u4eba\u5f62\u673a\u5668\u4eba\u8dcc\u5012\u6062\u590d\u662f\u5173\u952e\u6280\u80fd\u3002\u73b0\u6709\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u4e3a\u6bcf\u79cd\u673a\u5668\u4eba\u5f62\u6001\u8bad\u7ec3\u5355\u72ec\u7b56\u7565\uff0c\u8fd9\u9650\u5236\u4e86\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u4f7f\u7528CrossQ\u8bad\u7ec3\u4e00\u4e2a\u7edf\u4e00\u7684DRL\u7b56\u7565\uff0c\u8986\u76d6\u4e03\u79cd\u4e0d\u540c\u9ad8\u5ea6\u3001\u91cd\u91cf\u548c\u52a8\u529b\u5b66\u7279\u6027\u7684\u4eba\u5f62\u673a\u5668\u4eba\u3002\u901a\u8fc7\u7559\u4e00\u6cd5\u5b9e\u9a8c\u3001\u5f62\u6001\u7f29\u653e\u5206\u6790\u548c\u591a\u6837\u6027\u6d88\u878d\u7814\u7a76\u6765\u9a8c\u8bc1\u65b9\u6cd5\u3002", "result": "\u7edf\u4e00\u7b56\u7565\u5728\u672a\u89c1\u8fc7\u7684\u673a\u5668\u4eba\u5f62\u6001\u4e0a\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u6210\u529f\u7387\u9ad8\u8fbe86\u00b17%\uff0895%\u7f6e\u4fe1\u533a\u95f4[81,89]\uff09\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u5171\u4eab\u7b56\u7565\u751a\u81f3\u8d85\u8fc7\u4e86\u4e13\u95e8\u8bad\u7ec3\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u6709\u9488\u5bf9\u6027\u7684\u5f62\u6001\u8986\u76d6\u53ef\u4ee5\u6539\u5584\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5f62\u6001\u65e0\u5173\u7684\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u901a\u7528\u4eba\u5f62\u63a7\u5236\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2512.12444", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12444", "abs": "https://arxiv.org/abs/2512.12444", "authors": ["Veronica Mangiaterra", "Hamad Al-Azary", "Chiara Barattieri di San Pietro", "Paolo Canal", "Valentina Bambini"], "title": "Can GPT replace human raters? Validity and reliability of machine-generated norms for metaphors", "comment": "30 pages, 5 figures", "summary": "As Large Language Models (LLMs) are increasingly being used in scientific research, the issue of their trustworthiness becomes crucial. In psycholinguistics, LLMs have been recently employed in automatically augmenting human-rated datasets, with promising results obtained by generating ratings for single words. Yet, performance for ratings of complex items, i.e., metaphors, is still unexplored. Here, we present the first assessment of the validity and reliability of ratings of metaphors on familiarity, comprehensibility, and imageability, generated by three GPT models for a total of 687 items gathered from the Italian Figurative Archive and three English studies. We performed a thorough validation in terms of both alignment with human data and ability to predict behavioral and electrophysiological responses. We found that machine-generated ratings positively correlated with human-generated ones. Familiarity ratings reached moderate-to-strong correlations for both English and Italian metaphors, although correlations weakened for metaphors with high sensorimotor load. Imageability showed moderate correlations in English and moderate-to-strong in Italian. Comprehensibility for English metaphors exhibited the strongest correlations. Overall, larger models outperformed smaller ones and greater human-model misalignment emerged with familiarity and imageability. Machine-generated ratings significantly predicted response times and the EEG amplitude, with a strength comparable to human ratings. Moreover, GPT ratings obtained across independent sessions were highly stable. We conclude that GPT, especially larger models, can validly and reliably replace - or augment - human subjects in rating metaphor properties. Yet, LLMs align worse with humans when dealing with conventionality and multimodal aspects of metaphorical meaning, calling for careful consideration of the nature of stimuli.", "AI": {"tldr": "\u8bc4\u4f30GPT\u6a21\u578b\u5728\u751f\u6210\u9690\u55bb\u5c5e\u6027\u8bc4\u5206\uff08\u719f\u6089\u5ea6\u3001\u53ef\u7406\u89e3\u6027\u3001\u5f62\u8c61\u6027\uff09\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u8f83\u5927\u6a21\u578b\u4e0e\u4eba\u7c7b\u8bc4\u5206\u6709\u826f\u597d\u76f8\u5173\u6027\uff0c\u5e76\u80fd\u9884\u6d4b\u884c\u4e3a\u53cd\u5e94\uff0c\u4f46\u5904\u7406\u5e38\u89c4\u6027\u548c\u591a\u6a21\u6001\u9690\u55bb\u65f6\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u5176\u53ef\u4fe1\u5ea6\u95ee\u9898\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u5728\u5fc3\u7406\u8bed\u8a00\u5b66\u4e2d\uff0cLLMs\u5df2\u88ab\u7528\u4e8e\u81ea\u52a8\u6269\u5145\u4eba\u7c7b\u8bc4\u5206\u6570\u636e\u96c6\uff0c\u4f46\u5728\u590d\u6742\u9879\u76ee\uff08\u5982\u9690\u55bb\uff09\u7684\u8bc4\u5206\u6027\u80fd\u5c1a\u672a\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u4e09\u4e2aGPT\u6a21\u578b\u5bf9687\u4e2a\u6765\u81ea\u610f\u5927\u5229\u6bd4\u55bb\u6863\u6848\u548c\u4e09\u4e2a\u82f1\u8bed\u7814\u7a76\u7684\u9690\u55bb\u9879\u76ee\u751f\u6210\u719f\u6089\u5ea6\u3001\u53ef\u7406\u89e3\u6027\u548c\u5f62\u8c61\u6027\u8bc4\u5206\u3002\u901a\u8fc7\u4e0e\u4eba\u6570\u636e\u5bf9\u9f50\u5ea6\u3001\u9884\u6d4b\u884c\u4e3a\u548c\u7535\u751f\u7406\u53cd\u5e94\u80fd\u529b\u8fdb\u884c\u5168\u9762\u9a8c\u8bc1\u3002", "result": "\u673a\u5668\u751f\u6210\u8bc4\u5206\u4e0e\u4eba\u7c7b\u8bc4\u5206\u5448\u6b63\u76f8\u5173\uff1a\u719f\u6089\u5ea6\u8bc4\u5206\u5728\u82f1\u8bed\u548c\u610f\u5927\u5229\u8bed\u9690\u55bb\u4e2d\u8fbe\u5230\u4e2d\u7b49\u5230\u5f3a\u76f8\u5173\uff08\u9ad8\u611f\u89c9\u8fd0\u52a8\u8d1f\u8377\u9690\u55bb\u76f8\u5173\u51cf\u5f31\uff09\uff1b\u5f62\u8c61\u6027\u5728\u82f1\u8bed\u4e2d\u4e2d\u7b49\u76f8\u5173\uff0c\u610f\u5927\u5229\u8bed\u4e2d\u4e2d\u7b49\u5230\u5f3a\u76f8\u5173\uff1b\u82f1\u8bed\u9690\u55bb\u53ef\u7406\u89e3\u6027\u76f8\u5173\u6700\u5f3a\u3002\u8f83\u5927\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u673a\u5668\u8bc4\u5206\u80fd\u663e\u8457\u9884\u6d4b\u53cd\u5e94\u65f6\u95f4\u548cEEG\u632f\u5e45\uff0c\u4e0e\u4eba\u7c7b\u8bc4\u5206\u9884\u6d4b\u5f3a\u5ea6\u76f8\u5f53\u3002GPT\u8bc4\u5206\u5728\u4e0d\u540c\u4f1a\u8bdd\u95f4\u9ad8\u5ea6\u7a33\u5b9a\u3002", "conclusion": "GPT\uff08\u5c24\u5176\u662f\u8f83\u5927\u6a21\u578b\uff09\u53ef\u4ee5\u6709\u6548\u53ef\u9760\u5730\u66ff\u4ee3\u6216\u589e\u5f3a\u4eba\u7c7b\u88ab\u8bd5\u8fdb\u884c\u9690\u55bb\u5c5e\u6027\u8bc4\u5206\u3002\u4f46\u5f53\u5904\u7406\u9690\u55bb\u610f\u4e49\u7684\u5e38\u89c4\u6027\u548c\u591a\u6a21\u6001\u65b9\u9762\u65f6\uff0cLLMs\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u8f83\u5dee\uff0c\u9700\u8981\u4ed4\u7ec6\u8003\u8651\u523a\u6fc0\u6027\u8d28\u3002"}}
{"id": "2512.12233", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12233", "abs": "https://arxiv.org/abs/2512.12233", "authors": ["Murad Mehrab Abrar", "Trevor W. Harrison"], "title": "Robust Underwater Localization of Buoyancy Driven microFloats Using Acoustic Time-of-Flight Measurements", "comment": "9 pages", "summary": "Accurate underwater localization remains a challenge for inexpensive autonomous platforms that require highfrequency position updates. In this paper, we present a robust, low-cost localization pipeline for buoyancy-driven microFloats operating in coastal waters. We build upon previous work by introducing a bidirectional acoustic Time-of-Flight (ToF) localization framework, which incorporates both float-to-buoy and buoy-to-float transmissions, thereby increasing the number of usable measurements. The method integrates nonlinear trilateration with a filtering of computed position estimates based on geometric cost and Cramer-Rao Lower Bounds (CRLB). This approach removes outliers caused by multipath effects and other acoustic errors from the ToF estimation and improves localization robustness without relying on heavy smoothing. We validate the framework in two field deployments in Puget Sound, Washington, USA. The localization pipeline achieves median positioning errors below 4 m relative to GPS positions. The filtering technique shows a reduction in mean error from 139.29 m to 12.07 m, and improved alignment of trajectories with GPS paths. Additionally, we demonstrate a Time-Difference-of-Arrival (TDoA) localization for unrecovered floats that were transmitting during the experiment. Range-based acoustic localization techniques are widely used and generally agnostic to hardware-this work aims to maximize their utility by improving positioning frequency and robustness through careful algorithmic design.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9c81\u68d2\u7684\u6c34\u4e0b\u58f0\u5b66\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411ToF\u6d4b\u91cf\u548c\u51e0\u4f55\u6210\u672c/CRLB\u6ee4\u6ce2\uff0c\u5728\u6cbf\u6d77\u6c34\u57df\u5b9e\u73b04\u7c73\u4e2d\u503c\u5b9a\u4f4d\u7cbe\u5ea6", "motivation": "\u4f4e\u6210\u672c\u81ea\u4e3b\u5e73\u53f0\u9700\u8981\u9ad8\u9891\u4f4d\u7f6e\u66f4\u65b0\uff0c\u4f46\u51c6\u786e\u7684\u6c34\u4e0b\u5b9a\u4f4d\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6cbf\u6d77\u6c34\u57df\u53d7\u591a\u5f84\u6548\u5e94\u548c\u58f0\u5b66\u8bef\u5dee\u5f71\u54cd\uff0c\u9700\u8981\u63d0\u9ad8\u5b9a\u4f4d\u9c81\u68d2\u6027\u548c\u9891\u7387", "method": "\u91c7\u7528\u53cc\u5411\u58f0\u5b66\u98de\u884c\u65f6\u95f4(ToF)\u5b9a\u4f4d\u6846\u67b6\uff0c\u7ed3\u5408\u6d6e\u6807\u5230\u6d6e\u4f53\u548c\u6d6e\u4f53\u5230\u6d6e\u6807\u7684\u53cc\u5411\u4f20\u8f93\u589e\u52a0\u53ef\u7528\u6d4b\u91cf\u3002\u96c6\u6210\u975e\u7ebf\u6027\u4e09\u8fb9\u6d4b\u91cf\uff0c\u5e76\u57fa\u4e8e\u51e0\u4f55\u6210\u672c\u548c\u514b\u62c9\u7f8e\u7f57\u4e0b\u754c(CRLB)\u5bf9\u8ba1\u7b97\u7684\u4f4d\u7f6e\u4f30\u8ba1\u8fdb\u884c\u6ee4\u6ce2\uff0c\u53bb\u9664\u591a\u5f84\u6548\u5e94\u548c\u58f0\u5b66\u8bef\u5dee\u5bfc\u81f4\u7684\u5f02\u5e38\u503c", "result": "\u5728\u7f8e\u56fd\u534e\u76db\u987f\u5dde\u666e\u5409\u7279\u6e7e\u7684\u4e24\u6b21\u5b9e\u5730\u90e8\u7f72\u4e2d\uff0c\u5b9a\u4f4d\u7ba1\u9053\u5b9e\u73b0\u4e86\u76f8\u5bf9\u4e8eGPS\u4f4d\u7f6e\u7684\u4e2d\u503c\u5b9a\u4f4d\u8bef\u5dee\u4f4e\u4e8e4\u7c73\u3002\u6ee4\u6ce2\u6280\u672f\u5c06\u5e73\u5747\u8bef\u5dee\u4ece139.29\u7c73\u964d\u4f4e\u523012.07\u7c73\uff0c\u8f68\u8ff9\u4e0eGPS\u8def\u5f84\u7684\u5bf9\u9f50\u5ea6\u663e\u8457\u6539\u5584\u3002\u8fd8\u5c55\u793a\u4e86\u672a\u56de\u6536\u6d6e\u4f53\u7684\u5230\u8fbe\u65f6\u95f4\u5dee(TDoA)\u5b9a\u4f4d", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7b97\u6cd5\uff0c\u57fa\u4e8e\u8ddd\u79bb\u7684\u58f0\u5b66\u5b9a\u4f4d\u6280\u672f\u53ef\u4ee5\u63d0\u9ad8\u5b9a\u4f4d\u9891\u7387\u548c\u9c81\u68d2\u6027\u3002\u53cc\u5411ToF\u6846\u67b6\u7ed3\u5408\u51e0\u4f55/CRLB\u6ee4\u6ce2\u80fd\u6709\u6548\u5904\u7406\u591a\u5f84\u6548\u5e94\uff0c\u4e3a\u4f4e\u6210\u672c\u5fae\u6d6e\u4f53\u5728\u6cbf\u6d77\u6c34\u57df\u63d0\u4f9b\u53ef\u9760\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.12447", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12447", "abs": "https://arxiv.org/abs/2512.12447", "authors": ["Gary Lupyan"], "title": "Large language models have learned to use language", "comment": "Commentary on Futrell & Mahowald's How Linguistics Learned to Stop Worrying and Love the Language Models (BBS, Forthcoming)", "summary": "Acknowledging that large language models have learned to use language can open doors to breakthrough language science. Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u627f\u8ba4\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u5b66\u4f1a\u4f7f\u7528\u8bed\u8a00\uff0c\u8fd9\u80fd\u5f00\u542f\u8bed\u8a00\u79d1\u5b66\u7a81\u7834\uff0c\u4f46\u9700\u8981\u653e\u5f03\u4e00\u4e9b\u4f20\u7edf\u8bc4\u4f30\u89c2\u5ff5\u5e76\u63a5\u53d7\u540e\u56fe\u7075\u6d4b\u8bd5\u65f6\u4ee3", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u80fd\u529b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4f20\u7edf\u8bed\u8a00\u79d1\u5b66\u8bc4\u4f30\u65b9\u6cd5\u53ef\u80fd\u5df2\u4e0d\u9002\u5e94\u65b0\u65f6\u4ee3\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u5982\u4f55\u8bc4\u4f30\u8bed\u8a00\u77e5\u8bc6", "method": "\u901a\u8fc7\u54f2\u5b66\u548c\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51fa\u9700\u8981\u653e\u5f03\u67d0\u4e9b\u957f\u671f\u6301\u6709\u7684\u8bed\u8a00\u77e5\u8bc6\u8bc4\u4f30\u89c2\u5ff5\uff0c\u627f\u8ba4\u540e\u56fe\u7075\u6d4b\u8bd5\u65f6\u4ee3\u7684\u5230\u6765", "result": "\u8ba4\u8bc6\u5230\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u771f\u6b63\u5b66\u4f1a\u4f7f\u7528\u8bed\u8a00\uff0c\u8fd9\u4e3a\u8bed\u8a00\u79d1\u5b66\u7a81\u7834\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u4f46\u9700\u8981\u76f8\u5e94\u7684\u7406\u8bba\u548c\u65b9\u6cd5\u8bba\u8f6c\u53d8", "conclusion": "\u4e3a\u4e86\u5728\u8bed\u8a00\u79d1\u5b66\u9886\u57df\u53d6\u5f97\u7a81\u7834\uff0c\u5fc5\u987b\u63a5\u53d7\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u5177\u5907\u8bed\u8a00\u80fd\u529b\u7684\u4e8b\u5b9e\uff0c\u5e76\u76f8\u5e94\u8c03\u6574\u8bc4\u4f30\u6846\u67b6\u548c\u7814\u7a76\u8303\u5f0f\uff0c\u8fce\u63a5\u540e\u56fe\u7075\u6d4b\u8bd5\u65f6\u4ee3"}}
{"id": "2512.12243", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12243", "abs": "https://arxiv.org/abs/2512.12243", "authors": ["HT To", "S Nguyen", "NH Pham"], "title": "CAR-CHASE: Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement", "comment": null, "summary": "Multi-Agent Path Finding (MAPF) for car-like robots, addressed by algorithms such as Conflict-Based Search with Continuous Time (CL-CBS), faces significant computational challenges due to expensive kinematic heuristic calculations. Traditional heuristic caching assumes that the heuristic function depends only on the state, which is incorrect in CBS where constraints from conflict resolution make the search space context-dependent. We propose \\textbf{CAR-CHASE} (Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement), a novel approach that combines \\textbf{conflict-aware heuristic caching} -- which caches heuristic values based on both state and relevant constraint context -- with an \\textbf{adaptive hybrid heuristic} that intelligently switches between fast approximate and exact computations. Our key innovations are (1) a compact \\emph{conflict fingerprint} that efficiently encodes which constraints affect a state's heuristic, (2) a relevance filter using spatial, temporal, and geometric criteria, and (3) an adaptive switching strategy with theoretical quality bounds. Experimental evaluation on 480 benchmark instances with varying agent counts (10 to 30) and obstacle densities (0\\% and 50\\%) demonstrates a geometric mean speedup of 2.46$\\times$ over the baseline CL-CBS implementation while maintaining solution optimality. The optimizations improve success rate from 77.9\\% to 84.8\\% (+6.9 percentage points), reduce total runtime by 70.1\\%, and enable solving 33 additional instances that previously timed out. Performance gains scale with problem complexity, reaching up to 4.06$\\times$ speedup for challenging 30-agent obstacle scenarios. Our techniques are general and applicable to other CBS variants.", "AI": {"tldr": "\u63d0\u51faCAR-CHASE\u65b9\u6cd5\uff0c\u901a\u8fc7\u51b2\u7a81\u611f\u77e5\u542f\u53d1\u5f0f\u7f13\u5b58\u548c\u81ea\u9002\u5e94\u6df7\u5408\u542f\u53d1\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u7c7b\u8f66\u673a\u5668\u4eba\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u4fdd\u6301\u6700\u4f18\u6027\u7684\u540c\u65f6\u83b7\u5f972.46\u500d\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u3002", "motivation": "\u4f20\u7edf\u7c7b\u8f66\u673a\u5668\u4ebaMAPF\u7b97\u6cd5\uff08\u5982CL-CBS\uff09\u9762\u4e34\u8ba1\u7b97\u6311\u6218\uff0c\u4e3b\u8981\u56e0\u4e3a\u6602\u8d35\u7684\u8fd0\u52a8\u5b66\u542f\u53d1\u5f0f\u8ba1\u7b97\u3002\u4f20\u7edf\u542f\u53d1\u5f0f\u7f13\u5b58\u5047\u8bbe\u542f\u53d1\u51fd\u6570\u4ec5\u4f9d\u8d56\u4e8e\u72b6\u6001\uff0c\u8fd9\u5728CBS\u4e2d\u4e0d\u6210\u7acb\uff0c\u56e0\u4e3a\u51b2\u7a81\u89e3\u51b3\u5f15\u5165\u7684\u7ea6\u675f\u4f7f\u641c\u7d22\u7a7a\u95f4\u5177\u6709\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\u3002", "method": "\u63d0\u51faCAR-CHASE\u65b9\u6cd5\uff1a1) \u51b2\u7a81\u611f\u77e5\u542f\u53d1\u5f0f\u7f13\u5b58\uff0c\u57fa\u4e8e\u72b6\u6001\u548c\u76f8\u5173\u7ea6\u675f\u4e0a\u4e0b\u6587\u7f13\u5b58\u542f\u53d1\u503c\uff1b2) \u81ea\u9002\u5e94\u6df7\u5408\u542f\u53d1\u5f0f\uff0c\u667a\u80fd\u5207\u6362\u5feb\u901f\u8fd1\u4f3c\u548c\u7cbe\u786e\u8ba1\u7b97\uff1b3) \u7d27\u51d1\u7684\u51b2\u7a81\u6307\u7eb9\u7f16\u7801\u7ea6\u675f\u5f71\u54cd\uff1b4) \u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u51e0\u4f55\u76f8\u5173\u6027\u7684\u8fc7\u6ee4\uff1b5) \u5177\u6709\u7406\u8bba\u8d28\u91cf\u754c\u7684\u81ea\u9002\u5e94\u5207\u6362\u7b56\u7565\u3002", "result": "\u5728480\u4e2a\u57fa\u51c6\u5b9e\u4f8b\u4e0a\u8bc4\u4f30\uff0c\u51e0\u4f55\u5e73\u5747\u52a0\u901f2.46\u500d\uff0c\u6210\u529f\u7387\u4ece77.9%\u63d0\u5347\u81f384.8%\uff08+6.9\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u603b\u8fd0\u884c\u65f6\u95f4\u51cf\u5c1170.1%\uff0c\u89e3\u51b3\u4e8633\u4e2a\u5148\u524d\u8d85\u65f6\u7684\u5b9e\u4f8b\u3002\u6027\u80fd\u63d0\u5347\u968f\u95ee\u9898\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u572830\u667a\u80fd\u4f53\u969c\u788d\u573a\u666f\u4e2d\u8fbe\u52304.06\u500d\u52a0\u901f\u3002", "conclusion": "CAR-CHASE\u901a\u8fc7\u51b2\u7a81\u611f\u77e5\u542f\u53d1\u5f0f\u7f13\u5b58\u548c\u81ea\u9002\u5e94\u6df7\u5408\u542f\u53d1\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7c7b\u8f66\u673a\u5668\u4ebaMAPF\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u7684\u6700\u4f18\u6027\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6CBS\u53d8\u4f53\u3002"}}
{"id": "2512.12488", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12488", "abs": "https://arxiv.org/abs/2512.12488", "authors": ["James Luther", "Donald Brown"], "title": "The American Ghost in the Machine: How language models align culturally and the effects of cultural prompting", "comment": null, "summary": "Culture is the bedrock of human interaction; it dictates how we perceive and respond to everyday interactions. As the field of human-computer interaction grows via the rise of generative Large Language Models (LLMs), the cultural alignment of these models become an important field of study. This work, using the VSM13 International Survey and Hofstede's cultural dimensions, identifies the cultural alignment of popular LLMs (DeepSeek-V3, V3.1, GPT-5, GPT-4.1, GPT-4, Claude Opus 4, Llama 3.1, and Mistral Large). We then use cultural prompting, or using system prompts to shift the cultural alignment of a model to a desired country, to test the adaptability of these models to other cultures, namely China, France, India, Iran, Japan, and the United States. We find that the majority of the eight LLMs tested favor the United States when the culture is not specified, with varying results when prompted for other cultures. When using cultural prompting, seven of the eight models shifted closer to the expected culture. We find that models had trouble aligning with Japan and China, despite two of the models tested originating with the Chinese company DeepSeek.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528VSM13\u56fd\u9645\u8c03\u67e5\u548c\u970d\u592b\u65af\u6cf0\u5fb7\u6587\u5316\u7ef4\u5ea6\u5206\u6790\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u5bf9\u9f50\u6027\uff0c\u53d1\u73b0\u591a\u6570\u6a21\u578b\u9ed8\u8ba4\u504f\u5411\u7f8e\u56fd\u6587\u5316\uff0c\u901a\u8fc7\u6587\u5316\u63d0\u793a\u53ef\u4ee5\u8c03\u6574\u6a21\u578b\u5411\u76ee\u6807\u6587\u5316\u5bf9\u9f50\uff0c\u4f46\u5bf9\u4e2d\u56fd\u548c\u65e5\u672c\u6587\u5316\u7684\u5bf9\u9f50\u6548\u679c\u8f83\u5dee\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6a21\u578b\u7684\u6587\u5316\u5bf9\u9f50\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u4e0d\u540c\u6587\u5316\u80cc\u666f\u7684\u7528\u6237\u9700\u8981\u6a21\u578b\u80fd\u591f\u7406\u89e3\u548c\u9002\u5e94\u5176\u6587\u5316\u4ef7\u503c\u89c2\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u4e3b\u6d41LLM\u6587\u5316\u5bf9\u9f50\u6027\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u3002", "method": "\u4f7f\u7528VSM13\u56fd\u9645\u8c03\u67e5\u548c\u970d\u592b\u65af\u6cf0\u5fb7\u6587\u5316\u7ef4\u5ea6\u6846\u67b6\uff0c\u8bc4\u4f30DeepSeek-V3\u3001GPT-5\u3001Claude Opus 4\u7b498\u4e2a\u4e3b\u6d41LLM\u7684\u6587\u5316\u5bf9\u9f50\u6027\u3002\u901a\u8fc7\u6587\u5316\u63d0\u793a\u6280\u672f\uff0c\u6d4b\u8bd5\u6a21\u578b\u5411\u4e2d\u56fd\u3001\u6cd5\u56fd\u3001\u5370\u5ea6\u3001\u4f0a\u6717\u3001\u65e5\u672c\u548c\u7f8e\u56fd\u7b49\u76ee\u6807\u6587\u5316\u8c03\u6574\u7684\u80fd\u529b\u3002", "result": "1. \u672a\u6307\u5b9a\u6587\u5316\u65f6\uff0c\u591a\u6570\u6a21\u578b\u9ed8\u8ba4\u504f\u5411\u7f8e\u56fd\u6587\u5316\uff1b2. \u4f7f\u7528\u6587\u5316\u63d0\u793a\u540e\uff0c8\u4e2a\u6a21\u578b\u4e2d\u67097\u4e2a\u80fd\u591f\u5411\u76ee\u6807\u6587\u5316\u5bf9\u9f50\uff1b3. \u6a21\u578b\u5bf9\u4e2d\u56fd\u548c\u65e5\u672c\u6587\u5316\u7684\u5bf9\u9f50\u6548\u679c\u8f83\u5dee\uff0c\u5373\u4f7fDeepSeek\u6a21\u578b\u6e90\u81ea\u4e2d\u56fd\u516c\u53f8\u4e5f\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u660e\u663e\u7684\u6587\u5316\u504f\u5411\u6027\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u8fdb\u884c\u8c03\u6574\u3002\u7136\u800c\uff0c\u6a21\u578b\u5bf9\u67d0\u4e9b\u7279\u5b9a\u6587\u5316\uff08\u7279\u522b\u662f\u4e2d\u56fd\u548c\u65e5\u672c\uff09\u7684\u9002\u5e94\u80fd\u529b\u6709\u9650\uff0c\u8868\u660e\u9700\u8981\u66f4\u6df1\u5165\u7684\u6587\u5316\u5bf9\u9f50\u7814\u7a76\u548c\u6a21\u578b\u4f18\u5316\u3002"}}
{"id": "2512.12320", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12320", "abs": "https://arxiv.org/abs/2512.12320", "authors": ["Canqi Meng", "Weibang Bai"], "title": "Programmable Deformation Design of Porous Soft Actuator through Volumetric-Pattern-Induced Anisotropy", "comment": null, "summary": "Conventional soft pneumatic actuators, typically based on hollow elastomeric chambers, often suffer from small structural support and require costly geometry-specific redesigns for multimodal functionality. Porous materials such as foam, filled into chambers, can provide structural stability for the actuators. However, methods to achieve programmable deformation by tailoring the porous body itself remain underexplored. In this paper, a novel design method is presented to realize soft porous actuators with programmable deformation by incising specific patterns into the porous foam body. This approach introduces localized structural anisotropy of the foam guiding the material's deformation under a global vacuum input. Furthermore, three fundamental patterns on a cylindrical foam substrate are discussed: transverse for bending, longitudinal for tilting, and diagonal for twisting. A computational model is built with Finite Element Analysis (FEA), to investigate the mechanism of the incision-patterning method. Experiments demonstrate that with a potential optimal design of the pattern array number N, actuators can achieve bending up to $80^{\\circ}$ (N=2), tilting of $18^{\\circ}$ (N=1), and twisting of $115^{\\circ}$ (N=8). The versatility of our approach is demonstrated via pattern transferability, scalability, and mold-less rapid prototyping of complex designs. As a comprehensive application, we translate the human hand crease map into a functional incision pattern, creating a bio-inspired soft robot hand capable of human-like adaptive grasping. Our work provides a new, efficient, and scalable paradigm for the design of multi-functional soft porous robots.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7\u5728\u591a\u5b54\u6ce1\u6cab\u4f53\u4e0a\u5207\u5272\u7279\u5b9a\u56fe\u6848\u6765\u5b9e\u73b0\u53ef\u7f16\u7a0b\u53d8\u5f62\u7684\u8f6f\u591a\u5b54\u81f4\u52a8\u5668\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5229\u7528\u771f\u7a7a\u9a71\u52a8\u5b9e\u73b0\u5f2f\u66f2\u3001\u503e\u659c\u548c\u626d\u8f6c\u7b49\u591a\u79cd\u53d8\u5f62\u6a21\u5f0f\u3002", "motivation": "\u4f20\u7edf\u8f6f\u6c14\u52a8\u81f4\u52a8\u5668\u5b58\u5728\u7ed3\u6784\u652f\u6491\u4e0d\u8db3\u3001\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u529f\u80fd\u8fdb\u884c\u6602\u8d35\u7684\u51e0\u4f55\u91cd\u65b0\u8bbe\u8ba1\u7b49\u95ee\u9898\u3002\u867d\u7136\u591a\u5b54\u6750\u6599\u53ef\u4ee5\u63d0\u4f9b\u7ed3\u6784\u7a33\u5b9a\u6027\uff0c\u4f46\u5982\u4f55\u901a\u8fc7\u8c03\u63a7\u591a\u5b54\u4f53\u672c\u8eab\u5b9e\u73b0\u53ef\u7f16\u7a0b\u53d8\u5f62\u4ecd\u7f3a\u4e4f\u63a2\u7d22\u3002", "method": "\u5728\u591a\u5b54\u6ce1\u6cab\u5706\u67f1\u4f53\u4e0a\u5207\u5272\u7279\u5b9a\u56fe\u6848\uff08\u6a2a\u5411\u3001\u7eb5\u5411\u3001\u5bf9\u89d2\u7ebf\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u5c40\u90e8\u7ed3\u6784\u5404\u5411\u5f02\u6027\u6765\u5f15\u5bfc\u6750\u6599\u5728\u5168\u5c40\u771f\u7a7a\u8f93\u5165\u4e0b\u7684\u53d8\u5f62\u3002\u5efa\u7acb\u6709\u9650\u5143\u5206\u6790\u6a21\u578b\u7814\u7a76\u5207\u53e3\u56fe\u6848\u65b9\u6cd5\u7684\u673a\u7406\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e0d\u540c\u56fe\u6848\u9635\u5217\u6570\u91cf\u4e0b\u7684\u53d8\u5f62\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u901a\u8fc7\u4f18\u5316\u56fe\u6848\u9635\u5217\u6570\u91cf\uff0c\u81f4\u52a8\u5668\u53ef\u5b9e\u73b0\u9ad8\u8fbe80\u00b0\u7684\u5f2f\u66f2\uff08N=2\uff09\u300118\u00b0\u7684\u503e\u659c\uff08N=1\uff09\u548c115\u00b0\u7684\u626d\u8f6c\uff08N=8\uff09\u3002\u65b9\u6cd5\u5177\u6709\u56fe\u6848\u53ef\u8f6c\u79fb\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u65e0\u9700\u6a21\u5177\u7684\u5feb\u901f\u539f\u578b\u5236\u4f5c\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u591a\u529f\u80fd\u8f6f\u591a\u5b54\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u3001\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u4eba\u624b\u8936\u76b1\u56fe\u8f6c\u5316\u4e3a\u529f\u80fd\u6027\u5207\u53e3\u56fe\u6848\uff0c\u6210\u529f\u521b\u5efa\u4e86\u5177\u6709\u7c7b\u4eba\u81ea\u9002\u5e94\u6293\u53d6\u80fd\u529b\u7684\u4eff\u751f\u8f6f\u673a\u5668\u4eba\u624b\u3002"}}
{"id": "2512.12537", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12537", "abs": "https://arxiv.org/abs/2512.12537", "authors": ["Agniva Maiti", "Manya Pandey", "Murari Mandal"], "title": "NagaNLP: Bootstrapping NLP for Low-Resource Nagamese Creole with Human-in-the-Loop Synthetic Data", "comment": null, "summary": "The vast majority of the world's languages, particularly creoles like Nagamese, remain severely under-resourced in Natural Language Processing (NLP), creating a significant barrier to their representation in digital technology. This paper introduces NagaNLP, a comprehensive open-source toolkit for Nagamese, bootstrapped through a novel methodology that relies on LLM-driven but human-validated synthetic data generation. We detail a multi-stage pipeline where an expert-guided LLM (Gemini) generates a candidate corpus, which is then refined and annotated by native speakers. This synthetic-hybrid approach yielded a 10K pair conversational dataset and a high-quality annotated corpus for foundational tasks. To assess the effectiveness of our methodology, we trained both discriminative and generative models. Our fine-tuned XLM-RoBERTa-base model establishes a new benchmark for Nagamese, achieving a 93.81\\% accuracy (0.90 F1-Macro) on Part-of-Speech tagging and a 0.75 F1-Macro on Named Entity Recognition, massively outperforming strong zero-shot baselines. Furthermore, we fine-tuned a Llama-3.2-3B Instruct model, named NagaLLaMA, which demonstrates superior performance on conversational tasks, achieving a Perplexity of 3.85, an order of magnitude improvement over its few-shot counterpart (96.76). We release the NagaNLP toolkit, including all datasets, models, and code, providing a foundational resource for a previously underserved language and a reproducible framework for reducing data scarcity in other low-resource contexts.", "AI": {"tldr": "NagaNLP\u662f\u4e00\u4e2a\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00Nagamese\u7684\u5f00\u6e90NLP\u5de5\u5177\u5305\uff0c\u901a\u8fc7LLM\u751f\u6210+\u4eba\u5de5\u9a8c\u8bc1\u7684\u5408\u6210\u6570\u636e\u65b9\u6cd5\u6784\u5efa\u4e86\u9996\u4e2a\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3\u4e86SOTA\u7684POS\u3001NER\u548c\u5bf9\u8bdd\u6a21\u578b\u3002", "motivation": "\u4e16\u754c\u4e0a\u5927\u591a\u6570\u8bed\u8a00\uff08\u7279\u522b\u662f\u50cfNagamese\u8fd9\u6837\u7684\u514b\u91cc\u5965\u5c14\u8bed\uff09\u5728NLP\u9886\u57df\u4e25\u91cd\u7f3a\u4e4f\u8d44\u6e90\uff0c\u8fd9\u963b\u788d\u4e86\u5b83\u4eec\u5728\u6570\u5b57\u6280\u672f\u4e2d\u7684\u4ee3\u8868\u6027\u3002\u9700\u8981\u4e3a\u8fd9\u4e9b\u4f4e\u8d44\u6e90\u8bed\u8a00\u5f00\u53d1\u53ef\u590d\u73b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e13\u5bb6\u6307\u5bfc\u7684LLM\uff08Gemini\uff09\u751f\u6210\u5019\u9009\u8bed\u6599\u5e93\uff0c\u7136\u540e\u7531\u6bcd\u8bed\u8005\u8fdb\u884c\u7cbe\u70bc\u548c\u6807\u6ce8\u7684\u591a\u9636\u6bb5\u5408\u6210-\u6df7\u5408\u65b9\u6cd5\u3002\u751f\u6210\u4e8610K\u5bf9\u8bdd\u5bf9\u6570\u636e\u96c6\u548c\u9ad8\u8d28\u91cf\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u8bad\u7ec3\u5224\u522b\u5f0f\u6a21\u578b\uff08XLM-RoBERTa-base\uff09\u548c\u751f\u6210\u5f0f\u6a21\u578b\uff08Llama-3.2-3B\uff09\u3002", "result": "XLM-RoBERTa-base\u6a21\u578b\u5728POS\u6807\u6ce8\u4e0a\u8fbe\u523093.81%\u51c6\u786e\u7387\uff080.90 F1-Macro\uff09\uff0cNER\u8fbe\u52300.75 F1-Macro\uff0c\u5927\u5e45\u8d85\u8d8a\u96f6\u6837\u672c\u57fa\u7ebf\u3002NagaLLaMA\u5bf9\u8bdd\u6a21\u578b\u56f0\u60d1\u5ea6\u8fbe\u52303.85\uff0c\u6bd4\u5c11\u6837\u672c\u7248\u672c\uff0896.76\uff09\u63d0\u5347\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "NagaNLP\u4e3aNagamese\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u7684NLP\u8d44\u6e90\uff0c\u540c\u65f6\u4e3a\u5176\u4ed6\u4f4e\u8d44\u6e90\u8bed\u8a00\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u51cf\u5c11\u6570\u636e\u7a00\u7f3a\u6027\u7684\u6846\u67b6\u3002\u6240\u6709\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u4ee3\u7801\u5747\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.12377", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12377", "abs": "https://arxiv.org/abs/2512.12377", "authors": ["Haichuan Li", "Changda Tian", "Panos Trahanias", "Tomi Westerlund"], "title": "INDOOR-LiDAR: Bridging Simulation and Reality for Robot-Centric 360 degree Indoor LiDAR Perception -- A Robot-Centric Hybrid Dataset", "comment": null, "summary": "We present INDOOR-LIDAR, a comprehensive hybrid dataset of indoor 3D LiDAR point clouds designed to advance research in robot perception. Existing indoor LiDAR datasets often suffer from limited scale, inconsistent annotation formats, and human-induced variability during data collection. INDOOR-LIDAR addresses these limitations by integrating simulated environments with real-world scans acquired using autonomous ground robots, providing consistent coverage and realistic sensor behavior under controlled variations. Each sample consists of dense point cloud data enriched with intensity measurements and KITTI-style annotations. The annotation schema encompasses common indoor object categories within various scenes. The simulated subset enables flexible configuration of layouts, point densities, and occlusions, while the real-world subset captures authentic sensor noise, clutter, and domain-specific artifacts characteristic of real indoor settings. INDOOR-LIDAR supports a wide range of applications including 3D object detection, bird's-eye-view (BEV) perception, SLAM, semantic scene understanding, and domain adaptation between simulated and real indoor domains. By bridging the gap between synthetic and real-world data, INDOOR-LIDAR establishes a scalable, realistic, and reproducible benchmark for advancing robotic perception in complex indoor environments.", "AI": {"tldr": "INDOOR-LIDAR\u662f\u4e00\u4e2a\u6df7\u5408\u5ba4\u51853D LiDAR\u70b9\u4e91\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4eff\u771f\u548c\u771f\u5b9e\u626b\u63cf\u6570\u636e\uff0c\u63d0\u4f9b\u4e00\u81f4\u7684\u8986\u76d6\u548c\u771f\u5b9e\u4f20\u611f\u5668\u884c\u4e3a\uff0c\u652f\u6301\u591a\u79cd\u673a\u5668\u4eba\u611f\u77e5\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u5ba4\u5185LiDAR\u6570\u636e\u96c6\u5b58\u5728\u89c4\u6a21\u6709\u9650\u3001\u6807\u6ce8\u683c\u5f0f\u4e0d\u4e00\u81f4\u3001\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u4e2d\u4eba\u4e3a\u53d8\u5f02\u6027\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5168\u9762\u3001\u4e00\u81f4\u7684\u6570\u636e\u96c6\u6765\u63a8\u8fdb\u673a\u5668\u4eba\u611f\u77e5\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u4eff\u771f\u73af\u5883\u548c\u81ea\u4e3b\u5730\u9762\u673a\u5668\u4eba\u91c7\u96c6\u7684\u771f\u5b9e\u4e16\u754c\u626b\u63cf\u6570\u636e\uff0c\u63d0\u4f9b\u5bc6\u96c6\u70b9\u4e91\u6570\u636e\uff0c\u5305\u542b\u5f3a\u5ea6\u6d4b\u91cf\u548cKITTI\u98ce\u683c\u6807\u6ce8\uff0c\u6db5\u76d6\u591a\u79cd\u5ba4\u5185\u573a\u666f\u548c\u7269\u4f53\u7c7b\u522b\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u4eff\u771f\u548c\u771f\u5b9e\u5b50\u96c6\u7684\u6df7\u5408\u6570\u636e\u96c6\uff0c\u4eff\u771f\u90e8\u5206\u652f\u6301\u7075\u6d3b\u914d\u7f6e\u5e03\u5c40\u3001\u70b9\u5bc6\u5ea6\u548c\u906e\u6321\uff0c\u771f\u5b9e\u90e8\u5206\u6355\u6349\u4e86\u771f\u5b9e\u5ba4\u5185\u73af\u5883\u7684\u4f20\u611f\u5668\u566a\u58f0\u3001\u6742\u4e71\u548c\u7279\u5b9a\u9886\u57df\u4f2a\u5f71\u3002", "conclusion": "INDOOR-LIDAR\u901a\u8fc7\u5f25\u5408\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u611f\u77e5\u7814\u7a76\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u3001\u771f\u5b9e\u4e14\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u3002"}}
{"id": "2512.12544", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12544", "abs": "https://arxiv.org/abs/2512.12544", "authors": ["Yiming Zeng", "Jinghan Cao", "Zexin Li", "Wanhao Yu", "Zhankai Ye", "Dawei Xiang", "Ting Hua", "Xin Liu", "Shangqian Gao", "Tingting Yu"], "title": "HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks", "comment": null, "summary": "Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.", "AI": {"tldr": "HyperEdit\uff1a\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u52a8\u6001\u9002\u5e94\u548c\u5dee\u5f02\u611f\u77e5\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6307\u4ee4\u6587\u672c\u7f16\u8f91\u6027\u80fd\uff0c\u5728\u4fee\u6539\u533a\u57df\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53479%-30% BLEU\u5206\u6570", "motivation": "\u73b0\u6709LLMs\u5728\u6307\u4ee4\u6587\u672c\u7f16\u8f91\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c06\u7f16\u8f91\u89c6\u4e3a\u901a\u7528\u6587\u672c\u751f\u6210\u5bfc\u81f4\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u96be\u4ee5\u5fe0\u5b9e\u5bf9\u9f50\u7528\u6237\u610f\u56fe\uff0c\u4ee5\u53ca\u8fc7\u5ea6\u7f16\u8f91\u672a\u6539\u53d8\u533a\u57df\u3002\u8fd9\u5728\u4ee3\u7801\u7f16\u8f91\u5668\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u5c24\u4e3a\u5173\u952e\uff0c\u56e0\u4e3a\u5373\u4f7f\u662f\u5fae\u5c0f\u610f\u5916\u4fee\u6539\u4e5f\u53ef\u80fd\u7834\u574f\u529f\u80fd\u3002", "method": "1. \u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u52a8\u6001\u9002\u5e94\uff1a\u751f\u6210\u8bf7\u6c42\u7279\u5b9a\u7684\u53c2\u6570\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u6bcf\u4e2a\u6307\u4ee4\u5b9a\u5236\u7f16\u8f91\u7b56\u7565\uff1b2. \u5dee\u5f02\u611f\u77e5\u6b63\u5219\u5316\uff1a\u5c06\u76d1\u7763\u96c6\u4e2d\u5728\u4fee\u6539\u7684\u6587\u672c\u7247\u6bb5\u4e0a\uff0c\u9632\u6b62\u8fc7\u5ea6\u7f16\u8f91\u540c\u65f6\u786e\u4fdd\u7cbe\u786e\u7684\u6700\u5c0f\u5316\u66f4\u6539\u3002", "result": "HyperEdit\u5728\u4fee\u6539\u533a\u57df\u4e0a\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u5bf9\u63d0\u53479%-30% BLEU\u5206\u6570\uff0c\u5c3d\u7ba1\u4ec5\u4f7f\u752830\u4ebf\u53c2\u6570\u3002", "conclusion": "HyperEdit\u901a\u8fc7\u8d85\u7f51\u7edc\u52a8\u6001\u9002\u5e94\u548c\u5dee\u5f02\u611f\u77e5\u6b63\u5219\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6307\u4ee4\u6587\u672c\u7f16\u8f91\u4e2d\u7684\u610f\u56fe\u5bf9\u9f50\u548c\u8fc7\u5ea6\u7f16\u8f91\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f16\u8f91\u6027\u80fd\u3002"}}
{"id": "2512.12427", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12427", "abs": "https://arxiv.org/abs/2512.12427", "authors": ["Rudolf Reiter", "Chao Qin", "Leonard Bauersfeld", "Davide Scaramuzza"], "title": "Unifying Quadrotor Motion Planning and Control by Chaining Different Fidelity Models", "comment": null, "summary": "Many aerial tasks involving quadrotors demand both instant reactivity and long-horizon planning. High-fidelity models enable accurate control but are too slow for long horizons; low-fidelity planners scale but degrade closed-loop performance. We present Unique, a unified MPC that cascades models of different fidelity within a single optimization: a short-horizon, high-fidelity model for accurate control, and a long-horizon, low-fidelity model for planning. We align costs across horizons, derive feasibility-preserving thrust and body-rate constraints for the point-mass model, and introduce transition constraints that match the different states, thrust-induced acceleration, and jerk-body-rate relations. To prevent local minima emerging from nonsmooth clutter, we propose a 3D progressive smoothing schedule that morphs norm-based obstacles along the horizon. In addition, we deploy parallel randomly initialized MPC solvers to discover lower-cost local minima on the long, low-fidelity horizon. In simulation and real flights, under equal computational budgets, Unique improves closed-loop position or velocity tracking by up to 75% compared with standard MPC and hierarchical planner-tracker baselines. Ablations and Pareto analyses confirm robust gains across horizon variations, constraint approximations, and smoothing schedules.", "AI": {"tldr": "Unique\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00MPC\u6846\u67b6\uff0c\u901a\u8fc7\u7ea7\u8054\u4e0d\u540c\u4fdd\u771f\u5ea6\u6a21\u578b\u5b9e\u73b0\u56db\u65cb\u7ffc\u7684\u5373\u65f6\u53cd\u5e94\u6027\u548c\u957f\u65f6\u57df\u89c4\u5212\uff0c\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u5c06\u95ed\u73af\u8ddf\u8e2a\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe75%\u3002", "motivation": "\u56db\u65cb\u7ffc\u7a7a\u4e2d\u4efb\u52a1\u9700\u8981\u540c\u65f6\u5177\u5907\u5373\u65f6\u53cd\u5e94\u6027\u548c\u957f\u65f6\u57df\u89c4\u5212\u80fd\u529b\u3002\u9ad8\u4fdd\u771f\u6a21\u578b\u63a7\u5236\u7cbe\u786e\u4f46\u8ba1\u7b97\u91cf\u5927\uff0c\u4f4e\u4fdd\u771f\u6a21\u578b\u53ef\u6269\u5c55\u4f46\u95ed\u73af\u6027\u80fd\u5dee\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u517c\u987e\u4e24\u8005\u3002", "method": "1) \u5728\u5355\u4e00\u4f18\u5316\u4e2d\u7ea7\u8054\u4e0d\u540c\u4fdd\u771f\u5ea6\u6a21\u578b\uff1a\u77ed\u65f6\u57df\u9ad8\u4fdd\u771f\u6a21\u578b\u7528\u4e8e\u7cbe\u786e\u63a7\u5236\uff0c\u957f\u65f6\u57df\u4f4e\u4fdd\u771f\u6a21\u578b\u7528\u4e8e\u89c4\u5212\uff1b2) \u5bf9\u9f50\u8de8\u65f6\u57df\u6210\u672c\u51fd\u6570\uff1b3) \u63a8\u5bfc\u70b9\u8d28\u91cf\u6a21\u578b\u7684\u53ef\u884c\u6027\u4fdd\u6301\u7ea6\u675f\uff1b4) \u5f15\u5165\u72b6\u6001\u3001\u63a8\u529b\u52a0\u901f\u5ea6\u548c\u6025\u52a8-\u4f53\u901f\u7387\u5173\u7cfb\u7684\u8fc7\u6e21\u7ea6\u675f\uff1b5) \u63d0\u51fa3D\u6e10\u8fdb\u5e73\u6ed1\u8c03\u5ea6\u5904\u7406\u975e\u5e73\u6ed1\u969c\u788d\u7269\uff1b6) \u90e8\u7f72\u5e76\u884c\u968f\u673a\u521d\u59cb\u5316MPC\u6c42\u89e3\u5668\u53d1\u73b0\u66f4\u4f4e\u6210\u672c\u5c40\u90e8\u6700\u5c0f\u503c\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u98de\u884c\u4e2d\uff0c\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0c\u76f8\u6bd4\u6807\u51c6MPC\u548c\u5206\u5c42\u89c4\u5212\u5668-\u8ddf\u8e2a\u5668\u57fa\u7ebf\uff0cUnique\u5c06\u95ed\u73af\u4f4d\u7f6e\u6216\u901f\u5ea6\u8ddf\u8e2a\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe75%\u3002\u6d88\u878d\u5b9e\u9a8c\u548c\u5e15\u7d2f\u6258\u5206\u6790\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u65f6\u57df\u53d8\u5316\u3001\u7ea6\u675f\u8fd1\u4f3c\u548c\u5e73\u6ed1\u8c03\u5ea6\u4e0b\u7684\u9c81\u68d2\u589e\u76ca\u3002", "conclusion": "Unique\u901a\u8fc7\u7ea7\u8054\u4e0d\u540c\u4fdd\u771f\u5ea6\u6a21\u578b\u7684\u7edf\u4e00MPC\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56db\u65cb\u7ffc\u540c\u65f6\u9700\u8981\u5373\u65f6\u53cd\u5e94\u6027\u548c\u957f\u65f6\u57df\u89c4\u5212\u7684\u96be\u9898\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u95ed\u73af\u6027\u80fd\u3002"}}
{"id": "2512.12576", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12576", "abs": "https://arxiv.org/abs/2512.12576", "authors": ["Xueru Wen", "Jie Lou", "Yanjiang Liu", "Hongyu Lin", "Ben He", "Xianpei Han", "Le Sun", "Yaojie Lu", "Debing Zhang"], "title": "Coupled Variational Reinforcement Learning for Language Model General Reasoning", "comment": null, "summary": "While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \\textit{\\b{Co}upled \\b{V}ariational \\b{R}einforcement \\b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\\% over the base model and achieves an additional 2.3\\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.", "AI": {"tldr": "CoVRL\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u63a8\u65ad\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u8026\u5408\u53d8\u5206\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u91c7\u6837\u7b56\u7565\u8fde\u63a5\u5148\u9a8c\u548c\u540e\u9a8c\u5206\u5e03\uff0c\u5728\u65e0\u9700\u9a8c\u8bc1\u5668\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65e0\u9700\u9a8c\u8bc1\u5668\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4ec5\u57fa\u4e8e\u95ee\u9898\u91c7\u6837\u63a8\u7406\u8f68\u8ff9\uff0c\u5bfc\u81f4\u63a8\u7406\u8f68\u8ff9\u4e0e\u6700\u7ec8\u7b54\u6848\u5206\u79bb\uff0c\u9020\u6210\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u548c\u8f68\u8ff9-\u7b54\u6848\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "CoVRL\u901a\u8fc7\u6784\u5efa\u548c\u4f18\u5316\u6574\u5408\u5148\u9a8c\u548c\u540e\u9a8c\u5206\u5e03\u7684\u590d\u5408\u5206\u5e03\uff0c\u91c7\u7528\u6df7\u5408\u91c7\u6837\u7b56\u7565\u8026\u5408\u8fd9\u4e24\u4e2a\u5206\u5e03\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u5e76\u4fdd\u6301\u601d\u7ef4-\u7b54\u6848\u4e00\u81f4\u6027\u3002", "result": "\u5728\u6570\u5b66\u548c\u901a\u7528\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoVRL\u76f8\u6bd4\u57fa\u7840\u6a21\u578b\u6027\u80fd\u63d0\u534712.4%\uff0c\u76f8\u6bd4\u73b0\u6709\u65e0\u9700\u9a8c\u8bc1\u5668\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u989d\u5916\u63d0\u53472.3%\u3002", "conclusion": "CoVRL\u4e3a\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u901a\u8fc7\u8026\u5408\u53d8\u5206\u63a8\u65ad\u548c\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u63a2\u7d22\u6548\u7387\u548c\u4e00\u81f4\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.12437", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12437", "abs": "https://arxiv.org/abs/2512.12437", "authors": ["Jonathan Spraggett"], "title": "Sim2Real Reinforcement Learning for Soccer skills", "comment": "Undergrad Thesis", "summary": "This thesis work presents a more efficient and effective approach to training control-related tasks for humanoid robots using Reinforcement Learning (RL). The traditional RL methods are limited in adapting to real-world environments, complexity, and natural motions, but the proposed approach overcomes these limitations by using curriculum training and Adversarial Motion Priors (AMP) technique. The results show that the developed RL policies for kicking, walking, and jumping are more dynamic, and adaptive, and outperformed previous methods. However, the transfer of the learned policy from simulation to the real world was unsuccessful, highlighting the limitations of current RL methods in fully adapting to real-world scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u8bfe\u7a0b\u5b66\u4e60\u548c\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\u6280\u672f\u6765\u66f4\u9ad8\u6548\u8bad\u7ec3\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u6a21\u62df\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u597d\u7684\u52a8\u6001\u6027\u548c\u9002\u5e94\u6027\uff0c\u4f46\u672a\u80fd\u6210\u529f\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u73af\u5883\u3001\u5904\u7406\u590d\u6742\u6027\u548c\u751f\u6210\u81ea\u7136\u8fd0\u52a8\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u6765\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u8bfe\u7a0b\u8bad\u7ec3\u548c\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\u6280\u672f\u6765\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u9488\u5bf9\u8e22\u7403\u3001\u884c\u8d70\u548c\u8df3\u8dc3\u7b49\u63a7\u5236\u4efb\u52a1\u5f00\u53d1\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5f00\u53d1\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u52a8\u6001\u6027\u548c\u9002\u5e94\u6027\uff0c\u5728\u8e22\u7403\u3001\u884c\u8d70\u548c\u8df3\u8dc3\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u5148\u524d\u65b9\u6cd5\uff0c\u4f46\u672a\u80fd\u6210\u529f\u8fc1\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\u3002", "conclusion": "\u867d\u7136\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6a21\u62df\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4f46\u6a21\u62df\u5230\u771f\u5b9e\u7684\u8fc1\u79fb\u5931\u8d25\u63ed\u793a\u4e86\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5b8c\u5168\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u573a\u666f\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.12608", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12608", "abs": "https://arxiv.org/abs/2512.12608", "authors": ["Hong Su"], "title": "Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery", "comment": null, "summary": "Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53d7\u4eba\u7c7b\u5b66\u4e60\u542f\u53d1\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u8bb0\u5fc6\u5b58\u50a8\u548c\u6700\u5927\u71b5\u65b9\u6cd5\u53d1\u73b0\u673a\u5236\uff0c\u89e3\u51b3LLMs\u5728\u7f55\u89c1\u3001\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u5b66\u4e60\u95ee\u9898\u3002", "motivation": "LLMs\u5728\u5904\u7406\u7f55\u89c1\u3001\u4f4e\u8d44\u6e90\u6216\u672a\u89c1\u8fc7\u7684\u573a\u666f\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u60c5\u51b5\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u7a00\u758f\uff1b\u540c\u65f6LLMs\u4e3b\u8981\u4f9d\u8d56\u9690\u5f0f\u53c2\u6570\u8bb0\u5fc6\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u663e\u5f0f\u83b7\u53d6\u3001\u56de\u5fc6\u548c\u7cbe\u70bc\u65b9\u6cd5\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u66f4\u50cf\u662f\u76f4\u89c9\u9a71\u52a8\u7684\u9884\u6d4b\u5668\u800c\u975e\u65b9\u6cd5\u5bfc\u5411\u7684\u5b66\u4e60\u8005\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u673a\u5236\uff1a1) Obvious Record\uff1a\u663e\u5f0f\u5b58\u50a8\u56e0\u679c\uff08\u6216\u95ee\u9898-\u89e3\u51b3\u65b9\u6848\uff09\u5173\u7cfb\u4f5c\u4e3a\u7b26\u53f7\u8bb0\u5fc6\uff0c\u652f\u6301\u4ece\u5355\u6b21\u6216\u7f55\u89c1\u906d\u9047\u4e2d\u6301\u7eed\u5b66\u4e60\uff1b2) Maximum-Entropy Method Discovery\uff1a\u4f18\u5148\u4fdd\u7559\u8bed\u4e49\u5dee\u5f02\u5ea6\u9ad8\u7684\u65b9\u6cd5\uff0c\u6355\u6349\u901a\u5e38\u88ab\u4e0b\u4e00\u4e2atoken\u9884\u6d4b\u5ffd\u7565\u7684\u591a\u6837\u5316\u548c\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7b56\u7565\u3002", "result": "\u5728\u5305\u542b60\u4e2a\u8bed\u4e49\u591a\u6837\u5316\u95ee\u9898-\u89e3\u51b3\u65b9\u6848\u5bf9\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u63d0\u51fa\u7684\u71b5\u5f15\u5bfc\u65b9\u6cd5\u6bd4\u968f\u673a\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u5f3a\u7684\u672a\u89c1\u95ee\u9898\u8986\u76d6\u7387\u548c\u663e\u8457\u66f4\u9ad8\u7684\u5185\u90e8\u591a\u6837\u6027\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u53d1\u73b0\u66f4\u901a\u7528\u548c\u4eba\u7c7b\u542f\u53d1\u65b9\u6cd5\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u4eba\u7c7b\u542f\u53d1\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u663e\u5f0f\u8bb0\u5fc6\u5b58\u50a8\u548c\u6700\u5927\u71b5\u65b9\u6cd5\u53d1\u73b0\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLMs\u5728\u7f55\u89c1\u573a\u666f\u4e0b\u7684\u5b66\u4e60\u9650\u5236\uff0c\u4f7f\u5176\u80fd\u591f\u4ece\u7a00\u758f\u6570\u636e\u4e2d\u5b66\u4e60\u5e76\u6355\u6349\u591a\u6837\u5316\u7b56\u7565\uff0c\u4ece\u800c\u66f4\u50cf\u4eba\u7c7b\u5b66\u4e60\u8005\u800c\u975e\u5355\u7eaf\u7684\u9884\u6d4b\u5668\u3002"}}
{"id": "2512.12468", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12468", "abs": "https://arxiv.org/abs/2512.12468", "authors": ["Tina Tian", "Xinyu Wang", "Andrew L. Orekhov", "Fujun Ruan", "Lu Li", "Oliver Kroemer", "Howie Choset"], "title": "Autonomously Unweaving Multiple Cables Using Visual Feedback", "comment": "6 pages, 5 figures", "summary": "Many cable management tasks involve separating out the different cables and removing tangles. Automating this task is challenging because cables are deformable and can have combinations of knots and multiple interwoven segments. Prior works have focused on untying knots in one cable, which is one subtask of cable management. However, in this paper, we focus on a different subtask called multi-cable unweaving, which refers to removing the intersections among multiple interwoven cables to separate them and facilitate further manipulation. We propose a method that utilizes visual feedback to unweave a bundle of loosely entangled cables. We formulate cable unweaving as a pick-and-place problem, where the grasp position is selected from discrete nodes in a graph-based cable state representation. Our cable state representation encodes both topological and geometric information about the cables from the visual image. To predict future cable states and identify valid actions, we present a novel state transition model that takes into account the straightening and bending of cables during manipulation. Using this state transition model, we select between two high-level action primitives and calculate predicted immediate costs to optimize the lower-level actions. We experimentally demonstrate that iterating the above perception-planning-action process enables unweaving electric cables and shoelaces with an 84% success rate on average.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u53cd\u9988\u7684\u591a\u7ebf\u7f06\u89e3\u4ea4\u7ec7\u65b9\u6cd5\uff0c\u5c06\u7ebf\u7f06\u89e3\u4ea4\u7ec7\u5efa\u6a21\u4e3a\u6293\u53d6\u653e\u7f6e\u95ee\u9898\uff0c\u4f7f\u7528\u56fe\u7ed3\u6784\u8868\u793a\u7ebf\u7f06\u72b6\u6001\uff0c\u901a\u8fc7\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\u9884\u6d4b\u672a\u6765\u72b6\u6001\u5e76\u9009\u62e9\u52a8\u4f5c\uff0c\u5b9e\u9a8c\u6210\u529f\u738784%", "motivation": "\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u5355\u6839\u7ebf\u7f06\u7684\u89e3\u7ed3\uff0c\u800c\u5b9e\u9645\u7535\u7f06\u7ba1\u7406\u4efb\u52a1\u4e2d\u5e38\u6d89\u53ca\u591a\u6839\u7ebf\u7f06\u7684\u4ea4\u7ec7\u7f20\u7ed5\u3002\u591a\u7ebf\u7f06\u89e3\u4ea4\u7ec7\u662f\u7535\u7f06\u7ba1\u7406\u7684\u91cd\u8981\u5b50\u4efb\u52a1\uff0c\u9700\u8981\u5c06\u4ea4\u7ec7\u7684\u591a\u6839\u7ebf\u7f06\u5206\u79bb\u4ee5\u4fbf\u540e\u7eed\u64cd\u4f5c\u3002", "method": "1) \u5c06\u7ebf\u7f06\u89e3\u4ea4\u7ec7\u5efa\u6a21\u4e3a\u6293\u53d6\u653e\u7f6e\u95ee\u9898\uff1b2) \u4f7f\u7528\u57fa\u4e8e\u56fe\u7684\u7ebf\u7f06\u72b6\u6001\u8868\u793a\uff0c\u7f16\u7801\u62d3\u6251\u548c\u51e0\u4f55\u4fe1\u606f\uff1b3) \u63d0\u51fa\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\u9884\u6d4b\u7ebf\u7f06\u5728\u64cd\u4f5c\u4e2d\u7684\u4f38\u76f4\u548c\u5f2f\u66f2\uff1b4) \u901a\u8fc7\u72b6\u6001\u8f6c\u79fb\u6a21\u578b\u9009\u62e9\u9ad8\u5c42\u52a8\u4f5c\u57fa\u5143\u5e76\u8ba1\u7b97\u9884\u6d4b\u6210\u672c\u4ee5\u4f18\u5316\u5e95\u5c42\u52a8\u4f5c\u3002", "result": "\u5728\u7535\u6e90\u7ebf\u548c\u978b\u5e26\u7b49\u7ebf\u7f06\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5e73\u5747\u6210\u529f\u738784%\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u611f\u77e5-\u89c4\u5212-\u52a8\u4f5c\u8fed\u4ee3\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u591a\u7ebf\u7f06\u89e3\u4ea4\u7ec7\u95ee\u9898\uff0c\u4e3a\u81ea\u52a8\u5316\u7535\u7f06\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u89c6\u89c9\u53cd\u9988\u548c\u72b6\u6001\u9884\u6d4b\u5b9e\u73b0\u4e86\u5bf9\u53ef\u53d8\u5f62\u7ebf\u7f06\u7684\u667a\u80fd\u64cd\u4f5c\u3002"}}
{"id": "2512.12613", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12613", "abs": "https://arxiv.org/abs/2512.12613", "authors": ["Yucan Guo", "Saiping Guan", "Miao Su", "Zeya Zhao", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "title": "StruProKGR: A Structural and Probabilistic Framework for Sparse Knowledge Graph Reasoning", "comment": null, "summary": "Sparse Knowledge Graphs (KGs) are commonly encountered in real-world applications, where knowledge is often incomplete or limited. Sparse KG reasoning, the task of inferring missing knowledge over sparse KGs, is inherently challenging due to the scarcity of knowledge and the difficulty of capturing relational patterns in sparse scenarios. Among all sparse KG reasoning methods, path-based ones have attracted plenty of attention due to their interpretability. Existing path-based methods typically rely on computationally intensive random walks to collect paths, producing paths of variable quality. Additionally, these methods fail to leverage the structured nature of graphs by treating paths independently. To address these shortcomings, we propose a Structural and Probabilistic framework named StruProKGR, tailored for efficient and interpretable reasoning on sparse KGs. StruProKGR utilizes a distance-guided path collection mechanism to significantly reduce computational costs while exploring more relevant paths. It further enhances the reasoning process by incorporating structural information through probabilistic path aggregation, which prioritizes paths that reinforce each other. Extensive experiments on five sparse KG reasoning benchmarks reveal that StruProKGR surpasses existing path-based methods in both effectiveness and efficiency, providing an effective, efficient, and interpretable solution for sparse KG reasoning.", "AI": {"tldr": "\u63d0\u51faStruProKGR\u6846\u67b6\uff0c\u901a\u8fc7\u8ddd\u79bb\u5f15\u5bfc\u8def\u5f84\u6536\u96c6\u548c\u6982\u7387\u8def\u5f84\u805a\u5408\uff0c\u89e3\u51b3\u7a00\u758f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u8def\u5f84\u8d28\u91cf\u4e0d\u4e00\u548c\u7ed3\u6784\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5728\u6548\u679c\u548c\u6548\u7387\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u77e5\u8bc6\u56fe\u8c31\u5f80\u5f80\u662f\u7a00\u758f\u7684\uff0c\u73b0\u6709\u57fa\u4e8e\u8def\u5f84\u7684\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u4f9d\u8d56\u8ba1\u7b97\u5bc6\u96c6\u7684\u968f\u673a\u6e38\u8d70\u6536\u96c6\u8def\u5f84\uff0c\u8def\u5f84\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff1b2) \u5c06\u8def\u5f84\u72ec\u7acb\u5904\u7406\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u56fe\u7684\u7ed3\u6784\u4fe1\u606f\u3002", "method": "\u63d0\u51faStruProKGR\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u8ddd\u79bb\u5f15\u5bfc\u7684\u8def\u5f84\u6536\u96c6\u673a\u5236\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63a2\u7d22\u66f4\u76f8\u5173\u7684\u8def\u5f84\uff1b2) \u6982\u7387\u8def\u5f84\u805a\u5408\uff0c\u901a\u8fc7\u7ed3\u6784\u4fe1\u606f\u589e\u5f3a\u63a8\u7406\u8fc7\u7a0b\uff0c\u4f18\u5148\u8003\u8651\u76f8\u4e92\u5f3a\u5316\u7684\u8def\u5f84\u3002", "result": "\u5728\u4e94\u4e2a\u7a00\u758f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cStruProKGR\u5728\u6548\u679c\u548c\u6548\u7387\u4e0a\u90fd\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u57fa\u4e8e\u8def\u5f84\u7684\u65b9\u6cd5\u3002", "conclusion": "StruProKGR\u4e3a\u7a00\u758f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u3001\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u4fe1\u606f\u548c\u6982\u7387\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u8def\u5f84\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.12632", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12632", "abs": "https://arxiv.org/abs/2512.12632", "authors": ["Rishit Agnihotri", "Sandeep Kumar Sharma"], "title": "Optimized Conflict Management for Urban Air Mobility Using Swarm UAV Networks", "comment": "Preprint. Under review for conference submission", "summary": "Urban Air Mobility (UAM) poses unprecedented traffic coordination challenges, especially with increasing UAV densities in dense urban corridors. This paper introduces a mathematical model using a control algorithm to optimize an Edge AI-driven decentralized swarm architecture for intelligent conflict resolution, enabling real-time decision-making with low latency. Using lightweight neural networks, the system leverages edge nodes to perform distributed conflict detection and resolution. A simulation platform was developed to evaluate the scheme under various UAV densities. Results indicate that the conflict resolution time is dramatically minimized up to 3.8 times faster, and accuracy is enhanced compared to traditional centralized control models. The proposed architecture is highly promising for scalable, efficient, and safe aerial traffic management in future UAM systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fb9\u7f18AI\u7684\u53bb\u4e2d\u5fc3\u5316\u65e0\u4eba\u673a\u7fa4\u67b6\u6784\uff0c\u7528\u4e8e\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u7684\u5b9e\u65f6\u51b2\u7a81\u68c0\u6d4b\u4e0e\u89e3\u51b3\uff0c\u76f8\u6bd4\u4f20\u7edf\u96c6\u4e2d\u5f0f\u63a7\u5236\u6a21\u578b\uff0c\u51b2\u7a81\u89e3\u51b3\u65f6\u95f4\u6700\u591a\u51cf\u5c113.8\u500d\u3002", "motivation": "\u968f\u7740\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\uff08UAM\uff09\u7684\u53d1\u5c55\uff0c\u5bc6\u96c6\u57ce\u5e02\u8d70\u5eca\u4e2d\u7684\u65e0\u4eba\u673a\u5bc6\u5ea6\u4e0d\u65ad\u589e\u52a0\uff0c\u5e26\u6765\u4e86\u524d\u6240\u672a\u6709\u7684\u4ea4\u901a\u534f\u8c03\u6311\u6218\uff0c\u9700\u8981\u4f4e\u5ef6\u8fdf\u7684\u5b9e\u65f6\u51b3\u7b56\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u63a7\u5236\u7b97\u6cd5\u6784\u5efa\u6570\u5b66\u6a21\u578b\uff0c\u8bbe\u8ba1\u8fb9\u7f18AI\u9a71\u52a8\u7684\u53bb\u4e2d\u5fc3\u5316\u7fa4\u67b6\u6784\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5728\u8fb9\u7f18\u8282\u70b9\u8fdb\u884c\u5206\u5e03\u5f0f\u51b2\u7a81\u68c0\u6d4b\u4e0e\u89e3\u51b3\uff0c\u5e76\u5f00\u53d1\u4e86\u4eff\u771f\u5e73\u53f0\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u67b6\u6784\u5728\u4e0d\u540c\u65e0\u4eba\u673a\u5bc6\u5ea6\u4e0b\u90fd\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u51b2\u7a81\u89e3\u51b3\u65f6\u95f4\u6700\u591a\u51cf\u5c113.8\u500d\uff0c\u51c6\u786e\u7387\u4e5f\u4f18\u4e8e\u4f20\u7edf\u96c6\u4e2d\u5f0f\u63a7\u5236\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u8fb9\u7f18AI\u53bb\u4e2d\u5fc3\u5316\u67b6\u6784\u4e3a\u672a\u6765UAM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u5b89\u5168\u7684\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u7406\u65b9\u6848\uff0c\u5177\u6709\u5f88\u9ad8\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2512.12620", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12620", "abs": "https://arxiv.org/abs/2512.12620", "authors": ["Aheli Poddar", "Saptarshi Sahoo", "Sujata Ghosh"], "title": "Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives", "comment": "9 pages, 4 figures, 5 tables. Submitted to AAAI 2026 Bridge Program on Logic & AI. Code available at https://github.com/XAheli/Logic-in-LLMs", "summary": "We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6f14\u7ece\u63a8\u7406\u65b9\u9762\u7684\u80fd\u529b\uff0c\u4ece\u903b\u8f91\u548c\u81ea\u7136\u8bed\u8a00\u4e24\u4e2a\u89d2\u5ea6\u5206\u6790\u4e8614\u4e2aLLM\u7684\u4e09\u6bb5\u8bba\u63a8\u7406\u8868\u73b0\uff0c\u53d1\u73b0\u867d\u7136\u63a8\u7406\u80fd\u529b\u5e76\u975e\u6240\u6709\u6a21\u578b\u7684\u666e\u904d\u7279\u6027\uff0c\u4f46\u67d0\u4e9b\u6a21\u578b\u7684\u5b8c\u7f8e\u7b26\u53f7\u63a8\u7406\u8868\u73b0\u5f15\u53d1\u4e86\u5173\u4e8eLLM\u662f\u5426\u6b63\u5728\u6210\u4e3a\u5f62\u5f0f\u63a8\u7406\u673a\u5236\u800c\u975e\u6a21\u62df\u4eba\u7c7b\u63a8\u7406\u7684\u601d\u8003\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u672c\u63a8\u7406\u80fd\u529b\uff0c\u7279\u522b\u662f\u4e09\u6bb5\u8bba\u63a8\u7406\u80fd\u529b\uff0c\u4ece\u903b\u8f91\u63a8\u7406\u548c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e24\u4e2a\u7ef4\u5ea6\u8bc4\u4f30LLM\u7684\u63a8\u7406\u8868\u73b0\uff0c\u5e76\u4e86\u89e3\u8be5\u7814\u7a76\u9886\u57df\u7684\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u4f7f\u752814\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7b26\u53f7\u63a8\u7406\u548c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4e24\u4e2a\u7ef4\u5ea6\u6765\u8bc4\u4f30\u5b83\u4eec\u7684\u4e09\u6bb5\u8bba\u63a8\u7406\u80fd\u529b\uff0c\u5206\u6790\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e09\u6bb5\u8bba\u63a8\u7406\u80fd\u529b\u5e76\u975e\u6240\u6709LLM\u7684\u666e\u904d\u6d8c\u73b0\u7279\u6027\uff0c\u4f46\u67d0\u4e9b\u6a21\u578b\u5728\u7b26\u53f7\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u5b8c\u7f8e\u6027\u80fd\uff0c\u8fd9\u5f15\u53d1\u4e86\u5173\u4e8eLLM\u662f\u5426\u6b63\u5728\u6f14\u53d8\u4e3a\u5f62\u5f0f\u63a8\u7406\u673a\u5236\u800c\u975e\u6a21\u62df\u4eba\u7c7b\u63a8\u7406\u7ec6\u5fae\u5dee\u522b\u7684\u7591\u95ee\u3002", "conclusion": "LLM\u7684\u4e09\u6bb5\u8bba\u63a8\u7406\u80fd\u529b\u5b58\u5728\u6a21\u578b\u95f4\u5dee\u5f02\uff0c\u67d0\u4e9b\u6a21\u578b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u5f62\u5f0f\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u53ef\u80fd\u8868\u660eLLM\u6b63\u5728\u5411\u5f62\u5f0f\u63a8\u7406\u673a\u5236\u53d1\u5c55\uff0c\u800c\u975e\u5b8c\u5168\u6a21\u62df\u4eba\u7c7b\u63a8\u7406\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2512.12649", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12649", "abs": "https://arxiv.org/abs/2512.12649", "authors": ["Zhewen Zheng", "Wenjing Cao", "Hongkang Yu", "Mo Chen", "Takashi Suzuki"], "title": "Bayesian Optimization Parameter Tuning Framework for a Lyapunov Based Path Following Controller", "comment": null, "summary": "Parameter tuning in real-world experiments is constrained by the limited evaluation budget available on hardware. The path-following controller studied in this paper reflects a typical situation in nonlinear geometric controller, where multiple gains influence the dynamics through coupled nonlinear terms. Such interdependence makes manual tuning inefficient and unlikely to yield satisfactory performance within a practical number of trials. To address this challenge, we propose a Bayesian optimization (BO) framework that treats the closed-loop system as a black box and selects controller gains using a Gaussian-process surrogate. BO offers model-free exploration, quantified uncertainty, and data-efficient search, making it well suited for tuning tasks where each evaluation is costly. The framework is implemented on Honda's AI-Formula three-wheeled robot and assessed through repeated full-lap experiments on a fixed test track. The results show that BO improves controller performance within 32 trials, including 15 warm-start initial evaluations, indicating that it can efficiently locate high-performing regions of the parameter space under real-world conditions. These findings demonstrate that BO provides a practical, reliable, and data-efficient tuning approach for nonlinear path-following controllers on real robotic platforms.", "AI": {"tldr": "\u4f7f\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\u81ea\u52a8\u8c03\u6574\u975e\u7ebf\u6027\u8def\u5f84\u8ddf\u968f\u63a7\u5236\u5668\u7684\u589e\u76ca\u53c2\u6570\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u901a\u8fc7\u6709\u9650\u6b21\u5b9e\u9a8c\u5b9e\u73b0\u9ad8\u6548\u8c03\u53c2", "motivation": "\u5b9e\u9645\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u53c2\u6570\u8c03\u6574\u53d7\u9650\u4e8e\u6709\u9650\u7684\u8bc4\u4f30\u9884\u7b97\u3002\u975e\u7ebf\u6027\u51e0\u4f55\u63a7\u5236\u5668\u4e2d\u591a\u4e2a\u589e\u76ca\u901a\u8fc7\u8026\u5408\u975e\u7ebf\u6027\u9879\u5f71\u54cd\u7cfb\u7edf\u52a8\u6001\uff0c\u8fd9\u79cd\u76f8\u4e92\u4f9d\u8d56\u4f7f\u5f97\u624b\u52a8\u8c03\u53c2\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u5728\u6709\u9650\u8bd5\u9a8c\u6b21\u6570\u5185\u83b7\u5f97\u6ee1\u610f\u6027\u80fd", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u95ed\u73af\u7cfb\u7edf\u89c6\u4e3a\u9ed1\u76d2\uff0c\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u4ee3\u7406\u6a21\u578b\u9009\u62e9\u63a7\u5236\u5668\u589e\u76ca\u3002BO\u63d0\u4f9b\u65e0\u6a21\u578b\u63a2\u7d22\u3001\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u548c\u6570\u636e\u9ad8\u6548\u641c\u7d22\uff0c\u9002\u5408\u8bc4\u4f30\u6210\u672c\u9ad8\u7684\u8c03\u53c2\u4efb\u52a1", "result": "\u5728Honda AI-Formula\u4e09\u8f6e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\uff0c\u901a\u8fc7\u5728\u56fa\u5b9a\u6d4b\u8bd5\u8f68\u9053\u4e0a\u91cd\u590d\u5168\u5708\u5b9e\u9a8c\u8bc4\u4f30\u3002\u7ed3\u679c\u663e\u793aBO\u572832\u6b21\u8bd5\u9a8c\uff08\u5305\u62ec15\u6b21\u9884\u70ed\u521d\u59cb\u8bc4\u4f30\uff09\u5185\u6539\u5584\u4e86\u63a7\u5236\u5668\u6027\u80fd\uff0c\u8868\u660e\u5176\u80fd\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u9ad8\u6548\u5b9a\u4f4d\u53c2\u6570\u7a7a\u95f4\u7684\u9ad8\u6027\u80fd\u533a\u57df", "conclusion": "\u8d1d\u53f6\u65af\u4f18\u5316\u4e3a\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u975e\u7ebf\u6027\u8def\u5f84\u8ddf\u968f\u63a7\u5236\u5668\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u53ef\u9760\u4e14\u6570\u636e\u9ad8\u6548\u7684\u8c03\u53c2\u65b9\u6cd5"}}
{"id": "2512.12641", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12641", "abs": "https://arxiv.org/abs/2512.12641", "authors": ["Sander Land", "Yuval Pinter"], "title": "Which Pieces Does Unigram Tokenization Really Need?", "comment": "10 pages, 1 figure. For associated code, see https://github.com/sanderland/script_tok", "summary": "The Unigram tokenization algorithm offers a probabilistic alternative to the greedy heuristics of Byte-Pair Encoding. Despite its theoretical elegance, its implementation in practice is complex, limiting its adoption to the SentencePiece package and adapters thereof. We bridge this gap between theory and practice by providing a clear guide to implementation and parameter choices. We also identify a simpler algorithm that accepts slightly higher training loss in exchange for improved compression.", "AI": {"tldr": "\u672c\u6587\u63d0\u4f9b\u4e86Unigram\u5206\u8bcd\u7b97\u6cd5\u7684\u6e05\u6670\u5b9e\u73b0\u6307\u5357\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u7684\u66ff\u4ee3\u7b97\u6cd5\uff0c\u5728\u7565\u5fae\u589e\u52a0\u8bad\u7ec3\u635f\u5931\u7684\u540c\u65f6\u63d0\u5347\u538b\u7f29\u6548\u7387\u3002", "motivation": "Unigram\u5206\u8bcd\u7b97\u6cd5\u867d\u7136\u7406\u8bba\u4e0a\u4f18\u96c5\uff0c\u4f46\u5b9e\u9645\u5b9e\u73b0\u590d\u6742\uff0c\u9650\u5236\u4e86\u5176\u5728SentencePiece\u5305\u4e4b\u5916\u7684\u91c7\u7528\u3002\u672c\u6587\u65e8\u5728\u5f25\u5408\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u4f9b\u4e86Unigram\u7b97\u6cd5\u7684\u6e05\u6670\u5b9e\u73b0\u6307\u5357\u548c\u53c2\u6570\u9009\u62e9\u5efa\u8bae\uff0c\u540c\u65f6\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7b80\u5355\u7684\u66ff\u4ee3\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u63a5\u53d7\u7a0d\u9ad8\u7684\u8bad\u7ec3\u635f\u5931\u4ee5\u6362\u53d6\u66f4\u597d\u7684\u538b\u7f29\u6548\u679c\u3002", "result": "\u63d0\u51fa\u7684\u7b80\u5316\u7b97\u6cd5\u5728\u7565\u5fae\u589e\u52a0\u8bad\u7ec3\u635f\u5931\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u538b\u7f29\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u9009\u62e9\u3002", "conclusion": "\u901a\u8fc7\u63d0\u4f9b\u6e05\u6670\u7684\u5b9e\u73b0\u6307\u5357\u548c\u66f4\u7b80\u5355\u7684\u66ff\u4ee3\u7b97\u6cd5\uff0c\u672c\u6587\u4f7fUnigram\u5206\u8bcd\u7b97\u6cd5\u66f4\u6613\u4e8e\u5b9e\u9645\u5e94\u7528\uff0c\u4fc3\u8fdb\u4e86\u8be5\u7b97\u6cd5\u5728\u66f4\u5e7f\u6cdb\u573a\u666f\u4e2d\u7684\u91c7\u7528\u3002"}}
{"id": "2512.12717", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12717", "abs": "https://arxiv.org/abs/2512.12717", "authors": ["Mattia Catellani", "Marta Gabbi", "Lorenzo Sabattini"], "title": "HMPCC: Human-Aware Model Predictive Coverage Control", "comment": null, "summary": "We address the problem of coordinating a team of robots to cover an unknown environment while ensuring safe operation and avoiding collisions with non-cooperative agents. Traditional coverage strategies often rely on simplified assumptions, such as known or convex environments and static density functions, and struggle to adapt to real-world scenarios, especially when humans are involved. In this work, we propose a human-aware coverage framework based on Model Predictive Control (MPC), namely HMPCC, where human motion predictions are integrated into the planning process. By anticipating human trajectories within the MPC horizon, robots can proactively coordinate their actions %avoid redundant exploration, and adapt to dynamic conditions. The environment is modeled as a Gaussian Mixture Model (GMM), representing regions of interest. Team members operate in a fully decentralized manner, without relying on explicit communication, an essential feature in hostile or communication-limited scenarios. Our results show that human trajectory forecasting enables more efficient and adaptive coverage, improving coordination between human and robotic agents.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u4eba\u7c7b\u611f\u77e5\u8986\u76d6\u6846\u67b6HMPCC\uff0c\u901a\u8fc7\u9884\u6d4b\u4eba\u7c7b\u8f68\u8ff9\u6765\u534f\u8c03\u673a\u5668\u4eba\u56e2\u961f\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5b89\u5168\u8986\u76d6", "motivation": "\u4f20\u7edf\u8986\u76d6\u7b56\u7565\u901a\u5e38\u4f9d\u8d56\u7b80\u5316\u5047\u8bbe\uff08\u5982\u5df2\u77e5\u6216\u51f8\u73af\u5883\u3001\u9759\u6001\u5bc6\u5ea6\u51fd\u6570\uff09\uff0c\u96be\u4ee5\u9002\u5e94\u73b0\u5b9e\u573a\u666f\uff0c\u7279\u522b\u662f\u5728\u6d89\u53ca\u4eba\u7c7b\u7684\u60c5\u51b5\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b89\u5168\u534f\u8c03\u673a\u5668\u4eba\u56e2\u961f\u3001\u907f\u514d\u4e0e\u975e\u5408\u4f5c\u4ee3\u7406\uff08\u5982\u4eba\u7c7b\uff09\u78b0\u649e\u7684\u8986\u76d6\u65b9\u6cd5\u3002", "method": "\u63d0\u51faHMPCC\u6846\u67b6\uff1a1) \u5c06\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\u96c6\u6210\u5230MPC\u89c4\u5212\u8fc7\u7a0b\u4e2d\uff0c\u5728MPC\u65f6\u95f4\u8303\u56f4\u5185\u9884\u6d4b\u4eba\u7c7b\u8f68\u8ff9\uff1b2) \u4f7f\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b(GMM)\u8868\u793a\u611f\u5174\u8da3\u533a\u57df\uff1b3) \u673a\u5668\u4eba\u56e2\u961f\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u8fd0\u884c\uff0c\u65e0\u9700\u663e\u5f0f\u901a\u4fe1\uff0c\u9002\u5e94\u654c\u5bf9\u6216\u901a\u4fe1\u53d7\u9650\u573a\u666f", "result": "\u4eba\u7c7b\u8f68\u8ff9\u9884\u6d4b\u4f7f\u8986\u76d6\u66f4\u9ad8\u6548\u548c\u81ea\u9002\u5e94\uff0c\u6539\u5584\u4e86\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4ee3\u7406\u4e4b\u95f4\u7684\u534f\u8c03\u3002\u673a\u5668\u4eba\u80fd\u591f\u4e3b\u52a8\u534f\u8c03\u884c\u52a8\uff0c\u907f\u514d\u5197\u4f59\u63a2\u7d22\uff0c\u9002\u5e94\u52a8\u6001\u6761\u4ef6", "conclusion": "HMPCC\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u4eba\u7c7b\u8fd0\u52a8\u9884\u6d4b\u5230MPC\u89c4\u5212\u4e2d\uff0c\u5b9e\u73b0\u4e86\u5728\u672a\u77e5\u73af\u5883\u4e2d\u673a\u5668\u4eba\u56e2\u961f\u7684\u5b89\u5168\u3001\u9ad8\u6548\u8986\u76d6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6d89\u53ca\u4eba\u7c7b\u4ea4\u4e92\u7684\u73b0\u5b9e\u573a\u666f"}}
{"id": "2512.12643", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12643", "abs": "https://arxiv.org/abs/2512.12643", "authors": ["Yida Cai", "Ranjuexiao Hu", "Huiyuan Xie", "Chenyang Li", "Yun Liu", "Yuxiao Ye", "Zhenghao Liu", "Weixing Shen", "Zhiyuan Liu"], "title": "LexRel: Benchmarking Legal Relation Extraction for Chinese Civil Cases", "comment": null, "summary": "Legal relations form a highly consequential analytical framework of civil law system, serving as a crucial foundation for resolving disputes and realizing values of the rule of law in judicial practice. However, legal relations in Chinese civil cases remain underexplored in the field of legal artificial intelligence (legal AI), largely due to the absence of comprehensive schemas. In this work, we firstly introduce a comprehensive schema, which contains a hierarchical taxonomy and definitions of arguments, for AI systems to capture legal relations in Chinese civil cases. Based on this schema, we then formulate legal relation extraction task and present LexRel, an expert-annotated benchmark for legal relation extraction in Chinese civil law. We use LexRel to evaluate state-of-the-art large language models (LLMs) on legal relation extractions, showing that current LLMs exhibit significant limitations in accurately identifying civil legal relations. Furthermore, we demonstrate that incorporating legal relations information leads to consistent performance gains on other downstream legal AI tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LexRel\uff0c\u4e00\u4e2a\u7528\u4e8e\u4e2d\u6587\u6c11\u4e8b\u6cd5\u5f8b\u5173\u7cfb\u63d0\u53d6\u7684\u4e13\u5bb6\u6807\u6ce8\u57fa\u51c6\uff0c\u5e76\u8bc4\u4f30\u4e86LLMs\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u8bc6\u522b\u6c11\u4e8b\u6cd5\u5f8b\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002", "motivation": "\u6cd5\u5f8b\u5173\u7cfb\u662f\u6c11\u6cd5\u4f53\u7cfb\u7684\u91cd\u8981\u5206\u6790\u6846\u67b6\uff0c\u4f46\u5728\u6cd5\u5f8bAI\u9886\u57df\uff0c\u4e2d\u6587\u6c11\u4e8b\u6848\u4ef6\u7684\u6cd5\u5f8b\u5173\u7cfb\u7814\u7a76\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u7f3a\u4e4f\u5168\u9762\u7684\u6a21\u5f0f\u5b9a\u4e49\u3002", "method": "\u9996\u5148\u5f15\u5165\u5305\u542b\u5c42\u6b21\u5206\u7c7b\u548c\u8bba\u636e\u5b9a\u4e49\u7684\u7efc\u5408\u6a21\u5f0f\uff0c\u7136\u540e\u57fa\u4e8e\u6b64\u6a21\u5f0f\u5236\u5b9a\u6cd5\u5f8b\u5173\u7cfb\u63d0\u53d6\u4efb\u52a1\uff0c\u521b\u5efaLexRel\u4e13\u5bb6\u6807\u6ce8\u57fa\u51c6\uff0c\u5e76\u7528\u5176\u8bc4\u4f30\u6700\u5148\u8fdb\u7684LLMs\u3002", "result": "\u5f53\u524dLLMs\u5728\u51c6\u786e\u8bc6\u522b\u6c11\u4e8b\u6cd5\u5f8b\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff0c\u4f46\u5c06\u6cd5\u5f8b\u5173\u7cfb\u4fe1\u606f\u6574\u5408\u5230\u5176\u4ed6\u4e0b\u6e38\u6cd5\u5f8bAI\u4efb\u52a1\u4e2d\u80fd\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u6cd5\u5f8b\u5173\u7cfb\u63d0\u53d6\u662f\u6cd5\u5f8bAI\u7684\u91cd\u8981\u4efb\u52a1\uff0cLexRel\u57fa\u51c6\u4e3a\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u672a\u6765\u9700\u8981\u5f00\u53d1\u66f4\u4e13\u4e1a\u7684\u6cd5\u5f8bAI\u7cfb\u7edf\u6765\u5904\u7406\u590d\u6742\u7684\u6cd5\u5f8b\u5173\u7cfb\u3002"}}
{"id": "2512.12722", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12722", "abs": "https://arxiv.org/abs/2512.12722", "authors": ["Tarik Viehmann", "Daniel Swoboda", "Samridhi Kalra", "Himanshu Grover", "Gerhard Lakemeyer"], "title": "Making Robots Play by the Rules: The ROS 2 CLIPS-Executive", "comment": null, "summary": "CLIPS is a rule-based programming language for building knowledge-driven applications, well suited for the complex task of coordinating autonomous robots. Inspired by the CLIPS-Executive originally developed for the lesser known Fawkes robotics framework, we present an Integration of CLIPS into the ROS ecosystem. Additionally, we show the flexibility of CLIPS by describing a PDDL-based planning framework integration.", "AI": {"tldr": "\u5c06CLIPS\u89c4\u5219\u7f16\u7a0b\u8bed\u8a00\u96c6\u6210\u5230ROS\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\uff0c\u5e76\u5c55\u793a\u4e86\u4e0ePDDL\u89c4\u5212\u6846\u67b6\u7684\u96c6\u6210", "motivation": "CLIPS\u4f5c\u4e3a\u57fa\u4e8e\u89c4\u5219\u7684\u77e5\u8bc6\u9a71\u52a8\u7f16\u7a0b\u8bed\u8a00\uff0c\u975e\u5e38\u9002\u5408\u534f\u8c03\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u9700\u8981\u66f4\u597d\u5730\u96c6\u6210\u5230\u4e3b\u6d41\u7684ROS\u751f\u6001\u7cfb\u7edf\u4e2d", "method": "\u53d7Fawkes\u673a\u5668\u4eba\u6846\u67b6\u4e2dCLIPS-Executive\u7684\u542f\u53d1\uff0c\u5c06CLIPS\u96c6\u6210\u5230ROS\u751f\u6001\u7cfb\u7edf\uff0c\u5e76\u63cf\u8ff0\u57fa\u4e8ePDDL\u7684\u89c4\u5212\u6846\u67b6\u96c6\u6210\u65b9\u6848", "result": "\u6210\u529f\u5b9e\u73b0\u4e86CLIPS\u5728ROS\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u96c6\u6210\uff0c\u5c55\u793a\u4e86CLIPS\u7684\u7075\u6d3b\u6027\uff0c\u7279\u522b\u662f\u4e0ePDDL\u89c4\u5212\u6846\u67b6\u7684\u96c6\u6210\u80fd\u529b", "conclusion": "CLIPS\u53ef\u4ee5\u6709\u6548\u5730\u96c6\u6210\u5230ROS\u751f\u6001\u7cfb\u7edf\u4e2d\uff0c\u4e3a\u673a\u5668\u4eba\u534f\u8c03\u4efb\u52a1\u63d0\u4f9b\u5f3a\u5927\u7684\u77e5\u8bc6\u9a71\u52a8\u7f16\u7a0b\u80fd\u529b\uff0c\u540c\u65f6\u5c55\u793a\u4e86\u4e0e\u89c4\u5212\u6846\u67b6\u7684\u826f\u597d\u517c\u5bb9\u6027"}}
{"id": "2512.12654", "categories": ["cs.CL", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.12654", "abs": "https://arxiv.org/abs/2512.12654", "authors": ["Hassan Mujtaba", "Hamza Naveed", "Hanzlah Munir"], "title": "Modeling Authorial Style in Urdu Novels Using Character Interaction Graphs and Graph Neural Networks", "comment": "6 pages", "summary": "Authorship analysis has traditionally focused on lexical and stylistic cues within text, while higher-level narrative structure remains underexplored, particularly for low-resource languages such as Urdu. This work proposes a graph-based framework that models Urdu novels as character interaction networks to examine whether authorial style can be inferred from narrative structure alone. Each novel is represented as a graph where nodes correspond to characters and edges denote their co-occurrence within narrative proximity. We systematically compare multiple graph representations, including global structural features, node-level semantic summaries, unsupervised graph embeddings, and supervised graph neural networks. Experiments on a dataset of 52 Urdu novels written by seven authors show that learned graph representations substantially outperform hand-crafted and unsupervised baselines, achieving up to 0.857 accuracy under a strict author-aware evaluation protocol.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u6846\u67b6\uff0c\u5c06\u4e4c\u5c14\u90fd\u8bed\u5c0f\u8bf4\u5efa\u6a21\u4e3a\u89d2\u8272\u4e92\u52a8\u7f51\u7edc\uff0c\u901a\u8fc7\u53d9\u4e8b\u7ed3\u6784\u5206\u6790\u4f5c\u8005\u98ce\u683c\uff0c\u572852\u90e8\u5c0f\u8bf4\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.857\u51c6\u786e\u7387", "motivation": "\u4f20\u7edf\u4f5c\u8005\u5206\u6790\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u7684\u8bcd\u6c47\u548c\u98ce\u683c\u7279\u5f81\uff0c\u800c\u66f4\u9ad8\u5c42\u6b21\u7684\u53d9\u4e8b\u7ed3\u6784\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e4c\u5c14\u90fd\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002\u9700\u8981\u63a2\u7d22\u662f\u5426\u4ec5\u4ece\u53d9\u4e8b\u7ed3\u6784\u5c31\u80fd\u63a8\u65ad\u4f5c\u8005\u98ce\u683c\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u6846\u67b6\uff0c\u5c06\u4e4c\u5c14\u90fd\u8bed\u5c0f\u8bf4\u5efa\u6a21\u4e3a\u89d2\u8272\u4e92\u52a8\u7f51\u7edc\uff1a\u8282\u70b9\u5bf9\u5e94\u89d2\u8272\uff0c\u8fb9\u8868\u793a\u5728\u53d9\u4e8b\u90bb\u8fd1\u8303\u56f4\u5185\u7684\u5171\u73b0\u3002\u7cfb\u7edf\u6bd4\u8f83\u591a\u79cd\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u5305\u62ec\u5168\u5c40\u7ed3\u6784\u7279\u5f81\u3001\u8282\u70b9\u7ea7\u8bed\u4e49\u6458\u8981\u3001\u65e0\u76d1\u7763\u56fe\u5d4c\u5165\u548c\u76d1\u7763\u56fe\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u57287\u4f4d\u4f5c\u8005\u64b0\u5199\u768452\u90e8\u4e4c\u5c14\u90fd\u8bed\u5c0f\u8bf4\u6570\u636e\u96c6\u4e0a\uff0c\u5b66\u4e60\u7684\u56fe\u8868\u793a\u663e\u8457\u4f18\u4e8e\u624b\u5de5\u5236\u4f5c\u548c\u65e0\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u4e25\u683c\u7684\u4f5c\u8005\u611f\u77e5\u8bc4\u4f30\u534f\u8bae\u4e0b\u8fbe\u52300.857\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u53d9\u4e8b\u7ed3\u6784\u786e\u5b9e\u5305\u542b\u4f5c\u8005\u98ce\u683c\u4fe1\u606f\uff0c\u57fa\u4e8e\u56fe\u7684\u8868\u793a\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6355\u6349\u8fd9\u4e9b\u7279\u5f81\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4f5c\u8005\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2512.12793", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12793", "abs": "https://arxiv.org/abs/2512.12793", "authors": ["Mizuho Aoki", "Kohei Honda", "Yasuhiro Yoshimura", "Takeshi Ishita", "Ryo Yonetani"], "title": "VLG-Loc: Vision-Language Global Localization from Labeled Footprint Maps", "comment": null, "summary": "This paper presents Vision-Language Global Localization (VLG-Loc), a novel global localization method that uses human-readable labeled footprint maps containing only names and areas of distinctive visual landmarks in an environment. While humans naturally localize themselves using such maps, translating this capability to robotic systems remains highly challenging due to the difficulty of establishing correspondences between observed landmarks and those in the map without geometric and appearance details. To address this challenge, VLG-Loc leverages a vision-language model (VLM) to search the robot's multi-directional image observations for the landmarks noted in the map. The method then identifies robot poses within a Monte Carlo localization framework, where the found landmarks are used to evaluate the likelihood of each pose hypothesis. Experimental validation in simulated and real-world retail environments demonstrates superior robustness compared to existing scan-based methods, particularly under environmental changes. Further improvements are achieved through the probabilistic fusion of visual and scan-based localization.", "AI": {"tldr": "VLG-Loc\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5168\u5c40\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4f7f\u7528\u4ec5\u5305\u542b\u89c6\u89c9\u5730\u6807\u540d\u79f0\u548c\u533a\u57df\u7684\u4eba\u7c7b\u53ef\u8bfb\u8db3\u8ff9\u5730\u56fe\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u65b9\u5411\u56fe\u50cf\u4e2d\u641c\u7d22\u5730\u56fe\u5730\u6807\uff0c\u5e76\u5728\u8499\u7279\u5361\u6d1b\u5b9a\u4f4d\u6846\u67b6\u4e2d\u8bc4\u4f30\u4f4d\u59ff\u5047\u8bbe\u3002", "motivation": "\u4eba\u7c7b\u80fd\u591f\u4f7f\u7528\u7b80\u5355\u7684\u5730\u6807\u540d\u79f0\u548c\u533a\u57df\u5730\u56fe\u8fdb\u884c\u5b9a\u4f4d\uff0c\u4f46\u673a\u5668\u4eba\u7cfb\u7edf\u96be\u4ee5\u5728\u6ca1\u6709\u51e0\u4f55\u548c\u5916\u89c2\u7ec6\u8282\u7684\u60c5\u51b5\u4e0b\u5efa\u7acb\u89c2\u6d4b\u5730\u6807\u4e0e\u5730\u56fe\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u8fd9\u9650\u5236\u4e86\u673a\u5668\u4eba\u4f7f\u7528\u6b64\u7c7b\u5730\u56fe\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u7684\u591a\u65b9\u5411\u56fe\u50cf\u89c2\u6d4b\u4e2d\u641c\u7d22\u5730\u56fe\u4e2d\u6807\u6ce8\u7684\u5730\u6807\uff0c\u7136\u540e\u5728\u8499\u7279\u5361\u6d1b\u5b9a\u4f4d\u6846\u67b6\u4e2d\uff0c\u5229\u7528\u627e\u5230\u7684\u5730\u6807\u8bc4\u4f30\u6bcf\u4e2a\u4f4d\u59ff\u5047\u8bbe\u7684\u4f3c\u7136\u5ea6\uff0c\u5b9e\u73b0\u5168\u5c40\u5b9a\u4f4d\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u96f6\u552e\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u7684\u57fa\u4e8e\u626b\u63cf\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u73af\u5883\u53d8\u5316\u60c5\u51b5\u4e0b\u3002\u901a\u8fc7\u89c6\u89c9\u548c\u626b\u63cf\u5b9a\u4f4d\u7684\u6982\u7387\u878d\u5408\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "conclusion": "VLG-Loc\u6210\u529f\u5b9e\u73b0\u4e86\u4f7f\u7528\u7b80\u5355\u4eba\u7c7b\u53ef\u8bfb\u5730\u56fe\u7684\u673a\u5668\u4eba\u5168\u5c40\u5b9a\u4f4d\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u5730\u6807\u5bf9\u5e94\u5173\u7cfb\uff0c\u5728\u73af\u5883\u53d8\u5316\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.12677", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12677", "abs": "https://arxiv.org/abs/2512.12677", "authors": ["Amirhossein Yousefiramandi", "Ciaran Cooney"], "title": "Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches", "comment": "18 pages, 6 figures", "summary": "We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u5728\u8d44\u6e90\u53d7\u9650\u4e0b\u5fae\u8c03\u4ec5\u89e3\u7801\u5668LLM\u8fdb\u884c\u6587\u672c\u5206\u7c7b\u7684\u9ad8\u6548\u7b56\u7565\uff0c\u6bd4\u8f83\u4e86\u5d4c\u5165\u5206\u7c7b\u5934\u5fae\u8c03\u548c\u6307\u4ee4\u5fae\u8c03\u4e24\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0\u5d4c\u5165\u65b9\u6cd5\u5728F1\u5206\u6570\u4e0a\u663e\u8457\u4f18\u4e8e\u6307\u4ee4\u5fae\u8c03\uff0c\u4e14\u80fd\u4e0e\u9886\u57df\u7279\u5b9a\u6a21\u578b\u7ade\u4e89\u3002", "motivation": "\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u9ad8\u6548\u5730\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u4e0b\u6e38\u6587\u672c\u5206\u7c7b\u4efb\u52a1\uff0c\u63a2\u7d22\u4e0d\u540c\u5fae\u8c03\u7b56\u7565\u7684\u6027\u80fd\u5dee\u5f02\u548c\u5b9e\u7528\u6027\u3002", "method": "\u7814\u7a76\u4e24\u79cd\u5fae\u8c03\u7b56\u7565\uff1a1) \u5728\u9884\u8bad\u7ec3\u56e0\u679cLLM\u4e0a\u9644\u52a0\u5206\u7c7b\u5934\uff0c\u4f7f\u7528\u6700\u7ec8token\u5d4c\u5165\u4f5c\u4e3a\u5e8f\u5217\u8868\u793a\u8fdb\u884c\u5fae\u8c03\uff1b2) \u4ee5\u63d0\u793a->\u54cd\u5e94\u683c\u5f0f\u5bf9LLM\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03\u8fdb\u884c\u5206\u7c7b\u3002\u91c7\u75284\u4f4d\u6a21\u578b\u91cf\u5316\u548c\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5b9e\u73b0\u5355GPU\u4e0a\u6700\u9ad88B\u53c2\u6570\u6a21\u578b\u7684\u53c2\u6570\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\uff08\u4e13\u6709\u5355\u6807\u7b7e\u6570\u636e\u96c6\u548c\u516c\u5f00WIPO-Alpha\u4e13\u5229\u6570\u636e\u96c6\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u5d4c\u5165\u7684\u65b9\u6cd5\u5728F1\u5206\u6570\u4e0a\u663e\u8457\u4f18\u4e8e\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0e\u5728\u76f8\u540c\u4efb\u52a1\u4e0a\u5fae\u8c03\u7684\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff08\u5982BERT\uff09\u76f8\u6bd4\u975e\u5e38\u6709\u7ade\u4e89\u529b\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8d85\u8d8a\u5b83\u4eec\u3002", "conclusion": "\u76f4\u63a5\u5229\u7528\u56e0\u679cLLM\u7684\u5185\u90e8\u8868\u793a\uff0c\u7ed3\u5408\u9ad8\u6548\u7684\u5fae\u8c03\u6280\u672f\uff0c\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u80fd\u5b9e\u73b0\u4ee4\u4eba\u5370\u8c61\u6df1\u523b\u7684\u5206\u7c7b\u6027\u80fd\u3002\u7814\u7a76\u8ba8\u8bba\u4e86\u6bcf\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5e76\u4e3a\u5206\u7c7b\u573a\u666f\u4e2d\u4f18\u5316LLM\u5fae\u8c03\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u548c\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2512.12842", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12842", "abs": "https://arxiv.org/abs/2512.12842", "authors": ["Kuan Fang", "Yuxin Chen", "Xinghao Zhu", "Farzad Niroui", "Lingfeng Sun", "Jiuguang Wang"], "title": "SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding", "comment": "9 pages, 7 figures", "summary": "We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.", "AI": {"tldr": "SAGA\u662f\u4e00\u4e2a\u901a\u7528\u7684\u81ea\u9002\u5e94\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9ad8\u5c42\u8bed\u4e49\u610f\u56fe\u4e0e\u4f4e\u5c42\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u89e3\u8026\uff0c\u4f7f\u7528\u57fa\u4e8e\u53ef\u4f9b\u6027\u7684\u4efb\u52a1\u8868\u793a\uff0c\u5e76\u5229\u7528\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5c06\u4efb\u52a1\u8868\u793a\u63a5\u5730\u4e3a3D\u53ef\u4f9b\u6027\u70ed\u56fe\uff0c\u4ece\u800c\u5728\u5404\u79cd\u73af\u5883\u3001\u4efb\u52a1\u76ee\u6807\u548c\u7528\u6237\u89c4\u8303\u4e2d\u5b9e\u73b0\u6cdb\u5316\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u89c6\u89c9\u8fd0\u52a8\u63a7\u5236\u5728\u591a\u6837\u5316\u73af\u5883\u3001\u4efb\u52a1\u76ee\u6807\u548c\u7528\u6237\u89c4\u8303\u4e2d\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u89e3\u8026\u9ad8\u5c42\u8bed\u4e49\u610f\u56fe\u4e0e\u4f4e\u5c42\u63a7\u5236\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u5916\u89c2\u53d8\u5316\u5bf9\u6cdb\u5316\u7684\u963b\u788d\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u53ef\u4f9b\u6027\u7684\u4efb\u52a1\u8868\u793a\u7edf\u4e00\u8868\u8fbe\u590d\u6742\u884c\u4e3a\uff1b\u5229\u7528\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u5c06\u4efb\u52a1\u8868\u793a\u63a5\u5730\u4e3a3D\u53ef\u4f9b\u6027\u70ed\u56fe\uff0c\u7a81\u51fa\u4efb\u52a1\u76f8\u5173\u5b9e\u4f53\uff1b\u5728\u591a\u4efb\u52a1\u6f14\u793a\u6570\u636e\u4e0a\u8bad\u7ec3\u6761\u4ef6\u7b56\u7565\u8fdb\u884c\u5168\u8eab\u63a7\u5236\uff1b\u652f\u6301\u8bed\u8a00\u6307\u4ee4\u3001\u9009\u62e9\u70b9\u548c\u793a\u4f8b\u6f14\u793a\u7b49\u591a\u79cd\u4efb\u52a1\u6307\u5b9a\u65b9\u5f0f\u3002", "result": "\u5728\u56db\u8db3\u673a\u68b0\u81c2\u4e0a\u5b9e\u73b0SAGA\uff0c\u5e76\u572811\u4e2a\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002SAGA\u59cb\u7ec8\u663e\u8457\u4f18\u4e8e\u7aef\u5230\u7aef\u548c\u6a21\u5757\u5316\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u7ed3\u6784\u5316\u53ef\u4f9b\u6027\u63a5\u5730\u4e3a\u901a\u7528\u79fb\u52a8\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u9014\u5f84\u3002", "conclusion": "\u7ed3\u6784\u5316\u53ef\u4f9b\u6027\u63a5\u5730\u4e3a\u901a\u7528\u79fb\u52a8\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u9014\u5f84\uff0c\u80fd\u591f\u5b9e\u73b0\u96f6\u6837\u672c\u6267\u884c\u548c\u5c11\u6837\u672c\u9002\u5e94\uff0c\u89e3\u51b3\u591a\u79cd\u5f62\u5f0f\u7684\u4efb\u52a1\u6307\u5b9a\u3002"}}
{"id": "2512.12716", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12716", "abs": "https://arxiv.org/abs/2512.12716", "authors": ["Xuanzhang Liu", "Jianglun Feng", "Zhuoran Zhuang", "Junzhe Zhao", "Maofei Que", "Jieting Li", "Dianlei Wang", "Hao Tong", "Ye Chen", "Pan Li"], "title": "CoDA: A Context-Decoupled Hierarchical Agent with Reinforcement Learning", "comment": "Accepted to WSDM '26 Oral", "summary": "Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by \"Context Explosion\", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.", "AI": {"tldr": "CoDA\u662f\u4e00\u4e2a\u4e0a\u4e0b\u6587\u89e3\u8026\u7684\u5206\u5c42\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u9ad8\u5c42\u89c4\u5212\u4e0e\u4f4e\u5c42\u6267\u884c\u5206\u79bb\u6765\u7f13\u89e3LLM\u667a\u80fd\u4f53\u4e2d\u7684\"\u4e0a\u4e0b\u6587\u7206\u70b8\"\u95ee\u9898\uff0c\u4f7f\u7528\u5355\u4e00\u5171\u4eabLLM\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u89d2\u8272\u4e2d\u64cd\u4f5c\uff0c\u901a\u8fc7PECO\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8054\u5408\u4f18\u5316\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u4fdd\u6301\u957f\u4e0a\u4e0b\u6587\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u9762\u4e34\u7684\"\u4e0a\u4e0b\u6587\u7206\u70b8\"\u95ee\u9898\u2014\u2014\u957f\u6587\u672c\u8f93\u51fa\u79ef\u7d2f\u4f1a\u6df9\u6ca1\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5bfc\u81f4\u63a8\u7406\u5931\u8d25\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u591a\u6b65\u4efb\u52a1\u4e2d\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u7ba1\u7406\u7b56\u7565\u3002", "method": "\u63d0\u51faCoDA\uff08Context-Decoupled hierarchical Agent\uff09\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u5355\u4e00\u5171\u4eabLLM\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u89d2\u8272\u4e2d\u64cd\u4f5c\uff1a\u9ad8\u5c42\u89c4\u5212\u5668\u5728\u7b80\u6d01\u6218\u7565\u4e0a\u4e0b\u6587\u4e2d\u5206\u89e3\u4efb\u52a1\uff0c\u4f4e\u5c42\u6267\u884c\u5668\u5728\u4e34\u65f6\u9694\u79bb\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u5904\u7406\u5de5\u5177\u4ea4\u4e92\uff1b2\uff09\u91c7\u7528PECO\uff08Planner-Executor Co-Optimization\uff09\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f68\u8ff9\u7ea7\u5956\u52b1\u8054\u5408\u4f18\u5316\u4e24\u4e2a\u89d2\u8272\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u4f9d\u8d56\u7684\u7b56\u7565\u66f4\u65b0\u3002", "result": "\u5728\u590d\u6742\u591a\u8df3\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\uff1b\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\uff0c\u5f53\u6240\u6709\u5176\u4ed6\u57fa\u7ebf\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u65f6\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5206\u5c42\u8bbe\u8ba1\u5728\u7f13\u89e3\u4e0a\u4e0b\u6587\u8fc7\u8f7d\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "CoDA\u901a\u8fc7\u4e0a\u4e0b\u6587\u89e3\u8026\u7684\u5206\u5c42\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u7684\u4e0a\u4e0b\u6587\u7206\u70b8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5c42\u89c4\u5212\u4e0e\u4f4e\u5c42\u6267\u884c\u7684\u5206\u79bb\u4f18\u5316\uff0c\u4e3a\u590d\u6742\u591a\u6b65\u4efb\u52a1\u63d0\u4f9b\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5206\u5c42\u67b6\u6784\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2512.12855", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.12855", "abs": "https://arxiv.org/abs/2512.12855", "authors": ["Patrick Kostelac", "Xuerui Wang", "Anahita Jamshidnejad"], "title": "MPC-Guided Safe Reinforcement Learning and Lipschitz-Based Filtering for Structured Nonlinear Systems", "comment": null, "summary": "Modern engineering systems, such as autonomous vehicles, flexible robotics, and intelligent aerospace platforms, require controllers that are robust to uncertainties, adaptive to environmental changes, and safety-aware under real-time constraints. RL offers powerful data-driven adaptability for systems with nonlinear dynamics that interact with uncertain environments. RL, however, lacks built-in mechanisms for dynamic constraint satisfaction during exploration. MPC offers structured constraint handling and robustness, but its reliance on accurate models and computationally demanding online optimization may pose significant challenges. This paper proposes an integrated MPC-RL framework that combines stability and safety guarantees of MPC with the adaptability of RL. During training, MPC defines safe control bounds that guide the RL component and that enable constraint-aware policy learning. At deployment, the learned policy operates in real time with a lightweight safety filter based on Lipschitz continuity to ensure constraint satisfaction without heavy online optimizations. The approach, which is validated on a nonlinear aeroelastic wing system, demonstrates improved disturbance rejection, reduced actuator effort, and robust performance under turbulence. The architecture generalizes to other domains with structured nonlinearities and bounded disturbances, offering a scalable solution for safe artificial-intelligence-driven control in engineering applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408MPC\u4e0eRL\u7684\u96c6\u6210\u6846\u67b6\uff0c\u5c06MPC\u7684\u7a33\u5b9a\u6027\u3001\u5b89\u5168\u6027\u4fdd\u8bc1\u4e0eRL\u7684\u9002\u5e94\u6027\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7MPC\u5b9a\u4e49\u5b89\u5168\u63a7\u5236\u8fb9\u754c\u5f15\u5bfcRL\u8bad\u7ec3\uff0c\u90e8\u7f72\u65f6\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5b89\u5168\u6ee4\u6ce2\u5668\u786e\u4fdd\u5b9e\u65f6\u7ea6\u675f\u6ee1\u8db3\u3002", "motivation": "\u73b0\u4ee3\u5de5\u7a0b\u7cfb\u7edf\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u3001\u67d4\u6027\u673a\u5668\u4eba\u3001\u667a\u80fd\u822a\u7a7a\u822a\u5929\u5e73\u53f0\uff09\u9700\u8981\u80fd\u591f\u5e94\u5bf9\u4e0d\u786e\u5b9a\u6027\u3001\u9002\u5e94\u73af\u5883\u53d8\u5316\u5e76\u5728\u5b9e\u65f6\u7ea6\u675f\u4e0b\u4fdd\u8bc1\u5b89\u5168\u7684\u63a7\u5236\u5668\u3002RL\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u6570\u636e\u9a71\u52a8\u9002\u5e94\u6027\uff0c\u4f46\u7f3a\u4e4f\u52a8\u6001\u7ea6\u675f\u6ee1\u8db3\u673a\u5236\uff1bMPC\u5177\u6709\u7ed3\u6784\u5316\u7ea6\u675f\u5904\u7406\u80fd\u529b\uff0c\u4f46\u4f9d\u8d56\u7cbe\u786e\u6a21\u578b\u4e14\u5728\u7ebf\u8ba1\u7b97\u8d1f\u62c5\u91cd\u3002", "method": "\u63d0\u51faMPC-RL\u96c6\u6210\u6846\u67b6\uff1a\u8bad\u7ec3\u9636\u6bb5\uff0cMPC\u5b9a\u4e49\u5b89\u5168\u63a7\u5236\u8fb9\u754c\u6765\u5f15\u5bfcRL\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u7ea6\u675f\u611f\u77e5\u7684\u7b56\u7565\u5b66\u4e60\uff1b\u90e8\u7f72\u9636\u6bb5\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5b9e\u65f6\u8fd0\u884c\uff0c\u5e76\u57fa\u4e8eLipschitz\u8fde\u7eed\u6027\u7684\u8f7b\u91cf\u7ea7\u5b89\u5168\u6ee4\u6ce2\u5668\u786e\u4fdd\u7ea6\u675f\u6ee1\u8db3\uff0c\u65e0\u9700\u7e41\u91cd\u7684\u5728\u7ebf\u4f18\u5316\u3002", "result": "\u5728\u975e\u7ebf\u6027\u6c14\u52a8\u5f39\u6027\u7ffc\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u6539\u8fdb\u7684\u6270\u52a8\u6291\u5236\u3001\u51cf\u5c11\u7684\u4f5c\u52a8\u5668\u52aa\u529b\u4ee5\u53ca\u5728\u6e4d\u6d41\u4e0b\u7684\u9c81\u68d2\u6027\u80fd\u3002\u8be5\u67b6\u6784\u53ef\u63a8\u5e7f\u5230\u5176\u4ed6\u5177\u6709\u7ed3\u6784\u5316\u975e\u7ebf\u6027\u548c\u6709\u754c\u6270\u52a8\u7684\u9886\u57df\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86MPC\u7684\u7a33\u5b9a\u6027\u5b89\u5168\u4fdd\u8bc1\u4e0eRL\u7684\u9002\u5e94\u6027\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u7ea6\u675f\u6ee1\u8db3\u800c\u65e0\u9700\u7e41\u91cd\u5728\u7ebf\u8ba1\u7b97\u3002"}}
{"id": "2512.12730", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12730", "abs": "https://arxiv.org/abs/2512.12730", "authors": ["Jingzhe Ding", "Shengda Long", "Changxin Pu", "Huan Zhou", "Hongwan Gao", "Xiang Gao", "Chao He", "Yue Hou", "Fei Hu", "Zhaojian Li", "Weiran Shi", "Zaiyuan Wang", "Daoguang Zan", "Chenchen Zhang", "Xiaoxu Zhang", "Qizhi Chen", "Xianfu Cheng", "Bo Deng", "Qingshui Gu", "Kai Hua", "Juntao Lin", "Pai Liu", "Mingchen Li", "Xuanguang Pan", "Zifan Peng", "Yujia Qin", "Yong Shan", "Zhewen Tan", "Weihao Xie", "Zihan Wang", "Yishuo Yuan", "Jiayu Zhang", "Enduo Zhao", "Yunfei Zhao", "He Zhu", "Chenyang Zou", "Ming Ding", "Jianpeng Jiao", "Jiaheng Liu", "Minghao Liu", "Qian Liu", "Chongyao Tao", "Jian Yang", "Tong Yang", "Zhaoxiang Zhang", "Xinjie Chen", "Wenhao Huang", "Ge Zhang"], "title": "NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents", "comment": null, "summary": "Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.", "AI": {"tldr": "NL2Repo Bench\uff1a\u4e13\u4e3a\u8bc4\u4f30\u7f16\u7801\u4ee3\u7406\u957f\u65f6\u7a0b\u4ed3\u5e93\u751f\u6210\u80fd\u529b\u800c\u8bbe\u8ba1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8981\u6c42\u4ee3\u7406\u4ec5\u51ed\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u6587\u6863\u5728\u7a7a\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u6784\u5efa\u5b8c\u6574\u53ef\u5b89\u88c5\u7684Python\u5e93\uff0c\u5b9e\u9a8c\u663e\u793a\u5f53\u524d\u6700\u5148\u8fdb\u4ee3\u7406\u6210\u529f\u7387\u4f4e\u4e8e40%\uff0c\u63ed\u793a\u4e86\u957f\u65f6\u7a0b\u63a8\u7406\u662f\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u7684\u5173\u952e\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u7f16\u7801\u4ee3\u7406\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5c40\u90e8\u4ee3\u7801\u751f\u6210\u3001\u811a\u624b\u67b6\u5b8c\u6210\u6216\u77ed\u671f\u4fee\u590d\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u6784\u5efa\u5b8c\u6574\u8f6f\u4ef6\u7cfb\u7edf\u6240\u9700\u7684\u957f\u65f6\u7a0b\u80fd\u529b\u7684\u4e25\u683c\u8bc4\u4f30\u3002\u9700\u8981\u65b0\u7684\u57fa\u51c6\u6765\u6d4b\u8bd5\u4ee3\u7406\u5728\u771f\u5b9e\u4e16\u754c\u4ed3\u5e93\u6784\u5efa\u4e2d\u6240\u9700\u7684\u6301\u7eed\u63a8\u7406\u3001\u89c4\u5212\u548c\u6267\u884c\u80fd\u529b\u3002", "method": "\u63d0\u51faNL2Repo Bench\u57fa\u51c6\uff0c\u8981\u6c42\u7f16\u7801\u4ee3\u7406\u4ec5\u57fa\u4e8e\u5355\u4e2a\u81ea\u7136\u8bed\u8a00\u9700\u6c42\u6587\u6863\u548c\u7a7a\u5de5\u4f5c\u7a7a\u95f4\uff0c\u81ea\u4e3b\u5b8c\u6210\u67b6\u6784\u8bbe\u8ba1\u3001\u4f9d\u8d56\u7ba1\u7406\u3001\u591a\u6a21\u5757\u903b\u8f91\u5b9e\u73b0\uff0c\u6700\u7ec8\u751f\u6210\u5b8c\u5168\u53ef\u5b89\u88c5\u7684Python\u5e93\u3002\u57fa\u51c6\u901a\u8fc7\u6d4b\u8bd5\u901a\u8fc7\u7387\u7b49\u6307\u6807\u91cf\u5316\u8bc4\u4f30\u4ee3\u7406\u7684\u957f\u65f6\u7a0b\u4ed3\u5e93\u751f\u6210\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u957f\u65f6\u7a0b\u4ed3\u5e93\u751f\u6210\u95ee\u9898\u8fdc\u672a\u89e3\u51b3\uff1a\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u5f00\u653e\u548c\u95ed\u6e90\u6a21\u578b\u4ee3\u7406\uff0c\u5e73\u5747\u6d4b\u8bd5\u901a\u8fc7\u7387\u4e5f\u4f4e\u4e8e40%\uff0c\u5f88\u5c11\u80fd\u6b63\u786e\u5b8c\u6210\u6574\u4e2a\u4ed3\u5e93\u3002\u5206\u6790\u63ed\u793a\u4e86\u5173\u952e\u5931\u8d25\u6a21\u5f0f\uff1a\u8fc7\u65e9\u7ec8\u6b62\u3001\u5168\u5c40\u4e00\u81f4\u6027\u4e22\u5931\u3001\u8106\u5f31\u7684\u8de8\u6587\u4ef6\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u53ca\u5728\u6570\u767e\u4e2a\u4ea4\u4e92\u6b65\u9aa4\u4e2d\u89c4\u5212\u4e0d\u8db3\u3002", "conclusion": "NL2Repo Bench\u4e3a\u8861\u91cf\u6301\u7eed\u4ee3\u7406\u80fd\u529b\u5efa\u7acb\u4e86\u4e25\u683c\u53ef\u9a8c\u8bc1\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7a81\u663e\u957f\u65f6\u7a0b\u63a8\u7406\u662f\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u7f16\u7801\u4ee3\u7406\u7684\u6838\u5fc3\u74f6\u9888\u3002\u8be5\u57fa\u51c6\u586b\u8865\u4e86\u73b0\u6709\u8bc4\u4f30\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2512.12945", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12945", "abs": "https://arxiv.org/abs/2512.12945", "authors": ["Anja Sheppard", "Parker Ewen", "Joey Wilson", "Advaith V. Sethuraman", "Benard Adewole", "Anran Li", "Yuzhen Chen", "Ram Vasudevan", "Katherine A. Skinner"], "title": "SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework", "comment": "Accepted into R-AL", "summary": "This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.", "AI": {"tldr": "SLIM-VDB\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8bed\u4e49\u5efa\u56fe\u7cfb\u7edf\uff0c\u5229\u7528OpenVDB\u6570\u636e\u7ed3\u6784\uff0c\u652f\u6301\u5c01\u95ed\u96c6\u548c\u5f00\u653e\u96c6\u8bed\u4e49\u878d\u5408\uff0c\u663e\u8457\u964d\u4f4e\u5185\u5b58\u4f7f\u7528\u548c\u5efa\u56fe\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u5efa\u56fe\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u672a\u5145\u5206\u5229\u7528\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66\u4e2d\u9ad8\u6548\u7684OpenVDB\u6570\u636e\u7ed3\u6784\uff1b2) \u7f3a\u4e4f\u540c\u65f6\u652f\u6301\u56fa\u5b9a\u7c7b\u522b\u548c\u5f00\u653e\u8bed\u8a00\u6807\u7b7e\u9884\u6d4b\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eOpenVDB\u6570\u636e\u7ed3\u6784\u76843D\u8bed\u4e49\u5efa\u56fe\u7cfb\u7edf\uff0c\u91c7\u7528\u7edf\u4e00\u7684\u8d1d\u53f6\u65af\u66f4\u65b0\u6846\u67b6\uff0c\u540c\u65f6\u652f\u6301\u5c01\u95ed\u96c6\u548c\u5f00\u653e\u96c6\u8bed\u4e49\u878d\u5408\u3002", "result": "\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u8bed\u4e49\u5efa\u56fe\u65b9\u6cd5\uff0cSLIM-VDB\u5728\u4fdd\u6301\u53ef\u6bd4\u5efa\u56fe\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u4f7f\u7528\u548c\u5efa\u56fe\u65f6\u95f4\u3002", "conclusion": "SLIM-VDB\u6210\u529f\u5c06OpenVDB\u6570\u636e\u7ed3\u6784\u5e94\u7528\u4e8e\u8bed\u4e49\u5efa\u56fe\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u4e14\u652f\u6301\u5c01\u95ed/\u5f00\u653e\u96c6\u8bed\u4e49\u878d\u5408\u7684\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2512.12770", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12770", "abs": "https://arxiv.org/abs/2512.12770", "authors": ["Thales Sales Almeida", "Rodrigo Nogueira", "H\u00e9lio Pedrini"], "title": "Curi\u00f3-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining", "comment": null, "summary": "Continued pretraining extends a language model's capabilities by further exposing it to additional data, often tailored to a specific linguistic or domain context. This strategy has emerged as an efficient alternative to full retraining when adapting general-purpose models to new settings. In this work, we investigate this paradigm through Curi\u00f3 7B, a 7-billion-parameter model derived from LLaMA-2 and trained on 100 billion Portuguese tokens from the ClassiCC-PT corpus - the most extensive Portuguese-specific continued-pretraining effort above the three-billion-parameter scale to date. Beyond scale, we investigate whether quantity alone suffices or whether data quality plays a decisive role in linguistic adaptation. To this end, we introduce Curi\u00f3-Edu 7B, a variant trained exclusively on the educational and STEM-filtered subset of the same corpus, totaling just 10 billion tokens. Despite using only 10% of the data and 20% of the computation, Curi\u00f3-Edu 7B surpasses the full-corpus model in our evaluations, demonstrating that data selection can be fundamental even when adapting models with limited prior exposure to the target language. The developed models are available at https://huggingface.co/collections/ClassiCC-Corpus/curio-edu", "AI": {"tldr": "Curi\u00f3 7B\u662f\u57fa\u65bcLLaMA-2\u768470\u5104\u53c3\u6578\u6a21\u578b\uff0c\u4f7f\u75281000\u5104\u8461\u8404\u7259\u8a9etoken\u9032\u884c\u6301\u7e8c\u9810\u8a13\u7df4\u3002\u5176\u8b8a\u9ad4Curi\u00f3-Edu 7B\u50c5\u4f7f\u7528\u6559\u80b2/STEM\u9818\u57df\u7684100\u5104token\uff0c\u537b\u5728\u8a55\u4f30\u4e2d\u8868\u73fe\u66f4\u597d\uff0c\u8b49\u660e\u6578\u64da\u8cea\u91cf\u6bd4\u6578\u91cf\u66f4\u91cd\u8981\u3002", "motivation": "\u7814\u7a76\u6301\u7e8c\u9810\u8a13\u7df4\u5728\u8a9e\u8a00\u6a21\u578b\u9069\u61c9\u7279\u5b9a\u8a9e\u8a00\u6216\u9818\u57df\u6642\u7684\u6548\u679c\uff0c\u7279\u5225\u63a2\u8a0e\u5728\u6578\u64da\u6709\u9650\u7684\u60c5\u6cc1\u4e0b\uff0c\u6578\u64da\u8cea\u91cf\u662f\u5426\u6bd4\u6578\u91cf\u66f4\u91cd\u8981\u3002\u91dd\u5c0d\u8461\u8404\u7259\u8a9e\u9019\u4e00\u8cc7\u6e90\u76f8\u5c0d\u8f03\u5c11\u7684\u8a9e\u8a00\uff0c\u7814\u7a76\u5982\u4f55\u6709\u6548\u9069\u61c9\u8a9e\u8a00\u6a21\u578b\u3002", "method": "\u958b\u767c\u5169\u500b\u6a21\u578b\uff1aCuri\u00f3 7B\uff08\u4f7f\u75281000\u5104\u8461\u8404\u7259\u8a9etoken\u7684\u5b8c\u6574ClassiCC-PT\u8a9e\u6599\u5eab\uff09\u548cCuri\u00f3-Edu 7B\uff08\u50c5\u4f7f\u7528\u6559\u80b2/STEM\u9818\u57df\u7684100\u5104token\u5b50\u96c6\uff09\u3002\u6bd4\u8f03\u5169\u8005\u5728\u76f8\u540c\u53c3\u6578\u898f\u6a21\uff0870\u5104\uff09\u4e0b\u7684\u8868\u73fe\uff0c\u8a55\u4f30\u6578\u64da\u9078\u64c7\u5c0d\u8a9e\u8a00\u9069\u61c9\u7684\u5f71\u97ff\u3002", "result": "Curi\u00f3-Edu 7B\u96d6\u7136\u53ea\u4f7f\u7528\u4e8610%\u7684\u6578\u64da\u548c20%\u7684\u8a08\u7b97\u8cc7\u6e90\uff0c\u4f46\u5728\u8a55\u4f30\u4e2d\u8d85\u8d8a\u4e86\u4f7f\u7528\u5b8c\u6574\u8a9e\u6599\u5eab\u7684Curi\u00f3 7B\u3002\u9019\u8868\u660e\u5728\u8a9e\u8a00\u9069\u61c9\u4efb\u52d9\u4e2d\uff0c\u7cbe\u5fc3\u9078\u64c7\u7684\u9ad8\u8cea\u91cf\u6578\u64da\u6bd4\u5927\u91cf\u666e\u901a\u6578\u64da\u66f4\u6709\u6548\u3002", "conclusion": "\u6578\u64da\u8cea\u91cf\u5728\u8a9e\u8a00\u6a21\u578b\u7684\u6301\u7e8c\u9810\u8a13\u7df4\u4e2d\u8d77\u6c7a\u5b9a\u6027\u4f5c\u7528\uff0c\u7279\u5225\u662f\u5728\u9069\u61c9\u8cc7\u6e90\u6709\u9650\u7684\u8a9e\u8a00\u6642\u3002\u7cbe\u5fc3\u7be9\u9078\u7684\u9818\u57df\u7279\u5b9a\u6578\u64da\u53ef\u4ee5\u986f\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u5373\u4f7f\u6578\u64da\u91cf\u5927\u5e45\u6e1b\u5c11\u3002\u9019\u70ba\u8cc7\u6e90\u6709\u9650\u8a9e\u8a00\u7684\u6a21\u578b\u9069\u61c9\u63d0\u4f9b\u4e86\u9ad8\u6548\u7b56\u7565\u3002"}}
{"id": "2512.12987", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12987", "abs": "https://arxiv.org/abs/2512.12987", "authors": ["Amin Jalal Aghdasian", "Farzaneh Abdollahi", "Ali Kamali Iglie"], "title": "Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning", "comment": null, "summary": "This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u96ea\u5730\u8f66\u9053\u4fdd\u6301\u7b97\u6cd5\uff1aAR-RDPG\uff08\u611f\u77e5-\u63a7\u5236\u5206\u79bb\uff09\u548cAR-CADPG\uff08\u7aef\u5230\u7aef\u6ce8\u610f\u529b\u673a\u5236\uff09\uff0c\u5728CARLA\u6a21\u62df\u548c\u771f\u5b9eJetson Nano\u8f66\u8f86\u4e0a\u9a8c\u8bc1\uff0cAR-CADPG\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u96ea\u5730\u9053\u8def\u6761\u4ef6\u4e0b\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u6253\u6ed1\u95ee\u9898\uff0c\u4f20\u7edf\u8f66\u9053\u4fdd\u6301\u7cfb\u7edf\u96be\u4ee5\u5e94\u5bf9\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u7684\u9c81\u68d2\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u4f5c\u9c81\u68d2\u65b9\u6cd5\uff1a1) AR-RDPG\uff1a\u611f\u77e5\u5c42\u4f7f\u7528\u591a\u5c3a\u5ea6\u795e\u7ecf\u7f51\u7edc\u53bb\u566a\uff0c\u9884\u8bad\u7ec3DCNN\u63d0\u53d6\u4e2d\u5fc3\u7ebf\u7cfb\u6570\uff0c\u4e0e\u63a7\u5236\u5c42\u7ed3\u5408\uff1b2) AR-CADPG\uff1a\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u5c06CNN\u548c\u6ce8\u610f\u529b\u673a\u5236\u96c6\u6210\u5230DRL\u6846\u67b6\u4e2d\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e2d\u8bad\u7ec3\u5e76\u5728\u591a\u79cd\u96ea\u5730\u573a\u666f\u9a8c\u8bc1\uff0cJetson Nano\u771f\u5b9e\u8f66\u8f86\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u5b66\u4e60\u7b56\u7565\u7684\u53ef\u884c\u6027\u548c\u7a33\u5b9a\u6027\u3002AR-CADPG\u5728\u8def\u5f84\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u7ed3\u5408\u65f6\u95f4\u8bb0\u5fc6\u3001\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u6ce8\u610f\u529b\u673a\u5236\u7684AR-CADPG\u65b9\u6cd5\u5728\u96ea\u5730\u8f66\u9053\u4fdd\u6301\u4e2d\u6548\u679c\u6700\u4f73\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5728\u6076\u52a3\u5929\u6c14\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.12775", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12775", "abs": "https://arxiv.org/abs/2512.12775", "authors": ["Pedro Henrique Luz de Araujo", "Michael A. Hedderich", "Ali Modarressi", "Hinrich Schuetze", "Benjamin Roth"], "title": "Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions", "comment": "31 pages, 35 figures", "summary": "Persona-assigned large language models (LLMs) are used in domains such as education, healthcare, and sociodemographic simulation. Yet, they are typically evaluated only in short, single-round settings that do not reflect real-world usage. We introduce an evaluation protocol that combines long persona dialogues (over 100 rounds) and evaluation datasets to create dialogue-conditioned benchmarks that can robustly measure long-context effects. We then investigate the effects of dialogue length on persona fidelity, instruction-following, and safety of seven state-of-the-art open- and closed-weight LLMs. We find that persona fidelity degrades over the course of dialogues, especially in goal-oriented conversations, where models must sustain both persona fidelity and instruction following. We identify a trade-off between fidelity and instruction following, with non-persona baselines initially outperforming persona-assigned models; as dialogues progress and fidelity fades, persona responses become increasingly similar to baseline responses. Our findings highlight the fragility of persona applications in extended interactions and our work provides a protocol to systematically measure such failures.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u957f\u5bf9\u8bdd\uff08\u8d85\u8fc7100\u8f6e\uff09\u548c\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u65b0\u8bc4\u4f30\u534f\u8bae\uff0c\u7528\u4e8e\u7cfb\u7edf\u6d4b\u91cfLLM\u5728\u957f\u5bf9\u8bdd\u4e2d\u89d2\u8272\u4fdd\u771f\u5ea6\u3001\u6307\u4ee4\u9075\u5faa\u548c\u5b89\u5168\u6027\u7684\u9000\u5316\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u89d2\u8272\u5206\u914d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u771f\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\u901a\u5e38\u6d89\u53ca\u957f\u5bf9\u8bdd\u4ea4\u4e92\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u5728\u77ed\u5bf9\u8bdd\u3001\u5355\u8f6e\u8bbe\u7f6e\uff0c\u65e0\u6cd5\u53cd\u6620\u5b9e\u9645\u4f7f\u7528\u60c5\u51b5\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u6d4b\u91cf\u957f\u4e0a\u4e0b\u6587\u6548\u5e94\u7684\u8bc4\u4f30\u534f\u8bae\u3002", "method": "\u7814\u7a76\u8005\u5f15\u5165\u4e86\u4e00\u79cd\u8bc4\u4f30\u534f\u8bae\uff0c\u7ed3\u5408\u957f\u89d2\u8272\u5bf9\u8bdd\uff08\u8d85\u8fc7100\u8f6e\uff09\u548c\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u521b\u5efa\u5bf9\u8bdd\u6761\u4ef6\u5316\u57fa\u51c6\uff0c\u7528\u4e8e\u9c81\u68d2\u5730\u6d4b\u91cf\u957f\u4e0a\u4e0b\u6587\u6548\u5e94\u3002\u7136\u540e\u7814\u7a76\u4e86\u5bf9\u8bdd\u957f\u5ea6\u5bf97\u4e2a\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u95ed\u6e90LLM\u5728\u89d2\u8272\u4fdd\u771f\u5ea6\u3001\u6307\u4ee4\u9075\u5faa\u548c\u5b89\u5168\u6027\u65b9\u9762\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u89d2\u8272\u4fdd\u771f\u5ea6\u5728\u5bf9\u8bdd\u8fc7\u7a0b\u4e2d\u4f1a\u9000\u5316\uff0c\u7279\u522b\u662f\u5728\u76ee\u6807\u5bfc\u5411\u5bf9\u8bdd\u4e2d\uff1b2\uff09\u5b58\u5728\u89d2\u8272\u4fdd\u771f\u5ea6\u4e0e\u6307\u4ee4\u9075\u5faa\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u975e\u89d2\u8272\u57fa\u7ebf\u6a21\u578b\u5728\u521d\u59cb\u9636\u6bb5\u4f18\u4e8e\u89d2\u8272\u5206\u914d\u6a21\u578b\uff1b3\uff09\u968f\u7740\u5bf9\u8bdd\u8fdb\u5c55\u548c\u4fdd\u771f\u5ea6\u8870\u51cf\uff0c\u89d2\u8272\u54cd\u5e94\u9010\u6e10\u53d8\u5f97\u4e0e\u57fa\u7ebf\u54cd\u5e94\u76f8\u4f3c\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u89d2\u8272\u5e94\u7528\u5728\u6269\u5c55\u4ea4\u4e92\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u7684\u534f\u8bae\u80fd\u591f\u7cfb\u7edf\u6d4b\u91cf\u6b64\u7c7b\u5931\u8d25\uff0c\u4e3aLLM\u5728\u957f\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u53ef\u9760\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2512.12993", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12993", "abs": "https://arxiv.org/abs/2512.12993", "authors": ["Guillermo A. Castillo", "Himanshu Lodha", "Ayonga Hereid"], "title": "Learning Terrain Aware Bipedal Locomotion via Reduced Dimensional Perceptual Representations", "comment": null, "summary": "This work introduces a hierarchical strategy for terrain-aware bipedal locomotion that integrates reduced-dimensional perceptual representations to enhance reinforcement learning (RL)-based high-level (HL) policies for real-time gait generation. Unlike end-to-end approaches, our framework leverages latent terrain encodings via a Convolutional Variational Autoencoder (CNN-VAE) alongside reduced-order robot dynamics, optimizing the locomotion decision process with a compact state. We systematically analyze the impact of latent space dimensionality on learning efficiency and policy robustness. Additionally, we extend our method to be history-aware, incorporating sequences of recent terrain observations into the latent representation to improve robustness. To address real-world feasibility, we introduce a distillation method to learn the latent representation directly from depth camera images and provide preliminary hardware validation by comparing simulated and real sensor data. We further validate our framework using the high-fidelity Agility Robotics (AR) simulator, incorporating realistic sensor noise, state estimation, and actuator dynamics. The results confirm the robustness and adaptability of our method, underscoring its potential for hardware deployment.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u5730\u5f62\u611f\u77e5\u53cc\u8db3\u8fd0\u52a8\u7b56\u7565\uff0c\u7ed3\u5408\u964d\u7ef4\u611f\u77e5\u8868\u793a\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u9ad8\u5c42\u7b56\u7565\uff0c\u901a\u8fc7CNN-VAE\u63d0\u53d6\u5730\u5f62\u6f5c\u5728\u7f16\u7801\uff0c\u4f18\u5316\u8fd0\u52a8\u51b3\u7b56\u8fc7\u7a0b", "motivation": "\u4f20\u7edf\u7aef\u5230\u7aef\u65b9\u6cd5\u5728\u590d\u6742\u5730\u5f62\u53cc\u8db3\u8fd0\u52a8\u63a7\u5236\u4e2d\u6548\u7387\u4f4e\u4e14\u9c81\u68d2\u6027\u5dee\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5730\u5f62\u611f\u77e5\u548c\u51b3\u7b56\u6846\u67b6\u6765\u63d0\u5347\u5b66\u4e60\u6548\u7387\u548c\u786c\u4ef6\u90e8\u7f72\u53ef\u884c\u6027", "method": "1) \u4f7f\u7528CNN-VAE\u63d0\u53d6\u5730\u5f62\u6f5c\u5728\u7f16\u7801\uff1b2) \u7ed3\u5408\u964d\u9636\u673a\u5668\u4eba\u52a8\u529b\u5b66\u6784\u5efa\u7d27\u51d1\u72b6\u6001\u7a7a\u95f4\uff1b3) \u5f15\u5165\u5386\u53f2\u611f\u77e5\u673a\u5236\uff0c\u6574\u5408\u8fd1\u671f\u5730\u5f62\u89c2\u6d4b\u5e8f\u5217\uff1b4) \u63d0\u51fa\u4ece\u6df1\u5ea6\u76f8\u673a\u56fe\u50cf\u5b66\u4e60\u6f5c\u5728\u8868\u793a\u7684\u84b8\u998f\u65b9\u6cd5\uff1b5) \u5728\u9ad8\u4fdd\u771fAgility Robotics\u6a21\u62df\u5668\u4e2d\u9a8c\u8bc1", "result": "\u7cfb\u7edf\u5206\u6790\u4e86\u6f5c\u5728\u7a7a\u95f4\u7ef4\u5ea6\u5bf9\u5b66\u4e60\u6548\u7387\u548c\u7b56\u7565\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u65b9\u6cd5\u5c55\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u6a21\u62df\u4e0e\u771f\u5b9e\u4f20\u611f\u5668\u6570\u636e\u5bf9\u6bd4\u9a8c\u8bc1\u4e86\u786c\u4ef6\u90e8\u7f72\u6f5c\u529b", "conclusion": "\u8be5\u5206\u5c42\u5730\u5f62\u611f\u77e5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u53cc\u8db3\u8fd0\u52a8\u63a7\u5236\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u786c\u4ef6\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5728\u590d\u6742\u5730\u5f62\u4e2d\u5b9e\u65f6\u6b65\u6001\u751f\u6210\u7684\u6f5c\u529b"}}
{"id": "2512.12777", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12777", "abs": "https://arxiv.org/abs/2512.12777", "authors": ["Mosh Levy", "Zohar Elyoseph", "Shauli Ravfogel", "Yoav Goldberg"], "title": "State over Tokens: Characterizing the Role of Reasoning Tokens", "comment": null, "summary": "Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faState over Tokens (SoT)\u6846\u67b6\uff0c\u5c06LLM\u7684\u63a8\u7406token\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5916\u90e8\u5316\u8ba1\u7b97\u72b6\u6001\u800c\u975e\u8bed\u8a00\u53d9\u8ff0\uff0c\u89e3\u91ca\u5176\u4e3a\u4f55\u80fd\u9a71\u52a8\u6b63\u786e\u63a8\u7406\u5374\u975e\u5fe0\u5b9e\u89e3\u91ca\u3002", "motivation": "\u73b0\u6709LLM\u751f\u6210\u7684\u63a8\u7406token\u5e8f\u5217\u770b\u4f3c\u4eba\u7c7b\u601d\u8003\u8fc7\u7a0b\uff0c\u4f46\u5b9e\u8bc1\u8868\u660e\u5b83\u4eec\u5e76\u975e\u6a21\u578b\u5b9e\u9645\u63a8\u7406\u8fc7\u7a0b\u7684\u5fe0\u5b9e\u89e3\u91ca\u3002\u9700\u8981\u5f25\u5408\u8fd9\u79cd\u8868\u8c61\u4e0e\u529f\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u5f15\u5165State over Tokens (SoT)\u6982\u5ff5\u6846\u67b6\uff0c\u5c06\u63a8\u7406token\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5916\u90e8\u5316\u8ba1\u7b97\u72b6\u6001\u2014\u2014\u8fd9\u662f\u6a21\u578b\u65e0\u72b6\u6001\u751f\u6210\u5468\u671f\u4e2d\u552f\u4e00\u6301\u4e45\u7684\u4fe1\u606f\u8f7d\u4f53\u3002", "result": "SoT\u6846\u67b6\u89e3\u91ca\u4e86\u63a8\u7406token\u5982\u4f55\u5728\u4e0d\u4f5c\u4e3a\u5fe0\u5b9e\u6587\u672c\u89e3\u91ca\u7684\u60c5\u51b5\u4e0b\u9a71\u52a8\u6b63\u786e\u63a8\u7406\uff0c\u5e76\u63ed\u793a\u4e86\u4e4b\u524d\u88ab\u5ffd\u89c6\u7684\u7814\u7a76\u95ee\u9898\u3002", "conclusion": "\u8981\u771f\u6b63\u7406\u89e3LLM\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u7814\u7a76\u5fc5\u987b\u8d85\u8d8a\u5c06\u63a8\u7406token\u4f5c\u4e3a\u6587\u672c\u6765\u9605\u8bfb\uff0c\u800c\u5e94\u5c06\u5176\u89e3\u7801\u4e3a\u72b6\u6001\u3002"}}
{"id": "2512.13009", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13009", "abs": "https://arxiv.org/abs/2512.13009", "authors": ["O\u011fuzhan Akb\u0131y\u0131k", "Naseem Alhousani", "Fares J. Abu-Dakka"], "title": "K-VARK: Kernelized Variance-Aware Residual Kalman Filter for Sensorless Force Estimation in Collaborative Robots", "comment": null, "summary": "Reliable estimation of contact forces is crucial for ensuring safe and precise interaction of robots with unstructured environments. However, accurate sensorless force estimation remains challenging due to inherent modeling errors and complex residual dynamics and friction. To address this challenge, in this paper, we propose K-VARK (Kernelized Variance-Aware Residual Kalman filter), a novel approach that integrates a kernelized, probabilistic model of joint residual torques into an adaptive Kalman filter framework. Through Kernelized Movement Primitives trained on optimized excitation trajectories, K-VARK captures both the predictive mean and input-dependent heteroscedastic variance of residual torques, reflecting data variability and distance-to-training effects. These statistics inform a variance-aware virtual measurement update by augmenting the measurement noise covariance, while the process noise covariance adapts online via variational Bayesian optimization to handle dynamic disturbances. Experimental validation on a 6-DoF collaborative manipulator demonstrates that K-VARK achieves over 20% reduction in RMSE compared to state-of-the-art sensorless force estimation methods, yielding robust and accurate external force/torque estimation suitable for advanced tasks such as polishing and assembly.", "AI": {"tldr": "K-VARK\u662f\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u65e0\u4f20\u611f\u5668\u529b\u4f30\u8ba1\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6838\u5316\u6982\u7387\u6a21\u578b\u548c\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u57286\u81ea\u7531\u5ea6\u534f\u4f5c\u673a\u68b0\u81c2\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u8d85\u8fc720%\u7684RMSE\u964d\u4f4e\u3002", "motivation": "\u673a\u5668\u4eba\u5b89\u5168\u7cbe\u786e\u5730\u4e0e\u65e0\u7ed3\u6784\u73af\u5883\u4ea4\u4e92\u9700\u8981\u53ef\u9760\u7684\u63a5\u89e6\u529b\u4f30\u8ba1\uff0c\u4f46\u7531\u4e8e\u5efa\u6a21\u8bef\u5dee\u3001\u590d\u6742\u6b8b\u4f59\u52a8\u529b\u5b66\u548c\u6469\u64e6\uff0c\u51c6\u786e\u7684\u65e0\u4f20\u611f\u5668\u529b\u4f30\u8ba1\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faK-VARK\u65b9\u6cd5\uff0c\u5c06\u5173\u8282\u6b8b\u4f59\u626d\u77e9\u7684\u6838\u5316\u6982\u7387\u6a21\u578b\u96c6\u6210\u5230\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u6846\u67b6\u4e2d\u3002\u901a\u8fc7\u57fa\u4e8e\u4f18\u5316\u6fc0\u52b1\u8f68\u8ff9\u8bad\u7ec3\u7684\u6838\u5316\u8fd0\u52a8\u57fa\u5143\uff0c\u6355\u6349\u6b8b\u4f59\u626d\u77e9\u7684\u9884\u6d4b\u5747\u503c\u548c\u8f93\u5165\u4f9d\u8d56\u7684\u5f02\u65b9\u5dee\u65b9\u5dee\uff0c\u8fd9\u4e9b\u7edf\u8ba1\u4fe1\u606f\u901a\u8fc7\u589e\u5f3a\u6d4b\u91cf\u566a\u58f0\u534f\u65b9\u5dee\u6765\u901a\u77e5\u65b9\u5dee\u611f\u77e5\u7684\u865a\u62df\u6d4b\u91cf\u66f4\u65b0\uff0c\u540c\u65f6\u8fc7\u7a0b\u566a\u58f0\u534f\u65b9\u5dee\u901a\u8fc7\u53d8\u5206\u8d1d\u53f6\u65af\u4f18\u5316\u5728\u7ebf\u9002\u5e94\u4ee5\u5904\u7406\u52a8\u6001\u6270\u52a8\u3002", "result": "\u57286\u81ea\u7531\u5ea6\u534f\u4f5c\u673a\u68b0\u81c2\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cK-VARK\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u65e0\u4f20\u611f\u5668\u529b\u4f30\u8ba1\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8d85\u8fc720%\u7684RMSE\u964d\u4f4e\uff0c\u4e3a\u629b\u5149\u548c\u88c5\u914d\u7b49\u9ad8\u7ea7\u4efb\u52a1\u63d0\u4f9b\u4e86\u9c81\u68d2\u51c6\u786e\u7684\u5916\u90e8\u529b/\u626d\u77e9\u4f30\u8ba1\u3002", "conclusion": "K-VARK\u901a\u8fc7\u96c6\u6210\u6838\u5316\u6982\u7387\u6a21\u578b\u548c\u81ea\u9002\u5e94\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u65e0\u4f20\u611f\u5668\u529b\u4f30\u8ba1\u4e2d\u7684\u5efa\u6a21\u8bef\u5dee\u548c\u590d\u6742\u52a8\u6001\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f30\u8ba1\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.12812", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12812", "abs": "https://arxiv.org/abs/2512.12812", "authors": ["Hanyu Cai", "Binqi Shen", "Lier Jin", "Lan Hu", "Xiaojing Fan"], "title": "Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA", "comment": null, "summary": "Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.\n  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e0d\u540c\u8bed\u6c14\uff08\u53cb\u597d\u3001\u4e2d\u6027\u3001\u7c97\u9c81\uff09\u5bf9\u4e09\u79cd\u4e3b\u6d41LLM\u5728MMMLU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8bed\u6c14\u654f\u611f\u6027\u56e0\u6a21\u578b\u548c\u9886\u57df\u800c\u5f02\uff0c\u4f46\u73b0\u4ee3LLM\u603b\u4f53\u4e0a\u5bf9\u8bed\u6c14\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u63d0\u793a\u5de5\u7a0b\u5bf9LLM\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8bed\u8a00\u8bed\u6c14\u548c\u793c\u8c8c\u7b49\u8bed\u7528\u5143\u7d20\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u4e4b\u95f4\u3002\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u4ea4\u4e92\u8bed\u6c14\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u5728MMMLU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u4e09\u79cdLLM\uff08GPT-4o mini\u3001Gemini 2.0 Flash\u3001Llama 4 Scout\uff09\u5728\"\u975e\u5e38\u53cb\u597d\"\u3001\"\u4e2d\u6027\"\u548c\"\u975e\u5e38\u7c97\u9c81\"\u4e09\u79cd\u8bed\u6c14\u53d8\u4f53\u4e0b\u7684\u8868\u73b0\uff0c\u6db5\u76d6STEM\u548c\u4eba\u6587\u9886\u57df\u516d\u4e2a\u4efb\u52a1\uff0c\u5e76\u8fdb\u884c\u7edf\u8ba1\u663e\u8457\u6027\u68c0\u9a8c\u3002", "result": "\u8bed\u6c14\u654f\u611f\u6027\u662f\u6a21\u578b\u4f9d\u8d56\u548c\u9886\u57df\u7279\u5b9a\u7684\uff1a\u4e2d\u6027\u6216\u53cb\u597d\u63d0\u793a\u901a\u5e38\u6bd4\u7c97\u9c81\u63d0\u793a\u51c6\u786e\u7387\u66f4\u9ad8\uff0c\u4f46\u7edf\u8ba1\u663e\u8457\u6548\u5e94\u4ec5\u51fa\u73b0\u5728\u90e8\u5206\u4eba\u6587\u4efb\u52a1\u4e2d\uff1b\u7c97\u9c81\u8bed\u6c14\u4f1a\u964d\u4f4eGPT\u548cLlama\u7684\u51c6\u786e\u7387\uff0c\u800cGemini\u76f8\u5bf9\u4e0d\u654f\u611f\uff1b\u8de8\u4efb\u52a1\u805a\u5408\u540e\u8bed\u6c14\u6548\u5e94\u51cf\u5f31\u5e76\u5931\u53bb\u7edf\u8ba1\u663e\u8457\u6027\u3002", "conclusion": "\u867d\u7136\u4ea4\u4e92\u8bed\u6c14\u5728\u7279\u5b9a\u89e3\u91ca\u6027\u573a\u666f\u4e2d\u53ef\u80fd\u91cd\u8981\uff0c\u4f46\u73b0\u4ee3LLM\u603b\u4f53\u4e0a\u5bf9\u8bed\u6c14\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u63d0\u793a\u8bbe\u8ba1\u548c\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2512.13080", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13080", "abs": "https://arxiv.org/abs/2512.13080", "authors": ["Yicheng Feng", "Wanpeng Zhang", "Ye Wang", "Hao Luo", "Haoqi Yuan", "Sipeng Zheng", "Zongqing Lu"], "title": "Spatial-Aware VLA Pretraining through Visual-Physical Alignment from Human Videos", "comment": null, "summary": "Vision-Language-Action (VLA) models provide a promising paradigm for robot learning by integrating visual perception with language-guided policy learning. However, most existing approaches rely on 2D visual inputs to perform actions in 3D physical environments, creating a significant gap between perception and action grounding. To bridge this gap, we propose a Spatial-Aware VLA Pretraining paradigm that performs explicit alignment between visual space and physical space during pretraining, enabling models to acquire 3D spatial understanding before robot policy learning. Starting from pretrained vision-language models, we leverage large-scale human demonstration videos to extract 3D visual and 3D action annotations, forming a new source of supervision that aligns 2D visual observations with 3D spatial reasoning. We instantiate this paradigm with VIPA-VLA, a dual-encoder architecture that incorporates a 3D visual encoder to augment semantic visual representations with 3D-aware features. When adapted to downstream robot tasks, VIPA-VLA achieves significantly improved grounding between 2D vision and 3D action, resulting in more robust and generalizable robotic policies.", "AI": {"tldr": "\u63d0\u51fa\u7a7a\u95f4\u611f\u77e5\u7684VLA\u9884\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc72D\u89c6\u89c9\u4e0e3D\u7a7a\u95f4\u7684\u663e\u5f0f\u5bf9\u9f50\uff0c\u89e3\u51b3\u73b0\u6709VLA\u6a21\u578b\u57283D\u7269\u7406\u73af\u5883\u4e2d\u611f\u77e5\u4e0e\u52a8\u4f5c\u7684\u9e3f\u6c9f\u95ee\u9898\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5927\u591a\u4f9d\u8d562D\u89c6\u89c9\u8f93\u5165\u57283D\u7269\u7406\u73af\u5883\u4e2d\u6267\u884c\u52a8\u4f5c\uff0c\u5bfc\u81f4\u611f\u77e5\u4e0e\u52a8\u4f5c\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u9700\u8981\u5f25\u54082D\u89c6\u89c9\u89c2\u5bdf\u4e0e3D\u7a7a\u95f4\u63a8\u7406\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u63d0\u51fa\u7a7a\u95f4\u611f\u77e5VLA\u9884\u8bad\u7ec3\u8303\u5f0f\uff1a1) \u5229\u7528\u5927\u89c4\u6a21\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u63d0\u53d63D\u89c6\u89c9\u548c3D\u52a8\u4f5c\u6807\u6ce8\uff1b2) \u6784\u5efaVIPA-VLA\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5305\u542b3D\u89c6\u89c9\u7f16\u7801\u5668\u589e\u5f3a\u8bed\u4e49\u89c6\u89c9\u8868\u793a\uff1b3) \u5728\u9884\u8bad\u7ec3\u671f\u95f4\u663e\u5f0f\u5bf9\u9f50\u89c6\u89c9\u7a7a\u95f4\u4e0e\u7269\u7406\u7a7a\u95f4\u3002", "result": "VIPA-VLA\u5728\u4e0b\u6e38\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u663e\u8457\u6539\u5584\u4e862D\u89c6\u89c9\u4e0e3D\u52a8\u4f5c\u4e4b\u95f4\u7684\u63a5\u5730\u6027\uff0c\u4ea7\u751f\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u7b56\u7565\u3002", "conclusion": "\u901a\u8fc7\u7a7a\u95f4\u611f\u77e5\u9884\u8bad\u7ec3\u8303\u5f0f\uff0cVLA\u6a21\u578b\u80fd\u591f\u5728\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u524d\u83b7\u5f973D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u6709\u6548\u5f25\u5408\u4e86\u611f\u77e5\u4e0e\u52a8\u4f5c\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002"}}
{"id": "2512.12818", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12818", "abs": "https://arxiv.org/abs/2512.12818", "authors": ["Chris Latimer", "Nicol\u00f3 Boschi", "Andrew Neeser", "Chris Bartholomew", "Gaurav Srivastava", "Xuan Wang", "Naren Ramakrishnan"], "title": "Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects", "comment": null, "summary": "Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.", "AI": {"tldr": "Hindsight\u662f\u4e00\u79cd\u65b0\u578b\u667a\u80fd\u4f53\u8bb0\u5fc6\u67b6\u6784\uff0c\u901a\u8fc7\u56db\u4e2a\u903b\u8f91\u7f51\u7edc\u7ed3\u6784\u5316\u7ec4\u7ec7\u8bb0\u5fc6\uff0c\u652f\u6301\u4fdd\u7559\u3001\u56de\u5fc6\u548c\u53cd\u601d\u4e09\u79cd\u6838\u5fc3\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u957f\u671f\u5bf9\u8bdd\u8bb0\u5fc6\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u8bb0\u5fc6\u7cfb\u7edf\u5c06\u8bb0\u5fc6\u4f5c\u4e3a\u5916\u90e8\u5c42\u5904\u7406\uff0c\u5b58\u5728\u8bc1\u636e\u4e0e\u63a8\u7406\u754c\u9650\u6a21\u7cca\u3001\u957f\u671f\u4fe1\u606f\u7ec4\u7ec7\u56f0\u96be\u3001\u63a8\u7406\u89e3\u91ca\u652f\u6301\u6709\u9650\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7ed3\u6784\u5316\u7684\u8bb0\u5fc6\u67b6\u6784\u6765\u652f\u6301\u957f\u671f\u63a8\u7406\u3002", "method": "\u63d0\u51faHindsight\u8bb0\u5fc6\u67b6\u6784\uff0c\u5c06\u8bb0\u5fc6\u7ec4\u7ec7\u4e3a\u56db\u4e2a\u903b\u8f91\u7f51\u7edc\uff1a\u4e16\u754c\u4e8b\u5b9e\u3001\u667a\u80fd\u4f53\u7ecf\u9a8c\u3001\u5408\u6210\u5b9e\u4f53\u6458\u8981\u548c\u6f14\u5316\u4fe1\u5ff5\u3002\u5305\u542b\u65f6\u95f4\u611f\u77e5\u8bb0\u5fc6\u5c42\u5c06\u5bf9\u8bdd\u6d41\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u8bb0\u5fc6\u5e93\uff0c\u53cd\u601d\u5c42\u8fdb\u884c\u53ef\u8ffd\u6eaf\u7684\u63a8\u7406\u548c\u66f4\u65b0\u3002", "result": "\u5728LongMemEval\u548cLoCoMo\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHindsight\u5c0620B\u5f00\u6e90\u6a21\u578b\u7684\u51c6\u786e\u7387\u4ece39%\u63d0\u5347\u523083.6%\uff0c\u8d85\u8d8a\u5b8c\u6574\u4e0a\u4e0b\u6587GPT-4o\u3002\u8fdb\u4e00\u6b65\u6269\u5c55\u6a21\u578b\u53ef\u5c06\u6027\u80fd\u63d0\u5347\u81f391.4%\uff08LongMemEval\uff09\u548c89.61%\uff08LoCoMo\uff09\u3002", "conclusion": "Hindsight\u901a\u8fc7\u7ed3\u6784\u5316\u8bb0\u5fc6\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u5728\u957f\u671f\u5bf9\u8bdd\u4e2d\u7684\u8bb0\u5fc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u4f1a\u8bdd\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2512.13090", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13090", "abs": "https://arxiv.org/abs/2512.13090", "authors": ["Jebeom Chae", "Junwoo Chang", "Seungho Yeom", "Yujin Kim", "Jongeun Choi"], "title": "Multi-Robot Motion Planning from Vision and Language using Heat-Inspired Diffusion", "comment": null, "summary": "Diffusion models have recently emerged as powerful tools for robot motion planning by capturing the multi-modal distribution of feasible trajectories. However, their extension to multi-robot settings with flexible, language-conditioned task specifications remains limited. Furthermore, current diffusion-based approaches incur high computational cost during inference and struggle with generalization because they require explicit construction of environment representations and lack mechanisms for reasoning about geometric reachability. To address these limitations, we present Language-Conditioned Heat-Inspired Diffusion (LCHD), an end-to-end vision-based framework that generates language-conditioned, collision-free trajectories. LCHD integrates CLIP-based semantic priors with a collision-avoiding diffusion kernel serving as a physical inductive bias that enables the planner to interpret language commands strictly within the reachable workspace. This naturally handles out-of-distribution scenarios -- in terms of reachability -- by guiding robots toward accessible alternatives that match the semantic intent, while eliminating the need for explicit obstacle information at inference time. Extensive evaluations on diverse real-world-inspired maps, along with real-robot experiments, show that LCHD consistently outperforms prior diffusion-based planners in success rate, while reducing planning latency.", "AI": {"tldr": "LCHD\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u8bed\u8a00\u6761\u4ef6\u6269\u6563\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408CLIP\u8bed\u4e49\u5148\u9a8c\u548c\u78b0\u649e\u907f\u514d\u6269\u6563\u6838\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u663e\u5f0f\u73af\u5883\u8868\u793a\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u751f\u6210\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u89c4\u5212\u6548\u7387\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u9650\u5236\uff1a1\uff09\u96be\u4ee5\u6269\u5c55\u5230\u591a\u673a\u5668\u4eba\u573a\u666f\u548c\u7075\u6d3b\u7684\u8bed\u8a00\u6761\u4ef6\u4efb\u52a1\u89c4\u8303\uff1b2\uff09\u63a8\u7406\u65f6\u8ba1\u7b97\u6210\u672c\u9ad8\uff1b3\uff09\u9700\u8981\u663e\u5f0f\u73af\u5883\u8868\u793a\u4e14\u7f3a\u4e4f\u51e0\u4f55\u53ef\u8fbe\u6027\u63a8\u7406\u673a\u5236\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faLCHD\u6846\u67b6\uff0c\u6574\u5408CLIP\u8bed\u4e49\u5148\u9a8c\u4f5c\u4e3a\u8bed\u8a00\u7406\u89e3\u6a21\u5757\uff0c\u8bbe\u8ba1\u78b0\u649e\u907f\u514d\u6269\u6563\u6838\u4f5c\u4e3a\u7269\u7406\u5f52\u7eb3\u504f\u7f6e\uff0c\u4f7f\u89c4\u5212\u5668\u80fd\u5728\u53ef\u8fbe\u5de5\u4f5c\u7a7a\u95f4\u5185\u4e25\u683c\u89e3\u91ca\u8bed\u8a00\u6307\u4ee4\u3002\u8be5\u6846\u67b6\u65e0\u9700\u63a8\u7406\u65f6\u63d0\u4f9b\u663e\u5f0f\u969c\u788d\u7269\u4fe1\u606f\uff0c\u901a\u8fc7\u5f15\u5bfc\u673a\u5668\u4eba\u671d\u5411\u8bed\u4e49\u610f\u56fe\u5339\u914d\u7684\u53ef\u8fbe\u66ff\u4ee3\u65b9\u6848\u6765\u5904\u7406\u5206\u5e03\u5916\u573a\u666f\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u542f\u53d1\u5730\u56fe\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cLCHD\u5728\u6210\u529f\u7387\u4e0a\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6269\u6563\u89c4\u5212\u5668\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u89c4\u5212\u5ef6\u8fdf\u3002", "conclusion": "LCHD\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u7406\u89e3\u548c\u7269\u7406\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8bed\u8a00\u6761\u4ef6\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\uff0c\u80fd\u591f\u5904\u7406\u53ef\u8fbe\u6027\u76f8\u5173\u7684\u5206\u5e03\u5916\u573a\u666f\uff0c\u4e3a\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7075\u6d3b\u4efb\u52a1\u89c4\u8303\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12839", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12839", "abs": "https://arxiv.org/abs/2512.12839", "authors": ["Dingyi Yang", "Qin Jin"], "title": "What Matters in Evaluating Book-Length Stories? A Systematic Study of Long Story Evaluation", "comment": "24 pages, 7 figures, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics", "summary": "In this work, we conduct systematic research in a challenging area: the automatic evaluation of book-length stories (>100K tokens). Our study focuses on two key questions: (1) understanding which evaluation aspects matter most to readers, and (2) exploring effective methods for evaluating lengthy stories. We introduce the first large-scale benchmark, LongStoryEval, comprising 600 newly published books with an average length of 121K tokens (maximum 397K). Each book includes its average rating and multiple reader reviews, presented as critiques organized by evaluation aspects. By analyzing all user-mentioned aspects, we propose an evaluation criteria structure and conduct experiments to identify the most significant aspects among the 8 top-level criteria. For evaluation methods, we compare the effectiveness of three types: aggregation-based, incremental-updated, and summary-based evaluations. Our findings reveal that aggregation- and summary-based evaluations perform better, with the former excelling in detail assessment and the latter offering greater efficiency. Building on these insights, we further propose NovelCritique, an 8B model that leverages the efficient summary-based framework to review and score stories across specified aspects. NovelCritique outperforms commercial models like GPT-4o in aligning with human evaluations. Our datasets and codes are available at https://github.com/DingyiYang/LongStoryEval.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5927\u89c4\u6a21\u957f\u7bc7\u6545\u4e8b\u81ea\u52a8\u8bc4\u4f30\u57fa\u51c6LongStoryEval\uff0c\u5305\u542b600\u672c\u5e73\u574712.1\u4e07token\u7684\u4e66\u7c4d\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u6458\u8981\u5f0f\u8bc4\u4f30\u6846\u67b6\u548c8B\u6a21\u578bNovelCritique\uff0c\u5728\u4eba\u7c7b\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8eGPT-4o\u3002", "motivation": "\u957f\u7bc7\u6545\u4e8b\uff08>10\u4e07token\uff09\u7684\u81ea\u52a8\u8bc4\u4f30\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u7406\u89e3\u8bfb\u8005\u6700\u5173\u6ce8\u54ea\u4e9b\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u5e76\u63a2\u7d22\u6709\u6548\u7684\u957f\u6587\u672c\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6784\u5efaLongStoryEval\u57fa\u51c6\uff08600\u672c\u4e66\u7c4d\uff0c\u5e73\u574712.1\u4e07token\uff09\uff0c\u5206\u6790\u8bfb\u8005\u8bc4\u8bba\u63d0\u51fa8\u4e2a\u9876\u5c42\u8bc4\u4f30\u6807\u51c6\uff0c\u6bd4\u8f83\u805a\u5408\u5f0f\u3001\u589e\u91cf\u66f4\u65b0\u5f0f\u548c\u6458\u8981\u5f0f\u4e09\u79cd\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6458\u8981\u5f0f\u6846\u67b6\u5f00\u53d18B\u6a21\u578bNovelCritique\u3002", "result": "\u805a\u5408\u5f0f\u548c\u6458\u8981\u5f0f\u8bc4\u4f30\u8868\u73b0\u66f4\u597d\uff0c\u524d\u8005\u5728\u7ec6\u8282\u8bc4\u4f30\u4e0a\u66f4\u4f18\uff0c\u540e\u8005\u6548\u7387\u66f4\u9ad8\u3002NovelCritique\u5728\u4eba\u7c7b\u8bc4\u4f30\u5bf9\u9f50\u65b9\u9762\u4f18\u4e8eGPT-4o\u7b49\u5546\u4e1a\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u957f\u7bc7\u6545\u4e8b\u81ea\u52a8\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u548c\u6709\u6548\u65b9\u6cd5\uff0cNovelCritique\u5c55\u793a\u4e86\u9ad8\u6548\u8bc4\u4f30\u6846\u67b6\u7684\u6f5c\u529b\uff0c\u63a8\u52a8\u4e86\u957f\u6587\u672c\u8bc4\u4f30\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.13093", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13093", "abs": "https://arxiv.org/abs/2512.13093", "authors": ["Mingqi Yuan", "Tao Yu", "Haolin Song", "Bo Li", "Xin Jin", "Hua Chen", "Wenjun Zeng"], "title": "PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations", "comment": "13 pages, 12 figures", "summary": "Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.", "AI": {"tldr": "\u63d0\u51faPvP\u6846\u67b6\uff0c\u901a\u8fc7\u672c\u4f53\u611f\u77e5\u4e0e\u7279\u6743\u72b6\u6001\u5bf9\u6bd4\u5b66\u4e60\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u6548\u7387\uff0c\u5e76\u5f00\u53d1SRL4Humanoid\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5728\u901f\u5ea6\u548c\u8fd0\u52a8\u6a21\u4eff\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u9700\u8981\u9ad8\u6548\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u4f46\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u6837\u672c\u6548\u7387\u4f4e\u4e0b\u7684\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u673a\u5668\u4eba\u590d\u6742\u52a8\u529b\u5b66\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6837\u672c\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faPvP\uff08\u672c\u4f53\u611f\u77e5-\u7279\u6743\u5bf9\u6bd4\u5b66\u4e60\uff09\u6846\u67b6\uff0c\u5229\u7528\u672c\u4f53\u611f\u77e5\u72b6\u6001\u4e0e\u7279\u6743\u72b6\u6001\u7684\u5185\u5728\u4e92\u8865\u6027\u5b66\u4e60\u7d27\u51d1\u4efb\u52a1\u76f8\u5173\u6f5c\u5728\u8868\u793a\uff0c\u65e0\u9700\u624b\u5de5\u6570\u636e\u589e\u5f3a\u3002\u540c\u65f6\u5f00\u53d1SRL4Humanoid\u7edf\u4e00\u6a21\u5757\u5316\u6846\u67b6\uff0c\u96c6\u6210\u4ee3\u8868\u6027\u72b6\u6001\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728LimX Oli\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u901f\u5ea6\u8ddf\u8e2a\u548c\u8fd0\u52a8\u6a21\u4eff\u4efb\u52a1\u5b9e\u9a8c\uff0cPvP\u76f8\u6bd4\u57fa\u7ebfSRL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u3002\u4e3a\u6570\u636e\u9ad8\u6548\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "PvP\u6846\u67b6\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u6837\u672c\u6548\u7387\u95ee\u9898\uff0cSRL4Humanoid\u6846\u67b6\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u652f\u6301\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u7684\u6570\u636e\u9ad8\u6548\u5b66\u4e60\u63d0\u4f9b\u6709\u4ef7\u503c\u6307\u5bfc\u3002"}}
{"id": "2512.12868", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12868", "abs": "https://arxiv.org/abs/2512.12868", "authors": ["Furong Jia", "Yuan Pu", "Finn Guo", "Monica Agrawal"], "title": "Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM", "comment": null, "summary": "Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.", "AI": {"tldr": "LLMs\u5728\u4e34\u5e8a\u8bca\u65ad\u57fa\u51c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46FBPR\u65b9\u6cd5\u4f7f\u7528\u7b80\u5355\u7684\u6982\u5ff5-\u8bca\u65ad\u5171\u73b0\u7edf\u8ba1\u5c31\u80fd\u8fbe\u5230\u76f8\u4f3c\u6027\u80fd\uff0c\u8868\u660eLLMs\u7684\u6027\u80fd\u5e76\u975e\u5b8c\u5168\u6765\u81ea\u6982\u7387\u63a8\u7406", "motivation": "\u7814\u7a76LLMs\u5728\u4e34\u5e8a\u8bca\u65ad\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u662f\u5426\u771f\u6b63\u53cd\u6620\u4e86\u6982\u7387\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u57fa\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u9891\u7387\u7edf\u8ba1", "method": "\u63d0\u51faFBPR\u65b9\u6cd5\uff1a\u4f7f\u7528\u5e73\u6ed1\u6734\u7d20\u8d1d\u53f6\u65af\u5bf9\u6982\u5ff5-\u8bca\u65ad\u5171\u73b0\u7edf\u8ba1\u8fdb\u884c\u8bc4\u5206\uff0c\u4ece\u9884\u8bad\u7ec3\u8bed\u6599\u4e2d\u63d0\u53d6\u5171\u73b0\u6570\u636e", "result": "FBPR\u4e0e\u5bf9\u5e94LLMs\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u4e24\u8005\u6b63\u786e\u56de\u7b54\u7684\u95ee\u9898\u91cd\u53e0\u5ea6\u4ec5\u7565\u9ad8\u4e8e\u968f\u673a\uff0c\u8868\u660e\u65b9\u6cd5\u4e92\u8865", "conclusion": "\u663e\u5f0f\u6982\u7387\u57fa\u7ebf\u4ecd\u6709\u4ef7\u503c\uff1a\u63d0\u4f9b\u6027\u80fd\u53c2\u8003\u70b9\u548c\u4e92\u8865\u4fe1\u53f7\uff1bLLMs\u6027\u80fd\u673a\u5236\u4e0d\u540c\u4e8e\u7b80\u5355\u9891\u7387\u805a\u5408\uff0c\u4f46\u4f20\u7edf\u4e13\u5bb6\u7cfb\u7edf\u65b9\u6cd5\u4ecd\u5360\u57fa\u51c6\u6027\u80fd\u7684\u91cd\u8981\u90e8\u5206"}}
{"id": "2512.13094", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13094", "abs": "https://arxiv.org/abs/2512.13094", "authors": ["Xiang Li", "Gang Liu", "Weitao Zhou", "Hongyi Zhu", "Zhong Cao"], "title": "Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation", "comment": null, "summary": "Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.", "AI": {"tldr": "SoE\u65b9\u6cd5\u901a\u8fc7\u65f6\u5e8f\u4e13\u5bb6\u5e8f\u5217\u7b56\u7565\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u95ed\u73af\u6027\u80fd\uff0c\u65e0\u9700\u589e\u52a0\u6a21\u578b\u89c4\u6a21\u6216\u6570\u636e\u9700\u6c42\uff0c\u5728nuPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u95ed\u73af\u73af\u5883\u4e2d\u7531\u4e8e\u5fae\u5c0f\u8bef\u5dee\u7684\u7d2f\u79ef\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5355\u65f6\u95f4\u70b9\u7684\u72b6\u6001\u7ea7\u9c81\u68d2\u6027\uff0c\u800c\u81ea\u52a8\u9a7e\u9a76\u662f\u8fde\u7eed\u65f6\u95f4\u8fc7\u7a0b\uff0c\u9700\u8981\u4ece\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u589e\u5f3a\u9c81\u68d2\u6027", "method": "\u63d0\u51faSequence of Experts (SoE)\u65b9\u6cd5\uff0c\u4e00\u79cd\u65f6\u5e8f\u4ea4\u66ff\u7b56\u7565\uff0c\u901a\u8fc7\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684\u4e13\u5bb6\u5e8f\u5217\u6765\u589e\u5f3a\u95ed\u73af\u6027\u80fd\uff0c\u4e0d\u589e\u52a0\u6a21\u578b\u89c4\u6a21\u6216\u6570\u636e\u9700\u6c42", "result": "\u5728\u5927\u578b\u81ea\u52a8\u9a7e\u9a76\u57fa\u51c6nuPlan\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSoE\u65b9\u6cd5\u80fd\u4e00\u81f4\u4e14\u663e\u8457\u63d0\u5347\u6240\u6709\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73", "conclusion": "SoE\u6a21\u5757\u4e3a\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u63d0\u4f9b\u4e86\u5173\u952e\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u652f\u6301\uff0c\u901a\u8fc7\u65f6\u95f4\u5c3a\u5ea6\u589e\u5f3a\u9c81\u68d2\u6027\u4e3a\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2"}}
{"id": "2512.12950", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12950", "abs": "https://arxiv.org/abs/2512.12950", "authors": ["Lingyi Meng", "Maolin Liu", "Hao Wang", "Yilan Cheng", "Qi Yang", "Idlkaid Mohanmmed"], "title": "Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping", "comment": "43 pages, 6 fingures, accepted in Artificial Intelligence and Law (2025)", "summary": "Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4eba\u673a\u534f\u4f5c\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u591a\u8bed\u8a00\u6cd5\u5f8b\u672f\u8bed\u6570\u636e\u5e93\uff0c\u7279\u522b\u9488\u5bf9\u4e2d-\u65e5-\u82f1\u8bed\u8a00\u5bf9\uff0c\u901a\u8fc7AI\u5904\u7406\u91cd\u590d\u4efb\u52a1\u3001\u4e13\u5bb6\u63d0\u4f9b\u76d1\u7763\u7684\u65b9\u5f0f\u63d0\u9ad8\u672f\u8bed\u6620\u5c04\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u4e2d-\u65e5\u7b49\u8bed\u8a00\u5b58\u5728\u5927\u91cf\u540c\u5f62\u5f02\u4e49\u8bcd\uff0c\u73b0\u6709\u8d44\u6e90\u548c\u6807\u51c6\u5316\u5de5\u5177\u6709\u9650\uff0c\u51c6\u786e\u6620\u5c04\u8de8\u8bed\u8a00\u6cd5\u5f8b\u672f\u8bed\u9762\u4e34\u91cd\u5927\u6311\u6218\u3002", "method": "\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0cAI\u4ee3\u7406\u5904\u7406OCR\u3001\u6587\u672c\u5206\u5272\u3001\u8bed\u4e49\u5bf9\u9f50\u548c\u521d\u59cb\u672f\u8bed\u63d0\u53d6\u7b49\u91cd\u590d\u4efb\u52a1\uff0c\u6cd5\u5f8b\u9886\u57df\u4e13\u5bb6\u63d0\u4f9b\u76d1\u7763\u3001\u5ba1\u67e5\u548c\u4e0a\u4e0b\u6587\u77e5\u8bc6\u5224\u65ad\uff0c\u6db5\u76d6\u4ece\u539f\u59cb\u6587\u6863\u9884\u5904\u7406\u3001\u7bc7\u7ae0\u5bf9\u9f50\u5230\u672f\u8bed\u63d0\u53d6\u3001\u6620\u5c04\u548c\u8d28\u91cf\u4fdd\u8bc1\u7684\u5168\u8fc7\u7a0b\u3002", "result": "\u4f7f\u7528\u5305\u542b35\u90e8\u5173\u952e\u4e2d\u56fd\u6cd5\u89c4\u53ca\u5176\u82f1\u3001\u65e5\u8bd1\u6587\u7684\u5e73\u884c\u8bed\u6599\u5e93\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7ed3\u679c\u8868\u660e\u8fd9\u79cd\u4eba\u673a\u534f\u4f5c\u7684\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u591a\u8bed\u8a00\u6cd5\u5f8b\u672f\u8bed\u6620\u5c04\u7684\u7cbe\u786e\u6027\u548c\u4e00\u81f4\u6027\uff0c\u76f8\u6bd4\u4f20\u7edf\u4eba\u5de5\u65b9\u6cd5\u8fd8\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u4eba\u673a\u534f\u4f5c\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u8de8\u8bed\u8a00\u6cd5\u5f8b\u672f\u8bed\u6620\u5c04\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5408\u7406\u5206\u5de5\u5b9e\u73b0\u8d28\u91cf\u4e0e\u6548\u7387\u7684\u5e73\u8861\uff0c\u4e3a\u591a\u8bed\u8a00\u6cd5\u5f8b\u672f\u8bed\u6570\u636e\u5e93\u6784\u5efa\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.13100", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13100", "abs": "https://arxiv.org/abs/2512.13100", "authors": ["Guanhua Ji", "Harsha Polavaram", "Lawrence Yunliang Chen", "Sandeep Bajamahal", "Zehan Ma", "Simeon Adebola", "Chenfeng Xu", "Ken Goldberg"], "title": "OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning", "comment": null, "summary": "Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\\% of its real data, which risks overfitting to robot-scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $\u03c0_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot-gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.", "AI": {"tldr": "\u63d0\u51faAugE-Toolkit\u673a\u5668\u4eba\u589e\u5f3a\u6d41\u6c34\u7ebf\u548cOXE-AugE\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u589e\u52a09\u79cd\u673a\u5668\u4eba\u5f62\u6001\u5c06OXE\u6570\u636e\u96c6\u6269\u59273\u500d\u4ee5\u4e0a\uff0c\u6539\u5584\u8de8\u5f62\u6001\u5b66\u4e60\u6548\u679c", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u6570\u636e\u96c6\uff08\u5982OXE\uff09\u5b58\u5728\u4e25\u91cd\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u524d\u56db\u79cd\u673a\u5668\u4eba\u7c7b\u578b\u5360\u771f\u5b9e\u6570\u636e\u768485%\u4ee5\u4e0a\uff0c\u5bb9\u6613\u5bfc\u81f4\u5bf9\u7279\u5b9a\u673a\u5668\u4eba-\u573a\u666f\u7ec4\u5408\u7684\u8fc7\u62df\u5408\uff0c\u9700\u8981\u66f4\u5e73\u8861\u591a\u6837\u7684\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565", "method": "\u5f00\u53d1\u4e86AugE-Toolkit\u53ef\u6269\u5c55\u673a\u5668\u4eba\u589e\u5f3a\u6d41\u6c34\u7ebf\uff0c\u521b\u5efa\u4e86OXE-AugE\u9ad8\u8d28\u91cf\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u5728\u539f\u6709OXE\u57fa\u7840\u4e0a\u589e\u52a0\u4e869\u79cd\u4e0d\u540c\u7684\u673a\u5668\u4eba\u5f62\u6001\uff0c\u5305\u542b\u8d85\u8fc7440\u4e07\u6761\u8f68\u8ff9", "result": "\u589e\u5f3a\u6570\u636e\u96c6\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5728\u589e\u5f3a\u673a\u5668\u4eba\u4e0a\u7684\u7b56\u7565\u6027\u80fd\uff0c\u8fd8\u80fd\u63d0\u5347\u5728\u672a\u89c1\u8fc7\u7684\u673a\u5668\u4eba\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u751a\u81f3\u5bf9\u539f\u59cb\u673a\u5668\u4eba\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u8868\u73b0\u4e5f\u6709\u6539\u5584\u3002OpenVLA\u548c\u03c00\u7b49\u5148\u8fdb\u901a\u7528\u7b56\u7565\u5728OXE-AugE\u4e0a\u5fae\u8c03\u540e\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u673a\u5668\u4eba-\u5939\u722a\u7ec4\u5408\u4e0a\u7684\u6210\u529f\u7387\u63d0\u9ad8\u4e8624-45%", "conclusion": "\u901a\u8fc7\u673a\u5668\u4eba\u6570\u636e\u589e\u5f3a\u6269\u5c55\u6570\u636e\u96c6\u591a\u6837\u6027\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u8de8\u5f62\u6001\u5b66\u4e60\u6548\u679c\uff0c\u4e3a\u8bad\u7ec3\u66f4\u901a\u7528\u7684\u673a\u5668\u4eba\u7b56\u7565\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6570\u636e\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.12967", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12967", "abs": "https://arxiv.org/abs/2512.12967", "authors": ["Weizhou Shen", "Ziyi Yang", "Chenliang Li", "Zhiyuan Lu", "Miao Peng", "Huashan Sun", "Yingcheng Shi", "Shengyi Liao", "Shaopeng Lai", "Bo Zhang", "Dayiheng Liu", "Fei Huang", "Jingren Zhou", "Ming Yan"], "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management", "comment": null, "summary": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.", "AI": {"tldr": "QwenLong-L1.5\u901a\u8fc7\u7cfb\u7edf\u540e\u8bad\u7ec3\u521b\u65b0\u5b9e\u73b0\u5353\u8d8a\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u5305\u62ec\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u5408\u6210\u3001\u7a33\u5b9a\u5f3a\u5316\u5b66\u4e60\u548c\u5185\u5b58\u589e\u5f3a\u67b6\u6784\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230GPT-5\u548cGemini-2.5-Pro\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u68c0\u7d22\u4efb\u52a1\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u957f\u8ddd\u79bb\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u89e3\u51b3\u8d85\u957f\u5e8f\u5217\u5904\u7406\u95ee\u9898\u3002", "method": "1) \u957f\u4e0a\u4e0b\u6587\u6570\u636e\u5408\u6210\u7ba1\u9053\uff1a\u5c06\u6587\u6863\u5206\u89e3\u4e3a\u539f\u5b50\u4e8b\u5b9e\u548c\u5173\u7cfb\uff0c\u7f16\u7a0b\u751f\u6210\u53ef\u9a8c\u8bc1\u63a8\u7406\u95ee\u9898\uff1b2) \u7a33\u5b9a\u5f3a\u5316\u5b66\u4e60\uff1a\u4efb\u52a1\u5e73\u8861\u91c7\u6837\u548c\u81ea\u9002\u5e94\u71b5\u63a7\u5236\u7b56\u7565\u4f18\u5316\uff1b3) \u5185\u5b58\u589e\u5f3a\u67b6\u6784\uff1a\u591a\u9636\u6bb5\u878d\u5408RL\u8bad\u7ec3\uff0c\u96c6\u6210\u5355\u6b21\u63a8\u7406\u548c\u8fed\u4ee3\u5185\u5b58\u5904\u7406\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u57fa\u51c6\u4e0a\u6027\u80fd\u4e0eGPT-5\u548cGemini-2.5-Pro\u76f8\u5f53\uff0c\u6bd4\u57fa\u7ebf\u5e73\u5747\u63d0\u53479.90\u5206\uff1b\u5728\u8d85\u957f\u4efb\u52a1(1M~4M tokens)\u4e0a\uff0c\u5185\u5b58\u4ee3\u7406\u6846\u67b6\u6bd4\u4ee3\u7406\u57fa\u7ebf\u63d0\u53479.48\u5206\uff1b\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u4e5f\u63d0\u5347\u4e86\u79d1\u5b66\u63a8\u7406\u3001\u8bb0\u5fc6\u5de5\u5177\u4f7f\u7528\u548c\u6269\u5c55\u5bf9\u8bdd\u7b49\u901a\u7528\u9886\u57df\u6027\u80fd\u3002", "conclusion": "QwenLong-L1.5\u901a\u8fc7\u7cfb\u7edf\u540e\u8bad\u7ec3\u521b\u65b0\u6210\u529f\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u5176\u6280\u672f\u7a81\u7834\u4e3a\u5904\u7406\u8d85\u957f\u5e8f\u5217\u548c\u590d\u6742\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13153", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13153", "abs": "https://arxiv.org/abs/2512.13153", "authors": ["Ruiqi Yu", "Qianshi Wang", "Hongyi Li", "Zheng Jun", "Zhicheng Wang", "Jun Wu", "Qiuguo Zhu"], "title": "START: Traversing Sparse Footholds with Terrain Reconstruction", "comment": null, "summary": "Traversing terrains with sparse footholds like legged animals presents a promising yet challenging task for quadruped robots, as it requires precise environmental perception and agile control to secure safe foot placement while maintaining dynamic stability. Model-based hierarchical controllers excel in laboratory settings, but suffer from limited generalization and overly conservative behaviors. End-to-end learning-based approaches unlock greater flexibility and adaptability, but existing state-of-the-art methods either rely on heightmaps that introduce noise and complex, costly pipelines, or implicitly infer terrain features from egocentric depth images, often missing accurate critical geometric cues and leading to inefficient learning and rigid gaits. To overcome these limitations, we propose START, a single-stage learning framework that enables agile, stable locomotion on highly sparse and randomized footholds. START leverages only low-cost onboard vision and proprioception to accurately reconstruct local terrain heightmap, providing an explicit intermediate representation to convey essential features relevant to sparse foothold regions. This supports comprehensive environmental understanding and precise terrain assessment, reducing exploration cost and accelerating skill acquisition. Experimental results demonstrate that START achieves zero-shot transfer across diverse real-world scenarios, showcasing superior adaptability, precise foothold placement, and robust locomotion.", "AI": {"tldr": "START\uff1a\u4e00\u79cd\u5355\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u4f4e\u6210\u672c\u673a\u8f7d\u89c6\u89c9\u548c\u672c\u4f53\u611f\u77e5\u6765\u91cd\u5efa\u5c40\u90e8\u5730\u5f62\u9ad8\u5ea6\u56fe\uff0c\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u5728\u7a00\u758f\u7acb\u8db3\u70b9\u5730\u5f62\u4e0a\u7684\u654f\u6377\u7a33\u5b9a\u8fd0\u52a8\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u5728\u7a00\u758f\u7acb\u8db3\u70b9\u5730\u5f62\u4e0a\u8fd0\u52a8\u9700\u8981\u7cbe\u786e\u7684\u73af\u5883\u611f\u77e5\u548c\u654f\u6377\u63a7\u5236\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u6a21\u578b\u7684\u5206\u5c42\u63a7\u5236\u5668\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4e14\u884c\u4e3a\u4fdd\u5b88\uff1b\u7aef\u5230\u7aef\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u566a\u58f0\u9ad8\u5ea6\u56fe\uff0c\u8981\u4e48\u4ece\u6df1\u5ea6\u56fe\u50cf\u9690\u5f0f\u63a8\u65ad\u5730\u5f62\u7279\u5f81\uff0c\u7f3a\u5c11\u7cbe\u786e\u51e0\u4f55\u7ebf\u7d22\uff0c\u5bfc\u81f4\u5b66\u4e60\u6548\u7387\u4f4e\u548c\u6b65\u6001\u50f5\u786c\u3002", "method": "\u63d0\u51faSTART\u5355\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff0c\u4ec5\u4f7f\u7528\u4f4e\u6210\u672c\u673a\u8f7d\u89c6\u89c9\u548c\u672c\u4f53\u611f\u77e5\u51c6\u786e\u91cd\u5efa\u5c40\u90e8\u5730\u5f62\u9ad8\u5ea6\u56fe\uff0c\u63d0\u4f9b\u660e\u786e\u7684\u4e2d\u95f4\u8868\u793a\u6765\u4f20\u8fbe\u7a00\u758f\u7acb\u8db3\u70b9\u533a\u57df\u7684\u5173\u952e\u7279\u5f81\uff0c\u652f\u6301\u5168\u9762\u7684\u73af\u5883\u7406\u89e3\u548c\u7cbe\u786e\u5730\u5f62\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eSTART\u5b9e\u73b0\u4e86\u5728\u591a\u6837\u5316\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u9002\u5e94\u6027\u3001\u7cbe\u786e\u7684\u7acb\u8db3\u70b9\u653e\u7f6e\u548c\u9c81\u68d2\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "conclusion": "START\u901a\u8fc7\u663e\u5f0f\u5730\u5f62\u91cd\u5efa\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u51cf\u5c11\u4e86\u63a2\u7d22\u6210\u672c\uff0c\u52a0\u901f\u4e86\u6280\u80fd\u83b7\u53d6\uff0c\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u5728\u7a00\u758f\u7acb\u8db3\u70b9\u5730\u5f62\u4e0a\u7684\u654f\u6377\u7a33\u5b9a\u8fd0\u52a8\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12976", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12976", "abs": "https://arxiv.org/abs/2512.12976", "authors": ["Marcus Ma", "Cole Johnson", "Nolan Bridges", "Jackson Trager", "Georgios Chochlakis", "Shrikanth Narayanan"], "title": "Authors Should Annotate", "comment": null, "summary": "The status quo for labeling text is third-party annotation, but there are many cases where information directly from the document's source would be preferable over a third-person proxy, especially for egocentric features like sentiment and belief. We introduce author labeling, an annotation technique where the writer of the document itself annotates the data at the moment of creation. We collaborate with a commercial chatbot with over 10,000 users to deploy an author labeling annotation system for subjective features related to product recommendation. This system identifies task-relevant queries, generates on-the-fly labeling questions, and records authors' answers in real time. We train and deploy an online-learning model architecture for product recommendation that continuously improves from author labeling and find it achieved a 534% increase in click-through rate compared to an industry advertising baseline running concurrently. We then compare the quality and practicality of author labeling to three traditional annotation approaches for sentiment analysis and find author labeling to be higher quality, faster to acquire, and cheaper. These findings reinforce existing literature that annotations, especially for egocentric and subjective beliefs, are significantly higher quality when labeled by the author rather than a third party. To facilitate broader scientific adoption, we release an author labeling service for the research community at academic.echollm.io.", "AI": {"tldr": "\u4f5c\u8005\u6807\u6ce8\uff1a\u8ba9\u6587\u6863\u4f5c\u8005\u5728\u521b\u4f5c\u65f6\u76f4\u63a5\u6807\u6ce8\u6570\u636e\u7684\u65b0\u65b9\u6cd5\uff0c\u76f8\u6bd4\u7b2c\u4e09\u65b9\u6807\u6ce8\u5728\u4e3b\u89c2\u7279\u5f81\u4e0a\u8d28\u91cf\u66f4\u9ad8\u3001\u66f4\u5feb\u3001\u66f4\u4fbf\u5b9c", "motivation": "\u5f53\u524d\u6587\u672c\u6807\u6ce8\u4e3b\u8981\u4f9d\u8d56\u7b2c\u4e09\u65b9\u6807\u6ce8\uff0c\u4f46\u5bf9\u4e8e\u60c5\u611f\u3001\u4fe1\u5ff5\u7b49\u81ea\u6211\u4e2d\u5fc3\u7279\u5f81\uff0c\u76f4\u63a5\u4ece\u6587\u6863\u6765\u6e90\u83b7\u53d6\u4fe1\u606f\u6bd4\u7b2c\u4e09\u65b9\u4ee3\u7406\u66f4\u4f18", "method": "\u5f15\u5165\u4f5c\u8005\u6807\u6ce8\u6280\u672f\uff0c\u5728\u6587\u6863\u521b\u4f5c\u65f6\u8ba9\u4f5c\u8005\u76f4\u63a5\u6807\u6ce8\u6570\u636e\uff1b\u4e0e\u62e5\u670910,000+\u7528\u6237\u7684\u5546\u4e1a\u804a\u5929\u673a\u5668\u4eba\u5408\u4f5c\uff0c\u90e8\u7f72\u9488\u5bf9\u4ea7\u54c1\u63a8\u8350\u4e3b\u89c2\u7279\u5f81\u7684\u4f5c\u8005\u6807\u6ce8\u7cfb\u7edf\uff1b\u8be5\u7cfb\u7edf\u8bc6\u522b\u4efb\u52a1\u76f8\u5173\u67e5\u8be2\uff0c\u5b9e\u65f6\u751f\u6210\u6807\u6ce8\u95ee\u9898\u5e76\u8bb0\u5f55\u4f5c\u8005\u56de\u7b54", "result": "\u8bad\u7ec3\u5e76\u90e8\u7f72\u4e86\u5728\u7ebf\u5b66\u4e60\u6a21\u578b\u67b6\u6784\u7528\u4e8e\u4ea7\u54c1\u63a8\u8350\uff0c\u6301\u7eed\u4ece\u4f5c\u8005\u6807\u6ce8\u4e2d\u6539\u8fdb\uff0c\u70b9\u51fb\u7387\u6bd4\u884c\u4e1a\u5e7f\u544a\u57fa\u51c6\u63d0\u9ad8534%\uff1b\u4e0e\u4f20\u7edf\u6807\u6ce8\u65b9\u6cd5\u6bd4\u8f83\uff0c\u4f5c\u8005\u6807\u6ce8\u5728\u60c5\u611f\u5206\u6790\u4e0a\u8d28\u91cf\u66f4\u9ad8\u3001\u83b7\u53d6\u66f4\u5feb\u3001\u6210\u672c\u66f4\u4f4e", "conclusion": "\u4f5c\u8005\u6807\u6ce8\u5bf9\u4e8e\u81ea\u6211\u4e2d\u5fc3\u548c\u4e3b\u89c2\u4fe1\u5ff5\u7684\u6807\u6ce8\u8d28\u91cf\u663e\u8457\u9ad8\u4e8e\u7b2c\u4e09\u65b9\u6807\u6ce8\uff1b\u4e3a\u4fc3\u8fdb\u79d1\u7814\u91c7\u7528\uff0c\u53d1\u5e03\u4e86\u5b66\u672f\u754c\u7684\u4f5c\u8005\u6807\u6ce8\u670d\u52a1"}}
{"id": "2512.13170", "categories": ["cs.RO", "cs.LG", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.13170", "abs": "https://arxiv.org/abs/2512.13170", "authors": ["Deepak Ingole", "Valentin Bhend", "Shiva Ganesh Murali", "Oliver Dobrich", "Alisa Rupenayan"], "title": "Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks", "comment": null, "summary": "Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8fed\u4ee3\u5b66\u4e60\u7684\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6743\u91cd\u77e9\u9635\u81ea\u52a8\u8c03\u4f18\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u7ea7\u6027\u80fd\u53cd\u9988\u81ea\u9002\u5e94\u8c03\u6574\u6743\u91cd\uff0c\u57284\u6b21\u5728\u7ebf\u91cd\u590d\u4e2d\u8fbe\u5230\u63a5\u8fd1\u79bb\u7ebf\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u6027\u80fd", "motivation": "\u5236\u9020\u8fc7\u7a0b\u5e38\u53d7\u73af\u5883\u6f02\u79fb\u548c\u7cfb\u7edf\u78e8\u635f\u5f71\u54cd\uff0c\u5373\u4f7f\u5728\u91cd\u590d\u64cd\u4f5c\u4e2d\u4e5f\u9700\u8981\u91cd\u65b0\u8c03\u6574\u63a7\u5236\u53c2\u6570\u3002\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u79bb\u7ebf\u8bc4\u4f30\u6216\u590d\u6742\u7684\u68af\u5ea6\u8ba1\u7b97\u3002", "method": "\u53d7\u8303\u6570\u6700\u4f18\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\u542f\u53d1\uff0c\u6784\u5efa\u7ecf\u9a8c\u7075\u654f\u5ea6\u77e9\u9635\uff0c\u65e0\u9700\u901a\u8fc7NMPC\u6c42\u89e3\u5668\u8fdb\u884c\u5fae\u5206\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u7684\u6743\u91cd\u66f4\u65b0\u3002\u5728UR10e\u673a\u5668\u4eba\u78b3\u7ea4\u7ef4\u7f20\u7ed5\u4efb\u52a1\u4e0a\u8fdb\u884c\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4ec54\u6b21\u5728\u7ebf\u91cd\u590d\u4e2d\u6536\u655b\u5230\u63a5\u8fd1\u79bb\u7ebf\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u8ddf\u8e2a\u6027\u80fd\uff08RMSE\u5728BO\u76840.3%\u4ee5\u5185\uff09\uff0c\u800cBO\u7b97\u6cd5\u9700\u8981100\u6b21\u79bb\u7ebf\u8bc4\u4f30\u3002", "conclusion": "\u4e3a\u91cd\u590d\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u81ea\u9002\u5e94NMPC\u8c03\u4f18\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u7cbe\u5fc3\u4f18\u5316\u63a7\u5236\u5668\u7684\u7cbe\u5ea6\u548c\u5728\u7ebf\u9002\u5e94\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2512.13059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13059", "abs": "https://arxiv.org/abs/2512.13059", "authors": ["Ikuya Yamada", "Wataru Ikeda", "Ko Yoshida", "Mengyu Ye", "Hinata Sugimoto", "Masatoshi Suzuki", "Hisanori Ozaki", "Jun Suzuki"], "title": "An Open and Reproducible Deep Research Agent for Long-Form Question Answering", "comment": "Technical report of a winning system in the NeurIPS MMU-RAG competition", "summary": "We present an open deep research system for long-form question answering, selected as a winning system in the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system combines an open-source large language model (LLM) with an open web search API to perform iterative retrieval, reasoning, and synthesis in real-world open-domain settings. To enhance reasoning quality, we apply preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects, including clarity, insightfulness, and factuality. Our experimental results show that the proposed method consistently improves answer quality across all three aspects. Our source code is publicly available at https://github.com/efficient-deep-research/efficient-deep-research.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u957f\u5f62\u5f0f\u95ee\u7b54\u7684\u5f00\u653e\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\uff0c\u7ed3\u5408\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5f00\u653e\u7f51\u7edc\u641c\u7d22API\uff0c\u901a\u8fc7\u8fed\u4ee3\u68c0\u7d22\u3001\u63a8\u7406\u548c\u5408\u6210\u6765\u63d0\u5347\u56de\u7b54\u8d28\u91cf\uff0c\u5e76\u5728MMU-RAG\u7ade\u8d5b\u4e2d\u83b7\u5956\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u4e16\u754c\u5f00\u653e\u57df\u8bbe\u7f6e\u4e2d\u7684\u957f\u5f62\u5f0f\u95ee\u7b54\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u80fd\u591f\u8fdb\u884c\u6709\u6548\u7684\u68c0\u7d22\u3001\u63a8\u7406\u548c\u5408\u6210\uff0c\u540c\u65f6\u786e\u4fdd\u56de\u7b54\u7684\u6e05\u6670\u6027\u3001\u6d1e\u5bdf\u529b\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5f00\u653e\u7f51\u7edc\u641c\u7d22API\uff0c\u91c7\u7528\u8fed\u4ee3\u68c0\u7d22\u3001\u63a8\u7406\u548c\u5408\u6210\u7684\u65b9\u6cd5\uff0c\u5e76\u5e94\u7528\u57fa\u4e8eLLM-as-a-judge\u53cd\u9988\u7684\u504f\u597d\u8c03\u4f18\u6765\u8bc4\u4f30\u548c\u63d0\u5347\u56de\u7b54\u7684\u6e05\u6670\u6027\u3001\u6d1e\u5bdf\u529b\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6240\u6709\u4e09\u4e2a\u8bc4\u4f30\u65b9\u9762\uff08\u6e05\u6670\u6027\u3001\u6d1e\u5bdf\u529b\u548c\u4e8b\u5b9e\u51c6\u786e\u6027\uff09\u90fd\u80fd\u6301\u7eed\u63d0\u5347\u56de\u7b54\u8d28\u91cf\uff0c\u5e76\u5728NeurIPS 2025\u7684MMU-RAG\u7ade\u8d5b\u6587\u672c\u5230\u6587\u672c\u8d5b\u9053\u4e2d\u83b7\u5956\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u5f00\u653e\u57df\u957f\u5f62\u5f0f\u95ee\u7b54\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u57fa\u4e8e\u504f\u597d\u7684\u8c03\u4f18\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56de\u7b54\u8d28\u91cf\uff0c\u4e14\u4ee3\u7801\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2512.13183", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13183", "abs": "https://arxiv.org/abs/2512.13183", "authors": ["Alfredo Gonz\u00e1lez-Calvin", "Juan F. Jim\u00e9nez", "H\u00e9ctor Garc\u00eda de Marina"], "title": "Efficient Generation of Smooth Paths with Curvature Guarantees by Mollification", "comment": null, "summary": "Most path following and trajectory tracking algorithms in mobile robotics require the desired path or trajectory to be defined by at least twice continuously differentiable functions to guarantee key properties such as global convergence, especially for nonholonomic robots like unicycles with speed constraints. Consequently, these algorithms typically exclude continuous but non-differentiable paths, such as piecewise functions. Despite this exclusion, such paths provide convenient high-level inputs for describing robot missions or behavior. While techniques such as spline interpolation or optimization-based methods are commonly used to smooth non-differentiable paths or create feasible ones from sequences of waypoints, they either can produce unnecessarily complex trajectories or are computationally expensive. In this work, we present a method to regularize non-differentiable functions and generate feasible paths through mollification. Specifically, we approximate an arbitrary path with a differentiable function that can converge to it with arbitrary precision. Additionally, we provide a systematic method for bounding the curvature of generated paths, which we demonstrate by applying it to paths resulting from linking a sequence of waypoints with segments. The proposed approach is computationally efficient, enabling real-time implementation on microcontrollers and compatibility with standard trajectory tracking and path following algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u901a\u8fc7mollification\u6b63\u5219\u5316\u975e\u53ef\u5fae\u8def\u5f84\u7684\u65b9\u6cd5\uff0c\u751f\u6210\u53ef\u5fae\u8def\u5f84\u5e76\u63a7\u5236\u66f2\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5fae\u63a7\u5236\u5668\u5b9e\u73b0", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u8def\u5f84\u8ddf\u8e2a\u7b97\u6cd5\u901a\u5e38\u8981\u6c42\u8def\u5f84\u81f3\u5c11\u4e8c\u9636\u8fde\u7eed\u53ef\u5fae\uff0c\u4f46\u5b9e\u9645\u4efb\u52a1\u4e2d\u5e38\u4f7f\u7528\u5206\u6bb5\u8fde\u7eed\u7684\u975e\u53ef\u5fae\u8def\u5f84\u3002\u73b0\u6709\u5e73\u6ed1\u65b9\u6cd5\u8981\u4e48\u4ea7\u751f\u590d\u6742\u8f68\u8ff9\uff0c\u8981\u4e48\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "\u4f7f\u7528mollification\u6280\u672f\u5c06\u4efb\u610f\u8def\u5f84\u8fd1\u4f3c\u4e3a\u53ef\u5fae\u51fd\u6570\uff0c\u53ef\u4efb\u610f\u7cbe\u5ea6\u6536\u655b\u5230\u539f\u8def\u5f84\u3002\u63d0\u4f9b\u7cfb\u7edf\u65b9\u6cd5\u9650\u5236\u751f\u6210\u8def\u5f84\u7684\u66f2\u7387\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7531\u822a\u70b9\u5e8f\u5217\u8fde\u63a5\u7684\u5206\u6bb5\u8def\u5f84\u3002", "result": "\u65b9\u6cd5\u8ba1\u7b97\u9ad8\u6548\uff0c\u53ef\u5728\u5fae\u63a7\u5236\u5668\u4e0a\u5b9e\u65f6\u5b9e\u73b0\uff0c\u4e0e\u6807\u51c6\u8f68\u8ff9\u8ddf\u8e2a\u548c\u8def\u5f84\u8ddf\u968f\u7b97\u6cd5\u517c\u5bb9\u3002", "conclusion": "\u63d0\u51fa\u7684mollification\u65b9\u6cd5\u80fd\u6709\u6548\u6b63\u5219\u5316\u975e\u53ef\u5fae\u8def\u5f84\uff0c\u751f\u6210\u53ef\u5fae\u4e14\u66f2\u7387\u53ef\u63a7\u7684\u53ef\u884c\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5e73\u6ed1\u65b9\u6cd5\u7684\u590d\u6742\u6027\u548c\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002"}}
{"id": "2512.13063", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13063", "abs": "https://arxiv.org/abs/2512.13063", "authors": ["Cheril Shah", "Akshit Agarwal", "Kanak Garg", "Mourad Heddaya"], "title": "LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators", "comment": "Published in the First Workshop on Multi-Turn Interactions in Large Language Models at Neurips 2025", "summary": "Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u53cc\u66f2\u6b63\u5207\u66f2\u7ebf\u7684\u8ba9\u6b65\u52a8\u6001\u7edf\u4e00\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u7206\u53d1\u6027\u03c4\u548c\u8ba9\u6b65\u521a\u6027\u6307\u6570(CRI)\u4e24\u4e2a\u6307\u6807\u6765\u91cf\u5316\u62a5\u4ef7\u8f68\u8ff9\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u6bd4\u8f83\u4eba\u7c7b\u8c08\u5224\u8005\u4e0e\u56db\u79cd\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b(LLM)\uff0c\u53d1\u73b0LLM\u5728\u8c08\u5224\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u6781\u7aef\u951a\u5b9a\u3001\u7b56\u7565\u591a\u6837\u6027\u6709\u9650\u3001\u65e0\u6cd5\u6839\u636e\u60c5\u5883\u8c03\u6574\u7b49\u6839\u672c\u6027\u5c40\u9650\u3002", "motivation": "\u53cc\u8fb9\u8c08\u5224\u662f\u4e00\u4e2a\u590d\u6742\u7684\u60c5\u5883\u654f\u611f\u4efb\u52a1\uff0c\u4eba\u7c7b\u8c08\u5224\u8005\u4f1a\u6839\u636e\u6743\u529b\u4e0d\u5bf9\u79f0\u548c\u975e\u6b63\u5f0f\u7ebf\u7d22\u52a8\u6001\u8c03\u6574\u951a\u70b9\u3001\u8282\u594f\u548c\u7075\u6d3b\u6027\u3002\u5f53\u524d\u9700\u8981\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8c08\u5224\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0e\u4eba\u7c7b\u6709\u4f55\u5dee\u5f02\uff0c\u4ee5\u53ca\u5b83\u4eec\u662f\u5426\u5177\u5907\u60c5\u5883\u9002\u5e94\u548c\u5bf9\u624b\u63a8\u7406\u80fd\u529b\u3002", "method": "1. \u63d0\u51fa\u57fa\u4e8e\u53cc\u66f2\u6b63\u5207\u66f2\u7ebf\u7684\u8ba9\u6b65\u52a8\u6001\u7edf\u4e00\u6570\u5b66\u6a21\u578b\uff1b2. \u5f15\u5165\u7206\u53d1\u6027\u03c4\u548c\u8ba9\u6b65\u521a\u6027\u6307\u6570(CRI)\u4e24\u4e2a\u91cf\u5316\u6307\u6807\uff1b3. \u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u6bd4\u8f83\uff1a\u4eba\u7c7b\u8c08\u5224\u8005 vs \u56db\u79cd\u6700\u5148\u8fdbLLM\uff1b4. \u5b9e\u9a8c\u8bbe\u7f6e\uff1a\u81ea\u7136\u8bed\u8a00\u548c\u6570\u5b57\u62a5\u4ef7\u4e24\u79cd\u60c5\u5883\uff0c\u6709\u65e0\u4e30\u5bcc\u5e02\u573a\u80cc\u666f\uff0c\u4ee5\u53ca\u516d\u79cd\u53d7\u63a7\u7684\u6743\u529b\u4e0d\u5bf9\u79f0\u573a\u666f\u3002", "result": "1. \u4e0e\u4eba\u7c7b\u80fd\u591f\u5e73\u6ed1\u9002\u5e94\u60c5\u5883\u5e76\u63a8\u65ad\u5bf9\u624b\u7acb\u573a\u548c\u7b56\u7565\u4e0d\u540c\uff0cLLM\u7cfb\u7edf\u6027\u5730\u951a\u5b9a\u5728\u53ef\u80fd\u534f\u8bae\u533a\u7684\u6781\u7aef\u4f4d\u7f6e\uff1b2. LLM\u4f18\u5316\u56fa\u5b9a\u70b9\uff0c\u4e0d\u8003\u8651\u6760\u6746\u6216\u60c5\u5883\uff1b3. \u5b9a\u6027\u5206\u6790\u663e\u793aLLM\u7b56\u7565\u591a\u6837\u6027\u6709\u9650\uff0c\u5076\u5c14\u4f7f\u7528\u6b3a\u9a97\u6027\u7b56\u7565\uff1b4. LLM\u7684\u8c08\u5224\u80fd\u529b\u4e0d\u968f\u6a21\u578b\u6539\u8fdb\u800c\u63d0\u5347\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u8c08\u5224\u80fd\u529b\u4e0a\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u66f4\u597d\u5185\u5316\u5bf9\u624b\u63a8\u7406\u548c\u60c5\u5883\u4f9d\u8d56\u7b56\u7565\u7684\u6a21\u578b\u3002LLM\u65e0\u6cd5\u50cf\u4eba\u7c7b\u90a3\u6837\u52a8\u6001\u9002\u5e94\u6743\u529b\u4e0d\u5bf9\u79f0\u548c\u60c5\u5883\u53d8\u5316\uff0c\u663e\u793a\u51fa\u5728\u590d\u6742\u793e\u4f1a\u4e92\u52a8\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\u3002"}}
{"id": "2512.13198", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13198", "abs": "https://arxiv.org/abs/2512.13198", "authors": ["Hyun-Gi Lee", "Jaekyeong Han", "Minjun Kwon", "Hyeonuk Kwon", "Jooha Park", "Hoe Jin Ha", "Dong-Hwa Seo"], "title": "ALBATROSS: A robotised system for high-throughput electrolyte screening via automated electrolyte formulation, coin-cell fabrication, and electrochemical evaluation", "comment": null, "summary": "As battery technologies advance toward higher stability and energy density, the need for extensive cell-level testing across various component configurations becomes critical. To evaluate performance and understand the operating principles of batteries in laboratory scale, fabrication and evaluation of coin cells are essential processes. However, the conventional coin-cell assembly and testing processes require significant time and labor from researchers, posing challenges to high-throughput screening research. In this study, we introduce an Automated Li-ion BAttery Testing RObot SyStem (ALBATROSS), an automated system capable of electrolyte formulation, coin-cell assembly, and electrochemical evaluation. The system, integrated within a argon-filled glovebox, enables fully automated assembly and testing of up to 48 cells without researcher intervention. By incorporating custom-designed robot gripper and 3D-printed structures optimized for precise cell handling, ALBATROSS achieved high assembly reliability, yielding a relative standard deviation (RSD) of less than 1.2% in discharge capacity and a standard deviation of less than 3 \u03a9 in EIS measurements for NCM811||Li half cells. Owing to its high reliability and automation capability, ALBATROSS allows for the acquisition of high-quality coin-cell datasets, which are expected to accelerate the development of next-generation electrolytes.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u5168\u81ea\u52a8\u9502\u79bb\u5b50\u7535\u6c60\u6d4b\u8bd5\u673a\u5668\u4eba\u7cfb\u7edfALBATROSS\uff0c\u80fd\u591f\u5728\u624b\u5957\u7bb1\u5185\u81ea\u52a8\u5b8c\u6210\u7535\u89e3\u6db2\u914d\u5236\u3001\u7ebd\u6263\u7535\u6c60\u7ec4\u88c5\u548c\u7535\u5316\u5b66\u6d4b\u8bd5\uff0c\u5b9e\u73b0\u9ad8\u901a\u91cf\u7535\u6c60\u7814\u7a76\u3002", "motivation": "\u968f\u7740\u7535\u6c60\u6280\u672f\u5411\u66f4\u9ad8\u7a33\u5b9a\u6027\u548c\u80fd\u91cf\u5bc6\u5ea6\u53d1\u5c55\uff0c\u9700\u8981\u5bf9\u4e0d\u540c\u7ec4\u4ef6\u914d\u7f6e\u8fdb\u884c\u5927\u91cf\u7535\u6c60\u7ea7\u6d4b\u8bd5\u3002\u4f20\u7edf\u7684\u7ebd\u6263\u7535\u6c60\u7ec4\u88c5\u548c\u6d4b\u8bd5\u8fc7\u7a0b\u9700\u8981\u7814\u7a76\u4eba\u5458\u6295\u5165\u5927\u91cf\u65f6\u95f4\u548c\u7cbe\u529b\uff0c\u963b\u788d\u4e86\u9ad8\u901a\u91cf\u7b5b\u9009\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u4e86ALBATROSS\u5168\u81ea\u52a8\u7cfb\u7edf\uff0c\u96c6\u6210\u5728\u6c29\u6c14\u586b\u5145\u7684\u624b\u5957\u7bb1\u5185\uff0c\u5305\u542b\u5b9a\u5236\u8bbe\u8ba1\u7684\u673a\u5668\u4eba\u5939\u722a\u548c3D\u6253\u5370\u7ed3\u6784\uff0c\u7528\u4e8e\u7cbe\u786e\u7535\u6c60\u5904\u7406\u3002\u7cfb\u7edf\u80fd\u591f\u81ea\u52a8\u5b8c\u6210\u7535\u89e3\u6db2\u914d\u5236\u3001\u7ebd\u6263\u7535\u6c60\u7ec4\u88c5\u548c\u7535\u5316\u5b66\u6d4b\u8bd5\uff0c\u6700\u591a\u53ef\u5904\u740648\u4e2a\u7535\u6c60\u800c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9ad8\u7ec4\u88c5\u53ef\u9760\u6027\uff1aNCM811||Li\u534a\u7535\u6c60\u7684\u653e\u7535\u5bb9\u91cf\u76f8\u5bf9\u6807\u51c6\u504f\u5dee\u5c0f\u4e8e1.2%\uff0cEIS\u6d4b\u91cf\u6807\u51c6\u504f\u5dee\u5c0f\u4e8e3\u03a9\u3002\u7cfb\u7edf\u80fd\u591f\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u7ebd\u6263\u7535\u6c60\u6570\u636e\u96c6\u3002", "conclusion": "ALBATROSS\u51ed\u501f\u5176\u9ad8\u53ef\u9760\u6027\u548c\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u80fd\u591f\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u7ebd\u6263\u7535\u6c60\u6570\u636e\u96c6\uff0c\u6709\u671b\u52a0\u901f\u4e0b\u4e00\u4ee3\u7535\u89e3\u6db2\u7684\u5f00\u53d1\u3002"}}
{"id": "2512.13109", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13109", "abs": "https://arxiv.org/abs/2512.13109", "authors": ["Zewen Qiang", "Sendong Zhao", "Haochun Wang", "Bing Qin", "Ting Liu"], "title": "Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\\% in KV-Retrieval tasks.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u957f\u6587\u672c\u65f6\u5b58\u5728\"\u4e2d\u95f4\u8ff7\u5931\"\u95ee\u9898\uff0c\u9664\u4e86\u5df2\u77e5\u7684\u4f4d\u7f6e\u7f16\u7801\u504f\u5dee\u5916\uff0c\u8fd8\u9996\u6b21\u8bc6\u522b\u51fa\u521d\u59cb\u663e\u8457\u6027\u56e0\u7d20\uff0c\u901a\u8fc7\u8c03\u6574\u521d\u59cbtoken\u4e0e\u5176\u4ed6token\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u80fd\u63d0\u5347\u957f\u6587\u672c\u5904\u7406\u80fd\u529b\uff0c\u6700\u9ad8\u5728MDQA\u6570\u636e\u96c6\u4e0a\u63d0\u53473.6%\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5904\u7406\u957f\u6587\u672c\u5e8f\u5217\u65f6\u5b58\u5728\"\u4e2d\u95f4\u8ff7\u5931\"\u73b0\u8c61\uff0c\u5373\u6ce8\u610f\u529b\u8fc7\u5ea6\u96c6\u4e2d\u5728\u6587\u672c\u5f00\u5934\u548c\u7ed3\u5c3e\uff0c\u4e2d\u95f4\u90e8\u5206\u88ab\u5ffd\u89c6\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u5c06\u6b64\u5f52\u56e0\u4e8e\u4f4d\u7f6e\u7f16\u7801\u504f\u5dee\uff0c\u4f46\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5176\u4ed6\u6f5c\u5728\u56e0\u7d20\u3002", "method": "\u9996\u5148\u8bc6\u522b\u51fa\u521d\u59cb\u663e\u8457\u6027\u8fd9\u4e00\u65b0\u56e0\u7d20\uff1a\u5728\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\uff0c\u76f8\u5bf9\u4e8e\u521d\u59cbtoken\u5177\u6709\u66f4\u9ad8\u6ce8\u610f\u529b\u6743\u91cd\u7684token\uff0c\u5728\u9884\u6d4b\u4e0b\u4e00\u4e2atoken\u65f6\u4f1a\u83b7\u5f97\u66f4\u591a\u5173\u6ce8\u3002\u7136\u540e\u5229\u7528\u8fd9\u4e00\u7279\u6027\uff0c\u901a\u8fc7\u7f29\u653e\u521d\u59cbtoken\u4e0e\u5176\u4ed6token\u4e4b\u95f4\u7684\u6ce8\u610f\u529b\u6743\u91cd\u6765\u6539\u8fdb\u6a21\u578b\u7684\u957f\u6587\u672c\u5904\u7406\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728MDQA\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u9ad83.6%\u7684\u6027\u80fd\u63d0\u5347\u3002\u5f53\u4e0e\u73b0\u6709\u7684\u51cf\u5c11\u4f4d\u7f6e\u7f16\u7801\u504f\u5dee\u7684\u65b9\u6cd5\u7ed3\u5408\u65f6\uff0c\u5728KV-Retrieval\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u9ad83.4%\u7684\u8fdb\u4e00\u6b65\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u9664\u4e86\u4f4d\u7f6e\u7f16\u7801\u504f\u5dee\u5916\uff0c\u521d\u59cb\u663e\u8457\u6027\u4e5f\u662f\u5bfc\u81f4\u5927\u8bed\u8a00\u6a21\u578b\"\u4e2d\u95f4\u8ff7\u5931\"\u73b0\u8c61\u7684\u91cd\u8981\u56e0\u7d20\u3002\u901a\u8fc7\u8c03\u6574\u521d\u59cbtoken\u4e0e\u5176\u4ed6token\u7684\u6ce8\u610f\u529b\u6743\u91cd\uff0c\u53ef\u4ee5\u6709\u6548\u6539\u5584\u6a21\u578b\u7684\u957f\u6587\u672c\u5904\u7406\u80fd\u529b\uff0c\u4e14\u4e0e\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408\u80fd\u83b7\u5f97\u66f4\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2512.13214", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13214", "abs": "https://arxiv.org/abs/2512.13214", "authors": ["Diego Bolliger", "Gabriele Fadini", "Markus Bambach", "Alisa Rupenyan"], "title": "Differentiable Material Point Method for the Control of Deformable Objects", "comment": "7 Pages, 4 Figures, 1 Table", "summary": "Controlling the deformation of flexible objects is challenging due to their non-linear dynamics and high-dimensional configuration space. This work presents a differentiable Material Point Method (MPM) simulator targeted at control applications. We exploit the differentiability of the simulator to optimize a control trajectory in an active damping problem for a hyperelastic rope. The simulator effectively minimizes the kinetic energy of the rope around 2$\\times$ faster than a baseline MPPI method and to a 20% lower energy level, while using about 3% of the computation time.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u5fae\u5206MPM\u6a21\u62df\u5668\u7528\u4e8e\u67d4\u6027\u7269\u4f53\u63a7\u5236\uff0c\u5728\u8d85\u5f39\u6027\u7ef3\u7d22\u4e3b\u52a8\u963b\u5c3c\u95ee\u9898\u4e2d\u6bd4\u57fa\u7ebfMPPI\u65b9\u6cd5\u5feb2\u500d\u3001\u80fd\u8017\u4f4e20%\u3001\u8ba1\u7b97\u65f6\u95f4\u4ec5\u97003%", "motivation": "\u67d4\u6027\u7269\u4f53\u63a7\u5236\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5176\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u9ad8\u6548\u4f18\u5316\u63a7\u5236\u8f68\u8ff9\u3002", "method": "\u5f00\u53d1\u53ef\u5fae\u5206\u6750\u6599\u70b9\u6cd5\uff08MPM\uff09\u6a21\u62df\u5668\uff0c\u5229\u7528\u5176\u53ef\u5fae\u6027\u5728\u8d85\u5f39\u6027\u7ef3\u7d22\u4e3b\u52a8\u963b\u5c3c\u95ee\u9898\u4e2d\u4f18\u5316\u63a7\u5236\u8f68\u8ff9\u3002", "result": "\u6a21\u62df\u5668\u4f7f\u7ef3\u7d22\u52a8\u80fd\u6700\u5c0f\u5316\uff0c\u6bd4\u57fa\u7ebfMPPI\u65b9\u6cd5\u5feb\u7ea62\u500d\uff0c\u8fbe\u5230\u7684\u52a8\u80fd\u6c34\u5e73\u4f4e20%\uff0c\u8ba1\u7b97\u65f6\u95f4\u4ec5\u9700\u7ea63%\u3002", "conclusion": "\u53ef\u5fae\u5206MPM\u6a21\u62df\u5668\u4e3a\u67d4\u6027\u7269\u4f53\u63a7\u5236\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u4e0a\u90fd\u6709\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2512.13194", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13194", "abs": "https://arxiv.org/abs/2512.13194", "authors": ["Chendong Sun"], "title": "Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models", "comment": null, "summary": "Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant \"random rejection\" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \\(1 - \\max(P_{\\mathrm{target}})\\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.", "AI": {"tldr": "EARS\u63d0\u51fa\u81ea\u9002\u5e94\u62d2\u7edd\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u63a5\u53d7\u9608\u503c\u6765\u51cf\u5c11\u63a8\u6d4b\u89e3\u7801\u4e2d\u7684\u968f\u673a\u62d2\u7edd\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u4f7f\u7528\u56fa\u5b9a\u7684\u968f\u673a\u9608\u503c\u8fdb\u884c\u62d2\u7edd\u91c7\u6837\uff0c\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u751f\u6210\u573a\u666f\u4e2d\u4f1a\u5bfc\u81f4\u5927\u91cf\u5408\u7406\u7684\u5019\u9009token\u56e0\u968f\u673a\u6027\u88ab\u62d2\u7edd\uff0c\u4ece\u800c\u964d\u4f4e\u63a8\u7406\u6548\u7387\u3002", "method": "\u63d0\u51fa\u9ad8\u6548\u81ea\u9002\u5e94\u62d2\u7edd\u91c7\u6837(EARS)\uff0c\u6839\u636e\u76ee\u6807\u6a21\u578b\u81ea\u8eab\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027(1 - max(P_target))\u52a8\u6001\u8c03\u6574\u63a5\u53d7\u9608\u503c\uff0c\u5f15\u5165\u4e0e\u4e0d\u786e\u5b9a\u6027\u6210\u6bd4\u4f8b\u7684\u5bb9\u5fcd\u9879\uff0c\u5728\u6a21\u578b\u4e0d\u786e\u5b9a\u65f6\u667a\u80fd\u653e\u5bbd\u63a5\u53d7\u6807\u51c6\u3002", "result": "\u5728\u521b\u610f\u5199\u4f5c\u548c\u5f00\u653e\u57dfQA\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cEARS\u663e\u8457\u63d0\u5347\u63a8\u6d4b\u89e3\u7801\u6548\u7387\uff0c\u5728GSM8K\u57fa\u51c6\u4e0a\u5b9e\u73b0\u9ad8\u8fbe18.12%\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u51c6\u786e\u7387\u4ec5\u4e0b\u964d0.84%\u3002", "conclusion": "EARS\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u51cf\u5c11\u968f\u673a\u62d2\u7edd\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2512.13215", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13215", "abs": "https://arxiv.org/abs/2512.13215", "authors": ["Yinsong Qu", "Yunxiang Li", "Shanlin Zhong"], "title": "Multi-directional Safe Rectangle Corridor-Based MPC for Nonholonomic Robots Navigation in Cluttered Environment", "comment": "9 pages, 11 figures, conference paper for the 2025 International Conference on Advanced Robotics and Mechatronics (ICARM), accepted", "summary": "Autonomous Mobile Robots (AMRs) have become indispensable in industrial applications due to their operational flexibility and efficiency. Navigation serves as a crucial technical foundation for accomplishing complex tasks. However, navigating AMRs in dense, cluttered, and semi-structured environments remains challenging, primarily due to nonholonomic vehicle dynamics, interactions with mixed static/dynamic obstacles, and the non-convex constrained nature of such operational spaces. To solve these problems, this paper proposes an Improved Sequential Model Predictive Control (ISMPC) navigation framework that systematically reformulates navigation tasks as sequential switched optimal control problems. The framework addresses the aforementioned challenges through two key innovations: 1) Implementation of a Multi-Directional Safety Rectangular Corridor (MDSRC) algorithm, which encodes the free space through rectangular convex regions to avoid collision with static obstacles, eliminating redundant computational burdens and accelerating solver convergence; 2) A sequential MPC navigation framework that integrates corridor constraints with barrier function constraints is proposed to achieve static and dynamic obstacle avoidance. The ISMPC navigation framework enables direct velocity generation for AMRs, simplifying traditional navigation algorithm architectures. Comparative experiments demonstrate the framework's superiority in free-space utilization ( an increase of 41.05$\\%$ in the average corridor area) while maintaining real-time computational performance (average corridors generation latency of 3 ms).", "AI": {"tldr": "\u63d0\u51fa\u6539\u8fdb\u7684\u987a\u5e8f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u65b9\u5411\u5b89\u5168\u77e9\u5f62\u8d70\u5eca\u548c\u987a\u5e8fMPC\u5b9e\u73b0\u5bc6\u96c6\u534a\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684AMR\u5bfc\u822a", "motivation": "AMR\u5728\u5bc6\u96c6\u3001\u6742\u4e71\u3001\u534a\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u975e\u5b8c\u6574\u8f66\u8f86\u52a8\u529b\u5b66\u3001\u9759\u6001/\u52a8\u6001\u969c\u788d\u7269\u4ea4\u4e92\u4ee5\u53ca\u975e\u51f8\u7ea6\u675f\u7a7a\u95f4\u7b49\u95ee\u9898", "method": "\u63d0\u51faISMPC\u5bfc\u822a\u6846\u67b6\uff1a1) MDSRC\u7b97\u6cd5\u901a\u8fc7\u77e9\u5f62\u51f8\u533a\u57df\u7f16\u7801\u81ea\u7531\u7a7a\u95f4\u907f\u514d\u9759\u6001\u969c\u788d\u7269\u78b0\u649e\uff1b2) \u987a\u5e8fMPC\u6846\u67b6\u96c6\u6210\u8d70\u5eca\u7ea6\u675f\u548c\u5c4f\u969c\u51fd\u6570\u7ea6\u675f\u5b9e\u73b0\u52a8\u9759\u969c\u788d\u7269\u907f\u969c", "result": "\u81ea\u7531\u7a7a\u95f4\u5229\u7528\u7387\u63d0\u9ad841.05%\uff0c\u5e73\u5747\u8d70\u5eca\u751f\u6210\u5ef6\u8fdf\u4ec53ms\uff0c\u4fdd\u6301\u5b9e\u65f6\u8ba1\u7b97\u6027\u80fd\uff0c\u7b80\u5316\u4e86\u4f20\u7edf\u5bfc\u822a\u7b97\u6cd5\u67b6\u6784", "conclusion": "ISMPC\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86AMR\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6311\u6218\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u907f\u969c\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u8d8a"}}
{"id": "2512.13278", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13278", "abs": "https://arxiv.org/abs/2512.13278", "authors": ["Jiaru Zou", "Ling Yang", "Yunzhe Qi", "Sirui Chen", "Mengting Ai", "Ke Shen", "Jingrui He", "Mengdi Wang"], "title": "AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning", "comment": "Best Paper Award at ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic Intelligence", "summary": "Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.", "AI": {"tldr": "AutoTool\u6846\u67b6\u8ba9LLM\u4ee3\u7406\u5177\u5907\u52a8\u6001\u5de5\u5177\u9009\u62e9\u80fd\u529b\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u4f18\u5316\u548cKL\u6b63\u5219\u5316\u6392\u540d\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u56fa\u5b9a\u5de5\u5177\u96c6\uff0c\u9650\u5236\u4e86LLM\u4ee3\u7406\u5bf9\u65b0\u5de5\u5177\u6216\u6f14\u5316\u5de5\u5177\u96c6\u7684\u9002\u5e94\u6027\uff0c\u9700\u8981\u52a8\u6001\u5de5\u5177\u9009\u62e9\u80fd\u529b\u6765\u63d0\u5347\u4ee3\u7406\u7684\u9002\u5e94\u6027\u548c\u6027\u80fd\u3002", "method": "\u6784\u5efa\u5305\u542b20\u4e07\u6837\u672c\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d61000+\u5de5\u5177\u548c100+\u4efb\u52a1\uff1b\u91c7\u7528\u53cc\u9636\u6bb5\u4f18\u5316\uff1a\u76d1\u7763\u548c\u57fa\u4e8eRL\u7684\u8f68\u8ff9\u7a33\u5b9a\u5316\uff0c\u4ee5\u53caKL\u6b63\u5219\u5316Plackett-Luce\u6392\u540d\u6765\u4f18\u5316\u591a\u6b65\u5de5\u5177\u9009\u62e9\u3002", "result": "\u5728\u5341\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAutoTool\u5728\u6570\u5b66\u79d1\u5b66\u63a8\u7406\u63d0\u53476.4%\uff0c\u641c\u7d22QA\u63d0\u53474.5%\uff0c\u4ee3\u7801\u751f\u6210\u63d0\u53477.7%\uff0c\u591a\u6a21\u6001\u7406\u89e3\u63d0\u53476.9%\uff0c\u4e14\u80fd\u52a8\u6001\u5229\u7528\u63a8\u7406\u8fc7\u7a0b\u4e2d\u672a\u89c1\u8fc7\u7684\u5de5\u5177\u3002", "conclusion": "AutoTool\u901a\u8fc7\u52a8\u6001\u5de5\u5177\u9009\u62e9\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u9002\u5e94\u4e0d\u65ad\u6f14\u5316\u7684\u5de5\u5177\u96c6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13219", "categories": ["cs.RO", "cs.MS"], "pdf": "https://arxiv.org/pdf/2512.13219", "abs": "https://arxiv.org/abs/2512.13219", "authors": ["Christoph Hartmann", "Marios Demetriades", "Kevin Pr\u00fcfer", "Zichen Zhang", "Klaus Spindler", "Stefan Weltge"], "title": "A Unified Framework for Automated Assembly Sequence and Production Line Planning using Graph-based Optimization", "comment": "Code available at https://github.com/TUM-utg/PyCAALP (repository will be made public prior to publication)", "summary": "This paper presents PyCAALP (Python-based Computer-Aided Assembly Line Planning), a framework for automated Assembly Sequence Planning (ASP) and Production Line Planning (PLP), employing a graph-based approach to model components and joints within production modules. The framework integrates kinematic boundary conditions, such as potential part collisions, to guarantee the feasibility of automated assembly planning. The developed algorithm computes all feasible production sequences, integrating modules for detecting spatial relationships and formulating geometric constraints. The algorithm incorporates additional attributes, including handling feasibility, tolerance matching, and joint compatibility, to manage the high combinatorial complexity inherent in assembly sequence generation. Heuristics, such as Single-Piece Flow assembly and geometrical constraint enforcement, are utilized to further refine the solution space, facilitating more efficient planning for complex assemblies. The PLP stage is formulated as a Mixed-Integer Program (MIP), balancing the total times of a fixed number of manufacturing stations. While some complexity reduction techniques may sacrifice optimality, they significantly reduce the MIPs computational time. Furthermore, the framework enables customization of engineering constraints and supports a flexible trade-off between ASP and PLP. The open-source nature of the framework, available at https://github.com/TUM-utg/PyCAALP, promotes further collaboration and adoption in both industrial and production research applications.", "AI": {"tldr": "PyCAALP\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u88c5\u914d\u7ebf\u89c4\u5212\u6846\u67b6\uff0c\u91c7\u7528\u56fe\u8bba\u65b9\u6cd5\u5efa\u6a21\u7ec4\u4ef6\u548c\u8fde\u63a5\uff0c\u901a\u8fc7\u96c6\u6210\u8fd0\u52a8\u5b66\u8fb9\u754c\u6761\u4ef6\u548c\u51e0\u4f55\u7ea6\u675f\u6765\u81ea\u52a8\u5316\u88c5\u914d\u5e8f\u5217\u89c4\u5212\u4e0e\u751f\u4ea7\u7ebf\u89c4\u5212\u3002", "motivation": "\u89e3\u51b3\u88c5\u914d\u5e8f\u5217\u89c4\u5212\uff08ASP\uff09\u548c\u751f\u4ea7\u7ebf\u89c4\u5212\uff08PLP\uff09\u4e2d\u7684\u9ad8\u7ec4\u5408\u590d\u6742\u6027\u95ee\u9898\uff0c\u786e\u4fdd\u81ea\u52a8\u5316\u88c5\u914d\u89c4\u5212\u7684\u53ef\u884c\u6027\uff0c\u540c\u65f6\u5e73\u8861\u89c4\u5212\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u91c7\u7528\u56fe\u8bba\u65b9\u6cd5\u5efa\u6a21\u7ec4\u4ef6\u548c\u8fde\u63a5\uff0c\u96c6\u6210\u8fd0\u52a8\u5b66\u8fb9\u754c\u6761\u4ef6\uff08\u5982\u90e8\u4ef6\u78b0\u649e\u68c0\u6d4b\uff09\uff0c\u4f7f\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08\u5982\u5355\u4ef6\u6d41\u88c5\u914d\uff09\u4f18\u5316\u89e3\u7a7a\u95f4\uff0c\u5c06PLP\u9636\u6bb5\u5efa\u6a21\u4e3a\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u95ee\u9898\u3002", "result": "\u5f00\u53d1\u4e86\u5f00\u6e90\u6846\u67b6PyCAALP\uff0c\u80fd\u591f\u8ba1\u7b97\u6240\u6709\u53ef\u884c\u7684\u751f\u4ea7\u5e8f\u5217\uff0c\u652f\u6301\u5de5\u7a0b\u7ea6\u675f\u5b9a\u5236\u5316\uff0c\u5728ASP\u548cPLP\u4e4b\u95f4\u5b9e\u73b0\u7075\u6d3b\u6743\u8861\uff0c\u663e\u8457\u51cf\u5c11MIP\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "PyCAALP\u6846\u67b6\u4e3a\u590d\u6742\u88c5\u914d\u7684\u81ea\u52a8\u5316\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5f00\u6e90\u65b9\u5f0f\u4fc3\u8fdb\u5de5\u4e1a\u548c\u751f\u4ea7\u7814\u7a76\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u5408\u4f5c\u4e0e\u5e94\u7528\u3002"}}
{"id": "2512.13279", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13279", "abs": "https://arxiv.org/abs/2512.13279", "authors": ["Jinrui Liu", "Jeff Wu", "Xuanguang Pan", "Gavin Cheung", "Shuai Ma", "Chongyang Tao"], "title": "AIR: Post-training Data Selection for Reasoning via Attention Head Influence", "comment": "19 pages", "summary": "LLMs achieve remarkable multi-step reasoning capabilities, yet effectively transferring these skills via post-training distillation remains challenging. Existing data selection methods, ranging from manual curation to heuristics based on length, entropy, or overall loss, fail to capture the causal importance of individual reasoning steps, limiting distillation efficiency. To address this, we propose Attention Influence for Reasoning (AIR), a principled, unsupervised and training-free framework that leverages mechanistic insights of the retrieval head to select high-value post-training data. AIR first identifies reasoning-critical attention heads of an off-the-shelf model, then constructs a weakened reference model with disabled head influence, and finally quantifies the resulting loss divergence as the Attention Influence Score. This score enables fine-grained assessment at both the step and sample levels, supporting step-level weighted fine-tuning and global sample selection. Experiments across multiple reasoning benchmarks show that AIR consistently improves reasoning accuracy, surpassing heuristic baselines and effectively isolating the most critical steps and samples. Our work establishes a mechanism-driven, data-efficient approach for reasoning distillation in LLMs.", "AI": {"tldr": "\u63d0\u51faAIR\u6846\u67b6\uff0c\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\u9009\u62e9\u63a8\u7406\u5173\u952e\u6570\u636e\uff0c\u63d0\u5347LLM\u540e\u8bad\u7ec3\u84b8\u998f\u6548\u7387", "motivation": "\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\uff08\u624b\u52a8\u7b5b\u9009\u3001\u57fa\u4e8e\u957f\u5ea6/\u71b5/\u635f\u5931\u7b49\u542f\u53d1\u5f0f\u65b9\u6cd5\uff09\u65e0\u6cd5\u6355\u6349\u63a8\u7406\u6b65\u9aa4\u7684\u56e0\u679c\u91cd\u8981\u6027\uff0c\u9650\u5236\u4e86\u84b8\u998f\u6548\u7387", "method": "AIR\u6846\u67b6\uff1a1)\u8bc6\u522b\u73b0\u6210\u6a21\u578b\u7684\u63a8\u7406\u5173\u952e\u6ce8\u610f\u529b\u5934\uff1b2)\u6784\u5efa\u7981\u7528\u5934\u90e8\u5f71\u54cd\u7684\u5f31\u5316\u53c2\u8003\u6a21\u578b\uff1b3)\u91cf\u5316\u635f\u5931\u5dee\u5f02\u4f5c\u4e3a\u6ce8\u610f\u529b\u5f71\u54cd\u5206\u6570\uff0c\u652f\u6301\u6b65\u9aa4\u7ea7\u52a0\u6743\u5fae\u8c03\u548c\u5168\u5c40\u6837\u672c\u9009\u62e9", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAIR\u6301\u7eed\u63d0\u5347\u63a8\u7406\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u542f\u53d1\u5f0f\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6709\u6548\u8bc6\u522b\u6700\u5173\u952e\u6b65\u9aa4\u548c\u6837\u672c", "conclusion": "\u5efa\u7acb\u4e86\u4e00\u79cd\u673a\u5236\u9a71\u52a8\u3001\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u7528\u4e8eLLM\u7684\u63a8\u7406\u84b8\u998f"}}
{"id": "2512.13262", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13262", "abs": "https://arxiv.org/abs/2512.13262", "authors": ["Hyunki Seong", "Jeong-Kyun Lee", "Heesoo Myeong", "Yongho Shin", "Hyun-Mook Cho", "Duck Hoon Kim", "Pranav Desai", "Monu Surana"], "title": "Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving", "comment": "11 pages, 5 figures", "summary": "Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.", "AI": {"tldr": "\u63d0\u51faGRBO\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\u548cWarm-K\u91c7\u6837\u7b56\u7565\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u8fd0\u52a8\u884c\u4e3a\u7684\u5b89\u5168\u6027\u548c\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u6a21\u578b\u5b58\u5728\u6570\u636e\u96c6\u504f\u5dee\u95ee\u9898\uff0c\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e0b\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u4e14\u5927\u591a\u6570\u7814\u7a76\u91c7\u7528\u5f00\u73af\u8bc4\u4f30\uff0c\u5ffd\u7565\u4e86\u95ed\u73af\u6267\u884c\u4e2d\u7684\u7d2f\u79ef\u8bef\u5dee", "method": "1. GRBO\uff1a\u57fa\u4e8e\u7fa4\u4f53\u76f8\u5bf9\u4f18\u52bf\u6700\u5927\u5316\u548c\u4eba\u7c7b\u6b63\u5219\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5fae\u8c03\u9884\u8bad\u7ec3\u884c\u4e3a\u6a21\u578b\uff1b2. Warm-K\uff1a\u57fa\u4e8e\u70ed\u542f\u52a8\u7684Top-K\u91c7\u6837\u7b56\u7565\uff0c\u5e73\u8861\u8fd0\u52a8\u9009\u62e9\u7684\u4e00\u81f4\u6027\u548c\u591a\u6837\u6027", "result": "GRBO\u4ec5\u4f7f\u752810%\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u63d0\u534740%\u4ee5\u4e0a\u7684\u5b89\u5168\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u884c\u4e3a\u771f\u5b9e\u6027\uff1bWarm-K\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u589e\u5f3a\u884c\u4e3a\u4e00\u81f4\u6027\u548c\u53cd\u5e94\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3", "conclusion": "\u901a\u8fc7GRBO\u548cWarm-K\u4e24\u79cd\u4e92\u8865\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u8fd0\u52a8\u884c\u4e3a\u7684\u5b89\u5168\u6027\u548c\u95ed\u73af\u6267\u884c\u95ee\u9898"}}
{"id": "2512.13286", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13286", "abs": "https://arxiv.org/abs/2512.13286", "authors": ["Youssra Rebboud", "Pasquale Lisena", "Raphael Troncy"], "title": "Integrating Causal Reasoning into Automated Fact-Checking", "comment": "Extended version of the accepted ACM SAC paper", "summary": "In fact-checking applications, a common reason to reject a claim is to detect the presence of erroneous cause-effect relationships between the events at play. However, current automated fact-checking methods lack dedicated causal-based reasoning, potentially missing a valuable opportunity for semantically rich explainability. To address this gap, we propose a methodology that combines event relation extraction, semantic similarity computation, and rule-based reasoning to detect logical inconsistencies between chains of events mentioned in a claim and in an evidence. Evaluated on two fact-checking datasets, this method establishes the first baseline for integrating fine-grained causal event relationships into fact-checking and enhance explainability of verdict prediction.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u4e8b\u4ef6\u5173\u7cfb\u63d0\u53d6\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u548c\u89c4\u5219\u63a8\u7406\u7684\u65b9\u6cd5\uff0c\u5c06\u7ec6\u7c92\u5ea6\u56e0\u679c\u4e8b\u4ef6\u5173\u7cfb\u6574\u5408\u5230\u4e8b\u5b9e\u6838\u67e5\u4e2d\uff0c\u63d0\u5347\u5224\u51b3\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u6027", "motivation": "\u5f53\u524d\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u65b9\u6cd5\u7f3a\u4e4f\u4e13\u95e8\u7684\u56e0\u679c\u63a8\u7406\u673a\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u68c0\u6d4b\u4e8b\u4ef6\u95f4\u7684\u9519\u8bef\u56e0\u679c\u5173\u7cfb\uff0c\u9519\u5931\u4e86\u8bed\u4e49\u4e30\u5bcc\u7684\u53ef\u89e3\u91ca\u6027\u673a\u4f1a", "method": "\u7ed3\u5408\u4e8b\u4ef6\u5173\u7cfb\u63d0\u53d6\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u63a8\u7406\uff0c\u68c0\u6d4b\u58f0\u660e\u548c\u8bc1\u636e\u4e2d\u4e8b\u4ef6\u94fe\u4e4b\u95f4\u7684\u903b\u8f91\u4e0d\u4e00\u81f4\u6027", "result": "\u5728\u4e24\u4e2a\u4e8b\u5b9e\u6838\u67e5\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u8be5\u65b9\u6cd5\u4e3a\u6574\u5408\u7ec6\u7c92\u5ea6\u56e0\u679c\u4e8b\u4ef6\u5173\u7cfb\u5efa\u7acb\u4e86\u9996\u4e2a\u57fa\u51c6\uff0c\u5e76\u589e\u5f3a\u4e86\u5224\u51b3\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u6027", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u586b\u8865\u4e86\u4e8b\u5b9e\u6838\u67e5\u4e2d\u56e0\u679c\u63a8\u7406\u7684\u7a7a\u767d\uff0c\u4e3a\u68c0\u6d4b\u4e8b\u4ef6\u95f4\u9519\u8bef\u56e0\u679c\u5173\u7cfb\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027"}}
{"id": "2512.13271", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13271", "abs": "https://arxiv.org/abs/2512.13271", "authors": ["Fangju Yang", "Hang Yang", "Ibrahim Alsarraj", "Yuhao Wang", "Ke Wu"], "title": "Lightweight Dynamic Modeling of Cable-Driven Continuum Robots Based on Actuation-Space Energy Formulation", "comment": null, "summary": "Cable-driven continuum robots (CDCRs) require accurate, real-time dynamic models for high-speed dynamics prediction or model-based control, making such capability an urgent need. In this paper, we propose the Lightweight Actuation-Space Energy Modeling (LASEM) framework for CDCRs, which formulates actuation potential energy directly in actuation space to enable lightweight yet accurate dynamic modeling. Through a unified variational derivation, the governing dynamics reduce to a single partial differential equation (PDE), requiring only the Euler moment balance while implicitly incorporating the Newton force balance. By also avoiding explicit computation of cable-backbone contact forces, the formulation simplifies the model structure and improves computational efficiency while preserving geometric accuracy and physical consistency. Importantly, the proposed framework for dynamic modeling natively supports both force-input and displacement-input actuation modes, a capability seldom achieved in existing dynamic formulations. Leveraging this lightweight structure, a Galerkin space-time modal discretization with analytical time-domain derivatives of the reduced state further enables an average 62.3% computational speedup over state-of-the-art real-time dynamic modeling approaches.", "AI": {"tldr": "\u63d0\u51faLASEM\u6846\u67b6\uff0c\u901a\u8fc7\u9a71\u52a8\u7a7a\u95f4\u80fd\u91cf\u5efa\u6a21\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u5b9e\u65f6\u52a8\u6001\u5efa\u6a21\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u534762.3%", "motivation": "\u7535\u7f06\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u9700\u8981\u51c6\u786e\u3001\u5b9e\u65f6\u7684\u52a8\u6001\u6a21\u578b\u7528\u4e8e\u9ad8\u901f\u52a8\u6001\u9884\u6d4b\u6216\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\uff0c\u8fd9\u662f\u5f53\u524d\u8feb\u5207\u9700\u6c42", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u9a71\u52a8\u7a7a\u95f4\u80fd\u91cf\u5efa\u6a21\u6846\u67b6\uff0c\u76f4\u63a5\u5728\u9a71\u52a8\u7a7a\u95f4\u8868\u8ff0\u9a71\u52a8\u52bf\u80fd\uff0c\u901a\u8fc7\u53d8\u5206\u63a8\u5bfc\u5c06\u63a7\u5236\u52a8\u6001\u7b80\u5316\u4e3a\u5355\u4e00\u504f\u5fae\u5206\u65b9\u7a0b\uff0c\u907f\u514d\u663e\u5f0f\u8ba1\u7b97\u7535\u7f06-\u9aa8\u67b6\u63a5\u89e6\u529b", "result": "\u6846\u67b6\u652f\u6301\u529b\u548c\u4f4d\u79fb\u4e24\u79cd\u9a71\u52a8\u6a21\u5f0f\uff0c\u901a\u8fc7Galerkin\u65f6\u7a7a\u6a21\u6001\u79bb\u6563\u5316\uff0c\u8ba1\u7b97\u901f\u5ea6\u6bd4\u73b0\u6709\u5b9e\u65f6\u52a8\u6001\u5efa\u6a21\u65b9\u6cd5\u5e73\u5747\u63d0\u534762.3%", "conclusion": "LASEM\u6846\u67b6\u4e3a\u7535\u7f06\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u3001\u51c6\u786e\u3001\u9ad8\u6548\u7684\u52a8\u6001\u5efa\u6a21\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387"}}
{"id": "2512.13298", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13298", "abs": "https://arxiv.org/abs/2512.13298", "authors": ["Anna Aksenova", "Boris Zverkov", "Nicola Dainese", "Alexander Nikitin", "Pekka Marttinen"], "title": "MiniLingua: A Small Open-Source LLM for European Languages", "comment": "9+6 pages, 6 figures and 3 tables in the main text. Code at https://github.com/MiniLingua-ai/training_artifacts", "summary": "Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.", "AI": {"tldr": "MiniLingua\u662f\u4e00\u4e2a10\u4ebf\u53c2\u6570\u7684\u591a\u8bed\u8a00\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u4e3a13\u79cd\u6b27\u6d32\u8bed\u8a00\u8bbe\u8ba1\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u66f4\u5927\u8bad\u7ec3\u9884\u7b97\u7684EuroLLM\uff0c\u5e76\u4e0e\u5148\u8fdb\u6a21\u578b\u5728\u5f00\u653e\u751f\u6210\u4efb\u52a1\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u9690\u79c1\u95ee\u9898\u548c\u82f1\u8bed\u4e2d\u5fc3\u5316\u8bad\u7ec3\u7684\u9650\u5236\u3002\u5c0f\u578b\u9ad8\u6548\u6a21\u578b\uff08\u7ea610\u4ebf\u53c2\u6570\uff09\u5df2\u663e\u793a\u51fa\u5f3a\u5927\u6027\u80fd\u5e76\u652f\u6301\u8bbe\u5907\u7aef\u4f7f\u7528\uff0c\u4f46\u9700\u8981\u4e13\u95e8\u7684\u591a\u8bed\u8a00\u6a21\u578b\u6765\u5e73\u8861\u8bed\u8a00\u8986\u76d6\u8303\u56f4\u548c\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002", "method": "\u4ece\u96f6\u5f00\u59cb\u8bad\u7ec310\u4ebf\u53c2\u6570\u7684\u591a\u8bed\u8a00\u5f00\u6e90LLM\uff0c\u8986\u76d613\u79cd\u6b27\u6d32\u8bed\u8a00\u3002\u91c7\u7528\u6307\u4ee4\u8c03\u4f18\u7248\u672c\uff0c\u5e76\u53d1\u5e03\u4e86\u6a21\u578b\u6743\u91cd\u3001\u5206\u8bcd\u5668\u4ee5\u53ca\u6570\u636e\u5904\u7406\u548c\u6a21\u578b\u8bad\u7ec3\u7684\u6e90\u4ee3\u7801\u3002", "result": "\u6307\u4ee4\u8c03\u4f18\u7248\u7684MiniLingua\u5728\u6458\u8981\u3001\u5206\u7c7b\u3001\u5f00\u653e\u548c\u95ed\u5377\u95ee\u7b54\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u8bad\u7ec3\u9884\u7b97\u66f4\u5927\u7684EuroLLM\u3002\u5728\u5f00\u653e\u751f\u6210\u4efb\u52a1\u4e0a\u4e0e\u66f4\u5148\u8fdb\u7684SOTA\u6a21\u578b\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "MiniLingua\u8bc1\u660e\u4e86\u5c0f\u578b\u9ad8\u6548\u591a\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u3001\u9690\u79c1\u654f\u611f\u548c\u591a\u8bed\u8a00\u9700\u6c42\u7684\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4e86\u5b8c\u6574\u5de5\u5177\u94fe\u3002"}}
{"id": "2512.13293", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13293", "abs": "https://arxiv.org/abs/2512.13293", "authors": ["Hao Fua", "Wei Liu", "Shuai Zhoua"], "title": "Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration", "comment": null, "summary": "This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5185\u5728\u52a8\u673a\u63a2\u7d22\u7684\u591a\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u793e\u4ea4\u7f16\u961f\u5bfc\u822a\u4e2d\u673a\u5668\u4eba\u534f\u8c03\u63a2\u7d22\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u793e\u4ea4\u7f16\u961f\u5bfc\u822a\u5bf9\u4e8e\u5b9e\u73b0\u4eba\u673a\u5171\u5b58\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u884c\u4eba\u884c\u4e3a\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u548c\u975e\u5408\u4f5c\u6027\u7ed9\u673a\u5668\u4eba\u534f\u8c03\u63a2\u7d22\u5e26\u6765\u6311\u6218\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u63a2\u7d22\u6548\u7387\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51fa\u534f\u8c03\u63a2\u7d22\u591a\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u6838\u5fc3\u662f\u81ea\u5b66\u4e60\u5185\u5728\u5956\u52b1\u673a\u5236\u6765\u7f13\u89e3\u7b56\u7565\u4fdd\u5b88\u6027\uff1b\u91c7\u7528\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\u6846\u67b6\u4e0b\u7684\u53cc\u91c7\u6837\u6a21\u5f0f\uff0c\u901a\u8fc7\u4e24\u65f6\u95f4\u5c3a\u5ea6\u66f4\u65b0\u89c4\u5219\u89e3\u8026\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5728\u793e\u4ea4\u7f16\u961f\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u7b97\u6cd5\u5728\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u5185\u5728\u52a8\u673a\u63a2\u7d22\u7684\u534f\u8c03\u591a\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u793e\u4ea4\u7f16\u961f\u5bfc\u822a\u6027\u80fd\uff0c\u4e3a\u4eba\u673a\u5171\u5b58\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13330", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13330", "abs": "https://arxiv.org/abs/2512.13330", "authors": ["Joona Kyt\u00f6niemi", "Jousia Piha", "Akseli Reunamo", "Fedor Vitiugin", "Farrokh Mehryary", "Sampo Pyysalo"], "title": "FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models", "comment": null, "summary": "We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.", "AI": {"tldr": "FIN-bench-v2\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u82ac\u5170\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\u5957\u4ef6\uff0c\u6574\u5408\u4e86\u591a\u4e2a\u82ac\u5170\u8bed\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d6\u9605\u8bfb\u7406\u89e3\u3001\u5e38\u8bc6\u63a8\u7406\u3001\u60c5\u611f\u5206\u6790\u3001\u4e16\u754c\u77e5\u8bc6\u548c\u5bf9\u9f50\u7b49\u591a\u4e2a\u4efb\u52a1\u7c7b\u578b\u3002", "motivation": "\u9700\u8981\u4e3a\u82ac\u5170\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u6574\u5408\u73b0\u6709\u7684\u82ac\u5170\u8bed\u57fa\u51c6\u6d4b\u8bd5\u8d44\u6e90\uff0c\u5e76\u786e\u4fdd\u6570\u636e\u8d28\u91cf\u548c\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "method": "1) \u6574\u5408\u82ac\u5170\u8bed\u7248\u672c\u7684\u5e7f\u6cdb\u4f7f\u7528\u57fa\u51c6\u6d4b\u8bd5\u548c\u539f\u59cbFIN-bench\u7684\u66f4\u65b0\u6269\u5c55\u7248\u672c\uff1b2) \u5c06\u6240\u6709\u6570\u636e\u96c6\u8f6c\u6362\u4e3aHuggingFace\u683c\u5f0f\uff0c\u5305\u542b\u5b8c\u5f62\u586b\u7a7a\u548c\u591a\u9879\u9009\u62e9\u9898\u63d0\u793a\u7684\u4e94\u4e2a\u53d8\u4f53\uff1b3) \u5bf9\u673a\u5668\u7ffb\u8bd1\u8d44\u6e90\u8fdb\u884c\u4eba\u5de5\u6807\u6ce8\u6216\u5ba1\u67e5\uff1b4) \u4f7f\u75282.15B\u53c2\u6570\u7684\u81ea\u56de\u5f52\u6a21\u578b\u901a\u8fc7\u5b66\u4e60\u66f2\u7ebf\u5206\u6790\u4efb\u52a1\u8d28\u91cf\uff0c\u4fdd\u7559\u6ee1\u8db3\u5355\u8c03\u6027\u3001\u4fe1\u566a\u6bd4\u3001\u975e\u968f\u673a\u6027\u80fd\u548c\u6a21\u578b\u6392\u5e8f\u4e00\u81f4\u6027\u7684\u4efb\u52a1\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u3001\u683c\u5f0f\u4e00\u81f4\u7684\u82ac\u5170\u8bed\u8bc4\u4f30\u57fa\u51c6\u5957\u4ef6\uff0c\u5305\u542b\u7ecf\u8fc7\u8d28\u91cf\u7b5b\u9009\u7684\u4efb\u52a1\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u4e2a\u5927\u578b\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u63d0\u793a\u683c\u5f0f\u4e0b\u7684\u6027\u80fd\u3002\u6240\u6709\u6570\u636e\u96c6\u3001\u63d0\u793a\u548c\u8bc4\u4f30\u914d\u7f6e\u90fd\u5df2\u516c\u5f00\u3002", "conclusion": "FIN-bench-v2\u4e3a\u82ac\u5170\u8bed\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u3001\u53ef\u9760\u7684\u57fa\u51c6\u5957\u4ef6\uff0c\u901a\u8fc7\u4e25\u683c\u7684\u8d28\u91cf\u7b5b\u9009\u786e\u4fdd\u4e86\u8bc4\u4f30\u7684\u6709\u6548\u6027\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u82ac\u5170\u8bedNLP\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.13304", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13304", "abs": "https://arxiv.org/abs/2512.13304", "authors": ["Sait Sovukluk", "Johannes Englsberger", "Christian Ott"], "title": "Humanoid Robot Running Through Random Stepping Stones and Jumping Over Obstacles: Step Adaptation Using Spring-Mass Trajectories", "comment": "Accepted for publication in Biomimetic Intelligence and Robotics. Supplemental video: https://youtu.be/HlAg2nbNct4", "summary": "This study proposes a step adaptation framework for running through spring-mass trajectories and deadbeat control gain libraries. It includes four main parts: (1) Automatic spring-mass trajectory library generation; (2) Deadbeat control gain library generation through an actively controlled template model that resembles the whole-body dynamics well; (3) Trajectory selection policy development for step adaptation; (4) Mapping spring-mass trajectories to a humanoid model through a whole-body control (WBC) framework also accounting for closed-kinematic chain systems, self collisions, and reactive limb swinging. We show the inclusiveness and the robustness of the proposed framework through various challenging and agile behaviors such as running through randomly generated stepping stones, jumping over random obstacles, performing slalom motions, changing the running direction suddenly with a random leg, and rejecting significant disturbances and uncertainties through the MuJoCo physics simulator. We also perform additional simulations under a comprehensive set of uncertainties and noise to better justify the proposed method's robustness to real-world challenges, including signal noise, imprecision, modeling errors, and delays. All the aforementioned behaviors are performed with a single library and the same set of WBC control parameters without additional tuning. The spring-mass and the deadbeat control gain library are automatically computed in 4.5 seconds in total for 315 different trajectories.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5f39\u7c27\u8d28\u91cf\u8f68\u8ff9\u548c\u6b7b\u533a\u63a7\u5236\u589e\u76ca\u5e93\u7684\u8dd1\u6b65\u6b65\u6001\u81ea\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u8eab\u4f53\u63a7\u5236\u5b9e\u73b0\u5404\u79cd\u654f\u6377\u884c\u4e3a", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5e94\u5bf9\u590d\u6742\u5730\u5f62\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u8dd1\u6b65\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u53cc\u8db3\u673a\u5668\u4eba\u80fd\u591f\u6267\u884c\u5404\u79cd\u654f\u6377\u884c\u4e3a\uff0c\u5982\u968f\u673a\u8e0f\u77f3\u3001\u8df3\u8dc3\u969c\u788d\u3001\u6025\u8f6c\u5f2f\u7b49", "method": "\u5305\u542b\u56db\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a1) \u81ea\u52a8\u751f\u6210\u5f39\u7c27\u8d28\u91cf\u8f68\u8ff9\u5e93\uff1b2) \u901a\u8fc7\u4e3b\u52a8\u63a7\u5236\u6a21\u677f\u6a21\u578b\u751f\u6210\u6b7b\u533a\u63a7\u5236\u589e\u76ca\u5e93\uff1b3) \u5f00\u53d1\u6b65\u6001\u81ea\u9002\u5e94\u8f68\u8ff9\u9009\u62e9\u7b56\u7565\uff1b4) \u901a\u8fc7\u5168\u8eab\u4f53\u63a7\u5236\u6846\u67b6\u5c06\u5f39\u7c27\u8d28\u91cf\u8f68\u8ff9\u6620\u5c04\u5230\u4eba\u5f62\u673a\u5668\u4eba\u6a21\u578b", "result": "\u5728MuJoCo\u7269\u7406\u6a21\u62df\u5668\u4e2d\u5c55\u793a\u4e86\u6846\u67b6\u7684\u5305\u5bb9\u6027\u548c\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u6267\u884c\u591a\u79cd\u6311\u6218\u6027\u884c\u4e3a\uff0c\u5305\u62ec\u968f\u673a\u8e0f\u77f3\u3001\u8df3\u8dc3\u969c\u788d\u3001\u6025\u8f6c\u5f2f\u7b49\uff0c\u6240\u6709\u884c\u4e3a\u4f7f\u7528\u5355\u4e00\u5e93\u548c\u76f8\u540c\u63a7\u5236\u53c2\u6570\uff0c\u65e0\u9700\u989d\u5916\u8c03\u4f18", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u6311\u6218\uff0c\u5305\u62ec\u4fe1\u53f7\u566a\u58f0\u3001\u4e0d\u7cbe\u786e\u6027\u3001\u5efa\u6a21\u8bef\u5dee\u548c\u5ef6\u8fdf\uff0c\u4e3a\u53cc\u8db3\u673a\u5668\u4eba\u8dd1\u6b65\u63a7\u5236\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.13363", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13363", "abs": "https://arxiv.org/abs/2512.13363", "authors": ["Shibani Sankpal"], "title": "Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers", "comment": "14 pages, 12 figures", "summary": "This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\"\u60c5\u7eea\u6f02\u79fb\"\u6982\u5ff5\uff0c\u5206\u6790\u5fc3\u7406\u5065\u5eb7\u76f8\u5173\u6587\u672c\u4e2d\u60c5\u7eea\u72b6\u6001\u7684\u53d8\u5316\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u68c0\u6d4b\u53e5\u5b50\u7ea7\u60c5\u7eea\u5e76\u8ba1\u7b97\u6f02\u79fb\u5206\u6570", "motivation": "\u4f20\u7edf\u60c5\u611f\u5206\u6790\u901a\u5e38\u5c06\u6574\u6761\u6d88\u606f\u5206\u7c7b\u4e3a\u79ef\u6781\u3001\u6d88\u6781\u6216\u4e2d\u6027\uff0c\u5ffd\u7565\u4e86\u6d88\u606f\u5185\u90e8\u60c5\u7eea\u7684\u5fae\u5999\u53d8\u5316\u3002\u5728\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\u4e2d\uff0c\u7406\u89e3\u60c5\u7eea\u5982\u4f55\u968f\u65f6\u95f4\u6f14\u53d8\u5bf9\u4e8e\u8bc6\u522b\u60c5\u7eea\u5347\u7ea7\u6216\u7f13\u89e3\u6a21\u5f0f\u81f3\u5173\u91cd\u8981", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684Transformer\u6a21\u578b\uff08DistilBERT\u548cRoBERTa\uff09\u68c0\u6d4b\u53e5\u5b50\u7ea7\u60c5\u7eea\uff0c\u7136\u540e\u8ba1\u7b97\u60c5\u7eea\u6f02\u79fb\u5206\u6570\u6765\u8861\u91cf\u5355\u4e2a\u6587\u672c\u4e2d\u60c5\u7eea\u72b6\u6001\u7684\u53d8\u5316", "result": "\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\u4e2d\u60c5\u7eea\u5347\u7ea7\u6216\u7f13\u89e3\u7684\u6a21\u5f0f\uff0c\u4e3a\u7406\u89e3\u60c5\u7eea\u52a8\u6001\u63d0\u4f9b\u4e86\u65b0\u7684\u6d1e\u5bdf", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u7406\u89e3\u6587\u672c\u5185\u5bb9\u4e2d\u7684\u60c5\u7eea\u52a8\u6001\uff0c\u7279\u522b\u662f\u5728\u5fc3\u7406\u5065\u5eb7\u76f8\u5173\u6d88\u606f\u4e2d\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u60c5\u7eea\u53d8\u5316\u6a21\u5f0f\u5e76\u652f\u6301\u66f4\u7cbe\u7ec6\u7684\u60c5\u611f\u5206\u6790"}}
{"id": "2512.13356", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13356", "abs": "https://arxiv.org/abs/2512.13356", "authors": ["Zeyad Gamal", "Youssef Mahran", "Ayman El-Badawy"], "title": "Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)", "comment": "This is the Author Accepted Manuscript version of a paper accepted for publication. The final published version is available via IEEE Xplore", "summary": "This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.", "AI": {"tldr": "\u4f7f\u7528TD3\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u63a7\u5236\u53cc\u65cb\u7ffc\u6c14\u52a8\u7cfb\u7edf\uff0c\u5b9e\u73b0\u89d2\u5ea6\u7a33\u5b9a\u548c\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u76f8\u6bd4\u4f20\u7edfPID\u5728\u6297\u5e72\u6270\u65b9\u9762\u8868\u73b0\u66f4\u597d", "motivation": "\u53cc\u65cb\u7ffc\u6c14\u52a8\u7cfb\u7edf(TRAS)\u5177\u6709\u590d\u6742\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7279\u6027\uff0c\u4f20\u7edf\u63a7\u5236\u7b97\u6cd5\u96be\u4ee5\u6709\u6548\u63a7\u5236\u3002\u8fd1\u5e74\u6765\u5f3a\u5316\u5b66\u4e60\u5728\u591a\u65cb\u7ffc\u63a7\u5236\u9886\u57df\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u56e0\u6b64\u63a2\u7d22RL\u5728TRAS\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u91c7\u7528Twin Delayed Deep Deterministic Policy Gradient (TD3)\u7b97\u6cd5\u8bad\u7ec3RL\u667a\u80fd\u4f53\uff0c\u8be5\u7b97\u6cd5\u9002\u7528\u4e8e\u8fde\u7eed\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u7684\u73af\u5883\uff0c\u4e14\u4e0d\u9700\u8981\u7cfb\u7edf\u6a21\u578b\u3002\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u5e76\u4e0e\u4f20\u7edfPID\u63a7\u5236\u5668\u8fdb\u884c\u5bf9\u6bd4\u6d4b\u8bd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660eRL\u63a7\u5236\u65b9\u6cd5\u6709\u6548\u3002\u5728\u98ce\u6270\u7b49\u5916\u90e8\u5e72\u6270\u6d4b\u8bd5\u4e2d\uff0cRL\u63a7\u5236\u5668\u76f8\u6bd4\u4f20\u7edfPID\u63a7\u5236\u5668\u8868\u73b0\u66f4\u4f18\u3002\u5b9e\u9a8c\u5ba4\u5b9e\u9645\u88c5\u7f6e\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u63a7\u5236\u5668\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u63a7\u5236\u590d\u6742\u7684\u53cc\u65cb\u7ffc\u6c14\u52a8\u7cfb\u7edf\uff0cTD3\u7b97\u6cd5\u5728\u8fde\u7eed\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u591a\u65cb\u7ffc\u7cfb\u7edf\u7684\u667a\u80fd\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13441", "categories": ["cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.13441", "abs": "https://arxiv.org/abs/2512.13441", "authors": ["Johan J. Bolhuis", "Andrea Moro", "Stephen Crain", "Sandiway Fong"], "title": "Large language models are not about language", "comment": null, "summary": "Large Language Models are useless for linguistics, as they are probabilistic models that require a vast amount of data to analyse externalized strings of words. In contrast, human language is underpinned by a mind-internal computational system that recursively generates hierarchical thought structures. The language system grows with minimal external input and can readily distinguish between real language and impossible languages.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba4\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u8bed\u8a00\u5b66\u65e0\u7528\uff0c\u56e0\u4e3a\u5b83\u4eec\u662f\u4f9d\u8d56\u5927\u91cf\u6570\u636e\u7684\u6982\u7387\u6a21\u578b\uff0c\u800c\u4eba\u7c7b\u8bed\u8a00\u57fa\u4e8e\u5185\u5728\u8ba1\u7b97\u7cfb\u7edf\uff0c\u80fd\u9012\u5f52\u751f\u6210\u5c42\u6b21\u5316\u601d\u7ef4\u7ed3\u6784\u3002", "motivation": "\u4f5c\u8005\u65e8\u5728\u6279\u5224\u5f53\u524d\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u8bed\u8a00\u5b66\u7814\u7a76\u7684\u8d8b\u52bf\uff0c\u5f3a\u8c03\u4eba\u7c7b\u8bed\u8a00\u80fd\u529b\u7684\u5185\u5728\u672c\u8d28\u4e0e\u7edf\u8ba1\u6a21\u578b\u7684\u6839\u672c\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5bf9\u6bd4\u5206\u6790\uff1a\u6bd4\u8f83\u5927\u8bed\u8a00\u6a21\u578b\uff08\u6570\u636e\u9a71\u52a8\u7684\u6982\u7387\u6a21\u578b\uff09\u4e0e\u4eba\u7c7b\u8bed\u8a00\u80fd\u529b\uff08\u5185\u5728\u8ba1\u7b97\u7cfb\u7edf\uff09\u7684\u672c\u8d28\u533a\u522b\u3002", "result": "\u8bba\u8bc1\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u8bed\u8a00\u7684\u6838\u5fc3\u7279\u5f81\u2014\u2014\u9012\u5f52\u751f\u6210\u5c42\u6b21\u7ed3\u6784\u7684\u80fd\u529b\uff0c\u4e14\u65e0\u6cd5\u50cf\u4eba\u7c7b\u90a3\u6837\u533a\u5206\u771f\u5b9e\u8bed\u8a00\u4e0e\u4e0d\u53ef\u80fd\u8bed\u8a00\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u4e0d\u9002\u5408\u7528\u4e8e\u8bed\u8a00\u5b66\u7814\u7a76\uff0c\u56e0\u4e3a\u5b83\u4eec\u65e0\u6cd5\u6a21\u62df\u4eba\u7c7b\u8bed\u8a00\u7684\u5185\u5728\u8ba1\u7b97\u672c\u8d28\uff0c\u800c\u53ea\u662f\u5bf9\u5916\u90e8\u5316\u8bcd\u4e32\u7684\u7edf\u8ba1\u5efa\u6a21\u3002"}}
{"id": "2512.13359", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13359", "abs": "https://arxiv.org/abs/2512.13359", "authors": ["S\u00fcmer Tun\u00e7ay", "Alain Andres", "Ignacio Carlucho"], "title": "Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles", "comment": null, "summary": "Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eJAX\u548cMJX\u7684GPU\u52a0\u901fRL\u8bad\u7ec3\u6846\u67b6\uff0c\u5b9e\u73b02\u5206\u949f\u5185\u5b8c\u6210AUV\u516d\u81ea\u7531\u5ea6\u4f4d\u7f6e\u63a7\u5236\u7b56\u7565\u8bad\u7ec3\uff0c\u5e76\u5728\u771f\u5b9e\u6c34\u4e0b\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\u6548\u679c", "motivation": "\u4f20\u7edfAUV\u63a7\u5236\u5668\u5728\u672a\u5efa\u6a21\u52a8\u6001\u548c\u73af\u5883\u6270\u52a8\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u4f20\u7edfRL\u8bad\u7ec3\u901f\u5ea6\u6162\u4e14\u4eff\u771f\u5230\u771f\u5b9e\u8fc1\u79fb\u56f0\u96be\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5", "method": "\u4f7f\u7528JAX\u548cMuJoCo-XLA\u6784\u5efaGPU\u52a0\u901f\u7684RL\u8bad\u7ec3\u7ba1\u9053\uff0c\u901a\u8fc7\u8054\u5408JIT\u7f16\u8bd1\u5927\u89c4\u6a21\u5e76\u884c\u7269\u7406\u4eff\u771f\u548c\u5b66\u4e60\u66f4\u65b0\uff0c\u5b9e\u73b0\u5feb\u901f\u8bad\u7ec3", "result": "\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u81f32\u5206\u949f\u4ee5\u5185\uff0c\u5728\u771f\u5b9e\u6c34\u4e0b\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u516d\u81ea\u7531\u5ea6\u8f68\u8ff9\u8ddf\u8e2a\u548c\u6270\u52a8\u6291\u5236\uff0c\u7b56\u7565\u4ece\u4eff\u771f\u96f6\u6837\u672c\u8fc1\u79fb\u6210\u529f", "conclusion": "\u9996\u6b21\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5c55\u793a\u4e86\u57fa\u4e8eRL\u7684AUV\u516d\u81ea\u7531\u5ea6\u4f4d\u7f6e\u63a7\u5236\uff0c\u8bc1\u660e\u4e86\u5feb\u901f\u8bad\u7ec3\u548c\u96f6\u6837\u672c\u8fc1\u79fb\u7684\u53ef\u884c\u6027"}}
{"id": "2512.13472", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13472", "abs": "https://arxiv.org/abs/2512.13472", "authors": ["Jian Yang", "Shawn Guo", "Lin Jing", "Wei Zhang", "Aishan Liu", "Chuan Hao", "Zhoujun Li", "Wayne Xin Zhao", "Xianglong Liu", "Weifeng Lv", "Bryan Dai"], "title": "Scaling Laws for Code: Every Programming Language Matters", "comment": null, "summary": "Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u63a2\u7d22\u4e86\u591a\u8bed\u8a00\u4ee3\u7801\u9884\u8bad\u7ec3\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u901a\u8fc71000+\u5b9e\u9a8c\u53d1\u73b0\u89e3\u91ca\u578b\u8bed\u8a00\uff08\u5982Python\uff09\u6bd4\u7f16\u8bd1\u578b\u8bed\u8a00\uff08\u5982Rust\uff09\u66f4\u80fd\u4ece\u6a21\u578b\u89c4\u6a21\u548c\u6570\u636e\u589e\u957f\u4e2d\u53d7\u76ca\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6bd4\u4f8b\u7684\u591a\u8bed\u8a00\u7f29\u653e\u5b9a\u5f8b\u6765\u4f18\u5316\u8bad\u7ec3token\u5206\u914d\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u5927\u8bed\u8a00\u6a21\u578b\uff08Code LLMs\uff09\u7684\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u73b0\u6709\u7f29\u653e\u5b9a\u5f8b\u672a\u80fd\u8003\u8651\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u5728\u9884\u8bad\u7ec3\u4e2d\u7684\u5dee\u5f02\u5f71\u54cd\uff0c\u5bfc\u81f4\u6027\u80fd\u9884\u6d4b\u4e0d\u51c6\u786e\u3002\u540c\u65f6\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u8bed\u8a00\u65e0\u5173\u8bbe\u7f6e\uff0c\u5ffd\u89c6\u4e86\u73b0\u4ee3\u8f6f\u4ef6\u5f00\u53d1\u7684\u591a\u8bed\u8a00\u672c\u8d28\u3002", "method": "\u8fdb\u884c\u4e86\u8d85\u8fc71000\u4e2a\u5b9e\u9a8c\uff08\u76f8\u5f53\u4e8e336,000+ H800 GPU\u5c0f\u65f6\uff09\uff0c\u6db5\u76d6\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u3001\u6a21\u578b\u89c4\u6a21\uff080.2B\u523014B\u53c2\u6570\uff09\u548c\u6570\u636e\u96c6\u89c4\u6a21\uff081T tokens\uff09\u3002\u5efa\u7acb\u4e86\u5168\u9762\u7684\u591a\u8bed\u8a00\u4ee3\u7801LLM\u7f29\u653e\u5b9a\u5f8b\uff0c\u5e76\u63d0\u51fa\u4e86\u5e76\u884c\u914d\u5bf9\u9884\u8bad\u7ec3\u7b56\u7565\uff08\u5c06\u4ee3\u7801\u7247\u6bb5\u4e0e\u5176\u7ffb\u8bd1\u7248\u672c\u62fc\u63a5\uff09\u3002", "result": "\u53d1\u73b0\u89e3\u91ca\u578b\u8bed\u8a00\uff08\u5982Python\uff09\u6bd4\u7f16\u8bd1\u578b\u8bed\u8a00\uff08\u5982Rust\uff09\u66f4\u80fd\u4ece\u6a21\u578b\u89c4\u6a21\u548c\u6570\u636e\u589e\u957f\u4e2d\u53d7\u76ca\uff1b\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u5177\u6709\u534f\u540c\u6548\u5e94\uff0c\u7279\u522b\u662f\u5728\u8bed\u6cd5\u76f8\u4f3c\u7684\u7f16\u7a0b\u8bed\u8a00\u4e4b\u95f4\uff1b\u5e76\u884c\u914d\u5bf9\u7b56\u7565\u663e\u8457\u589e\u5f3a\u4e86\u8de8\u8bed\u8a00\u80fd\u529b\uff1b\u63d0\u51fa\u7684\u6bd4\u4f8b\u4f9d\u8d56\u591a\u8bed\u8a00\u7f29\u653e\u5b9a\u5f8b\u5728\u76f8\u540c\u8ba1\u7b97\u9884\u7b97\u4e0b\u6bd4\u5747\u5300\u5206\u914d\u83b7\u5f97\u66f4\u4f18\u7684\u5e73\u5747\u6027\u80fd\u3002", "conclusion": "\u9996\u6b21\u7cfb\u7edf\u5efa\u7acb\u4e86\u591a\u8bed\u8a00\u4ee3\u7801\u9884\u8bad\u7ec3\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u7684\u8bad\u7ec3\u7279\u6027\u5dee\u5f02\uff0c\u63d0\u51fa\u7684\u4f18\u5316token\u5206\u914d\u7b56\u7565\u80fd\u591f\u6709\u6548\u63d0\u5347\u591a\u8bed\u8a00\u4ee3\u7801LLM\u7684\u6574\u4f53\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u8bad\u7ec3\u591a\u8bed\u8a00\u4ee3\u7801\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2512.13380", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13380", "abs": "https://arxiv.org/abs/2512.13380", "authors": ["Chuan Mao", "Haoqi Yuan", "Ziye Huang", "Chaoyi Xu", "Kai Ma", "Zongqing Lu"], "title": "Universal Dexterous Functional Grasping via Demonstration-Editing Reinforcement Learning", "comment": "19 pages", "summary": "Reinforcement learning (RL) has achieved great success in dexterous grasping, significantly improving grasp performance and generalization from simulation to the real world. However, fine-grained functional grasping, which is essential for downstream manipulation tasks, remains underexplored and faces several challenges: the complexity of specifying goals and reward functions for functional grasps across diverse objects, the difficulty of multi-task RL exploration, and the challenge of sim-to-real transfer. In this work, we propose DemoFunGrasp for universal dexterous functional grasping. We factorize functional grasping conditions into two complementary components - grasping style and affordance - and integrate them into an RL framework that can learn to grasp any object with any functional grasping condition. To address the multi-task optimization challenge, we leverage a single grasping demonstration and reformulate the RL problem as one-step demonstration editing, substantially enhancing sample efficiency and performance. Experimental results in both simulation and the real world show that DemoFunGrasp generalizes to unseen combinations of objects, affordances, and grasping styles, outperforming baselines in both success rate and functional grasping accuracy. In addition to strong sim-to-real capability, by incorporating a vision-language model (VLM) for planning, our system achieves autonomous instruction-following grasp execution.", "AI": {"tldr": "DemoFunGrasp\uff1a\u901a\u8fc7\u6f14\u793a\u7f16\u8f91\u548c\u6761\u4ef6\u5206\u89e3\u5b9e\u73b0\u901a\u7528\u7075\u5de7\u529f\u80fd\u6293\u53d6\uff0c\u5c06\u529f\u80fd\u6293\u53d6\u6761\u4ef6\u5206\u89e3\u4e3a\u6293\u53d6\u98ce\u683c\u548c\u53ef\u4f9b\u6027\uff0c\u5229\u7528\u5355\u6b21\u6f14\u793a\u8fdb\u884c\u4e00\u6b65\u6f14\u793a\u7f16\u8f91\uff0c\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u5b9e\u73b0\u672a\u89c1\u7269\u4f53\u3001\u53ef\u4f9b\u6027\u548c\u6293\u53d6\u98ce\u683c\u7684\u6cdb\u5316\u3002", "motivation": "\u5c3d\u7ba1\u5f3a\u5316\u5b66\u4e60\u5728\u7075\u5de7\u6293\u53d6\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u4f46\u7cbe\u7ec6\u5316\u7684\u529f\u80fd\u6293\u53d6\uff08\u5bf9\u4e0b\u6e38\u64cd\u4f5c\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff09\u4ecd\u9762\u4e34\u6311\u6218\uff1a\u4e3a\u4e0d\u540c\u7269\u4f53\u7684\u529f\u80fd\u6293\u53d6\u6307\u5b9a\u76ee\u6807\u548c\u5956\u52b1\u51fd\u6570\u590d\u6742\uff0c\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\u56f0\u96be\uff0c\u4ee5\u53ca\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u7684\u6311\u6218\u3002", "method": "\u5c06\u529f\u80fd\u6293\u53d6\u6761\u4ef6\u5206\u89e3\u4e3a\u6293\u53d6\u98ce\u683c\u548c\u53ef\u4f9b\u6027\u4e24\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff0c\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\uff1b\u5229\u7528\u5355\u6b21\u6293\u53d6\u6f14\u793a\uff0c\u5c06\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e00\u6b65\u6f14\u793a\u7f16\u8f91\uff0c\u5927\u5e45\u63d0\u9ad8\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\uff1b\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u89c4\u5212\u3002", "result": "\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\uff0cDemoFunGrasp\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u3001\u53ef\u4f9b\u6027\u548c\u6293\u53d6\u98ce\u683c\u7ec4\u5408\uff0c\u5728\u6210\u529f\u7387\u548c\u529f\u80fd\u6293\u53d6\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff1b\u5177\u5907\u5f3a\u5927\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u80fd\u529b\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u81ea\u4e3b\u6307\u4ee4\u8ddf\u968f\u6293\u53d6\u6267\u884c\u3002", "conclusion": "DemoFunGrasp\u901a\u8fc7\u6761\u4ef6\u5206\u89e3\u548c\u6f14\u793a\u7f16\u8f91\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u901a\u7528\u7075\u5de7\u529f\u80fd\u6293\u53d6\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u6837\u5316\u529f\u80fd\u6293\u53d6\u6761\u4ef6\u7684\u6cdb\u5316\uff0c\u4e3a\u4e0b\u6e38\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u57fa\u7840\u3002"}}
{"id": "2512.13478", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13478", "abs": "https://arxiv.org/abs/2512.13478", "authors": ["Kei Saito"], "title": "Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models", "comment": "19 pages", "summary": "Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing \"Dr. Smith the cardiologist\" from \"Dr. Smith the researcher\"). These mechanisms are unified by an external Resolution Operator $\u03c1$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u975e\u6d88\u89e3\u63a8\u7406(NRR)\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5411\u91cf\u5d4c\u5165\u3001\u975e\u574d\u7f29\u6ce8\u610f\u529b\u548c\u4e0a\u4e0b\u6587\u8eab\u4efd\u8ffd\u8e2a\u6765\u9632\u6b62\u8bed\u8a00\u6a21\u578b\u8fc7\u65e9\u8bed\u4e49\u574d\u7f29\uff0c\u5c06\u6b67\u4e49\u4fdd\u7559\u4e3a\u663e\u5f0f\u8868\u793a\u72b6\u6001\u800c\u975e\u9519\u8bef\u6a21\u5f0f\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8fc7\u65e9\u8bed\u4e49\u574d\u7f29\u7684\u6838\u5fc3\u67b6\u6784\u9650\u5236\u2014\u2014\u5728\u4e0a\u4e0b\u6587\u4e0d\u8db3\u65f6\u8fc7\u65e9\u627f\u8bfa\u5355\u4e00\u542b\u4e49\uff0c\u5bfc\u81f4\u63a8\u7406\u8106\u5f31\u548c\u4e0a\u4e0b\u6587\u5931\u8d25\u3002Softmax\u9a71\u52a8\u7684\u7ade\u4e89\u548c\u8d2a\u5a6a\u89e3\u7801\u4f7f\u6a21\u578b\u5728\u83b7\u5f97\u8db3\u591f\u4e0a\u4e0b\u6587\u524d\u4e22\u5f03\u6709\u6548\u89e3\u91ca\u3002", "method": "\u5f15\u5165\u975e\u6d88\u89e3\u63a8\u7406(NRR)\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a1) \u591a\u5411\u91cf\u5d4c\u5165\uff1a\u4e3a\u6bcf\u4e2atoken\u7ef4\u62a4\u591a\u4e2a\u53ef\u884c\u89e3\u91ca\uff1b2) \u975e\u574d\u7f29\u6ce8\u610f\u529b\uff1a\u9632\u6b62\u8de8\u5c42\u7684\u8d62\u5bb6\u901a\u5403\u52a8\u6001\uff1b3) \u4e0a\u4e0b\u6587\u8eab\u4efd\u8ffd\u8e2a(CIT)\uff1a\u4e3a\u91cd\u590d\u5b9e\u4f53\u5206\u914d\u4e0a\u4e0b\u6587\u7279\u5b9a\u8eab\u4efd\u3002\u8fd9\u4e9b\u673a\u5236\u901a\u8fc7\u5916\u90e8\u6d88\u89e3\u7b97\u5b50\u03c1\u7edf\u4e00\uff0c\u4f7f\u8bed\u4e49\u627f\u8bfa\u53d8\u5f97\u663e\u5f0f\u3001\u53ef\u63a7\u4e14\u4efb\u52a1\u4f9d\u8d56\u3002", "result": "\u5408\u6210\u8bc4\u4f30\u663e\u793aNRR\u80fd\u6709\u6548\u4fdd\u7559\u6b67\u4e49\u548c\u8ffd\u8e2a\u4e0a\u4e0b\u6587\uff1aCIT\u589e\u5f3a\u6a21\u578b\u5728\u5206\u5e03\u5916\u8eab\u4efd\u8f6c\u79fb\u4efb\u52a1\u4e0a\u8fbe\u523090.9%\u51c6\u786e\u7387\uff0c\u800cTransformer\u57fa\u7ebf\u4ec5\u4e3a9.1%\u3002NRR\u5c06\u8868\u793a\u4e0e\u6d88\u89e3\u5206\u79bb\uff0c\u5141\u8bb8\u5355\u4e2a\u6a21\u578b\u5728\u4e0d\u540c\u63a8\u7406\u6a21\u5f0f\u95f4\u5207\u6362\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "conclusion": "NRR\u4e3a\u8fc7\u65e9\u574d\u7f29\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u66ff\u4ee3\u65b9\u6848\uff0c\u5c06\u6b67\u4e49\u91cd\u6784\u4e3a\u663e\u5f0f\u8868\u793a\u72b6\u6001\u800c\u975e\u5931\u8d25\u6a21\u5f0f\u3002\u5173\u952e\u95ee\u9898\u4e0d\u662fAI\u662f\u5426\u5e94\u8be5\u6d88\u89e3\u6b67\u4e49\uff0c\u800c\u662f\u4f55\u65f6\u3001\u5982\u4f55\u4ee5\u53ca\u5728\u8c01\u7684\u63a7\u5236\u4e0b\u8fdb\u884c\u6d88\u89e3\u3002"}}
{"id": "2512.13477", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.13477", "abs": "https://arxiv.org/abs/2512.13477", "authors": ["Timothy A. Brumfiel", "Revanth Konda", "Drew Elliott", "Jaydev P. Desai"], "title": "Evaluating the Navigation Capabilities of a Modified COAST Guidewire Robot in an Anatomical Phantom Model", "comment": "Presented at the 14th Conference on New Technologies for Computer and Robot Assisted Surgery (CRAS 2025)", "summary": "To address the issues that arise due to the manual navigation of guidewires in endovascular interventions, research in medical robotics has taken a strong interest in developing robotically steerable guidewires, which offer the possibility of enhanced maneuverability and navigation, as the tip of the guidewire can be actively steered. The COaxially Aligned STeerable (COAST) guidewire robot has the ability to generate a wide variety of motions including bending motion with different bending lengths, follow-the-leader motion, and feedforward motion. In our past studies, we have explored different designs of the COAST guidewire robot and developed modeling, control, and sensing strategies for the COAST guidewire robot. In this study, the performance of a modified COAST guidewire robot is evaluated by conducting navigation experiments in an anatomical phantom model with pulsatile flow. The modified COAST guidewire robot is a simplified version of the COAST guidewire robot and consists of two tubes as opposed to three tubes. Through this study, we demonstrate the effectiveness of the modified COAST guidewire robot in navigating the tortuous phantom vasculature.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u7b80\u5316\u7248COAST\u5bfc\u4e1d\u673a\u5668\u4eba\uff08\u4ece\u4e09\u7ba1\u7ed3\u6784\u6539\u4e3a\u4e24\u7ba1\uff09\u5728\u8109\u52a8\u8840\u6d41\u89e3\u5256\u6a21\u578b\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u8840\u7ba1\u7ed3\u6784\u4e2d\u5bfc\u822a\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8840\u7ba1\u5185\u4ecb\u5165\u624b\u672f\u4e2d\u624b\u52a8\u64cd\u4f5c\u5bfc\u4e1d\u7684\u95ee\u9898\uff0c\u533b\u7597\u673a\u5668\u4eba\u7814\u7a76\u81f4\u529b\u4e8e\u5f00\u53d1\u673a\u5668\u4eba\u53ef\u64cd\u7eb5\u5bfc\u4e1d\uff0c\u4ee5\u589e\u5f3a\u5bfc\u4e1d\u7684\u53ef\u64cd\u4f5c\u6027\u548c\u5bfc\u822a\u80fd\u529b\u3002COAST\u5bfc\u4e1d\u673a\u5668\u4eba\u80fd\u591f\u4ea7\u751f\u591a\u79cd\u8fd0\u52a8\u6a21\u5f0f\uff0c\u4f46\u9700\u8981\u7b80\u5316\u8bbe\u8ba1\u4ee5\u8bc4\u4f30\u5176\u5b9e\u9645\u6027\u80fd\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u7b80\u5316\u7248\u7684COAST\u5bfc\u4e1d\u673a\u5668\u4eba\uff08\u4ece\u4e09\u7ba1\u7ed3\u6784\u7b80\u5316\u4e3a\u4e24\u7ba1\u7ed3\u6784\uff09\uff0c\u5728\u5177\u6709\u8109\u52a8\u8840\u6d41\u7684\u89e3\u5256\u6a21\u578b\u4e2d\u8fdb\u884c\u5bfc\u822a\u5b9e\u9a8c\uff0c\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u7b80\u5316\u7248COAST\u5bfc\u4e1d\u673a\u5668\u4eba\u80fd\u591f\u6709\u6548\u5bfc\u822a\u590d\u6742\u7684\u6a21\u578b\u8840\u7ba1\u7ed3\u6784\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u7b80\u5316\u7248COAST\u5bfc\u4e1d\u673a\u5668\u4eba\u5728\u8109\u52a8\u8840\u6d41\u89e3\u5256\u6a21\u578b\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u4e3a\u8840\u7ba1\u5185\u4ecb\u5165\u624b\u672f\u7684\u673a\u5668\u4eba\u8f85\u52a9\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13487", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13487", "abs": "https://arxiv.org/abs/2512.13487", "authors": ["Ayon Roy", "Risat Rahaman", "Sadat Shibly", "Udoy Saha Joy", "Abdulla Al Kafi", "Farig Yousuf Sadeque"], "title": "Advancing Bangla Machine Translation Through Informal Datasets", "comment": "33 pages, 13 figures", "summary": "Bangla is the sixth most widely spoken language globally, with approximately 234 million native speakers. However, progress in open-source Bangla machine translation remains limited. Most online resources are in English and often remain untranslated into Bangla, excluding millions from accessing essential information. Existing research in Bangla translation primarily focuses on formal language, neglecting the more commonly used informal language. This is largely due to the lack of pairwise Bangla-English data and advanced translation models. If datasets and models can be enhanced to better handle natural, informal Bangla, millions of people will benefit from improved online information access. In this research, we explore current state-of-the-art models and propose improvements to Bangla translation by developing a dataset from informal sources like social media and conversational texts. This work aims to advance Bangla machine translation by focusing on informal language translation and improving accessibility for Bangla speakers in the digital world.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u673a\u5668\u7ffb\u8bd1\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u975e\u6b63\u5f0f\u8bed\u8a00\u7684\u7ffb\u8bd1\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u793e\u4ea4\u5a92\u4f53\u548c\u5bf9\u8bdd\u6587\u672c\u6570\u636e\u96c6\u6765\u6539\u8fdb\u5b5f\u52a0\u62c9\u8bed\u7ffb\u8bd1\u6a21\u578b\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u4f5c\u4e3a\u5168\u7403\u7b2c\u516d\u5927\u8bed\u8a00\uff0c\u62e5\u6709\u7ea62.34\u4ebf\u6bcd\u8bed\u8005\uff0c\u4f46\u5f00\u6e90\u673a\u5668\u7ffb\u8bd1\u8fdb\u5c55\u6709\u9650\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6b63\u5f0f\u8bed\u8a00\uff0c\u5ffd\u7565\u4e86\u66f4\u5e38\u7528\u7684\u975e\u6b63\u5f0f\u8bed\u8a00\uff0c\u5bfc\u81f4\u6570\u767e\u4e07\u5b5f\u52a0\u62c9\u8bed\u4f7f\u7528\u8005\u65e0\u6cd5\u5145\u5206\u83b7\u53d6\u5728\u7ebf\u4fe1\u606f\u3002", "method": "\u7814\u7a76\u63a2\u7d22\u5f53\u524d\u6700\u5148\u8fdb\u7684\u7ffb\u8bd1\u6a21\u578b\uff0c\u5e76\u4ece\u793e\u4ea4\u5a92\u4f53\u548c\u5bf9\u8bdd\u6587\u672c\u7b49\u975e\u6b63\u5f0f\u6765\u6e90\u5f00\u53d1\u5b5f\u52a0\u62c9\u8bed-\u82f1\u8bed\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u4ee5\u6539\u8fdb\u5bf9\u81ea\u7136\u3001\u975e\u6b63\u5f0f\u5b5f\u52a0\u62c9\u8bed\u7684\u7ffb\u8bd1\u80fd\u529b\u3002", "result": "\u8bba\u6587\u672a\u63d0\u4f9b\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u7814\u7a76\u76ee\u6807\u662f\u5f00\u53d1\u6539\u8fdb\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u975e\u6b63\u5f0f\u5b5f\u52a0\u62c9\u8bed\u7ffb\u8bd1\u3002", "conclusion": "\u901a\u8fc7\u4e13\u6ce8\u4e8e\u975e\u6b63\u5f0f\u8bed\u8a00\u7ffb\u8bd1\u5e76\u6539\u8fdb\u6570\u636e\u96c6\uff0c\u8fd9\u9879\u7814\u7a76\u65e8\u5728\u63a8\u8fdb\u5b5f\u52a0\u62c9\u8bed\u673a\u5668\u7ffb\u8bd1\uff0c\u63d0\u9ad8\u5b5f\u52a0\u62c9\u8bed\u4f7f\u7528\u8005\u5728\u6570\u5b57\u4e16\u754c\u4e2d\u7684\u4fe1\u606f\u53ef\u53ca\u6027\u3002"}}
{"id": "2512.13514", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13514", "abs": "https://arxiv.org/abs/2512.13514", "authors": ["Aman Arora", "Matteo El-Hariry", "Miguel Olivares-Mendez"], "title": "Reinforcement Learning based 6-DoF Maneuvers for Microgravity Intravehicular Docking: A Simulation Study with Int-Ball2 in ISS-JEM", "comment": "Presented at AI4OPA Workshop at the International Conference on Space Robotics (iSpaRo) 2025 at Sendai, Japan", "summary": "Autonomous free-flyers play a critical role in intravehicular tasks aboard the International Space Station (ISS), where their precise docking under sensing noise, small actuation mismatches, and environmental variability remains a nontrivial challenge. This work presents a reinforcement learning (RL) framework for six-degree-of-freedom (6-DoF) docking of JAXA's Int-Ball2 robot inside a high-fidelity Isaac Sim model of the Japanese Experiment Module (JEM). Using Proximal Policy Optimization (PPO), we train and evaluate controllers under domain-randomized dynamics and bounded observation noise, while explicitly modeling propeller drag-torque effects and polarity structure. This enables a controlled study of how Int-Ball2's propulsion physics influence RL-based docking performance in constrained microgravity interiors. The learned policy achieves stable and reliable docking across varied conditions and lays the groundwork for future extensions pertaining to Int-Ball2 in collision-aware navigation, safe RL, propulsion-accurate sim-to-real transfer, and vision-based end-to-end docking.", "AI": {"tldr": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5728JAXA Int-Ball2\u673a\u5668\u4eba\u4e0a\u5b9e\u73b06\u81ea\u7531\u5ea6\u7cbe\u786e\u5bf9\u63a5\uff0c\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u63a7\u5236\u5668\uff0c\u7814\u7a76\u63a8\u8fdb\u7269\u7406\u5bf9\u5fae\u91cd\u529b\u73af\u5883\u4e0b\u5bf9\u63a5\u6027\u80fd\u7684\u5f71\u54cd", "motivation": "\u56fd\u9645\u7a7a\u95f4\u7ad9\u5185\u81ea\u4e3b\u98de\u884c\u5668\u7684\u7cbe\u786e\u5bf9\u63a5\u9762\u4e34\u4f20\u611f\u566a\u58f0\u3001\u6267\u884c\u5668\u5fae\u5c0f\u4e0d\u5339\u914d\u548c\u73af\u5883\u53d8\u5316\u7b49\u6311\u6218\uff0c\u9700\u8981\u53ef\u9760\u7684\u63a7\u5236\u65b9\u6cd5", "method": "\u4f7f\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u9ad8\u4fdd\u771fIsaac Sim\u6a21\u62df\u7684\u65e5\u672c\u5b9e\u9a8c\u8231\u73af\u5883\u4e2d\u8bad\u7ec36\u81ea\u7531\u5ea6\u5bf9\u63a5\u63a7\u5236\u5668\uff0c\u91c7\u7528\u9886\u57df\u968f\u673a\u5316\u52a8\u529b\u5b66\u548c\u6709\u754c\u89c2\u6d4b\u566a\u58f0\uff0c\u660e\u786e\u5efa\u6a21\u87ba\u65cb\u6868\u963b\u529b\u626d\u77e9\u6548\u5e94\u548c\u6781\u6027\u7ed3\u6784", "result": "\u5b66\u4e60\u5230\u7684\u7b56\u7565\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u7a33\u5b9a\u53ef\u9760\u7684\u5bf9\u63a5\uff0c\u4e3a\u672a\u6765\u6269\u5c55\u5960\u5b9a\u4e86\u57fa\u7840", "conclusion": "\u8be5\u7814\u7a76\u4e3aInt-Ball2\u673a\u5668\u4eba\u5728\u78b0\u649e\u611f\u77e5\u5bfc\u822a\u3001\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u3001\u63a8\u8fdb\u7cbe\u786e\u7684\u6a21\u62df\u5230\u771f\u5b9e\u8f6c\u79fb\u4ee5\u53ca\u57fa\u4e8e\u89c6\u89c9\u7684\u7aef\u5230\u7aef\u5bf9\u63a5\u7b49\u672a\u6765\u6269\u5c55\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2512.13494", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13494", "abs": "https://arxiv.org/abs/2512.13494", "authors": ["Yu-Chen Lu", "Sheng-Feng Yu", "Hui-Hsien Weng", "Pei-Shuo Wang", "Yu-Fang Hu", "Liang Hung-Chun", "Hung-Yueh Chiang", "Kai-Chiang Wu"], "title": "SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping", "comment": "Accepted by AAAI 2026", "summary": "Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, na\u00efve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.", "AI": {"tldr": "SkipCat\u662f\u4e00\u79cd\u65b0\u9896\u7684\u4f4e\u79e9\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u5185\u5171\u4eab\u4f4e\u79e9\u6295\u5f71\u548c\u5757\u8df3\u8fc7\u6280\u672f\uff0c\u5728\u76f8\u540c\u538b\u7f29\u7387\u4e0b\u4fdd\u7559\u66f4\u591a\u6709\u6548\u79e9\uff0c\u663e\u8457\u63d0\u5347\u538b\u7f29\u6a21\u578b\u6027\u80fd", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u5e9e\u5927\uff0c\u96be\u4ee5\u90e8\u7f72\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002\u4f20\u7edf\u4f4e\u79e9\u538b\u7f29\u65b9\u6cd5\u9700\u8981\u5927\u5e45\u964d\u4f4e\u4fdd\u7559\u79e9\u624d\u80fd\u83b7\u5f97\u6548\u7387\u63d0\u5347\uff0c\u4f46\u8fd9\u6837\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u5728\u538b\u7f29\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u5bfb\u627e\u66f4\u597d\u7684\u5e73\u8861", "method": "\u63d0\u51faSkipCat\u6846\u67b6\uff1a1\uff09\u5c42\u5185\u5171\u4eab\u4f4e\u79e9\u6295\u5f71 - \u591a\u4e2a\u5171\u4eab\u76f8\u540c\u8f93\u5165\u7684\u77e9\u9635\u4f7f\u7528\u5171\u540c\u6295\u5f71\uff0c\u51cf\u5c11\u5197\u4f59\uff1b2\uff09\u5757\u8df3\u8fc7\u6280\u672f - \u5728\u4f4e\u79e9\u5206\u89e3\u4e2d\u7701\u7565\u9009\u5b9a\u5b50\u5757\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u4f20\u8f93\u3002\u8fd9\u4e24\u79cd\u6280\u672f\u5171\u540c\u4f7f\u538b\u7f29\u6a21\u578b\u5728\u76f8\u540c\u538b\u7f29\u9884\u7b97\u4e0b\u4fdd\u7559\u66f4\u591a\u6709\u6548\u79e9", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u76f8\u540c\u538b\u7f29\u7387\u4e0b\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\uff0c\u8be5\u65b9\u6cd5\u6bd4\u5148\u524d\u4f4e\u79e9\u538b\u7f29\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u63d0\u53477%\u3002\u8fd9\u8bc1\u660e\u4e86\u5728\u4e25\u683c\u8d44\u6e90\u7ea6\u675f\u4e0b\uff0c\u79e9\u6700\u5927\u5316\u538b\u7f29\u7b56\u7565\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027", "conclusion": "SkipCat\u901a\u8fc7\u521b\u65b0\u7684\u4f4e\u79e9\u538b\u7f29\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u76f8\u540c\u538b\u7f29\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u538b\u7f29\u6a21\u578b\u7684\u6027\u80fd\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.13561", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13561", "abs": "https://arxiv.org/abs/2512.13561", "authors": ["Li-Wei Shih", "Ruo-Syuan Mei", "Jesse Heidrich", "Hui-Ping Wang", "Joel Hooton", "Joshua Solomon", "Jorge Arinez", "Guangze Li", "Chenhui Shao"], "title": "Near-Field Perception for Safety Enhancement of Autonomous Mobile Robots in Manufacturing Environments", "comment": "Submitted to the 54th SME North American Manufacturing Research Conference (NAMRC 54)", "summary": "Near-field perception is essential for the safe operation of autonomous mobile robots (AMRs) in manufacturing environments. Conventional ranging sensors such as light detection and ranging (LiDAR) and ultrasonic devices provide broad situational awareness but often fail to detect small objects near the robot base. To address this limitation, this paper presents a three-tier near-field perception framework. The first approach employs light-discontinuity detection, which projects a laser stripe across the near-field zone and identifies interruptions in the stripe to perform fast, binary cutoff sensing for obstacle presence. The second approach utilizes light-displacement measurement to estimate object height by analyzing the geometric displacement of a projected stripe in the camera image, which provides quantitative obstacle height information with minimal computational overhead. The third approach employs a computer vision-based object detection model on embedded AI hardware to classify objects, enabling semantic perception and context-aware safety decisions. All methods are implemented on a Raspberry Pi 5 system, achieving real-time performance at 25 or 50 frames per second. Experimental evaluation and comparative analysis demonstrate that the proposed hierarchy balances precision, computation, and cost, thereby providing a scalable perception solution for enabling safe operations of AMRs in manufacturing environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u8fd1\u573a\u611f\u77e5\u7684\u4e09\u5c42\u6846\u67b6\uff0c\u5305\u62ec\u5149\u95f4\u65ad\u68c0\u6d4b\u3001\u5149\u4f4d\u79fb\u6d4b\u91cf\u548c\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5bf9\u8c61\u68c0\u6d4b\uff0c\u5728Raspberry Pi 5\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd\uff0c\u4e3a\u5236\u9020\u73af\u5883\u4e2d\u7684AMR\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u5b89\u5168\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5728\u5236\u9020\u73af\u5883\u4e2d\uff0c\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u9700\u8981\u53ef\u9760\u7684\u8fd1\u573a\u611f\u77e5\u4ee5\u786e\u4fdd\u5b89\u5168\u64cd\u4f5c\u3002\u4f20\u7edf\u7684\u6d4b\u8ddd\u4f20\u611f\u5668\uff08\u5982LiDAR\u548c\u8d85\u58f0\u6ce2\u8bbe\u5907\uff09\u867d\u7136\u80fd\u63d0\u4f9b\u5e7f\u6cdb\u7684\u60c5\u5883\u611f\u77e5\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u68c0\u6d4b\u5230\u673a\u5668\u4eba\u5e95\u5ea7\u9644\u8fd1\u7684\u5c0f\u7269\u4f53\uff0c\u5b58\u5728\u5b89\u5168\u9690\u60a3\u3002", "method": "\u63d0\u51fa\u4e09\u5c42\u8fd1\u573a\u611f\u77e5\u6846\u67b6\uff1a1\uff09\u5149\u95f4\u65ad\u68c0\u6d4b\uff1a\u901a\u8fc7\u6fc0\u5149\u6761\u7eb9\u6295\u5f71\u5230\u8fd1\u573a\u533a\u57df\uff0c\u68c0\u6d4b\u6761\u7eb9\u4e2d\u65ad\u6765\u5b9e\u73b0\u5feb\u901f\u4e8c\u8fdb\u5236\u969c\u788d\u7269\u5b58\u5728\u68c0\u6d4b\uff1b2\uff09\u5149\u4f4d\u79fb\u6d4b\u91cf\uff1a\u901a\u8fc7\u5206\u6790\u6295\u5f71\u6761\u7eb9\u5728\u76f8\u673a\u56fe\u50cf\u4e2d\u7684\u51e0\u4f55\u4f4d\u79fb\u6765\u4f30\u8ba1\u7269\u4f53\u9ad8\u5ea6\uff0c\u63d0\u4f9b\u5b9a\u91cf\u969c\u788d\u7269\u9ad8\u5ea6\u4fe1\u606f\uff1b3\uff09\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u5bf9\u8c61\u68c0\u6d4b\uff1a\u5728\u5d4c\u5165\u5f0fAI\u786c\u4ef6\u4e0a\u8fd0\u884c\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\uff0c\u5bf9\u7269\u4f53\u8fdb\u884c\u5206\u7c7b\uff0c\u5b9e\u73b0\u8bed\u4e49\u611f\u77e5\u548c\u60c5\u5883\u611f\u77e5\u7684\u5b89\u5168\u51b3\u7b56\u3002\u6240\u6709\u65b9\u6cd5\u5747\u5728Raspberry Pi 5\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u3002", "result": "\u7cfb\u7edf\u5b9e\u73b0\u4e8625\u621650\u5e27/\u79d2\u7684\u5b9e\u65f6\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc4\u4f30\u548c\u6bd4\u8f83\u5206\u6790\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u5c42\u6b21\u7ed3\u6784\u5728\u7cbe\u5ea6\u3001\u8ba1\u7b97\u548c\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\uff0c\u4e3a\u5236\u9020\u73af\u5883\u4e2d\u7684AMR\u5b89\u5168\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u611f\u77e5\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u4e09\u5c42\u8fd1\u573a\u611f\u77e5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5feb\u901f\u4e8c\u8fdb\u5236\u68c0\u6d4b\u3001\u5b9a\u91cf\u9ad8\u5ea6\u6d4b\u91cf\u548c\u8bed\u4e49\u5206\u7c7b\uff0c\u4e3a\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8fd1\u573a\u611f\u77e5\u80fd\u529b\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\u5e73\u8861\u4e86\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u5236\u9020\u73af\u5883\u7684\u5b89\u5168\u64cd\u4f5c\u9700\u6c42\u3002"}}
{"id": "2512.13552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13552", "abs": "https://arxiv.org/abs/2512.13552", "authors": ["Hour Kaing", "Raj Dabre", "Haiyue Song", "Van-Hien Tran", "Hideki Tanaka", "Masao Utiyama"], "title": "PrahokBART: A Pre-trained Sequence-to-Sequence Model for Khmer Natural Language Generation", "comment": "Published at COLING 2025, 14 pages", "summary": "This work introduces {\\it PrahokBART}, a compact pre-trained sequence-to-sequence model trained from scratch for Khmer using carefully curated Khmer and English corpora. We focus on improving the pre-training corpus quality and addressing the linguistic issues of Khmer, which are ignored in existing multilingual models, by incorporating linguistic components such as word segmentation and normalization. We evaluate PrahokBART on three generative tasks: machine translation, text summarization, and headline generation, where our results demonstrate that it outperforms mBART50, a strong multilingual pre-trained model. Additionally, our analysis provides insights into the impact of each linguistic module and evaluates how effectively our model handles space during text generation, which is crucial for the naturalness of texts in Khmer.", "AI": {"tldr": "\u63d0\u51faPrahokBART\uff0c\u4e00\u4e2a\u4e13\u95e8\u4e3a\u9ad8\u68c9\u8bed\u4ece\u5934\u8bad\u7ec3\u7684\u7d27\u51d1\u578b\u5e8f\u5217\u5230\u5e8f\u5217\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5165\u5206\u8bcd\u548c\u89c4\u8303\u5316\u7b49\u8bed\u8a00\u5b66\u7ec4\u4ef6\u89e3\u51b3\u73b0\u6709\u591a\u8bed\u8a00\u6a21\u578b\u5ffd\u7565\u7684\u9ad8\u68c9\u8bed\u8bed\u8a00\u95ee\u9898\uff0c\u5728\u673a\u5668\u7ffb\u8bd1\u3001\u6587\u672c\u6458\u8981\u548c\u6807\u9898\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8emBART50\u3002", "motivation": "\u73b0\u6709\u591a\u8bed\u8a00\u6a21\u578b\u5ffd\u7565\u9ad8\u68c9\u8bed\u7684\u8bed\u8a00\u5b66\u7279\u6027\uff0c\u7279\u522b\u662f\u5728\u9884\u8bad\u7ec3\u8bed\u6599\u8d28\u91cf\u548c\u8bed\u8a00\u5904\u7406\u65b9\u9762\u5b58\u5728\u95ee\u9898\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u9ad8\u68c9\u8bed\u5f00\u53d1\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u89e3\u51b3\u5206\u8bcd\u3001\u89c4\u8303\u5316\u7b49\u8bed\u8a00\u5904\u7406\u6311\u6218\u3002", "method": "\u4f7f\u7528\u7cbe\u5fc3\u7b5b\u9009\u7684\u9ad8\u68c9\u8bed\u548c\u82f1\u8bed\u8bed\u6599\u4ece\u5934\u8bad\u7ec3\u7d27\u51d1\u578b\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\uff0c\u878d\u5165\u5206\u8bcd\u548c\u89c4\u8303\u5316\u7b49\u8bed\u8a00\u5b66\u7ec4\u4ef6\uff0c\u4e13\u95e8\u5904\u7406\u9ad8\u68c9\u8bed\u7684\u8bed\u8a00\u7279\u6027\u3002", "result": "\u5728\u673a\u5668\u7ffb\u8bd1\u3001\u6587\u672c\u6458\u8981\u548c\u6807\u9898\u751f\u6210\u4e09\u4e2a\u751f\u6210\u4efb\u52a1\u4e0a\uff0cPrahokBART\u7684\u8868\u73b0\u4f18\u4e8e\u5f3a\u5927\u7684\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578bmBART50\u3002\u5206\u6790\u8fd8\u63ed\u793a\u4e86\u5404\u8bed\u8a00\u5b66\u6a21\u5757\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30\u4e86\u6a21\u578b\u5904\u7406\u7a7a\u683c\u7684\u6709\u6548\u6027\uff0c\u8fd9\u5bf9\u9ad8\u68c9\u8bed\u6587\u672c\u7684\u81ea\u7136\u6027\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u4e13\u95e8\u9488\u5bf9\u9ad8\u68c9\u8bed\u8bbe\u8ba1\u7684\u9884\u8bad\u7ec3\u6a21\u578bPrahokBART\u901a\u8fc7\u878d\u5165\u8bed\u8a00\u5b66\u7ec4\u4ef6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u9ad8\u68c9\u8bed\u7684\u8bed\u8a00\u7279\u6027\uff0c\u5728\u591a\u4e2a\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u901a\u7528\u591a\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u3002"}}
{"id": "2512.13644", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13644", "abs": "https://arxiv.org/abs/2512.13644", "authors": ["Raktim Gautam Goswami", "Amir Bar", "David Fan", "Tsung-Yen Yang", "Gaoyue Zhou", "Prashanth Krishnamurthy", "Michael Rabbat", "Farshad Khorrami", "Yann LeCun"], "title": "World Models Can Leverage Human Videos for Dexterous Manipulation", "comment": null, "summary": "Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.", "AI": {"tldr": "DexWM\u662f\u4e00\u4e2a\u7075\u5de7\u64cd\u4f5c\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u9884\u6d4b\u73af\u5883\u6f5c\u5728\u72b6\u6001\u6765\u63d0\u5347\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u80fd\u529b\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u64cd\u4f5c\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7075\u5de7\u64cd\u4f5c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u7406\u89e3\u7ec6\u5fae\u7684\u624b\u90e8\u52a8\u4f5c\u5982\u4f55\u901a\u8fc7\u7269\u4f53\u63a5\u89e6\u5f71\u54cd\u73af\u5883\u3002\u73b0\u6709\u7075\u5de7\u64cd\u4f5c\u6570\u636e\u96c6\u7a00\u7f3a\uff0c\u4e14\u4ec5\u9884\u6d4b\u89c6\u89c9\u7279\u5f81\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u7cbe\u7ec6\u7075\u5de7\u64cd\u4f5c\u3002", "method": "\u63d0\u51faDexWM\u6a21\u578b\uff0c\u57fa\u4e8e\u8fc7\u53bb\u72b6\u6001\u548c\u7075\u5de7\u52a8\u4f5c\u9884\u6d4b\u73af\u5883\u7684\u4e0b\u4e00\u4e2a\u6f5c\u5728\u72b6\u6001\u3002\u4f7f\u7528\u8d85\u8fc7900\u5c0f\u65f6\u7684\u4eba\u7c7b\u548c\u975e\u7075\u5de7\u673a\u5668\u4eba\u89c6\u9891\u8fdb\u884c\u8bad\u7ec3\u3002\u5f15\u5165\u8f85\u52a9\u624b\u90e8\u4e00\u81f4\u6027\u635f\u5931\u6765\u786e\u4fdd\u51c6\u786e\u7684\u624b\u90e8\u914d\u7f6e\u3002", "result": "DexWM\u5728\u9884\u6d4b\u672a\u6765\u72b6\u6001\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u3001\u5bfc\u822a\u548c\u5168\u8eab\u52a8\u4f5c\u7684\u5148\u524d\u4e16\u754c\u6a21\u578b\u3002\u5728Franka Panda\u673a\u68b0\u81c2\u548cAllegro\u5939\u722a\u4e0a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u5b9e\u9a8c\u4e2d\uff0c\u5728\u6293\u53d6\u3001\u653e\u7f6e\u548c\u5230\u8fbe\u4efb\u52a1\u4e0a\u5e73\u5747\u6bd4Diffusion Policy\u9ad8\u51fa50%\u4ee5\u4e0a\u3002", "conclusion": "DexWM\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u65b9\u6cd5\u548c\u624b\u90e8\u4e00\u81f4\u6027\u635f\u5931\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7075\u5de7\u64cd\u4f5c\u4e2d\u7684\u6311\u6218\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u64cd\u4f5c\u6280\u80fd\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.13559", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13559", "abs": "https://arxiv.org/abs/2512.13559", "authors": ["Gibson Nkhata", "Uttamasha Anjally Oyshi", "Quan Mai", "Susan Gauch"], "title": "Verifying Rumors via Stance-Aware Structural Modeling", "comment": "8 pages, 2 figures, published in The 24th IEEE/WIC International Conference on Web Intelligence and Intelligent Agent Technology (WI-IAT 2025), London, UK, 2025", "summary": "Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7acb\u573a\u611f\u77e5\u7684\u7ed3\u6784\u5316\u5efa\u6a21\u65b9\u6cd5\uff0c\u7528\u4e8e\u793e\u4ea4\u5a92\u4f53\u8c23\u8a00\u9a8c\u8bc1\uff0c\u901a\u8fc7\u7ed3\u5408\u7acb\u573a\u4fe1\u53f7\u3001\u5bf9\u8bdd\u7ed3\u6784\u548c\u5c42\u6b21\u6df1\u5ea6\u663e\u8457\u63d0\u5347\u8c23\u8a00\u771f\u5b9e\u6027\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u8c23\u8a00\u9a8c\u8bc1\u5bf9\u904f\u5236\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u540c\u65f6\u6355\u6349\u8bed\u4e49\u5185\u5bb9\u3001\u7acb\u573a\u4fe1\u606f\u548c\u5bf9\u8bdd\u7ed3\u6784\uff0c\u7279\u522b\u662f\u5728\u57fa\u4e8eTransformer\u7684\u7f16\u7801\u5668\u5e8f\u5217\u957f\u5ea6\u9650\u5236\u4e0b\u3002", "method": "\u63d0\u51fa\u7acb\u573a\u611f\u77e5\u7ed3\u6784\u5316\u5efa\u6a21\uff1a1) \u7528\u7acb\u573a\u4fe1\u53f7\u7f16\u7801\u6bcf\u4e2a\u5e16\u5b50\uff1b2) \u6309\u7acb\u573a\u7c7b\u522b\u805a\u5408\u56de\u590d\u5d4c\u5165\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u4e14\u8bed\u4e49\u4e30\u5bcc\u7684\u7ebf\u7a0b\u8868\u793a\uff1b3) \u5f15\u5165\u7acb\u573a\u5206\u5e03\u548c\u5c42\u6b21\u6df1\u5ea6\u4f5c\u4e3a\u534f\u53d8\u91cf\uff0c\u6355\u6349\u7acb\u573a\u4e0d\u5e73\u8861\u548c\u56de\u590d\u6df1\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u9884\u6d4b\u8c23\u8a00\u771f\u5b9e\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u8bc1\u660e\u6a21\u578b\u5728\u65e9\u671f\u68c0\u6d4b\u548c\u8de8\u5e73\u53f0\u6cdb\u5316\u65b9\u9762\u5177\u6709\u826f\u597d\u9002\u5e94\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7acb\u573a\u611f\u77e5\u7ed3\u6784\u5316\u5efa\u6a21\u65b9\u6cd5\u6709\u6548\u6574\u5408\u4e86\u8bed\u4e49\u5185\u5bb9\u3001\u7acb\u573a\u4fe1\u606f\u548c\u5bf9\u8bdd\u7ed3\u6784\uff0c\u4e3a\u793e\u4ea4\u5a92\u4f53\u8c23\u8a00\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.13660", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13660", "abs": "https://arxiv.org/abs/2512.13660", "authors": ["Enshen Zhou", "Cheng Chi", "Yibo Li", "Jingkun An", "Jiayuan Zhang", "Shanyu Rong", "Yi Han", "Yuheng Ji", "Mengzhen Liu", "Pengwei Wang", "Zhongyuan Wang", "Lu Sheng", "Shanghang Zhang"], "title": "RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics", "comment": "Project page: https://zhoues.github.io/RoboTracer", "summary": "Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.", "AI": {"tldr": "RoboTracer\uff1a\u9996\u4e2a\u5b9e\u73b03D\u7a7a\u95f4\u6307\u4ee3\u4e0e\u6d4b\u91cf\u76843D\u611f\u77e5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7a7a\u95f4\u7f16\u7801\u5668\u548c\u56de\u5f52\u76d1\u7763\u89e3\u7801\u5668\u589e\u5f3a\u5c3a\u5ea6\u611f\u77e5\uff0c\u7ed3\u5408\u5f3a\u5316\u5fae\u8c03\u5b9e\u73b0\u591a\u6b65\u5ea6\u91cf\u63a8\u7406\uff0c\u5728\u7a7a\u95f4\u8ffd\u8e2a\u4efb\u52a1\u4e0a\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7a7a\u95f4\u8ffd\u8e2a\u4f5c\u4e3a\u673a\u5668\u4eba\u57fa\u672c\u4ea4\u4e92\u80fd\u529b\uff0c\u9700\u8981\u591a\u6b65\u5ea6\u91cf\u63a8\u7406\u3001\u590d\u6742\u7a7a\u95f4\u6307\u4ee3\u548c\u771f\u5b9e\u4e16\u754c\u5ea6\u91cf\u6d4b\u91cf\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u8fd9\u79cd\u7ec4\u5408\u4efb\u52a1\u3002", "method": "\u63d0\u51faRoboTracer 3D\u611f\u77e5VLM\uff1a1\uff09\u901a\u7528\u7a7a\u95f4\u7f16\u7801\u5668\u548c\u56de\u5f52\u76d1\u7763\u89e3\u7801\u5668\u5b9e\u73b03D\u7a7a\u95f4\u6307\u4ee3\u4e0e\u6d4b\u91cf\uff1b2\uff09\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\u914d\u5408\u5ea6\u91cf\u654f\u611f\u8fc7\u7a0b\u5956\u52b1\uff0c\u76d1\u7763\u5173\u952e\u4e2d\u95f4\u611f\u77e5\u7ebf\u7d22\uff1b3\uff09\u6784\u5efaTraceSpatial\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0830M QA\u5bf9\uff09\u548cTraceSpatial-Bench\u57fa\u51c6\u3002", "result": "RoboTracer\u5728\u7a7a\u95f4\u7406\u89e3\u3001\u6d4b\u91cf\u548c\u6307\u4ee3\u65b9\u9762\u8d85\u8d8a\u57fa\u7ebf\uff0c\u5e73\u5747\u6210\u529f\u738779.1%\uff1b\u5728TraceSpatial-Bench\u4e0a\u5927\u5e45\u9886\u5148\uff0c\u6bd4Gemini-2.5-Pro\u51c6\u786e\u7387\u9ad836%\uff1b\u53ef\u96c6\u6210\u591a\u79cd\u63a7\u5236\u7b56\u7565\uff0c\u5728\u6742\u4e71\u771f\u5b9e\u573a\u666f\u4e2d\u6267\u884c\u957f\u65f6\u7a0b\u52a8\u6001\u4efb\u52a1\u3002", "conclusion": "RoboTracer\u901a\u8fc7\u521b\u65b0\u76843D\u611f\u77e5\u67b6\u6784\u548c\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u7a7a\u95f4\u8ffd\u8e2a\u7684\u5b8c\u6574\u80fd\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u590d\u6742\u7a7a\u95f4\u4ea4\u4e92\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13564", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13564", "abs": "https://arxiv.org/abs/2512.13564", "authors": ["Yuyang Hu", "Shichun Liu", "Yanwei Yue", "Guibin Zhang", "Boyang Liu", "Fangyi Zhu", "Jiahang Lin", "Honglin Guo", "Shihan Dou", "Zhiheng Xi", "Senjie Jin", "Jiejun Tan", "Yanbin Yin", "Jiongnan Liu", "Zeyu Zhang", "Zhongxiang Sun", "Yutao Zhu", "Hao Sun", "Boci Peng", "Zhenrong Cheng", "Xuanbo Fan", "Jiaxin Guo", "Xinlei Yu", "Zhenhong Zhou", "Zewen Hu", "Jiahao Huo", "Junhao Wang", "Yuwei Niu", "Yu Wang", "Zhenfei Yin", "Xiaobin Hu", "Yue Liao", "Qiankun Li", "Kun Wang", "Wangchunshu Zhou", "Yixin Liu", "Dawei Cheng", "Qi Zhang", "Tao Gui", "Shirui Pan", "Yan Zhang", "Philip Torr", "Zhicheng Dou", "Ji-Rong Wen", "Xuanjing Huang", "Yu-Gang Jiang", "Shuicheng Yan"], "title": "Memory in the Age of AI Agents", "comment": null, "summary": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u667a\u80fd\u4f53\u8bb0\u5fc6\u7814\u7a76\u8fdb\u884c\u4e86\u7cfb\u7edf\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f62\u5f0f\u3001\u529f\u80fd\u548c\u52a8\u6001\u7684\u4e09\u7ef4\u5206\u6790\u6846\u67b6\uff0c\u5e76\u603b\u7ed3\u4e86\u5f53\u524d\u8fdb\u5c55\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u667a\u80fd\u4f53\u7814\u7a76\u8fc5\u901f\u53d1\u5c55\uff0c\u8bb0\u5fc6\u4f5c\u4e3a\u5176\u6838\u5fc3\u80fd\u529b\u53d7\u5230\u5e7f\u6cdb\u5173\u6ce8\uff0c\u4f46\u8be5\u9886\u57df\u5b58\u5728\u6982\u5ff5\u6a21\u7cca\u3001\u5206\u7c7b\u6df7\u4e71\u3001\u8bc4\u4ef7\u6807\u51c6\u4e0d\u7edf\u4e00\u7b49\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u68b3\u7406\u548c\u6982\u5ff5\u6f84\u6e05\u3002", "method": "\u901a\u8fc7\u4e09\u7ef4\u5206\u6790\u6846\u67b6\uff1a1) \u5f62\u5f0f\u7ef4\u5ea6\uff1a\u8bc6\u522btoken\u7ea7\u3001\u53c2\u6570\u5316\u548c\u6f5c\u5728\u8bb0\u5fc6\u4e09\u79cd\u5b9e\u73b0\u65b9\u5f0f\uff1b2) \u529f\u80fd\u7ef4\u5ea6\uff1a\u63d0\u51fa\u4e8b\u5b9e\u8bb0\u5fc6\u3001\u7ecf\u9a8c\u8bb0\u5fc6\u548c\u5de5\u4f5c\u8bb0\u5fc6\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff1b3) \u52a8\u6001\u7ef4\u5ea6\uff1a\u5206\u6790\u8bb0\u5fc6\u7684\u5f62\u6210\u3001\u6f14\u5316\u548c\u68c0\u7d22\u8fc7\u7a0b\u3002", "result": "\u5efa\u7acb\u4e86\u6e05\u6670\u7684\u667a\u80fd\u4f53\u8bb0\u5fc6\u6982\u5ff5\u8fb9\u754c\uff0c\u533a\u5206\u4e86LLM\u8bb0\u5fc6\u3001RAG\u548c\u4e0a\u4e0b\u6587\u5de5\u7a0b\u7b49\u76f8\u5173\u6982\u5ff5\uff1b\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u8bb0\u5fc6\u57fa\u51c6\u6d4b\u8bd5\u548c\u5f00\u6e90\u6846\u67b6\u603b\u7ed3\uff1b\u8bc6\u522b\u4e86\u5f53\u524d\u7814\u7a76\u7684\u5173\u952e\u5b9e\u73b0\u65b9\u5f0f\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u4e0d\u4ec5\u4e3a\u73b0\u6709\u5de5\u4f5c\u63d0\u4f9b\u53c2\u8003\uff0c\u66f4\u91cd\u8981\u7684\u662f\u4e3a\u91cd\u65b0\u601d\u8003\u8bb0\u5fc6\u4f5c\u4e3a\u672a\u6765\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u4e00\u7b49\u516c\u6c11\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\uff0c\u5e76\u6307\u51fa\u4e86\u8bb0\u5fc6\u81ea\u52a8\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u96c6\u6210\u3001\u591a\u6a21\u6001\u8bb0\u5fc6\u3001\u591a\u667a\u80fd\u4f53\u8bb0\u5fc6\u548c\u53ef\u4fe1\u5ea6\u7b49\u524d\u6cbf\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2512.13670", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13670", "abs": "https://arxiv.org/abs/2512.13670", "authors": ["Licheng Luo", "Yu Xia", "Kaier Liang", "Mingyu Cai"], "title": "NL2SpaTiaL: Generating Geometric Spatio-Temporal Logic Specifications from Natural Language for Manipulation Tasks", "comment": null, "summary": "Spatio-Temporal Logic (SpaTiaL) offers a principled formalism for expressing geometric spatial requirements-an essential component of robotic manipulation, where object locations, neighborhood relations, pose constraints, and interactions directly determine task success. Yet prior works have largely relied on standard temporal logic (TL), which models only robot trajectories and overlooks object-level interactions. Existing datasets built from randomly generated TL formulas paired with natural-language descriptions therefore cover temporal operators but fail to represent the layered spatial relations that manipulation tasks depend on. To address this gap, we introduce a dataset generation framework that synthesizes SpaTiaL specifications and converts them into natural-language descriptions through a deterministic, semantics-preserving back-translation procedure. This pipeline produces the NL2SpaTiaL dataset, aligning natural language with multi-level spatial relations and temporal objectives to reflect the compositional structure of manipulation tasks. Building on this foundation, we propose a translation-verification framework equipped with a language-based semantic checker that ensures the generated SpaTiaL formulas faithfully encode the semantics specified by the input description. Experiments across a suite of manipulation tasks show that SpaTiaL-based representations yield more interpretable, verifiable, and compositional grounding for instruction following. Project website: https://sites.google.com/view/nl2spatial", "AI": {"tldr": "\u63d0\u51fa\u4e86NL2SpaTiaL\u6570\u636e\u96c6\u548c\u7ffb\u8bd1\u9a8c\u8bc1\u6846\u67b6\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3a\u65f6\u7a7a\u903b\u8f91\u516c\u5f0f\uff0c\u4ee5\u66f4\u597d\u5730\u8868\u8fbe\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u7a7a\u95f4\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6807\u51c6\u65f6\u5e8f\u903b\u8f91\u7684\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u5ffd\u7565\u4e86\u7269\u4f53\u5c42\u9762\u7684\u7a7a\u95f4\u4ea4\u4e92\u5173\u7cfb\uff0c\u800c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u9ad8\u5ea6\u4f9d\u8d56\u51e0\u4f55\u7a7a\u95f4\u7ea6\u675f\uff08\u5982\u7269\u4f53\u4f4d\u7f6e\u3001\u90bb\u63a5\u5173\u7cfb\u3001\u59ff\u6001\u7ea6\u675f\u7b49\uff09\u3002\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u5bf9\u591a\u5c42\u6b21\u7a7a\u95f4\u5173\u7cfb\u7684\u8868\u8fbe\u3002", "method": "1) \u63d0\u51fa\u6570\u636e\u96c6\u751f\u6210\u6846\u67b6\uff0c\u5408\u6210\u65f6\u7a7a\u903b\u8f91(SpaTiaL)\u89c4\u8303\u5e76\u901a\u8fc7\u786e\u5b9a\u6027\u3001\u8bed\u4e49\u4fdd\u6301\u7684\u53cd\u5411\u7ffb\u8bd1\u8fc7\u7a0b\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff0c\u521b\u5efaNL2SpaTiaL\u6570\u636e\u96c6\uff1b2) \u63d0\u51fa\u7ffb\u8bd1\u9a8c\u8bc1\u6846\u67b6\uff0c\u914d\u5907\u57fa\u4e8e\u8bed\u8a00\u7684\u8bed\u4e49\u68c0\u67e5\u5668\uff0c\u786e\u4fdd\u751f\u6210\u7684SpaTiaL\u516c\u5f0f\u51c6\u786e\u7f16\u7801\u8f93\u5165\u63cf\u8ff0\u7684\u8bed\u4e49\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eSpaTiaL\u7684\u8868\u793a\u5728\u591a\u79cd\u64cd\u4f5c\u4efb\u52a1\u4e2d\u80fd\u591f\u63d0\u4f9b\u66f4\u53ef\u89e3\u91ca\u3001\u53ef\u9a8c\u8bc1\u548c\u53ef\u7ec4\u5408\u7684\u6307\u4ee4\u57fa\u7840\uff0c\u66f4\u597d\u5730\u53cd\u6620\u64cd\u4f5c\u4efb\u52a1\u7684\u7ec4\u5408\u7ed3\u6784\u3002", "conclusion": "\u65f6\u7a7a\u903b\u8f91\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u7a7a\u95f4\u5173\u7cfb\u8868\u8fbe\u5f62\u5f0f\uff0cNL2SpaTiaL\u6570\u636e\u96c6\u548c\u7ffb\u8bd1\u9a8c\u8bc1\u6846\u67b6\u80fd\u591f\u6709\u6548\u5bf9\u9f50\u81ea\u7136\u8bed\u8a00\u4e0e\u591a\u5c42\u6b21\u7a7a\u95f4\u5173\u7cfb\u548c\u65f6\u5e8f\u76ee\u6807\uff0c\u63d0\u5347\u6307\u4ee4\u8ddf\u968f\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2512.13586", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13586", "abs": "https://arxiv.org/abs/2512.13586", "authors": ["Jia-Nan Li", "Jian Guan", "Wei Wu", "Chongxuan Li"], "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding", "comment": null, "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\\times$ average speedup.", "AI": {"tldr": "ReFusion\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a9\u7801\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u5e76\u884c\u89e3\u7801\u4ecetoken\u7ea7\u522b\u63d0\u5347\u5230slot\u7ea7\u522b\uff0c\u91c7\u7528\"\u89c4\u5212-\u586b\u5145\"\u7684\u4e24\u9636\u6bb5\u89e3\u7801\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u5f00\u9500\u5927\u548c\u751f\u6210\u4e0d\u8fde\u8d2f\u95ee\u9898\u3002", "motivation": "\u81ea\u56de\u5f52\u6a21\u578b\u5b58\u5728\u987a\u5e8f\u63a8\u7406\u6162\u7684\u95ee\u9898\uff0c\u800c\u63a9\u7801\u6269\u6563\u6a21\u578b\u867d\u7136\u63d0\u4f9b\u5e76\u884c\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7f3a\u9677\uff1a1\uff09\u7531\u4e8e\u65e0\u6cd5\u4f7f\u7528KV\u7f13\u5b58\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u9ad8\uff1b2\uff09\u5728\u96be\u4ee5\u5904\u7406\u7684token\u7ec4\u5408\u7a7a\u95f4\u4e0a\u5b66\u4e60\u4f9d\u8d56\u5173\u7cfb\u5bfc\u81f4\u751f\u6210\u4e0d\u8fde\u8d2f\u3002", "method": "ReFusion\u91c7\u7528slot\u7ea7\u522b\u7684\u5e76\u884c\u89e3\u7801\uff0c\u6bcf\u4e2aslot\u662f\u56fa\u5b9a\u957f\u5ea6\u7684\u8fde\u7eed\u5b50\u5e8f\u5217\u3002\u4f7f\u7528\u4e24\u9636\u6bb5\u89e3\u7801\u8fc7\u7a0b\uff1a1\uff09\u57fa\u4e8e\u6269\u6563\u7684\u89c4\u5212\u6b65\u9aa4\u8bc6\u522b\u4e00\u7ec4\u5f31\u4f9d\u8d56\u7684slot\uff1b2\uff09\u81ea\u56de\u5f52\u586b\u5145\u6b65\u9aa4\u5e76\u884c\u89e3\u7801\u8fd9\u4e9b\u9009\u5b9a\u7684slot\u3002slot\u8bbe\u8ba1\u5b9e\u73b0\u4e86KV\u7f13\u5b58\u91cd\u7528\uff0c\u5e76\u5c06\u5b66\u4e60\u590d\u6742\u5ea6\u4ecetoken\u7ec4\u5408\u7a7a\u95f4\u964d\u4f4e\u5230\u53ef\u7ba1\u7406\u7684slot\u6392\u5217\u7a7a\u95f4\u3002", "result": "\u5728\u4e03\u4e2a\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cReFusion\u4e0d\u4ec5\u4ee534%\u7684\u6027\u80fd\u63d0\u5347\u548c\u8d85\u8fc718\u500d\u7684\u5e73\u5747\u52a0\u901f\u663e\u8457\u8d85\u8d8a\u5148\u524d\u7684\u63a9\u7801\u6269\u6563\u6a21\u578b\uff0c\u800c\u4e14\u5728\u4fdd\u63012.33\u500d\u5e73\u5747\u52a0\u901f\u7684\u540c\u65f6\uff0c\u7f29\u5c0f\u4e86\u4e0e\u5f3a\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "ReFusion\u901a\u8fc7slot\u7ea7\u522b\u7684\u5e76\u884c\u89e3\u7801\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u548c\u751f\u6210\u8d28\u91cf\u95ee\u9898\uff0c\u5728\u6027\u80fd\u548c\u901f\u5ea6\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4e3a\u5e76\u884c\u6587\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13598", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13598", "abs": "https://arxiv.org/abs/2512.13598", "authors": ["Daniel Melcer", "Qi Chen", "Wen-Hao Chiang", "Shweta Garg", "Pranav Garg", "Christian Bock"], "title": "Textual Gradients are a Flawed Metaphor for Automatic Prompt Optimization", "comment": null, "summary": "A well-engineered prompt can increase the performance of large language models; automatic prompt optimization techniques aim to increase performance without requiring human effort to tune the prompts. One leading class of prompt optimization techniques introduces the analogy of textual gradients. We investigate the behavior of these textual gradient methods through a series of experiments and case studies. While such methods often result in a performance improvement, our experiments suggest that the gradient analogy does not accurately explain their behavior. Our insights may inform the selection of prompt optimization strategies, and development of new approaches.", "AI": {"tldr": "\u6587\u672c\u68af\u5ea6\u65b9\u6cd5\u80fd\u63d0\u5347LLM\u6027\u80fd\uff0c\u4f46\u68af\u5ea6\u7c7b\u6bd4\u5e76\u4e0d\u80fd\u51c6\u786e\u89e3\u91ca\u5176\u884c\u4e3a\u673a\u5236", "motivation": "\u7814\u7a76\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u6280\u672f\u4e2d\u7684\u6587\u672c\u68af\u5ea6\u65b9\u6cd5\uff0c\u63a2\u7a76\u5176\u5b9e\u9645\u5de5\u4f5c\u539f\u7406\u662f\u5426\u5982\u68af\u5ea6\u7c7b\u6bd4\u6240\u63cf\u8ff0", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u548c\u6848\u4f8b\u7814\u7a76\uff0c\u5206\u6790\u6587\u672c\u68af\u5ea6\u65b9\u6cd5\u7684\u5b9e\u9645\u884c\u4e3a\u8868\u73b0", "result": "\u6587\u672c\u68af\u5ea6\u65b9\u6cd5\u901a\u5e38\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u68af\u5ea6\u7c7b\u6bd4\u5e76\u4e0d\u80fd\u51c6\u786e\u89e3\u91ca\u5176\u884c\u4e3a\u673a\u5236", "conclusion": "\u7814\u7a76\u7ed3\u679c\u53ef\u4e3a\u63d0\u793a\u4f18\u5316\u7b56\u7565\u7684\u9009\u62e9\u548c\u65b0\u65b9\u6cd5\u7684\u5f00\u53d1\u63d0\u4f9b\u53c2\u8003"}}
{"id": "2512.13607", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13607", "abs": "https://arxiv.org/abs/2512.13607", "authors": ["Boxin Wang", "Chankyu Lee", "Nayeon Lee", "Sheng-Chieh Lin", "Wenliang Dai", "Yang Chen", "Yangyi Chen", "Zhuolin Yang", "Zihan Liu", "Mohammad Shoeybi", "Bryan Catanzaro", "Wei Ping"], "title": "Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models", "comment": "We publicly release the Nemotron-Cascade models and the full collection of training data at: https://huggingface.co/collections/nvidia/nemotron-cascade", "summary": "Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.", "AI": {"tldr": "\u63d0\u51faCascade RL\u65b9\u6cd5\u8bad\u7ec3\u901a\u7528\u63a8\u7406\u6a21\u578bNemotron-Cascade\uff0c\u901a\u8fc7\u987a\u5e8f\u57df\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u8de8\u57df\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u4f20\u7edfRL\u65b9\u6cd5\u5728\u5904\u7406\u8de8\u57df\u63a8\u7406\u4efb\u52a1\u65f6\u9762\u4e34\u5f02\u8d28\u6027\u6311\u6218\uff0c\u5305\u62ec\u54cd\u5e94\u957f\u5ea6\u5dee\u5f02\u5927\u3001\u9a8c\u8bc1\u5ef6\u8fdf\u4e0d\u540c\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u57fa\u7840\u8bbe\u65bd\u590d\u6742\u3001\u8bad\u7ec3\u7f13\u6162\u3001\u8bfe\u7a0b\u8bbe\u8ba1\u548c\u8d85\u53c2\u6570\u9009\u62e9\u56f0\u96be", "method": "\u63d0\u51fa\u7ea7\u8054\u57df\u5f3a\u5316\u5b66\u4e60(Cascade RL)\uff0c\u91c7\u7528\u987a\u5e8f\u57dfRL\u800c\u975e\u6df7\u5408\u5f02\u8d28\u63d0\u793a\uff0c\u5148\u8fdb\u884cRLHF\u5bf9\u9f50\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u518d\u8fdb\u884c\u57dfRLVR\u9636\u6bb5\uff0c\u4fdd\u6301\u6216\u63d0\u5347\u5148\u524d\u57df\u7684\u6027\u80fd", "result": "14B\u6a21\u578b\u5728RL\u540e\u8d85\u8d8a\u5176SFT\u6559\u5e08DeepSeek-R1-0528\uff0c\u5728LiveCodeBench v5/v6/Pro\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u57282025\u5e74IOI\u4e2d\u83b7\u5f97\u94f6\u724c\u6210\u7ee9", "conclusion": "Cascade RL\u80fd\u6709\u6548\u89e3\u51b3\u8de8\u57df\u5f02\u8d28\u6027\u95ee\u9898\uff0c\u7b80\u5316\u5de5\u7a0b\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u901a\u7528\u63a8\u7406\u6a21\u578b\u7684SOTA\u6027\u80fd\uff0c\u5e76\u516c\u5f00\u5206\u4eab\u4e86\u8bad\u7ec3\u548c\u6570\u636e\u65b9\u6848"}}
{"id": "2512.13618", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13618", "abs": "https://arxiv.org/abs/2512.13618", "authors": ["Zefang Liu", "Nam Nguyen", "Yinzhu Quan", "Austin Zhang"], "title": "Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models", "comment": null, "summary": "Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u4e8b\u4ef6\u5e8f\u5217\u7684\u65f6\u95f4\u6807\u8bb0\u5316\u8fdb\u884c\u5b9e\u8bc1\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u4e94\u79cd\u7f16\u7801\u7b56\u7565\u5728\u4e0d\u540c\u7edf\u8ba1\u5206\u5e03\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6ca1\u6709\u5355\u4e00\u6700\u4f18\u7b56\u7565\uff0c\u6027\u80fd\u53d6\u51b3\u4e8e\u6807\u8bb0\u5668\u4e0e\u6570\u636e\u7edf\u8ba1\u7279\u6027\u7684\u5339\u914d\u7a0b\u5ea6\u3002", "motivation": "\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5efa\u6a21\u65f6\u95f4\u4e8b\u4ef6\u5e8f\u5217\u65f6\uff0c\u8fde\u7eed\u65f6\u95f4\u8868\u793a\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u6311\u6218\u3002\u73b0\u6709\u7b56\u7565\u5982\u5b57\u8282\u7ea7\u8868\u793a\u6216\u65e5\u5386\u6807\u8bb0\u7b49\uff0c\u4f46\u6700\u4f18\u65b9\u6cd5\u4ecd\u4e0d\u660e\u786e\uff0c\u7279\u522b\u662f\u8003\u8651\u5230\u73b0\u5b9e\u4e16\u754c\u4e8b\u4ef6\u6570\u636e\u5177\u6709\u591a\u6837\u5316\u7684\u7edf\u8ba1\u5206\u5e03\uff08\u4ece\u5e73\u6ed1\u7684\u5bf9\u6570\u6b63\u6001\u5206\u5e03\u5230\u79bb\u6563\u7684\u5c16\u5cf0\u6a21\u5f0f\uff09\u3002", "method": "\u6bd4\u8f83\u4e94\u79cd\u4e0d\u540c\u7684\u65f6\u95f4\u7f16\u7801\u7b56\u7565\uff1a\u6734\u7d20\u6570\u5b57\u5b57\u7b26\u4e32\u3001\u9ad8\u7cbe\u5ea6\u5b57\u8282\u7ea7\u8868\u793a\u3001\u4eba\u7c7b\u8bed\u4e49\u65e5\u5386\u6807\u8bb0\u3001\u7ecf\u5178\u5747\u5300\u5206\u7bb1\u548c\u81ea\u9002\u5e94\u6b8b\u5dee\u6807\u91cf\u91cf\u5316\u3002\u901a\u8fc7\u5728\u4f53\u73b0\u8fd9\u4e9b\u591a\u6837\u5316\u5206\u5e03\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u8bc4\u4f30\u8fd9\u4e9b\u7b56\u7565\u3002", "result": "\u5206\u6790\u8868\u660e\u6ca1\u6709\u5355\u4e00\u7b56\u7565\u662f\u666e\u904d\u6700\u4f18\u7684\uff1b\u9884\u6d4b\u6027\u80fd\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u6807\u8bb0\u5668\u4e0e\u6570\u636e\u7edf\u8ba1\u7279\u6027\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002\u57fa\u4e8e\u5bf9\u6570\u7684\u7b56\u7565\u5728\u504f\u659c\u5206\u5e03\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u4eba\u7c7b\u4e2d\u5fc3\u683c\u5f0f\u5728\u6df7\u5408\u6a21\u6001\u4e0a\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "conclusion": "\u65f6\u95f4\u6807\u8bb0\u5316\u7b56\u7565\u7684\u9009\u62e9\u5e94\u57fa\u4e8e\u6570\u636e\u7684\u7edf\u8ba1\u7279\u6027\u8fdb\u884c\u5339\u914d\uff0c\u6ca1\u6709\u9002\u7528\u4e8e\u6240\u6709\u60c5\u51b5\u7684\u901a\u7528\u6700\u4f18\u65b9\u6cd5\u3002\u5bf9\u6570\u7b56\u7565\u9002\u5408\u504f\u659c\u5206\u5e03\uff0c\u4eba\u7c7b\u8bed\u4e49\u683c\u5f0f\u9002\u5408\u6df7\u5408\u6a21\u6001\u3002"}}
{"id": "2512.13654", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.13654", "abs": "https://arxiv.org/abs/2512.13654", "authors": ["John E. Ortega", "Dhruv D. Joshi", "Matt P. Borkowski"], "title": "Large-Language Memorization During the Classification of United States Supreme Court Cases", "comment": "7 pages, 1 figure, Appendix of Prompts", "summary": "Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called \"hallucinations\" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6700\u9ad8\u6cd5\u9662\u5224\u51b3\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8bb0\u5fc6\u51c6\u786e\u6027\uff0c\u53d1\u73b0\u57fa\u4e8e\u63d0\u793a\u7684\u6a21\u578b\u6bd4\u4f20\u7edfBERT\u6a21\u578b\u8868\u73b0\u66f4\u597d", "motivation": "\u7814\u7a76LLM\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8bb0\u5fc6\u7b56\u7565\u548c\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5177\u6709\u6311\u6218\u6027\u7684\u6700\u9ad8\u6cd5\u9662\u5224\u51b3\u5206\u7c7b\u4efb\u52a1\uff0c\u4ee5\u7406\u89e3LLM\u5982\u4f55\u54cd\u5e94\u548c\u5904\u7406\u590d\u6742\u9886\u57df\u7279\u5b9a\u5185\u5bb9", "method": "\u4f7f\u7528\u6700\u65b0\u7684LLM\u5fae\u8c03\u548c\u68c0\u7d22\u65b9\u6cd5\uff0c\u5305\u62ec\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3001\u81ea\u52a8\u5efa\u6a21\u7b49\uff0c\u5728\u4e24\u4e2a\u6700\u9ad8\u6cd5\u9662\u5224\u51b3\u5206\u7c7b\u4efb\u52a1\uff0815\u4e2a\u6807\u7b7e\u4e3b\u9898\u548c279\u4e2a\u6807\u7b7e\u4e3b\u9898\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c", "result": "\u57fa\u4e8e\u63d0\u793a\u7684\u6a21\u578b\uff08\u5982DeepSeek\uff09\u6bd4\u4e4b\u524d\u7684BERT\u6a21\u578b\u66f4\u7a33\u5065\uff0c\u5728\u4e24\u4e2a\u4efb\u52a1\u4e0a\u90fd\u6bd4\u975e\u63d0\u793a\u6a21\u578b\u9ad8\u51fa\u7ea62\u5206", "conclusion": "\u57fa\u4e8e\u63d0\u793a\u7684\u6a21\u578b\u5728\u590d\u6742\u6cd5\u5f8b\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u8bb0\u5fc6\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3aLLM\u5728\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3"}}
{"id": "2512.13655", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.13655", "abs": "https://arxiv.org/abs/2512.13655", "authors": ["Richard J. Young"], "title": "Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation", "comment": "25 pages, 6 figures, 8 tables", "summary": "Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.", "AI": {"tldr": "\u8bc4\u4f30\u56db\u79cd\u53bb\u62d2\u7edd\u5316\u5de5\u5177\uff08Heretic\u3001DECCP\u3001ErisForge\u3001FailSpy\uff09\u572816\u4e2a\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4e0a\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5355\u6b21\u901a\u8fc7\u65b9\u6cd5\u5728\u80fd\u529b\u4fdd\u7559\u4e0a\u8868\u73b0\u66f4\u597d\uff0c\u800c\u8d1d\u53f6\u65af\u4f18\u5316\u65b9\u6cd5\u4ea7\u751f\u53ef\u53d8\u5206\u5e03\u504f\u79fb\uff0c\u6570\u5b66\u63a8\u7406\u80fd\u529b\u5bf9\u53bb\u62d2\u7edd\u5316\u5e72\u9884\u6700\u654f\u611f\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u673a\u5236\u901a\u8fc7\u5b66\u4e60\u7684\u62d2\u7edd\u884c\u4e3a\u9632\u6b62\u5bf9\u6709\u5bb3\u67e5\u8be2\u7684\u54cd\u5e94\uff0c\u4f46\u8fd9\u4e9b\u673a\u5236\u4e5f\u963b\u788d\u4e86\u5305\u62ec\u8ba4\u77e5\u5efa\u6a21\u3001\u5bf9\u6297\u6d4b\u8bd5\u548c\u5b89\u5168\u5206\u6790\u5728\u5185\u7684\u5408\u6cd5\u7814\u7a76\u5e94\u7528\u3002\u867d\u7136\u53bb\u62d2\u7edd\u5316\u6280\u672f\u80fd\u591f\u901a\u8fc7\u65b9\u5411\u6b63\u4ea4\u5316\u624b\u672f\u5f0f\u5730\u79fb\u9664\u62d2\u7edd\u8868\u793a\uff0c\u4f46\u73b0\u6709\u5b9e\u73b0\u65b9\u6cd5\u7684\u76f8\u5bf9\u6548\u679c\u5c1a\u672a\u88ab\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u79cd\u53bb\u62d2\u7edd\u5316\u5de5\u5177\uff08Heretic\u3001DECCP\u3001ErisForge\u3001FailSpy\uff09\u572816\u4e2a\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\uff087B-14B\u53c2\u6570\uff09\u4e0a\u7684\u8868\u73b0\u3002\u62a5\u544a\u4e86\u6240\u670916\u4e2a\u6a21\u578b\u7684\u5de5\u5177\u517c\u5bb9\u6027\uff0c\u5e76\u6839\u636e\u5de5\u5177\u652f\u6301\u60c5\u51b5\u5728\u5b50\u96c6\u4e0a\u62a5\u544a\u4e86\u5b9a\u91cf\u6307\u6807\u3002\u8bc4\u4f30\u4e86\u5355\u6b21\u901a\u8fc7\u65b9\u6cd5\u548c\u8d1d\u53f6\u65af\u4f18\u5316\u53bb\u62d2\u7edd\u5316\u65b9\u6cd5\u7684\u6548\u679c\u3002", "result": "\u5355\u6b21\u901a\u8fc7\u65b9\u6cd5\u5728\u57fa\u51c6\u5b50\u96c6\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u80fd\u529b\u4fdd\u7559\uff08\u4e09\u4e2a\u6a21\u578b\u7684\u5e73\u5747GSM8K\u53d8\u5316\uff1aErisForge -0.28 pp\uff1bDECCP -0.13 pp\uff09\u3002\u8d1d\u53f6\u65af\u4f18\u5316\u53bb\u62d2\u7edd\u5316\u4ea7\u751f\u53ef\u53d8\u7684\u5206\u5e03\u504f\u79fb\uff08KL\u6563\u5ea6\uff1a0.043-1.646\uff09\uff0c\u5177\u6709\u6a21\u578b\u4f9d\u8d56\u7684\u80fd\u529b\u5f71\u54cd\u3002\u6570\u5b66\u63a8\u7406\u80fd\u529b\u5bf9\u53bb\u62d2\u7edd\u5316\u5e72\u9884\u6700\u654f\u611f\uff0cGSM8K\u53d8\u5316\u8303\u56f4\u4ece+1.51 pp\u5230-18.81 pp\uff08\u76f8\u5bf9-26.5%\uff09\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u5de5\u5177\u9009\u62e9\u548c\u6a21\u578b\u67b6\u6784\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u57fa\u4e8e\u8bc1\u636e\u7684\u53bb\u62d2\u7edd\u5316\u5de5\u5177\u9009\u62e9\u6807\u51c6\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u7684\u6a21\u578b\u67b6\u6784\u3002\u4e3b\u8981\u53d1\u73b0\u8868\u660e\u6570\u5b66\u63a8\u7406\u80fd\u529b\u5bf9\u53bb\u62d2\u7edd\u5316\u5e72\u9884\u6700\u4e3a\u654f\u611f\uff0c\u5de5\u5177\u9009\u62e9\u548c\u6a21\u578b\u67b6\u6784\u5bf9\u80fd\u529b\u4fdd\u7559\u6709\u663e\u8457\u5f71\u54cd\u3002"}}
{"id": "2512.13667", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13667", "abs": "https://arxiv.org/abs/2512.13667", "authors": ["Cristina Aggazzotti", "Elizabeth Allyn Smith"], "title": "A stylometric analysis of speaker attribution from speech transcripts", "comment": null, "summary": "Forensic scientists often need to identify an unknown speaker or writer in cases such as ransom calls, covert recordings, alleged suicide notes, or anonymous online communications, among many others. Speaker recognition in the speech domain usually examines phonetic or acoustic properties of a voice, and these methods can be accurate and robust under certain conditions. However, if a speaker disguises their voice or employs text-to-speech software, vocal properties may no longer be reliable, leaving only their linguistic content available for analysis. Authorship attribution methods traditionally use syntactic, semantic, and related linguistic information to identify writers of written text (authorship attribution). In this paper, we apply a content-based authorship approach to speech that has been transcribed into text, using what a speaker says to attribute speech to individuals (speaker attribution). We introduce a stylometric method, StyloSpeaker, which incorporates character, word, token, sentence, and style features from the stylometric literature on authorship, to assess whether two transcripts were produced by the same speaker. We evaluate this method on two types of transcript formatting: one approximating prescriptive written text with capitalization and punctuation and another normalized style that removes these conventions. The transcripts' conversation topics are also controlled to varying degrees. We find generally higher attribution performance on normalized transcripts, except under the strongest topic control condition, in which overall performance is highest. Finally, we compare this more explainable stylometric model to black-box neural approaches on the same data and investigate which stylistic features most effectively distinguish speakers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStyloSpeaker\u7684\u8bf4\u8bdd\u4eba\u5f52\u5c5e\u65b9\u6cd5\uff0c\u5c06\u4f5c\u8005\u5f52\u5c5e\u7684\u6587\u4f53\u8ba1\u91cf\u5b66\u65b9\u6cd5\u5e94\u7528\u4e8e\u8bed\u97f3\u8f6c\u5f55\u6587\u672c\uff0c\u901a\u8fc7\u5206\u6790\u8bf4\u8bdd\u5185\u5bb9\u800c\u975e\u58f0\u97f3\u7279\u5f81\u6765\u8bc6\u522b\u8bf4\u8bdd\u4eba\u3002", "motivation": "\u5f53\u8bf4\u8bdd\u4eba\u4f2a\u88c5\u58f0\u97f3\u6216\u4f7f\u7528\u6587\u672c\u8f6c\u8bed\u97f3\u8f6f\u4ef6\u65f6\uff0c\u4f20\u7edf\u7684\u58f0\u5b66\u8bed\u97f3\u8bc6\u522b\u65b9\u6cd5\u53ef\u80fd\u5931\u6548\uff0c\u6b64\u65f6\u53ea\u80fd\u4f9d\u8d56\u8bed\u8a00\u5185\u5bb9\u8fdb\u884c\u5206\u6790\u3002\u9700\u8981\u5f00\u53d1\u57fa\u4e8e\u5185\u5bb9\u7684\u8bf4\u8bdd\u4eba\u5f52\u5c5e\u65b9\u6cd5\u3002", "method": "\u63d0\u51faStyloSpeaker\u65b9\u6cd5\uff0c\u4ece\u6587\u4f53\u8ba1\u91cf\u5b66\u6587\u732e\u4e2d\u63d0\u53d6\u5b57\u7b26\u3001\u5355\u8bcd\u3001\u6807\u8bb0\u3001\u53e5\u5b50\u548c\u98ce\u683c\u7279\u5f81\uff0c\u8bc4\u4f30\u4e24\u4e2a\u8f6c\u5f55\u6587\u672c\u662f\u5426\u7531\u540c\u4e00\u8bf4\u8bdd\u4eba\u4ea7\u751f\u3002\u6bd4\u8f83\u4e24\u79cd\u8f6c\u5f55\u683c\u5f0f\uff08\u89c4\u8303\u4e66\u9762\u6587\u672c\u683c\u5f0f\u548c\u6807\u51c6\u5316\u683c\u5f0f\uff09\u548c\u4e0d\u540c\u8bdd\u9898\u63a7\u5236\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5728\u6807\u51c6\u5316\u8f6c\u5f55\u683c\u5f0f\u4e0b\u83b7\u5f97\u66f4\u9ad8\u7684\u5f52\u5c5e\u6027\u80fd\uff0c\u4f46\u5728\u6700\u5f3a\u8bdd\u9898\u63a7\u5236\u6761\u4ef6\u4e0b\u6574\u4f53\u6027\u80fd\u6700\u9ad8\u3002\u4e0e\u9ed1\u76d2\u795e\u7ecf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8fd9\u79cd\u53ef\u89e3\u91ca\u7684\u6587\u4f53\u8ba1\u91cf\u6a21\u578b\u8868\u73b0\u826f\u597d\uff0c\u5e76\u8bc6\u522b\u51fa\u6700\u80fd\u533a\u5206\u8bf4\u8bdd\u4eba\u7684\u98ce\u683c\u7279\u5f81\u3002", "conclusion": "\u57fa\u4e8e\u5185\u5bb9\u7684\u8bf4\u8bdd\u4eba\u5f52\u5c5e\u65b9\u6cd5\u5728\u8bed\u97f3\u8f6c\u5f55\u6587\u672c\u4e2d\u6709\u6548\uff0c\u7279\u522b\u662f\u5728\u6807\u51c6\u5316\u8f6c\u5f55\u683c\u5f0f\u548c\u9002\u5f53\u8bdd\u9898\u63a7\u5236\u6761\u4ef6\u4e0b\u3002StyloSpeaker\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u8bc6\u522b\u533a\u5206\u8bf4\u8bdd\u4eba\u7684\u5173\u952e\u98ce\u683c\u7279\u5f81\u3002"}}
{"id": "2512.13676", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13676", "abs": "https://arxiv.org/abs/2512.13676", "authors": ["Baixiang Huang", "Limeng Cui", "Jiapeng Liu", "Haoran Wang", "Jiawei Xu", "Zhuiyue Tan", "Yutong Chen", "Chen Luo", "Yi Liu", "Kai Shu"], "title": "Towards Effective Model Editing for LLM Personalization", "comment": "15 pages (including appendix), 7 figures. Code, data, results, and additional resources are available at: https://model-editing.github.io", "summary": "Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clustered preference representations. This design enables precise preference-aligned updates while preserving overall model capabilities. In addition, existing personalization benchmarks frequently rely on persona-based dialogs between LLMs rather than user-LLM interactions, or focus primarily on stylistic imitation while neglecting information-seeking tasks that require accurate recall of user-specific preferences. We introduce User Preference Question Answering (UPQA), a short-answer QA dataset constructed from in-situ user queries with varying levels of difficulty. Unlike prior benchmarks, UPQA directly evaluates a model's ability to recall and apply specific user preferences. Across experimental settings, Personalization Editing achieves higher editing accuracy and greater computational efficiency than fine-tuning, while outperforming prompting-based baselines in multi-turn conversations and implicit preference questions settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faPersonalization Editing\u6846\u67b6\uff0c\u5c06\u4e2a\u6027\u5316\u89c6\u4e3a\u6a21\u578b\u7f16\u8f91\u4efb\u52a1\uff0c\u901a\u8fc7\u805a\u7c7b\u504f\u597d\u8868\u793a\u6307\u5bfc\u5c40\u90e8\u7f16\u8f91\uff0c\u5b9e\u73b0\u7cbe\u786e\u504f\u597d\u5bf9\u9f50\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6574\u4f53\u80fd\u529b\uff0c\u5e76\u5728\u65b0\u57fa\u51c6UPQA\u4e0a\u9a8c\u8bc1\u6548\u679c\u3002", "motivation": "\u5f53\u524dLLM\u4e2a\u6027\u5316\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u6570\u636e\u5bc6\u96c6\u3001\u6613\u53d1\u751f\u707e\u96be\u6027\u9057\u5fd8\u3001\u591a\u8f6e\u4ea4\u4e92\u6216\u5904\u7406\u9690\u5f0f\u67e5\u8be2\u65f6\u6027\u80fd\u4e0b\u964d\u7b49\u95ee\u9898\u3002\u73b0\u6709\u57fa\u51c6\u591a\u4f9d\u8d56LLM\u95f4\u5bf9\u8bdd\u800c\u975e\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\uff0c\u4e14\u504f\u91cd\u98ce\u683c\u6a21\u4eff\u800c\u5ffd\u89c6\u9700\u8981\u51c6\u786e\u56de\u5fc6\u7528\u6237\u504f\u597d\u7684\u4fe1\u606f\u5bfb\u6c42\u4efb\u52a1\u3002", "method": "\u5c06\u4e2a\u6027\u5316\u6982\u5ff5\u5316\u4e3a\u6a21\u578b\u7f16\u8f91\u4efb\u52a1\uff0c\u63d0\u51faPersonalization Editing\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u504f\u597d\u8868\u793a\u6307\u5bfc\u5c40\u90e8\u7f16\u8f91\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u504f\u597d\u5bf9\u9f50\u66f4\u65b0\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6574\u4f53\u80fd\u529b\u3002\u5f15\u5165UPQA\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u6784\u5efa\u77ed\u7b54\u6848QA\u6570\u636e\u96c6\uff0c\u76f4\u63a5\u8bc4\u4f30\u6a21\u578b\u56de\u5fc6\u548c\u5e94\u7528\u7279\u5b9a\u7528\u6237\u504f\u597d\u7684\u80fd\u529b\u3002", "result": "Personalization Editing\u5728\u7f16\u8f91\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u5fae\u8c03\u65b9\u6cd5\uff0c\u5728\u591a\u8f6e\u5bf9\u8bdd\u548c\u9690\u5f0f\u504f\u597d\u95ee\u9898\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u5c06\u4e2a\u6027\u5316\u89c6\u4e3a\u6a21\u578b\u7f16\u8f91\u4efb\u52a1\u5e76\u901a\u8fc7\u805a\u7c7b\u504f\u597d\u8868\u793a\u6307\u5bfc\u5c40\u90e8\u7f16\u8f91\u662f\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5b9e\u73b0\u7cbe\u786e\u504f\u597d\u5bf9\u9f50\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u80fd\u529b\uff0cUPQA\u6570\u636e\u96c6\u4e3a\u8bc4\u4f30\u4e2a\u6027\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u57fa\u51c6\u3002"}}
{"id": "2512.13685", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13685", "abs": "https://arxiv.org/abs/2512.13685", "authors": ["Dylan Phelps", "Rodrigo Wilkens", "Edward Gow-Smith", "Lilian Hubner", "B\u00e1rbara Malcorra", "C\u00e9sar Renn\u00f3-Costa", "Marco Idiart", "Maria-Cruz Villa-Uriol", "Aline Villavicencio"], "title": "Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech", "comment": null, "summary": "Alzheimer's Disease (AD) is a progressive neurodegenerative condition that adversely affects cognitive abilities. Language-related changes can be automatically identified through the analysis of outputs from linguistic assessment tasks, such as picture description. Language models show promise as a basis for screening tools for AD, but their limited interpretability poses a challenge in distinguishing true linguistic markers of cognitive decline from surface-level textual patterns. To address this issue, we examine how surface form variation affects classification performance, with the goal of assessing the ability of language models to represent underlying semantic indicators. We introduce a novel approach where texts surface forms are transformed by altering syntax and vocabulary while preserving semantic content. The transformations significantly modify the structure and lexical content, as indicated by low BLEU and chrF scores, yet retain the underlying semantics, as reflected in high semantic similarity scores, isolating the effect of semantic information, and finding models perform similarly to if they were using the original text, with only small deviations in macro-F1. We also investigate whether language from picture descriptions retains enough detail to reconstruct the original image using generative models. We found that image-based transformations add substantial noise reducing classification accuracy. Our methodology provides a novel way of looking at what features influence model predictions, and allows the removal of possible spurious correlations. We find that just using semantic information, language model based classifiers can still detect AD. This work shows that difficult to detect semantic impairment can be identified, addressing an overlooked feature of linguistic deterioration, and opening new pathways for early detection systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8bed\u4e49\u4fdd\u6301\u7684\u6587\u672c\u8f6c\u6362\u65b9\u6cd5\uff0c\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b\u4e2d\u662f\u5426\u771f\u6b63\u6355\u6349\u8bed\u4e49\u7279\u5f81\u800c\u975e\u8868\u9762\u6587\u672c\u6a21\u5f0f\uff0c\u53d1\u73b0\u4ec5\u51ed\u8bed\u4e49\u4fe1\u606f\u6a21\u578b\u4ecd\u80fd\u6709\u6548\u68c0\u6d4bAD\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u4f1a\u5bfc\u81f4\u8bed\u8a00\u80fd\u529b\u4e0b\u964d\uff0c\u8bed\u8a00\u6a21\u578b\u6709\u671b\u7528\u4e8eAD\u7b5b\u67e5\uff0c\u4f46\u5176\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u96be\u4ee5\u533a\u5206\u771f\u6b63\u7684\u8ba4\u77e5\u8870\u9000\u8bed\u8a00\u6807\u8bb0\u4e0e\u8868\u9762\u6587\u672c\u6a21\u5f0f\u3002\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u662f\u5426\u771f\u6b63\u6355\u6349\u5e95\u5c42\u8bed\u4e49\u6307\u6807\u3002", "method": "\u63d0\u51fa\u65b0\u9896\u65b9\u6cd5\uff1a\u901a\u8fc7\u6539\u53d8\u53e5\u6cd5\u548c\u8bcd\u6c47\u4f46\u4fdd\u6301\u8bed\u4e49\u5185\u5bb9\u6765\u8f6c\u6362\u6587\u672c\u8868\u9762\u5f62\u5f0f\u3002\u4f7f\u7528BLEU\u3001chrF\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8bc4\u5206\u9a8c\u8bc1\u8f6c\u6362\u6548\u679c\u3002\u540c\u65f6\u7814\u7a76\u56fe\u7247\u63cf\u8ff0\u8bed\u8a00\u662f\u5426\u5305\u542b\u8db3\u591f\u7ec6\u8282\u7528\u4e8e\u56fe\u50cf\u91cd\u5efa\uff0c\u5e76\u5206\u6790\u56fe\u50cf\u8f6c\u6362\u5bf9\u5206\u7c7b\u7684\u5f71\u54cd\u3002", "result": "\u6587\u672c\u8f6c\u6362\u663e\u8457\u6539\u53d8\u4e86\u7ed3\u6784\u548c\u8bcd\u6c47\u5185\u5bb9\uff08\u4f4eBLEU/chrF\u8bc4\u5206\uff09\uff0c\u4f46\u4fdd\u6301\u4e86\u5e95\u5c42\u8bed\u4e49\uff08\u9ad8\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff09\u3002\u6a21\u578b\u5728\u8f6c\u6362\u6587\u672c\u4e0a\u7684\u8868\u73b0\u4e0e\u539f\u59cb\u6587\u672c\u76f8\u4f3c\uff08\u4ec5\u5fae\u5c0fF1\u504f\u5dee\uff09\u3002\u56fe\u50cf\u8f6c\u6362\u4f1a\u6dfb\u52a0\u5927\u91cf\u566a\u58f0\u964d\u4f4e\u5206\u7c7b\u51c6\u786e\u7387\u3002\u4ec5\u4f7f\u7528\u8bed\u4e49\u4fe1\u606f\uff0c\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u5206\u7c7b\u5668\u4ecd\u80fd\u68c0\u6d4bAD\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u89c6\u89d2\u6765\u5206\u6790\u5f71\u54cd\u6a21\u578b\u9884\u6d4b\u7684\u7279\u5f81\uff0c\u80fd\u591f\u6d88\u9664\u53ef\u80fd\u7684\u865a\u5047\u76f8\u5173\u6027\u3002\u7814\u7a76\u8868\u660e\u96be\u4ee5\u68c0\u6d4b\u7684\u8bed\u4e49\u635f\u4f24\u53ef\u4ee5\u88ab\u8bc6\u522b\uff0c\u8fd9\u89e3\u51b3\u4e86\u8bed\u8a00\u9000\u5316\u4e2d\u88ab\u5ffd\u89c6\u7684\u7279\u5f81\uff0c\u4e3a\u65e9\u671f\u68c0\u6d4b\u7cfb\u7edf\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
