{"id": "2511.21886", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21886", "abs": "https://arxiv.org/abs/2511.21886", "authors": ["Jingtian Yan", "Shuai Zhou", "Stephen F. Smith", "Jiaoyang Li"], "title": "Bridging Planning and Execution: Multi-Agent Path Finding Under Real-World Deadlines", "comment": null, "summary": "The Multi-Agent Path Finding (MAPF) problem aims to find collision-free paths for multiple agents while optimizing objectives such as the sum of costs or makespan. MAPF has wide applications in domains like automated warehouses, manufacturing systems, and airport logistics. However, most MAPF formulations assume a simplified robot model for planning, which overlooks execution-time factors such as kinodynamic constraints, communication latency, and controller variability. This gap between planning and execution is problematic for time-sensitive applications. To bridge this gap, we propose REMAP, an execution-informed MAPF planning framework that can be combined with leading search-based MAPF planners with minor changes. Our framework integrates the proposed ExecTimeNet to accurately estimate execution time based on planned paths. We demonstrate our method for solving MAPF with Real-world Deadlines (MAPF-RD) problem, where agents must reach their goals before a predefined wall-clock time. We integrate our framework with two popular MAPF methods, MAPF-LNS and CBS. Experiments show that REMAP achieves up to 20% improvement in solution quality over baseline methods (e.g., constant execution speed estimators) on benchmark maps with up to 300 agents.", "AI": {"tldr": "REMAP\u662f\u4e00\u4e2a\u6267\u884c\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7ExecTimeNet\u51c6\u786e\u4f30\u8ba1\u6267\u884c\u65f6\u95f4\uff0c\u89e3\u51b3\u89c4\u5212\u4e0e\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5728MAPF-RD\u95ee\u9898\u4e0a\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534720%\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfMAPF\u95ee\u9898\u5047\u8bbe\u7b80\u5316\u7684\u673a\u5668\u4eba\u6a21\u578b\uff0c\u5ffd\u7565\u4e86\u6267\u884c\u65f6\u7684\u52a8\u529b\u5b66\u7ea6\u675f\u3001\u901a\u4fe1\u5ef6\u8fdf\u548c\u63a7\u5236\u5668\u53d8\u5f02\u6027\u7b49\u5b9e\u9645\u56e0\u7d20\uff0c\u5bfc\u81f4\u89c4\u5212\u4e0e\u6267\u884c\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u8fd9\u5bf9\u65f6\u95f4\u654f\u611f\u5e94\u7528\u5c24\u5176\u6210\u95ee\u9898\u3002", "method": "\u63d0\u51faREMAP\u6267\u884c\u611f\u77e5MAPF\u89c4\u5212\u6846\u67b6\uff0c\u96c6\u6210ExecTimeNet\u795e\u7ecf\u7f51\u7edc\u51c6\u786e\u4f30\u8ba1\u57fa\u4e8e\u89c4\u5212\u8def\u5f84\u7684\u6267\u884c\u65f6\u95f4\uff0c\u53ef\u4e0e\u4e3b\u6d41\u641c\u7d22\u5f0fMAPF\u89c4\u5212\u5668\uff08\u5982MAPF-LNS\u548cCBS\uff09\u7ed3\u5408\u4f7f\u7528\uff0c\u4ec5\u9700\u5c11\u91cf\u4fee\u6539\u3002", "result": "\u5728\u6700\u591a300\u4e2a\u667a\u80fd\u4f53\u7684\u57fa\u51c6\u5730\u56fe\u4e0a\uff0cREMAP\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982\u6052\u5b9a\u6267\u884c\u901f\u5ea6\u4f30\u8ba1\u5668\uff09\u5728\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e0a\u63d0\u5347\u9ad8\u8fbe20%\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5177\u6709\u73b0\u5b9e\u4e16\u754c\u622a\u6b62\u65f6\u95f4\u7684MAPF-RD\u95ee\u9898\u3002", "conclusion": "REMAP\u6846\u67b6\u6210\u529f\u5f25\u5408\u4e86MAPF\u89c4\u5212\u4e0e\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u51c6\u786e\u4f30\u8ba1\u6267\u884c\u65f6\u95f4\uff0c\u4e3a\u65f6\u95f4\u654f\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4e0e\u73b0\u6709MAPF\u65b9\u6cd5\u6709\u6548\u96c6\u6210\u3002"}}
{"id": "2511.21925", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.21925", "abs": "https://arxiv.org/abs/2511.21925", "authors": ["Alex Richardson", "Jonathan Sprinkle"], "title": "OpenTwinMap: An Open-Source Digital Twin Generator for Urban Autonomous Driving", "comment": null, "summary": "Digital twins of urban environments play a critical role in advancing autonomous vehicle (AV) research by enabling simulation, validation, and integration with emerging generative world models. While existing tools have demonstrated value, many publicly available solutions are tightly coupled to specific simulators, difficult to extend, or introduce significant technical overhead. For example, CARLA-the most widely used open-source AV simulator-provides a digital twin framework implemented entirely as an Unreal Engine C++ plugin, limiting flexibility and rapid prototyping. In this work, we propose OpenTwinMap, an open-source, Python-based framework for generating high-fidelity 3D urban digital twins. The completed framework will ingest LiDAR scans and OpenStreetMap (OSM) data to produce semantically segmented static environment assets, including road networks, terrain, and urban structures, which can be exported into Unreal Engine for AV simulation. OpenTwinMap emphasizes extensibility and parallelization, lowering the barrier for researchers to adapt and scale the pipeline to diverse urban contexts. We describe the current capabilities of the OpenTwinMap, which includes preprocessing of OSM and LiDAR data, basic road mesh and terrain generation, and preliminary support for CARLA integration.", "AI": {"tldr": "OpenTwinMap\u662f\u4e00\u4e2a\u5f00\u6e90\u7684Python\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f3D\u57ce\u5e02\u6570\u5b57\u5b6a\u751f\uff0c\u65e8\u5728\u964d\u4f4e\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u4e2d\u7684\u4eff\u771f\u95e8\u69db\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u4e2d\uff0c\u8bb8\u591a\u516c\u5f00\u53ef\u7528\u7684\u6570\u5b57\u5b6a\u751f\u5de5\u5177\u4e0e\u7279\u5b9a\u4eff\u771f\u5668\u7d27\u5bc6\u8026\u5408\u3001\u96be\u4ee5\u6269\u5c55\u6216\u6280\u672f\u5f00\u9500\u5927\u3002\u4f8b\u5982CARLA\u5b8c\u5168\u4f5c\u4e3aUnreal Engine C++\u63d2\u4ef6\u5b9e\u73b0\uff0c\u9650\u5236\u4e86\u7075\u6d3b\u6027\u548c\u5feb\u901f\u539f\u578b\u5f00\u53d1\u3002", "method": "\u57fa\u4e8ePython\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u901a\u8fc7\u5904\u7406LiDAR\u626b\u63cf\u548cOpenStreetMap\u6570\u636e\uff0c\u751f\u6210\u8bed\u4e49\u5206\u5272\u7684\u9759\u6001\u73af\u5883\u8d44\u4ea7\uff08\u9053\u8def\u7f51\u7edc\u3001\u5730\u5f62\u3001\u57ce\u5e02\u7ed3\u6784\uff09\uff0c\u53ef\u5bfc\u51fa\u5230Unreal Engine\u8fdb\u884c\u81ea\u52a8\u9a7e\u9a76\u4eff\u771f\u3002", "result": "\u76ee\u524d\u6846\u67b6\u5df2\u5177\u5907OSM\u548cLiDAR\u6570\u636e\u9884\u5904\u7406\u3001\u57fa\u672c\u9053\u8def\u7f51\u683c\u548c\u5730\u5f62\u751f\u6210\u529f\u80fd\uff0c\u5e76\u521d\u6b65\u652f\u6301CARLA\u96c6\u6210\u3002\u5f3a\u8c03\u53ef\u6269\u5c55\u6027\u548c\u5e76\u884c\u5316\u80fd\u529b\u3002", "conclusion": "OpenTwinMap\u901a\u8fc7\u63d0\u4f9b\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684Python\u6846\u67b6\uff0c\u964d\u4f4e\u4e86\u7814\u7a76\u4eba\u5458\u5728\u4e0d\u540c\u57ce\u5e02\u73af\u5883\u4e2d\u9002\u5e94\u548c\u6269\u5c55\u6570\u5b57\u5b6a\u751f\u7ba1\u9053\u7684\u95e8\u69db\uff0c\u6709\u52a9\u4e8e\u63a8\u8fdb\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u3002"}}
{"id": "2511.21957", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.21957", "abs": "https://arxiv.org/abs/2511.21957", "authors": ["Cahit Ikbal Er", "Amin Kashiri", "Yasin Yazicioglu"], "title": "RSPECT: Robust and Scalable Planner for Energy-Aware Coordination of UAV-UGV Teams in Aerial Monitoring", "comment": null, "summary": "We consider the robust planning of energy-constrained unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs), which act as mobile charging stations, to perform long-horizon aerial monitoring missions. More specifically, given a set of points to be visited by the UAVs and desired final positions of the UAV-UGV teams, the objective is to find a robust plan (the vehicle trajectories) that can be realized without a major revision in the face of uncertainty (e.g., unknown obstacles/terrain, wind) to complete this mission in minimum time. We provide a formal description of this problem as a mixed-integer program (MIP), which is NP-hard. Since exact solution methods are computationally intractable for such problems, we propose RSPECT, a scalable and efficient heuristic. We provide theoretical results on the complexity of our algorithm and the feasibility and robustness of resulting plans. We also demonstrate the performance of our method via simulations and experiments.", "AI": {"tldr": "\u63d0\u51faRSPECT\u7b97\u6cd5\uff0c\u7528\u4e8e\u89c4\u5212\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u5145\u7535\u8f66\u7684\u534f\u540c\u8def\u5f84\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u5b8c\u6210\u957f\u671f\u7a7a\u4e2d\u76d1\u6d4b\u4efb\u52a1\uff0c\u5b9e\u73b0\u6700\u5c0f\u5316\u4efb\u52a1\u65f6\u95f4\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u5145\u7535\u8f66\u534f\u540c\u6267\u884c\u957f\u671f\u7a7a\u4e2d\u76d1\u6d4b\u4efb\u52a1\u65f6\u7684\u9c81\u68d2\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002\u65e0\u4eba\u673a\u80fd\u91cf\u6709\u9650\uff0c\u9700\u8981\u5730\u9762\u5145\u7535\u8f66\u4f5c\u4e3a\u79fb\u52a8\u5145\u7535\u7ad9\uff0c\u540c\u65f6\u9762\u4e34\u672a\u77e5\u969c\u788d\u3001\u5730\u5f62\u3001\u98ce\u529b\u7b49\u4e0d\u786e\u5b9a\u6027\u56e0\u7d20\uff0c\u9700\u8981\u5236\u5b9a\u65e0\u9700\u91cd\u5927\u4fee\u6539\u7684\u9c81\u68d2\u8ba1\u5212\u3002", "method": "\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u95ee\u9898\uff0c\u7531\u4e8e\u95ee\u9898NP\u96be\uff0c\u63d0\u51faRSPECT\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u9ad8\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u7b97\u6cd5\u590d\u6742\u5ea6\u3001\u53ef\u884c\u6027\u548c\u9c81\u68d2\u6027\u7684\u7406\u8bba\u5206\u6790\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86RSPECT\u7b97\u6cd5\u7684\u6027\u80fd\u3002\u7b97\u6cd5\u80fd\u591f\u5728\u4e0d\u786e\u5b9a\u6027\u73af\u5883\u4e0b\u751f\u6210\u9c81\u68d2\u7684\u65e0\u4eba\u673a-\u5730\u9762\u8f66\u534f\u540c\u8def\u5f84\uff0c\u5b9e\u73b0\u6700\u5c0f\u5316\u4efb\u52a1\u65f6\u95f4\u7684\u76ee\u6807\u3002", "conclusion": "RSPECT\u7b97\u6cd5\u4e3a\u80fd\u91cf\u53d7\u9650\u7684\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u5145\u7535\u8f66\u534f\u540c\u6267\u884c\u957f\u671f\u7a7a\u4e2d\u76d1\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9c81\u68d2\u8def\u5f84\u89c4\u5212\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5e94\u5bf9\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.22042", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22042", "abs": "https://arxiv.org/abs/2511.22042", "authors": ["Lei Li", "Jiale Gong", "Ziyang Li", "Hong Wang"], "title": "Constant-Volume Deformation Manufacturing for Material-Efficient Shaping", "comment": "46 pages, 27 figures", "summary": "Additive and subtractive manufacturing enable complex geometries but rely on discrete stacking or local removal, limiting continuous and controllable deformation and causing volume loss and shape deviations. We present a volumepreserving digital-mold paradigm that integrates real-time volume-consistency modeling with geometry-informed deformation prediction and an error-compensation strategy to achieve highly predictable shaping of plastic materials. By analyzing deformation patterns and error trends from post-formed point clouds, our method corrects elastic rebound and accumulation errors, maintaining volume consistency and surface continuity. Experiments on five representative geometries demonstrate that the system reproduces target shapes with high fidelity while achieving over 98% material utilization. This approach establishes a digitally driven, reproducible pathway for sustainable, zero-waste shaping of user-defined designs, bridging digital modeling, real-time sensing, and adaptive forming, and advancing next-generation sustainable and customizable manufacturing.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4f53\u79ef\u4fdd\u6301\u7684\u6570\u5b57\u6a21\u5177\u8303\u5f0f\uff0c\u901a\u8fc7\u5b9e\u65f6\u4f53\u79ef\u4e00\u81f4\u6027\u5efa\u6a21\u3001\u51e0\u4f55\u4fe1\u606f\u53d8\u5f62\u9884\u6d4b\u548c\u8bef\u5dee\u8865\u507f\u7b56\u7565\uff0c\u5b9e\u73b0\u5851\u6599\u6750\u6599\u7684\u9ad8\u53ef\u9884\u6d4b\u6027\u6210\u5f62\uff0c\u8fbe\u523098%\u4ee5\u4e0a\u7684\u6750\u6599\u5229\u7528\u7387\u3002", "motivation": "\u4f20\u7edf\u589e\u6750\u548c\u51cf\u6750\u5236\u9020\u4f9d\u8d56\u79bb\u6563\u5806\u53e0\u6216\u5c40\u90e8\u53bb\u9664\uff0c\u9650\u5236\u4e86\u8fde\u7eed\u53ef\u63a7\u53d8\u5f62\uff0c\u5bfc\u81f4\u4f53\u79ef\u635f\u5931\u548c\u5f62\u72b6\u504f\u5dee\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4fdd\u6301\u4f53\u79ef\u4e00\u81f4\u6027\u7684\u53ef\u6301\u7eed\u3001\u96f6\u6d6a\u8d39\u7684\u6210\u5f62\u65b9\u6cd5\u3002", "method": "\u96c6\u6210\u5b9e\u65f6\u4f53\u79ef\u4e00\u81f4\u6027\u5efa\u6a21\u3001\u51e0\u4f55\u4fe1\u606f\u53d8\u5f62\u9884\u6d4b\u548c\u8bef\u5dee\u8865\u507f\u7b56\u7565\u7684\u6570\u5b57\u6a21\u5177\u8303\u5f0f\uff0c\u901a\u8fc7\u5206\u6790\u6210\u5f62\u540e\u70b9\u4e91\u7684\u53d8\u5f62\u6a21\u5f0f\u548c\u8bef\u5dee\u8d8b\u52bf\uff0c\u6821\u6b63\u5f39\u6027\u56de\u5f39\u548c\u7d2f\u79ef\u8bef\u5dee\u3002", "result": "\u5728\u4e94\u79cd\u4ee3\u8868\u6027\u51e0\u4f55\u5f62\u72b6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u591f\u9ad8\u4fdd\u771f\u5730\u590d\u73b0\u76ee\u6807\u5f62\u72b6\uff0c\u540c\u65f6\u5b9e\u73b0\u8d85\u8fc798%\u7684\u6750\u6599\u5229\u7528\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5efa\u7acb\u4e86\u6570\u5b57\u9a71\u52a8\u3001\u53ef\u91cd\u590d\u7684\u53ef\u6301\u7eed\u96f6\u6d6a\u8d39\u6210\u5f62\u8def\u5f84\uff0c\u8fde\u63a5\u4e86\u6570\u5b57\u5efa\u6a21\u3001\u5b9e\u65f6\u4f20\u611f\u548c\u81ea\u9002\u5e94\u6210\u5f62\uff0c\u63a8\u52a8\u4e86\u4e0b\u4e00\u4ee3\u53ef\u6301\u7eed\u548c\u53ef\u5b9a\u5236\u5236\u9020\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.21695", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.21695", "abs": "https://arxiv.org/abs/2511.21695", "authors": ["Ruchira Dhar", "Danae Sanchez Villegas", "Antonia Karamolegkou", "Alice Schiavone", "Yifei Yuan", "Xinyi Chen", "Jiaang Li", "Stella Frank", "Laura De Grazia", "Monorama Swain", "Stephanie Brandl", "Daniel Hershcovich", "Anders S\u00f8gaard", "Desmond Elliott"], "title": "EvalCards: A Framework for Standardized Evaluation Reporting", "comment": "Under review", "summary": "Evaluation has long been a central concern in NLP, and transparent reporting practices are more critical than ever in today's landscape of rapidly released open-access models. Drawing on a survey of recent work on evaluation and documentation, we identify three persistent shortcomings in current reporting practices: reproducibility, accessibility, and governance. We argue that existing standardization efforts remain insufficient and introduce Evaluation Disclosure Cards (EvalCards) as a path forward. EvalCards are designed to enhance transparency for both researchers and practitioners while providing a practical foundation to meet emerging governance requirements.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEvalCards\u4f5c\u4e3a\u89e3\u51b3\u5f53\u524dNLP\u8bc4\u4f30\u62a5\u544a\u5b9e\u8df5\u4e0d\u8db3\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u9ad8\u8bc4\u4f30\u7684\u900f\u660e\u5ea6\u3001\u53ef\u590d\u73b0\u6027\u548c\u6cbb\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524dNLP\u8bc4\u4f30\u62a5\u544a\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u53ef\u590d\u73b0\u6027\u4e0d\u8db3\u3001\u53ef\u8bbf\u95ee\u6027\u5dee\u548c\u6cbb\u7406\u673a\u5236\u7f3a\u5931\u3002\u968f\u7740\u5f00\u6e90\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u900f\u660e\u62a5\u544a\u5b9e\u8df5\u53d8\u5f97\u6bd4\u4ee5\u5f80\u4efb\u4f55\u65f6\u5019\u90fd\u66f4\u52a0\u91cd\u8981\u3002", "method": "\u5f15\u5165Evaluation Disclosure Cards (EvalCards)\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002EvalCards\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u589e\u5f3a\u7684\u900f\u660e\u5ea6\uff0c\u540c\u65f6\u4e3a\u6ee1\u8db3\u65b0\u5174\u6cbb\u7406\u8981\u6c42\u63d0\u4f9b\u5b9e\u7528\u57fa\u7840\u3002", "result": "\u8bba\u6587\u8ba4\u4e3a\u73b0\u6709\u6807\u51c6\u5316\u52aa\u529b\u4ecd\u7136\u4e0d\u8db3\uff0cEvalCards\u4e3a\u6539\u5584\u8bc4\u4f30\u62a5\u544a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u8def\u5f84\u3002", "conclusion": "EvalCards\u662f\u63a8\u52a8NLP\u8bc4\u4f30\u62a5\u544a\u5b9e\u8df5\u5411\u524d\u53d1\u5c55\u7684\u91cd\u8981\u5de5\u5177\uff0c\u80fd\u591f\u89e3\u51b3\u5f53\u524d\u62a5\u544a\u5b9e\u8df5\u4e2d\u7684\u5173\u952e\u7f3a\u9677\uff0c\u4fc3\u8fdb\u66f4\u900f\u660e\u3001\u53ef\u590d\u73b0\u548c\u53ef\u6cbb\u7406\u7684\u8bc4\u4f30\u8fc7\u7a0b\u3002"}}
{"id": "2511.22043", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22043", "abs": "https://arxiv.org/abs/2511.22043", "authors": ["Xuchen Liu", "Ruocheng Li", "Bin Xin", "Weijia Yao", "Qigeng Duan", "Jinqiang Cui", "Ben M. Chen", "Jie Chen"], "title": "SwordRiding: A Unified Navigation Framework for Quadrotors in Unknown Complex Environments via Online Guiding Vector Fields", "comment": "For an experimental demo, see https://www.youtube.com/watch?v=tKYCg266c4o. For the lemma proof, see https://github.com/SmartGroupSystems/GVF_close_loop_planning/blob/main/proofs.md", "summary": "Although quadrotor navigation has achieved high performance in trajectory planning and control, real-time adaptability in unknown complex environments remains a core challenge. This difficulty mainly arises because most existing planning frameworks operate in an open-loop manner, making it hard to cope with environmental uncertainties such as wind disturbances or external perturbations. This paper presents a unified real-time navigation framework for quadrotors in unknown complex environments, based on the online construction of guiding vector fields (GVFs) from discrete reference path points. In the framework, onboard perception modules build a Euclidean Signed Distance Field (ESDF) representation of the environment, which enables obstacle awareness and path distance evaluation. The system first generates discrete, collision-free path points using a global planner, and then parameterizes them via uniform B-splines to produce a smooth and physically feasible reference trajectory. An adaptive GVF is then synthesized from the ESDF and the optimized B-spline trajectory. Unlike conventional approaches, the method adopts a closed-loop navigation paradigm, which significantly enhances robustness under external disturbances. Compared with conventional GVF methods, the proposed approach directly accommodates discretized paths and maintains compatibility with standard planning algorithms. Extensive simulations and real-world experiments demonstrate improved robustness against external disturbances and superior real-time performance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f15\u5bfc\u5411\u91cf\u573a\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5b9e\u65f6\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u6784\u5efaGVF\u5b9e\u73b0\u672a\u77e5\u590d\u6742\u73af\u5883\u4e2d\u7684\u95ed\u73af\u5bfc\u822a\uff0c\u63d0\u9ad8\u6297\u5e72\u6270\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u56db\u65cb\u7ffc\u5bfc\u822a\u6846\u67b6\u591a\u4e3a\u5f00\u73af\u8fd0\u884c\uff0c\u96be\u4ee5\u5e94\u5bf9\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff08\u5982\u98ce\u6270\u3001\u5916\u90e8\u6270\u52a8\uff09\uff0c\u5728\u672a\u77e5\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u65f6\u9002\u5e94\u6027\u4e0d\u8db3\u3002", "method": "1) \u673a\u8f7d\u611f\u77e5\u6a21\u5757\u6784\u5efaESDF\u73af\u5883\u8868\u793a\uff1b2) \u5168\u5c40\u89c4\u5212\u5668\u751f\u6210\u79bb\u6563\u65e0\u78b0\u649e\u8def\u5f84\u70b9\uff1b3) \u5747\u5300B\u6837\u6761\u53c2\u6570\u5316\u751f\u6210\u5e73\u6ed1\u53c2\u8003\u8f68\u8ff9\uff1b4) \u4eceESDF\u548c\u4f18\u5316B\u6837\u6761\u8f68\u8ff9\u5408\u6210\u81ea\u9002\u5e94GVF\uff1b5) \u91c7\u7528\u95ed\u73af\u5bfc\u822a\u8303\u5f0f\u3002", "result": "\u5927\u91cf\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5bf9\u5916\u90e8\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5177\u6709\u4f18\u8d8a\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eGVF\u7684\u7edf\u4e00\u5b9e\u65f6\u5bfc\u822a\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u56db\u65cb\u7ffc\u5728\u672a\u77e5\u590d\u6742\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u9002\u5e94\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u95ed\u73af\u5bfc\u822a\u8303\u5f0f\u663e\u8457\u589e\u5f3a\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.21699", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21699", "abs": "https://arxiv.org/abs/2511.21699", "authors": ["Zhiyao Ma", "In Gim", "Lin Zhong"], "title": "Cacheback: Speculative Decoding With Nothing But Cache", "comment": null, "summary": "We present Cacheback Decoding, a training-free and model-agnostic speculative decoding method that exploits the locality in language to accelerate Large Language Model (LLM) inference. Cacheback leverages only Least Recently Used (LRU) cache tables of token n-grams to generate draft sequences. Cacheback achieves state-of-the-art performance among comparable methods despite its minimalist design, and its simplicity allows easy integration into existing systems. Cacheback also shows potential for fast adaptation to new domains.", "AI": {"tldr": "Cacheback Decoding\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u5229\u7528\u8bed\u8a00\u4e2d\u7684\u5c40\u90e8\u6027\u6765\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u4ec5\u4f7f\u7528LRU\u7f13\u5b58\u8868\u751f\u6210\u8349\u7a3f\u5e8f\u5217\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u901f\u5ea6\u8f83\u6162\uff0c\u9700\u8981\u52a0\u901f\u65b9\u6cd5\u3002\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u989d\u5916\u8bad\u7ec3\u6216\u590d\u6742\u8bbe\u8ba1\uff0c\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u7b80\u5355\u3001\u65e0\u9700\u8bad\u7ec3\u4e14\u6a21\u578b\u65e0\u5173\u7684\u52a0\u901f\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u8bed\u8a00\u4e2d\u7684\u5c40\u90e8\u6027\uff0c\u4ec5\u4f7f\u7528\u6700\u8fd1\u6700\u5c11\u4f7f\u7528\uff08LRU\uff09\u7f13\u5b58\u8868\u6765\u5b58\u50a8token n-gram\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u7f13\u5b58\u751f\u6210\u63a8\u6d4b\u6027\u8349\u7a3f\u5e8f\u5217\uff0c\u7136\u540e\u7531\u4e3b\u6a21\u578b\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u53ef\u6bd4\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c3d\u7ba1\u8bbe\u8ba1\u6781\u7b80\u4f46\u6548\u679c\u663e\u8457\uff0c\u4e14\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u7cfb\u7edf\u4e2d\uff0c\u8fd8\u663e\u793a\u51fa\u5feb\u901f\u9002\u5e94\u65b0\u9886\u57df\u7684\u6f5c\u529b\u3002", "conclusion": "Cacheback Decoding\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u65e0\u9700\u8bad\u7ec3\u4e14\u6a21\u578b\u65e0\u5173\uff0c\u80fd\u591f\u663e\u8457\u52a0\u901fLLM\u63a8\u7406\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u7684\u4fbf\u5229\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2511.22087", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22087", "abs": "https://arxiv.org/abs/2511.22087", "authors": ["Tai Inui", "Jee-Hwan Ryu"], "title": "SoftNash: Entropy-Regularized Nash Games for Non-Fighting Virtual Fixtures", "comment": null, "summary": "Virtual fixtures (VFs) improve precision in teleoperation but often ``fight'' the user, inflating mental workload and eroding the sense of agency. We propose Soft-Nash Virtual Fixtures, a game-theoretic shared-control policy that softens the classic two-player linear-quadratic (LQ) Nash solution by inflating the fixture's effort weight with a single, interpretable scalar parameter $\u03c4$. This yields a continuous dial on controller assertiveness: $\u03c4=0$ recovers a hard, performance-focused Nash / virtual fixture controller, while larger $\u03c4$ reduce gains and pushback, yet preserve the equilibrium structure and continuity of closed-loop stability. We derive Soft-Nash from both a KL-regularized trust-region and a maximum-entropy viewpoint, obtaining a closed-form robot best response that shrinks authority and aligns the fixture with the operator's input as $\u03c4$ grows. We implement Soft-Nash on a 6-DoF haptic device in 3D tracking task ($n=12$). Moderate softness ($\u03c4\\approx 1-3$, especially $\u03c4=2$) maintains tracking error statistically indistinguishable from a tuned classic VF while sharply reducing controller-user conflict, lowering NASA-TLX workload, and increasing Sense of Agency (SoAS). A composite BalancedScore that combines normalized accuracy and non-fighting behavior peaks near $\u03c4=2-3$. These results show that a one-parameter Soft-Nash policy can preserve accuracy while improving comfort and perceived agency, providing a practical and interpretable pathway to personalized shared control in haptics and teleoperation.", "AI": {"tldr": "\u63d0\u51faSoft-Nash\u865a\u62df\u5939\u5177\uff0c\u901a\u8fc7\u5355\u4e00\u53ef\u89e3\u91ca\u53c2\u6570\u03c4\u8f6f\u5316\u4f20\u7edf\u865a\u62df\u5939\u5177\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u51cf\u5c11\u7528\u6237\u51b2\u7a81\u3001\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\u5e76\u63d0\u5347\u4ee3\u7406\u611f", "motivation": "\u4f20\u7edf\u865a\u62df\u5939\u5177\u867d\u7136\u80fd\u63d0\u9ad8\u9065\u64cd\u4f5c\u7cbe\u5ea6\uff0c\u4f46\u4f1a\u4e0e\u7528\u6237\u4ea7\u751f\"\u5bf9\u6297\"\uff0c\u589e\u52a0\u8ba4\u77e5\u8d1f\u8377\u5e76\u524a\u5f31\u4ee3\u7406\u611f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5e73\u8861\u7cbe\u5ea6\u4e0e\u7528\u6237\u4f53\u9a8c\u7684\u5171\u4eab\u63a7\u5236\u7b56\u7565", "method": "\u57fa\u4e8e\u535a\u5f08\u8bba\u7684Soft-Nash\u5171\u4eab\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u5355\u4e00\u53c2\u6570\u03c4\u81a8\u80c0\u5939\u5177\u7684\u52aa\u529b\u6743\u91cd\uff0c\u4eceKL\u6b63\u5219\u5316\u4fe1\u4efb\u533a\u57df\u548c\u6700\u5927\u71b5\u89c6\u89d2\u63a8\u5bfc\uff0c\u83b7\u5f97\u673a\u5668\u4eba\u6700\u4f73\u54cd\u5e94\u7684\u95ed\u5f0f\u89e3", "result": "\u57286\u81ea\u7531\u5ea6\u89e6\u89c9\u8bbe\u5907\u4e0a\u76843D\u8ddf\u8e2a\u4efb\u52a1\u4e2d\uff0c\u9002\u5ea6\u8f6f\u5316\uff08\u03c4\u22481-3\uff0c\u7279\u522b\u662f\u03c4=2\uff09\u4fdd\u6301\u8ddf\u8e2a\u8bef\u5dee\u4e0e\u4f20\u7edf\u865a\u62df\u5939\u5177\u65e0\u7edf\u8ba1\u5dee\u5f02\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u51b2\u7a81\u3001\u964d\u4f4eNASA-TLX\u8d1f\u8377\u3001\u63d0\u5347\u4ee3\u7406\u611f", "conclusion": "\u5355\u53c2\u6570Soft-Nash\u7b56\u7565\u80fd\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u6539\u5584\u8212\u9002\u5ea6\u548c\u611f\u77e5\u4ee3\u7406\u611f\uff0c\u4e3a\u89e6\u89c9\u548c\u9065\u64cd\u4f5c\u4e2d\u7684\u4e2a\u6027\u5316\u5171\u4eab\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u9014\u5f84"}}
{"id": "2511.21700", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21700", "abs": "https://arxiv.org/abs/2511.21700", "authors": ["Yuhao Zhan", "Yuqing Zhang", "Jing Yuan", "Qixiang Ma", "Zhiqi Yang", "Yu Gu", "Zemin Liu", "Fei Wu"], "title": "JELV: A Judge of Edit-Level Validity for Evaluation and Automated Reference Expansion in Grammatical Error Correction", "comment": null, "summary": "Existing Grammatical Error Correction (GEC) systems suffer from limited reference diversity, leading to underestimated evaluation and restricted model generalization. To address this issue, we introduce the Judge of Edit-Level Validity (JELV), an automated framework to validate correction edits from grammaticality, faithfulness, and fluency. Using our proposed human-annotated Pair-wise Edit-level Validity Dataset (PEVData) as benchmark, JELV offers two implementations: a multi-turn LLM-as-Judges pipeline achieving 90% agreement with human annotators, and a distilled DeBERTa classifier with 85% precision on valid edits. We then apply JELV to reclassify misjudged false positives in evaluation and derive a comprehensive evaluation metric by integrating false positive decoupling and fluency scoring, resulting in state-of-the-art correlation with human judgments. We also apply JELV to filter LLM-generated correction candidates, expanding the BEA19's single-reference dataset containing 38,692 source sentences. Retraining top GEC systems on this expanded dataset yields measurable performance gains. JELV provides a scalable solution for enhancing reference diversity and strengthening both evaluation and model generalization.", "AI": {"tldr": "\u63d0\u51faJELV\u6846\u67b6\uff0c\u901a\u8fc7\u7f16\u8f91\u7ea7\u6709\u6548\u6027\u9a8c\u8bc1\u89e3\u51b3GEC\u7cfb\u7edf\u53c2\u8003\u591a\u6837\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u8bed\u6cd5\u9519\u8bef\u7ea0\u6b63\u7cfb\u7edf\u56e0\u53c2\u8003\u591a\u6837\u6027\u6709\u9650\uff0c\u5bfc\u81f4\u8bc4\u4f30\u88ab\u4f4e\u4f30\u4e14\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u53d7\u9650", "method": "\u63d0\u51faJELV\u6846\u67b6\uff0c\u5305\u542b\u591a\u8f6eLLM-as-Judges\u7ba1\u9053\u548c\u84b8\u998f\u7684DeBERTa\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u9a8c\u8bc1\u8bed\u6cd5\u6027\u3001\u5fe0\u5b9e\u5ea6\u548c\u6d41\u7545\u6027\u4e09\u4e2a\u7ef4\u5ea6\u7684\u7f16\u8f91\u6709\u6548\u6027", "result": "JELV\u4e0e\u4eba\u5de5\u6807\u6ce8\u8005\u8fbe\u621090%\u4e00\u81f4\u7387\uff0cDeBERTa\u5206\u7c7b\u5668\u5bf9\u6709\u6548\u7f16\u8f91\u8fbe\u523085%\u7cbe\u786e\u5ea6\uff1b\u5e94\u7528JELV\u91cd\u65b0\u5206\u7c7b\u8bef\u5224\u5047\u9633\u6027\u5e76\u6574\u5408\u5047\u9633\u6027\u89e3\u8026\u548c\u6d41\u7545\u5ea6\u8bc4\u5206\uff0c\u83b7\u5f97\u4e0e\u4eba\u5de5\u5224\u65ad\u6700\u5148\u8fdb\u7684\u76f8\u5173\u6027\uff1b\u6269\u5c55BEA19\u6570\u636e\u96c6\u540e\u91cd\u8bad\u7ec3GEC\u7cfb\u7edf\u83b7\u5f97\u53ef\u6d4b\u91cf\u7684\u6027\u80fd\u63d0\u5347", "conclusion": "JELV\u4e3a\u589e\u5f3a\u53c2\u8003\u591a\u6837\u6027\u548c\u52a0\u5f3a\u8bc4\u4f30\u4e0e\u6a21\u578b\u6cdb\u5316\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.22100", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22100", "abs": "https://arxiv.org/abs/2511.22100", "authors": ["Zelong Zhou", "Wenrui Chen", "Zeyun Hu", "Qiang Diao", "Qixin Gao", "Yaonan Wang"], "title": "Design of an Adaptive Modular Anthropomorphic Dexterous Hand for Human-like Manipulation", "comment": "7 pages, 8 figures", "summary": "Biological synergies have emerged as a widely adopted paradigm for dexterous hand design, enabling human-like manipulation with a small number of actuators. Nonetheless, excessive coupling tends to diminish the dexterity of hands. This paper tackles the trade-off between actuation complexity and dexterity by proposing an anthropomorphic finger topology with 4 DoFs driven by 2 actuators, and by developing an adaptive, modular dexterous hand based on this finger topology. We explore the biological basis of hand synergies and human gesture analysis, translating joint-level coordination and structural attributes into a modular finger architecture. Leveraging these biomimetic mappings, we design a five-finger modular hand and establish its kinematic model to analyze adaptive grasping and in-hand manipulation. Finally, we construct a physical prototype and conduct preliminary experiments, which validate the effectiveness of the proposed design and analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u751f\u7269\u534f\u540c\u539f\u7406\u7684\u6a21\u5757\u5316\u7075\u5de7\u624b\u8bbe\u8ba1\uff0c\u901a\u8fc72\u4e2a\u9a71\u52a8\u5668\u63a7\u52364\u81ea\u7531\u5ea6\u7684\u4eff\u4eba\u624b\u6307\u62d3\u6251\u7ed3\u6784\uff0c\u5e73\u8861\u4e86\u9a71\u52a8\u590d\u6742\u6027\u548c\u7075\u5de7\u6027\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "motivation": "\u751f\u7269\u534f\u540c\u539f\u7406\u88ab\u5e7f\u6cdb\u7528\u4e8e\u7075\u5de7\u624b\u8bbe\u8ba1\uff0c\u4f46\u8fc7\u5ea6\u8026\u5408\u4f1a\u964d\u4f4e\u624b\u7684\u7075\u5de7\u6027\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u9a71\u52a8\u590d\u6742\u6027\u548c\u7075\u5de7\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa4\u81ea\u7531\u5ea62\u9a71\u52a8\u7684\u4eff\u4eba\u624b\u6307\u62d3\u6251\u7ed3\u6784\uff0c\u57fa\u4e8e\u6b64\u5f00\u53d1\u81ea\u9002\u5e94\u6a21\u5757\u5316\u7075\u5de7\u624b\u3002\u901a\u8fc7\u5206\u6790\u624b\u90e8\u751f\u7269\u534f\u540c\u548c\u4eba\u7c7b\u624b\u52bf\uff0c\u5c06\u5173\u8282\u7ea7\u534f\u8c03\u548c\u7ed3\u6784\u7279\u5f81\u8f6c\u5316\u4e3a\u6a21\u5757\u5316\u624b\u6307\u67b6\u6784\uff0c\u5efa\u7acb\u8fd0\u52a8\u5b66\u6a21\u578b\u5206\u6790\u81ea\u9002\u5e94\u6293\u53d6\u548c\u624b\u5185\u64cd\u4f5c\u3002", "result": "\u6784\u5efa\u4e86\u7269\u7406\u539f\u578b\u5e76\u8fdb\u884c\u521d\u6b65\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u8bbe\u8ba1\u548c\u5206\u6790\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4eff\u4eba\u624b\u6307\u62d3\u6251\u548c\u6a21\u5757\u5316\u7075\u5de7\u624b\u8bbe\u8ba1\u6210\u529f\u5e73\u8861\u4e86\u9a71\u52a8\u590d\u6742\u6027\u548c\u7075\u5de7\u6027\uff0c\u4e3a\u7075\u5de7\u624b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21701", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21701", "abs": "https://arxiv.org/abs/2511.21701", "authors": ["Chiung-Yi Tseng", "Danyang Zhang", "Tianyang Wang", "Hongying Luo", "Lu Chen", "Junming Huang", "Jibin Guan", "Junfeng Hao", "Junhao Song", "Ziqian Bi"], "title": "47B Mixture-of-Experts Beats 671B Dense Models on Chinese Medical Examinations", "comment": null, "summary": "The rapid advancement of large language models(LLMs) has prompted significant interest in their potential applications in medical domains. This paper presents a comprehensive benchmark evaluation of 27 state-of-the-art LLMs on Chinese medical examination questions, encompassing seven medical specialties across two professional levels. We introduce a robust evaluation framework that assesses model performance on 2,800 carefully curated questions from cardiovascular, gastroenterology, hematology, infectious diseases, nephrology, neurology, and respiratory medicine domains. Our dataset distinguishes between attending physician and senior physician difficulty levels, providing nuanced insights into model capabilities across varying complexity. Our empirical analysis reveals substantial performance variations among models, with Mixtral-8x7B achieving the highest overall accuracy of 74.25%, followed by DeepSeek-R1-671B at 64.07%. Notably, we observe no consistent correlation between model size and performance, as evidenced by the strong performance of smaller mixture-of-experts architectures. The evaluation demonstrates significant performance gaps between medical specialties, with models generally performing better on cardiovascular and neurology questions compared to gastroenterology and nephrology domains. Furthermore, our analysis indicates minimal performance degradation between attending and senior physician levels for top-performing models, suggesting robust generalization capabilities. This benchmark provides critical insights for the deployment of LLMs in medical education and clinical decision support systems, highlighting both the promise and current limitations of these technologies in specialized medical contexts.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf927\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u56fd\u533b\u5b66\u8003\u8bd5\u9898\u76ee\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u57fa\u51c6\u8bc4\u4f30\uff0c\u6db5\u76d67\u4e2a\u533b\u5b66\u4e13\u4e1a\u548c2\u4e2a\u4e13\u4e1a\u7ea7\u522b\uff0c\u53d1\u73b0Mixtral-8x7B\u8868\u73b0\u6700\u4f73\uff0c\u6a21\u578b\u5927\u5c0f\u4e0e\u6027\u80fd\u65e0\u4e00\u81f4\u76f8\u5173\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u5728\u533b\u5b66\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u5f15\u8d77\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u9700\u8981\u8bc4\u4f30\u8fd9\u4e9b\u6a21\u578b\u5728\u4e13\u4e1a\u533b\u5b66\u77e5\u8bc6\u4e0a\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u4e2d\u56fd\u533b\u5b66\u8003\u8bd5\u73af\u5883\u4e0b\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b2,800\u9053\u7cbe\u5fc3\u7b5b\u9009\u9898\u76ee\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u5fc3\u8840\u7ba1\u3001\u6d88\u5316\u3001\u8840\u6db2\u3001\u611f\u67d3\u3001\u80be\u810f\u3001\u795e\u7ecf\u548c\u547c\u54387\u4e2a\u533b\u5b66\u4e13\u4e1a\uff0c\u5206\u4e3a\u4e3b\u6cbb\u533b\u5e08\u548c\u4e3b\u4efb\u533b\u5e08\u4e24\u4e2a\u96be\u5ea6\u7ea7\u522b\u3002\u8bc4\u4f30\u4e8627\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5efa\u7acb\u4e86\u7a33\u5065\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "result": "Mixtral-8x7B\u4ee574.25%\u7684\u51c6\u786e\u7387\u8868\u73b0\u6700\u4f73\uff0cDeepSeek-R1-671B\u4ee564.07%\u6b21\u4e4b\u3002\u6a21\u578b\u5927\u5c0f\u4e0e\u6027\u80fd\u65e0\u4e00\u81f4\u76f8\u5173\u6027\uff0c\u5c0f\u578b\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u8868\u73b0\u4f18\u5f02\u3002\u4e0d\u540c\u533b\u5b66\u4e13\u4e1a\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff0c\u5fc3\u8840\u7ba1\u548c\u795e\u7ecf\u79d1\u8868\u73b0\u8f83\u597d\uff0c\u6d88\u5316\u548c\u80be\u810f\u79d1\u8f83\u5dee\u3002\u9876\u7ea7\u6a21\u578b\u5728\u4e24\u4e2a\u96be\u5ea6\u7ea7\u522b\u95f4\u6027\u80fd\u4e0b\u964d\u5f88\u5c0f\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3aLLMs\u5728\u533b\u5b66\u6559\u80b2\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5c55\u793a\u4e86\u8fd9\u4e9b\u6280\u672f\u5728\u4e13\u4e1a\u533b\u5b66\u73af\u5883\u4e2d\u7684\u6f5c\u529b\u548c\u5f53\u524d\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u9700\u8981\u9488\u5bf9\u7279\u5b9a\u533b\u5b66\u9886\u57df\u8fdb\u884c\u4f18\u5316\u3002"}}
{"id": "2511.22195", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22195", "abs": "https://arxiv.org/abs/2511.22195", "authors": ["Zhiyang Liu", "Ruiteng Zhao", "Lei Zhou", "Chengran Yuan", "Yuwei Wu", "Sheng Guo", "Zhengshen Zhang", "Chenchen Liu", "Marcelo H Ang"], "title": "3D Affordance Keypoint Detection for Robotic Manipulation", "comment": "Accepted to IROS 2024", "summary": "This paper presents a novel approach for affordance-informed robotic manipulation by introducing 3D keypoints to enhance the understanding of object parts' functionality. The proposed approach provides direct information about what the potential use of objects is, as well as guidance on where and how a manipulator should engage, whereas conventional methods treat affordance detection as a semantic segmentation task, focusing solely on answering the what question. To address this gap, we propose a Fusion-based Affordance Keypoint Network (FAKP-Net) by introducing 3D keypoint quadruplet that harnesses the synergistic potential of RGB and Depth image to provide information on execution position, direction, and extent. Benchmark testing demonstrates that FAKP-Net outperforms existing models by significant margins in affordance segmentation task and keypoint detection task. Real-world experiments also showcase the reliability of our method in accomplishing manipulation tasks with previously unseen objects.", "AI": {"tldr": "\u63d0\u51faFAKP-Net\uff0c\u901a\u8fc73D\u5173\u952e\u70b9\u56db\u5143\u7ec4\u589e\u5f3a\u673a\u5668\u4eba\u5bf9\u7269\u4f53\u529f\u80fd\u6027\u7684\u7406\u89e3\uff0c\u540c\u65f6\u89e3\u51b3\"\u662f\u4ec0\u4e48\u3001\u5728\u54ea\u91cc\u3001\u5982\u4f55\u64cd\u4f5c\"\u4e09\u4e2a\u95ee\u9898\uff0c\u8d85\u8d8a\u4f20\u7edf\u4ec5\u5173\u6ce8\u8bed\u4e49\u5206\u5272\u7684\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u53ef\u4f9b\u6027\u68c0\u6d4b\u89c6\u4e3a\u8bed\u4e49\u5206\u5272\u4efb\u52a1\uff0c\u53ea\u5173\u6ce8\"\u7269\u4f53\u80fd\u7528\u6765\u505a\u4ec0\u4e48\"\uff08what\uff09\uff0c\u4f46\u7f3a\u4e4f\u5bf9\"\u5728\u54ea\u91cc\u64cd\u4f5c\"\uff08where\uff09\u548c\"\u5982\u4f55\u64cd\u4f5c\"\uff08how\uff09\u7684\u6307\u5bfc\u3002\u9700\u8981\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u5b8c\u6574\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u878d\u5408\u7684\u53ef\u4f9b\u6027\u5173\u952e\u70b9\u7f51\u7edc\uff08FAKP-Net\uff09\uff0c\u5f15\u51653D\u5173\u952e\u70b9\u56db\u5143\u7ec4\uff0c\u5229\u7528RGB\u548c\u6df1\u5ea6\u56fe\u50cf\u7684\u534f\u540c\u6f5c\u529b\uff0c\u63d0\u4f9b\u6267\u884c\u4f4d\u7f6e\u3001\u65b9\u5411\u548c\u8303\u56f4\u4fe1\u606f\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793aFAKP-Net\u5728\u53ef\u4f9b\u6027\u5206\u5272\u4efb\u52a1\u548c\u5173\u952e\u70b9\u68c0\u6d4b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e5f\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u65f6\u80fd\u53ef\u9760\u5b8c\u6210\u64cd\u4f5c\u4efb\u52a1\u3002", "conclusion": "FAKP-Net\u901a\u8fc73D\u5173\u952e\u70b9\u65b9\u6cd5\u5168\u9762\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684what\u3001where\u3001how\u95ee\u9898\uff0c\u4e3a\u53ef\u4f9b\u6027\u611f\u77e5\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21702", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21702", "abs": "https://arxiv.org/abs/2511.21702", "authors": ["Dong Liu", "Yanxuan Yu", "Ben Lengerich"], "title": "CSV-Decode: Certifiable Sub-Vocabulary Decoding for Efficient Large Language Model Inference", "comment": null, "summary": "Large language models face significant computational bottlenecks during inference due to the expensive output layer computation over large vocabularies. We present CSV-Decode, a novel approach that uses geometric upper bounds to construct small sub-vocabularies for each decoding step, enabling efficient sparse computation while maintaining dual correctness guarantees: exact top-$k$ certification and $\\varepsilon$-certified softmax approximations. Our method clusters vocabulary embeddings offline and uses centroid-plus-radius bounds to identify which tokens can be safely omitted from computation. We provide a complete system implementation with sparse GEMV kernels, multi-GPU sharding, and CUDA Graph optimization. Experimental results demonstrate significant speedup over full vocabulary decoding while maintaining distributional guarantees and low fallback rates. Our code implementation available at \\href{https://github.com/FastLM/CSV-Decode}{https://github.com/FastLM/CSV-Decode}.", "AI": {"tldr": "CSV-Decode \u662f\u4e00\u79cd\u901a\u8fc7\u51e0\u4f55\u4e0a\u754c\u6784\u5efa\u5c0f\u5b50\u8bcd\u6c47\u8868\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6b63\u786e\u6027\u4fdd\u8bc1\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u9762\u4e34\u663e\u8457\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u8f93\u51fa\u5c42\u9700\u8981\u5728\u5927\u578b\u8bcd\u6c47\u8868\u4e0a\u8fdb\u884c\u6602\u8d35\u7684\u8ba1\u7b97\u3002", "method": "\u79bb\u7ebf\u805a\u7c7b\u8bcd\u6c47\u5d4c\u5165\uff0c\u4f7f\u7528\u8d28\u5fc3\u52a0\u534a\u5f84\u7684\u51e0\u4f55\u4e0a\u754c\u4e3a\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u6784\u5efa\u5c0f\u5b50\u8bcd\u6c47\u8868\uff0c\u5b9e\u73b0\u7a00\u758f\u8ba1\u7b97\uff0c\u540c\u65f6\u63d0\u4f9b\u7cbe\u786etop-k\u8ba4\u8bc1\u548c\u03b5\u8ba4\u8bc1\u7684softmax\u8fd1\u4f3c\u4fdd\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u76f8\u6bd4\u5b8c\u6574\u8bcd\u6c47\u8868\u89e3\u7801\u6709\u663e\u8457\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u5e03\u4fdd\u8bc1\u548c\u8f83\u4f4e\u7684\u56de\u9000\u7387\u3002", "conclusion": "CSV-Decode \u901a\u8fc7\u51e0\u4f55\u4e0a\u754c\u6784\u5efa\u5c0f\u5b50\u8bcd\u6c47\u8868\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u8bc1\u6b63\u786e\u6027\u7684\u540c\u65f6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u65f6\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2511.22225", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.22225", "abs": "https://arxiv.org/abs/2511.22225", "authors": ["Gabriel Aguirre", "Simay Atasoy Bing\u00f6l", "Heiko Hamann", "Jonas Kuckling"], "title": "Bayesian Decentralized Decision-making for Multi-Robot Systems: Sample-efficient Estimation of Event Rates", "comment": "7 pages, 3 figures, submitted to IEEE MRS 2025", "summary": "Effective collective decision-making in swarm robotics often requires balancing exploration, communication and individual uncertainty estimation, especially in hazardous environments where direct measurements are limited or costly. We propose a decentralized Bayesian framework that enables a swarm of simple robots to identify the safer of two areas, each characterized by an unknown rate of hazardous events governed by a Poisson process. Robots employ a conjugate prior to gradually predict the times between events and derive confidence estimates to adapt their behavior. Our simulation results show that the robot swarm consistently chooses the correct area while reducing exposure to hazardous events by being sample-efficient. Compared to baseline heuristics, our proposed approach shows better performance in terms of safety and speed of convergence. The proposed scenario has potential to extend the current set of benchmarks in collective decision-making and our method has applications in adaptive risk-aware sampling and exploration in hazardous, dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u8ba9\u7b80\u5355\u673a\u5668\u4eba\u7fa4\u4f53\u901a\u8fc7\u6cca\u677e\u8fc7\u7a0b\u4f30\u8ba1\u5371\u9669\u4e8b\u4ef6\u7387\uff0c\u9009\u62e9\u66f4\u5b89\u5168\u533a\u57df\uff0c\u5728\u51cf\u5c11\u5371\u9669\u66b4\u9732\u7684\u540c\u65f6\u9ad8\u6548\u6536\u655b\u3002", "motivation": "\u5728\u5371\u9669\u73af\u5883\u4e2d\uff0c\u7fa4\u4f53\u673a\u5668\u4eba\u9700\u8981\u5e73\u8861\u63a2\u7d22\u3001\u901a\u4fe1\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u7279\u522b\u662f\u5728\u76f4\u63a5\u6d4b\u91cf\u53d7\u9650\u6216\u6210\u672c\u9ad8\u6602\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8bc6\u522b\u66f4\u5b89\u5168\u533a\u57df\u7684\u96c6\u4f53\u51b3\u7b56\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53bb\u4e2d\u5fc3\u5316\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u673a\u5668\u4eba\u4f7f\u7528\u5171\u8f6d\u5148\u9a8c\u9010\u6b65\u9884\u6d4b\u5371\u9669\u4e8b\u4ef6\u95f4\u9694\u65f6\u95f4\uff0c\u5e76\u63a8\u5bfc\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u6765\u8c03\u6574\u884c\u4e3a\uff0c\u901a\u8fc7\u6cca\u677e\u8fc7\u7a0b\u5efa\u6a21\u672a\u77e5\u5371\u9669\u4e8b\u4ef6\u7387\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u673a\u5668\u4eba\u7fa4\u4f53\u80fd\u4e00\u81f4\u9009\u62e9\u6b63\u786e\u533a\u57df\uff0c\u540c\u65f6\u901a\u8fc7\u6837\u672c\u9ad8\u6548\u6027\u51cf\u5c11\u5371\u9669\u66b4\u9732\u3002\u76f8\u6bd4\u57fa\u51c6\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u5b89\u5168\u6027\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u96c6\u4f53\u51b3\u7b56\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u9002\u7528\u4e8e\u5371\u9669\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u98ce\u9669\u611f\u77e5\u91c7\u6837\u548c\u63a2\u7d22\u5e94\u7528\u3002"}}
{"id": "2511.21703", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21703", "abs": "https://arxiv.org/abs/2511.21703", "authors": ["Siyaxolisa Kabane"], "title": "Evaluating Embedding Generalization: How LLMs, LoRA, and SLERP Shape Representational Geometry", "comment": "20 pages, 16 figures", "summary": "We investigate the generalization properties of dense text embeddings when the embedding backbone is a large language model (LLM) versus when it is a non-LLM encoder, and we study the extent to which spherical linear interpolation (SLERP) model-merging mitigates over-specialization introduced by task-specific adaptation (e.g., LoRA). To make the comparison concrete and domain-agnostic, we design a controlled suite of experiments in which models embed short numerical sequences and are evaluated on their ability to cluster and classify those sequences according to well-defined number-theoretic properties. Our experimental protocol compares four families of models: (1) non-LLM encoders trained from scratch or fine-tuned for embeddings, (2) LLM-based encoders adapted with parameter-efficient methods (LoRA), (3) LLM-based encoders with LoRA followed by model souping merging into the base weights, and (4) the same LoRA-adapted LLMs merged using SLERP across checkpoints or stages. We evaluate representational quality with clustering indices (Silhouette and Davies Bouldin). We additionally analyze the use of kmeans labels to see if the embeddings encode any other information besides the one we are testing for. Empirically, we find that LLM-based backbones produce embeddings that better capture higher-order, compositional numeric patterns, but are prone to adapter dominance that degrades balanced generalization; SLERP merging consistently recovers base-model structure while retaining most task gains, yielding superior tradeoffs in clustering separability, and robustness compared to model souping or models that were not merged.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86LLM\u4e0e\u975eLLM\u7f16\u7801\u5668\u5728\u6587\u672c\u5d4c\u5165\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63a2\u8ba8\u4e86SLERP\u6a21\u578b\u5408\u5e76\u5982\u4f55\u7f13\u89e3\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u5e26\u6765\u7684\u8fc7\u4e13\u4e1a\u5316\u95ee\u9898\u3002\u901a\u8fc7\u6570\u503c\u5e8f\u5217\u5206\u7c7b\u5b9e\u9a8c\u53d1\u73b0\uff0cLLM\u80fd\u66f4\u597d\u6355\u6349\u9ad8\u9636\u7ec4\u5408\u6a21\u5f0f\uff0c\u4f46\u6613\u53d7\u9002\u914d\u5668\u4e3b\u5bfc\uff1bSLERP\u5408\u5e76\u80fd\u6062\u590d\u57fa\u7840\u6a21\u578b\u7ed3\u6784\u540c\u65f6\u4fdd\u7559\u4efb\u52a1\u589e\u76ca\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u5e73\u8861\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u975eLLM\u7f16\u7801\u5668\u5728\u6587\u672c\u5d4c\u5165\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u7279\u6027\u5dee\u5f02\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u6280\u672f\uff08\u7279\u522b\u662f\u7403\u5f62\u7ebf\u6027\u63d2\u503cSLERP\uff09\u6765\u7f13\u89e3\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\uff08\u5982LoRA\uff09\u5bfc\u81f4\u7684\u8fc7\u4e13\u4e1a\u5316\u95ee\u9898\uff0c\u4ece\u800c\u83b7\u5f97\u66f4\u597d\u7684\u6cdb\u5316\u5e73\u8861\u3002", "method": "\u8bbe\u8ba1\u4e86\u53d7\u63a7\u5b9e\u9a8c\u5957\u4ef6\uff0c\u8ba9\u6a21\u578b\u5d4c\u5165\u77ed\u6570\u503c\u5e8f\u5217\u5e76\u6839\u636e\u6570\u8bba\u5c5e\u6027\u8fdb\u884c\u5206\u7c7b\u548c\u805a\u7c7b\u3002\u6bd4\u8f83\u4e86\u56db\u7c7b\u6a21\u578b\uff1a1\uff09\u4ece\u5934\u8bad\u7ec3\u6216\u5fae\u8c03\u7684\u975eLLM\u7f16\u7801\u5668\uff1b2\uff09\u4f7f\u7528\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\uff08LoRA\uff09\u9002\u5e94\u7684LLM\u7f16\u7801\u5668\uff1b3\uff09LoRA\u9002\u5e94\u540e\u901a\u8fc7\u6a21\u578b\u878d\u5408\u5408\u5e76\u5230\u57fa\u7840\u6743\u91cd\u7684LLM\uff1b4\uff09\u4f7f\u7528SLERP\u5728\u68c0\u67e5\u70b9\u6216\u9636\u6bb5\u95f4\u5408\u5e76\u7684LoRA\u9002\u5e94LLM\u3002\u4f7f\u7528\u805a\u7c7b\u6307\u6807\uff08\u8f6e\u5ed3\u7cfb\u6570\u548cDavies Bouldin\u6307\u6570\uff09\u8bc4\u4f30\u8868\u793a\u8d28\u91cf\uff0c\u5e76\u5206\u6790kmeans\u6807\u7b7e\u4ee5\u68c0\u6d4b\u5d4c\u5165\u4e2d\u662f\u5426\u5305\u542b\u989d\u5916\u4fe1\u606f\u3002", "result": "\u5b9e\u8bc1\u53d1\u73b0\uff1aLLM\u57fa\u7840\u6a21\u578b\u80fd\u66f4\u597d\u5730\u6355\u6349\u9ad8\u9636\u3001\u7ec4\u5408\u6027\u6570\u503c\u6a21\u5f0f\uff0c\u4f46\u5bb9\u6613\u4ea7\u751f\u9002\u914d\u5668\u4e3b\u5bfc\u73b0\u8c61\uff0c\u635f\u5bb3\u5e73\u8861\u6cdb\u5316\u80fd\u529b\uff1bSLERP\u5408\u5e76\u80fd\u4e00\u81f4\u5730\u6062\u590d\u57fa\u7840\u6a21\u578b\u7ed3\u6784\uff0c\u540c\u65f6\u4fdd\u7559\u5927\u90e8\u5206\u4efb\u52a1\u589e\u76ca\uff0c\u5728\u805a\u7c7b\u53ef\u5206\u6027\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u5e73\u8861\u65b9\u9762\u4f18\u4e8e\u6a21\u578b\u878d\u5408\u6216\u672a\u5408\u5e76\u7684\u6a21\u578b\u3002", "conclusion": "LLM\u5728\u6355\u6349\u590d\u6742\u6a21\u5f0f\u65b9\u9762\u4f18\u4e8e\u975eLLM\u7f16\u7801\u5668\uff0c\u4f46\u9700\u8981\u8c28\u614e\u5904\u7406\u4efb\u52a1\u9002\u5e94\u5e26\u6765\u7684\u8fc7\u4e13\u4e1a\u5316\u95ee\u9898\uff1bSLERP\u6a21\u578b\u5408\u5e76\u662f\u4e00\u79cd\u6709\u6548\u7b56\u7565\uff0c\u80fd\u5728\u4fdd\u7559\u4efb\u52a1\u7279\u5b9a\u6539\u8fdb\u7684\u540c\u65f6\u7ef4\u6301\u57fa\u7840\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u6027\u80fd\u5e73\u8861\u3002"}}
{"id": "2511.22238", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22238", "abs": "https://arxiv.org/abs/2511.22238", "authors": ["Ryosuke Ofuchi", "Yuichiro Toda", "Naoki Masuyama", "Takayuki Matsuno"], "title": "MLATC: Fast Hierarchical Topological Mapping from 3D LiDAR Point Clouds Based on Adaptive Resonance Theory", "comment": null, "summary": "This paper addresses the problem of building global topological maps from 3D LiDAR point clouds for autonomous mobile robots operating in large-scale, dynamic, and unknown environments. Adaptive Resonance Theory-based Topological Clustering with Different Topologies (ATC-DT) builds global topological maps represented as graphs while mitigating catastrophic forgetting during sequential processing. However, its winner selection mechanism relies on an exhaustive nearest-neighbor search over all existing nodes, leading to scalability limitations as the map grows. To address this challenge, we propose a hierarchical extension called Multi-Layer ATC (MLATC). MLATC organizes nodes into a hierarchy, enabling the nearest-neighbor search to proceed from coarse to fine resolutions, thereby drastically reducing the number of distance evaluations per query. The number of layers is not fixed in advance. MLATC employs an adaptive layer addition mechanism that automatically deepens the hierarchy when lower layers become saturated, keeping the number of user-defined hyperparameters low. Simulation experiments on synthetic large-scale environments show that MLATC accelerates topological map building compared to the original ATC-DT and exhibits a sublinear, approximately logarithmic scaling of search time with respect to the number of nodes. Experiments on campus-scale real-world LiDAR datasets confirm that MLATC maintains a millisecond-level per-frame runtime and enables real-time global topological map building in large-scale environments, significantly outperforming the original ATC-DT in terms of computational efficiency.", "AI": {"tldr": "\u63d0\u51faMLATC\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7ed3\u6784\u52a0\u901fATC-DT\u62d3\u6251\u5efa\u56fe\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u5904\u7406", "motivation": "ATC-DT\u65b9\u6cd5\u5728\u6784\u5efa\u5927\u89c4\u6a21\u62d3\u6251\u5730\u56fe\u65f6\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5176\u7a77\u4e3e\u6700\u8fd1\u90bb\u641c\u7d22\u968f\u7740\u8282\u70b9\u6570\u91cf\u589e\u52a0\u800c\u6548\u7387\u4e0b\u964d", "method": "\u63d0\u51fa\u591a\u5c42ATC\uff08MLATC\uff09\uff0c\u5c06\u8282\u70b9\u7ec4\u7ec7\u6210\u5c42\u6b21\u7ed3\u6784\uff0c\u5b9e\u73b0\u4ece\u7c97\u5230\u7cbe\u7684\u5206\u8fa8\u7387\u641c\u7d22\uff1b\u91c7\u7528\u81ea\u9002\u5e94\u5c42\u6dfb\u52a0\u673a\u5236\uff0c\u5728\u4f4e\u5c42\u9971\u548c\u65f6\u81ea\u52a8\u52a0\u6df1\u5c42\u6b21", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793aMLATC\u6bd4\u539f\u59cbATC-DT\u52a0\u901f\u62d3\u6251\u5efa\u56fe\uff0c\u641c\u7d22\u65f6\u95f4\u968f\u8282\u70b9\u6570\u5448\u4e9a\u7ebf\u6027\uff08\u8fd1\u4f3c\u5bf9\u6570\uff09\u589e\u957f\uff1b\u771f\u5b9e\u6821\u56ed\u89c4\u6a21LiDAR\u6570\u636e\u96c6\u5b9e\u9a8c\u8bc1\u5b9eMLATC\u4fdd\u6301\u6beb\u79d2\u7ea7\u6bcf\u5e27\u8fd0\u884c\u65f6\u95f4\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u73af\u5883\u5b9e\u65f6\u5168\u5c40\u62d3\u6251\u5efa\u56fe", "conclusion": "MLATC\u663e\u8457\u63d0\u5347\u4e86ATC-DT\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u52a8\u6001\u672a\u77e5\u73af\u5883\u4e2d\u5b9e\u65f6\u62d3\u6251\u5efa\u56fe\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898"}}
{"id": "2511.21704", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.21704", "abs": "https://arxiv.org/abs/2511.21704", "authors": ["Jonatas Grosman", "Cassio Almeida", "Guilherme Schardong", "H\u00e9lio Lopes"], "title": "On the Cross-lingual Transferability of Pre-trained wav2vec2-based Models", "comment": null, "summary": "Using representations provided by a large pre-trained model has become the primary strategy for achieving state-of-the-art results in a wide range of tasks. A recently proposed large pre-trained model, wav2vec 2.0, was seminal for several other works on pre-training large models on speech data. Many models are being pre-trained using the same architecture as wav2vec 2.0 and are getting state-of-the-art in various speech-related tasks. Previous work has demonstrated that the data used during the pre-training of these wav2vec2-based models can impact the model's performance in downstream tasks, and this should be taken into consideration before utilizing these models. However, few works have proposed investigating further how the transfer knowledge of these pre-trained models behaves in different languages, even when the target language differs from the one used during the model's pre-training. Our work aims to investigate the cross-lingual transferability of these wav2vec2-based models. We performed several fine-tuning experiments on the speech recognition task in 18 languages using 15 large pre-trained models. The results of our experiments showed us that the size of data used during the pre-training of these models is not as important to the final performance as the diversity. We noticed that the performance of Indo-European languages is superior to non-Indo-European languages in the evaluated models. We have observed a positive cross-lingual transfer of knowledge using monolingual models, which was evident in all the languages we used, but more pronounced when the language used during pre-training was more similar to the downstream task language. With these findings, we aim to assist the scientific community in utilizing existing wav2vec2-based pre-trained models, as well as facilitate the pre-training of new ones.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5\u4e86\u57fa\u4e8ewav2vec 2.0\u67b6\u6784\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b\uff0c\u53d1\u73b0\u6570\u636e\u591a\u6837\u6027\u6bd4\u6570\u636e\u91cf\u66f4\u91cd\u8981\uff0c\u5370\u6b27\u8bed\u7cfb\u8868\u73b0\u4f18\u4e8e\u975e\u5370\u6b27\u8bed\u7cfb\uff0c\u4e14\u8bed\u8a00\u76f8\u4f3c\u6027\u4fc3\u8fdb\u77e5\u8bc6\u8fc1\u79fb\u3002", "motivation": "\u867d\u7136wav2vec 2.0\u7b49\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5404\u79cd\u8bed\u97f3\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86SOTA\u7ed3\u679c\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u6df1\u5165\u63a2\u8ba8\u8fd9\u4e9b\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b\uff0c\u7279\u522b\u662f\u5f53\u76ee\u6807\u8bed\u8a00\u4e0e\u9884\u8bad\u7ec3\u8bed\u8a00\u4e0d\u540c\u65f6\u3002\u4e86\u89e3\u8fd9\u79cd\u8fc1\u79fb\u7279\u6027\u5bf9\u4e8e\u6709\u6548\u5229\u7528\u73b0\u6709\u6a21\u578b\u548c\u6307\u5bfc\u65b0\u6a21\u578b\u9884\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u752815\u4e2a\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u572818\u79cd\u8bed\u8a00\u7684\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00\u4e0a\u7684\u8868\u73b0\uff0c\u5206\u6790\u9884\u8bad\u7ec3\u6570\u636e\u89c4\u6a21\u3001\u8bed\u8a00\u591a\u6837\u6027\u4ee5\u53ca\u8bed\u8a00\u76f8\u4f3c\u6027\u5bf9\u8fc1\u79fb\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "1. \u9884\u8bad\u7ec3\u6570\u636e\u7684\u591a\u6837\u6027\u6bd4\u6570\u636e\u89c4\u6a21\u5bf9\u6700\u7ec8\u6027\u80fd\u66f4\u91cd\u8981\uff1b2. \u5370\u6b27\u8bed\u7cfb\u8bed\u8a00\u7684\u8868\u73b0\u4f18\u4e8e\u975e\u5370\u6b27\u8bed\u7cfb\u8bed\u8a00\uff1b3. \u5355\u8bed\u6a21\u578b\u5b58\u5728\u6b63\u5411\u7684\u8de8\u8bed\u8a00\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4e14\u5f53\u9884\u8bad\u7ec3\u8bed\u8a00\u4e0e\u4e0b\u6e38\u4efb\u52a1\u8bed\u8a00\u66f4\u76f8\u4f3c\u65f6\uff0c\u8fc1\u79fb\u6548\u679c\u66f4\u660e\u663e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u57fa\u4e8ewav2vec 2.0\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u8fc1\u79fb\u7279\u6027\uff0c\u4e3a\u79d1\u5b66\u793e\u533a\u6709\u6548\u5229\u7528\u73b0\u6709\u6a21\u578b\u548c\u6307\u5bfc\u65b0\u6a21\u578b\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u6570\u636e\u591a\u6837\u6027\u548c\u8bed\u8a00\u76f8\u4f3c\u6027\u5728\u8de8\u8bed\u8a00\u8fc1\u79fb\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2511.22318", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22318", "abs": "https://arxiv.org/abs/2511.22318", "authors": ["Yuki Origane", "Koya Cho", "Hideyuki Tsukagoshi"], "title": "Soft Fluidic Sheet Transistor for Soft Robotic System Enabling Fluid Logic Operations", "comment": "7 pages, 16 figures", "summary": "Aiming to achieve both high functionality and flexibility in soft robot system, this paper presents a soft urethane sheet-like valve with an amplifier that can perform logical operations using only pneumatic signals. When the control chamber in the valve is pressurized, the main path is compressed along its central axis, buckling and being pressed,resulting in blockage. This allows control by a pressure signal smaller than that within the main channel. Furthermore, similar to transistors in electrical circuits, when combined, the proposed valve can perform a variety of logical operations. The basic type operates as a NOT logic element, which is named the fluidic sheet transistor (FST). By integrating multiple FSTs, logical operations such as positive logic, NAND, and NOR can be performed on a single sheet. This paper describes the operating principle, fabrication method, and characteristics of the FST,followed by a method for configuring logical operations.Moreover, we demonstrate the construction of a latch circuit(self-holding logic circuit) using FST, introducing a prototype of a fluid robot system that combines a tactile tube as a fluidic detector and fluid actuators. This demonstrates that it is possible to generate behavior that actively changes posture when hitting an obstacle using only air pressure from a single pipe, which verifies the effectiveness of the proposed methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8f6f\u8d28\u805a\u6c28\u916f\u7247\u72b6\u9600\u95e8\uff08FST\uff09\uff0c\u4ec5\u4f7f\u7528\u6c14\u52a8\u4fe1\u53f7\u5373\u53ef\u5b9e\u73b0\u903b\u8f91\u8fd0\u7b97\uff0c\u7c7b\u4f3c\u7535\u5b50\u7535\u8def\u4e2d\u7684\u6676\u4f53\u7ba1\uff0c\u53ef\u7528\u4e8e\u6784\u5efa\u6d41\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u9ad8\u529f\u80fd\u6027\u548c\u7075\u6d3b\u6027\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4ec5\u4f7f\u7528\u6c14\u52a8\u4fe1\u53f7\u6267\u884c\u903b\u8f91\u64cd\u4f5c\u7684\u5143\u4ef6\uff0c\u4ece\u800c\u51cf\u5c11\u7535\u6c14\u90e8\u4ef6\u7684\u4f9d\u8d56\u5e76\u63d0\u9ad8\u7cfb\u7edf\u7684\u67d4\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u8bbe\u8ba1\u8f6f\u8d28\u805a\u6c28\u916f\u7247\u72b6\u9600\u95e8\uff0c\u5f53\u63a7\u5236\u8154\u52a0\u538b\u65f6\uff0c\u4e3b\u901a\u9053\u6cbf\u4e2d\u5fc3\u8f74\u538b\u7f29\u3001\u5c48\u66f2\u5e76\u88ab\u538b\u7d27\uff0c\u5b9e\u73b0\u963b\u585e\u3002\u8fd9\u79cd\u9600\u95e8\u4f5c\u4e3aNOT\u903b\u8f91\u5143\u4ef6\uff08FST\uff09\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u4e2aFST\u53ef\u5728\u5355\u5f20\u7247\u4e0a\u5b9e\u73b0\u6b63\u903b\u8f91\u3001NAND\u3001NOR\u7b49\u903b\u8f91\u8fd0\u7b97\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51faFST\u5e76\u9a8c\u8bc1\u5176\u903b\u8f91\u8fd0\u7b97\u529f\u80fd\uff0c\u6784\u5efa\u4e86\u9501\u5b58\u7535\u8def\uff08\u81ea\u4fdd\u6301\u903b\u8f91\u7535\u8def\uff09\uff0c\u7ed3\u5408\u6d41\u4f53\u68c0\u6d4b\u5668\u548c\u6d41\u4f53\u6267\u884c\u5668\uff0c\u5b9e\u73b0\u4e86\u4ec5\u4f7f\u7528\u5355\u7ba1\u6c14\u538b\u5c31\u80fd\u5728\u9047\u5230\u969c\u788d\u7269\u65f6\u4e3b\u52a8\u6539\u53d8\u59ff\u6001\u7684\u884c\u4e3a\u3002", "conclusion": "\u63d0\u51fa\u7684FST\u80fd\u591f\u4ec5\u4f7f\u7528\u6c14\u52a8\u4fe1\u53f7\u5b9e\u73b0\u903b\u8f91\u8fd0\u7b97\uff0c\u4e3a\u6784\u5efa\u5b8c\u5168\u6c14\u52a8\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2511.21705", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21705", "abs": "https://arxiv.org/abs/2511.21705", "authors": ["Junjie Wu", "Yumeng Fu", "Chen Gong", "Guohong Fu"], "title": "Insight-A: Attribution-aware for Multimodal Misinformation Detection", "comment": null, "summary": "AI-generated content (AIGC) technology has emerged as a prevalent alternative to create multimodal misinformation on social media platforms, posing unprecedented threats to societal safety. However, standard prompting leverages multimodal large language models (MLLMs) to identify the emerging misinformation, which ignores the misinformation attribution. To this end, we present Insight-A, exploring attribution with MLLM insights for detecting multimodal misinformation. Insight-A makes two efforts: I) attribute misinformation to forgery sources, and II) an effective pipeline with hierarchical reasoning that detects distortions across modalities. Specifically, to attribute misinformation to forgery traces based on generation patterns, we devise cross-attribution prompting (CAP) to model the sophisticated correlations between perception and reasoning. Meanwhile, to reduce the subjectivity of human-annotated prompts, automatic attribution-debiased prompting (ADP) is used for task adaptation on MLLMs. Additionally, we design image captioning (IC) to achieve visual details for enhancing cross-modal consistency checking. Extensive experiments demonstrate the superiority of our proposal and provide a new paradigm for multimodal misinformation detection in the era of AIGC.", "AI": {"tldr": "\u63d0\u51faInsight-A\u65b9\u6cd5\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u6d1e\u5bdf\u529b\uff0c\u901a\u8fc7\u6eaf\u6e90\u4f2a\u9020\u6765\u6e90\u548c\u5206\u5c42\u63a8\u7406\u6765\u68c0\u6d4b\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f", "motivation": "AIGC\u6280\u672f\u5df2\u6210\u4e3a\u793e\u4ea4\u5a92\u4f53\u4e0a\u521b\u5efa\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u7684\u666e\u904d\u624b\u6bb5\uff0c\u5bf9\u793e\u4f1a\u5b89\u5168\u6784\u6210\u5a01\u80c1\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528MLLMs\u68c0\u6d4b\u865a\u5047\u4fe1\u606f\u4f46\u5ffd\u7565\u4e86\u865a\u5047\u4fe1\u606f\u7684\u6eaf\u6e90\u95ee\u9898", "method": "\u63d0\u51faInsight-A\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u8de8\u6eaf\u6e90\u63d0\u793a\uff08CAP\uff09\u5c06\u865a\u5047\u4fe1\u606f\u6eaf\u6e90\u5230\u4f2a\u9020\u6765\u6e90\uff1b2) \u4f7f\u7528\u81ea\u52a8\u6eaf\u6e90\u53bb\u504f\u63d0\u793a\uff08ADP\uff09\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u7684\u4e3b\u89c2\u6027\uff1b3) \u8bbe\u8ba1\u56fe\u50cf\u63cf\u8ff0\uff08IC\uff09\u589e\u5f3a\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u68c0\u67e5\uff1b4) \u6784\u5efa\u5206\u5c42\u63a8\u7406\u7ba1\u9053\u68c0\u6d4b\u8de8\u6a21\u6001\u5931\u771f", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u4e3aAIGC\u65f6\u4ee3\u7684\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f", "conclusion": "Insight-A\u901a\u8fc7\u6eaf\u6e90\u4f2a\u9020\u6765\u6e90\u548c\u5206\u5c42\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u7684\u6eaf\u6e90\u95ee\u9898\uff0c\u4e3a\u5e94\u5bf9AIGC\u65f6\u4ee3\u7684\u865a\u5047\u4fe1\u606f\u5a01\u80c1\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.22338", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22338", "abs": "https://arxiv.org/abs/2511.22338", "authors": ["Denghan Xiong", "Yanzhe Zhao", "Yutong Chen", "Zichun Wang"], "title": "Nonholonomic Narrow Dead-End Escape with Deep Reinforcement Learning", "comment": "14 pages, 5 figures, 1 table, submitted to arXiv", "summary": "Nonholonomic constraints restrict feasible velocities without reducing configuration-space dimension, which makes collision-free geometric paths generally non-executable for car-like robots. Ackermann steering further imposes curvature bounds and forbids in-place rotation, so escaping from narrow dead ends typically requires tightly sequenced forward and reverse maneuvers. Classical planners that decouple global search and local steering struggle in these settings because narrow passages occupy low-measure regions and nonholonomic reachability shrinks the set of valid connections, which degrades sampling efficiency and increases sensitivity to clearances. We study nonholonomic narrow dead-end escape for Ackermann vehicles and contribute three components. First, we construct a generator that samples multi-phase forward-reverse trajectories compatible with Ackermann kinematics and inflates their envelopes to synthesize families of narrow dead ends that are guaranteed to admit at least one feasible escape. Second, we construct a training environment that enforces kinematic constraints and train a policy using the soft actor-critic algorithm. Third, we evaluate against representative classical planners that combine global search with nonholonomic steering. Across parameterized dead-end families, the learned policy solves a larger fraction of instances, reduces maneuver count, and maintains comparable path length and planning time while under the same sensing and control limits. We provide our project as open source at https://github.com/gitagitty/cisDRL-RobotNav.git", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u975e\u5b8c\u6574\u7ea6\u675fAckermann\u8f66\u8f86\u7a84\u6b7b\u80e1\u540c\u9003\u9038\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u8bad\u7ec3\u73af\u5883\u5e76\u8bad\u7ec3\u7b56\u7565\uff0c\u76f8\u6bd4\u4f20\u7edf\u89c4\u5212\u5668\u5728\u9003\u9038\u6210\u529f\u7387\u548c\u673a\u52a8\u6b21\u6570\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u975e\u5b8c\u6574\u7ea6\u675f\u9650\u5236\u4e86Ackermann\u8f66\u8f86\u7684\u53ef\u884c\u901f\u5ea6\uff0c\u4f7f\u5176\u65e0\u6cd5\u6267\u884c\u51e0\u4f55\u8def\u5f84\uff1b\u72ed\u7a84\u6b7b\u80e1\u540c\u9003\u9038\u9700\u8981\u590d\u6742\u7684\u524d\u8fdb-\u540e\u9000\u673a\u52a8\u5e8f\u5217\uff1b\u4f20\u7edf\u89c4\u5212\u5668\u5c06\u5168\u5c40\u641c\u7d22\u4e0e\u5c40\u90e8\u8f6c\u5411\u89e3\u8026\uff0c\u5728\u72ed\u7a84\u901a\u9053\u4e2d\u91c7\u6837\u6548\u7387\u4f4e\u4e14\u5bf9\u95f4\u9699\u654f\u611f\u3002", "method": "1) \u6784\u5efa\u751f\u6210\u5668\uff0c\u91c7\u6837\u4e0eAckermann\u8fd0\u52a8\u5b66\u517c\u5bb9\u7684\u591a\u9636\u6bb5\u524d\u8fdb-\u540e\u9000\u8f68\u8ff9\uff0c\u5e76\u81a8\u80c0\u5176\u5305\u7edc\u4ee5\u5408\u6210\u4fdd\u8bc1\u81f3\u5c11\u6709\u4e00\u4e2a\u53ef\u884c\u9003\u9038\u8def\u5f84\u7684\u7a84\u6b7b\u80e1\u540c\u65cf\uff1b2) \u6784\u5efa\u5f3a\u5236\u6267\u884c\u8fd0\u52a8\u5b66\u7ea6\u675f\u7684\u8bad\u7ec3\u73af\u5883\uff0c\u4f7f\u7528\u8f6f\u6f14\u5458-\u8bc4\u8bba\u5bb6\u7b97\u6cd5\u8bad\u7ec3\u7b56\u7565\uff1b3) \u4e0e\u7ed3\u5408\u5168\u5c40\u641c\u7d22\u548c\u975e\u5b8c\u6574\u8f6c\u5411\u7684\u7ecf\u5178\u89c4\u5212\u5668\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u5728\u53c2\u6570\u5316\u7684\u6b7b\u80e1\u540c\u65cf\u4e2d\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u89e3\u51b3\u4e86\u66f4\u5927\u6bd4\u4f8b\u7684\u5b9e\u4f8b\uff0c\u51cf\u5c11\u4e86\u673a\u52a8\u6b21\u6570\uff0c\u5728\u4fdd\u6301\u53ef\u6bd4\u8def\u5f84\u957f\u5ea6\u548c\u89c4\u5212\u65f6\u95f4\u7684\u540c\u65f6\uff0c\u5728\u76f8\u540c\u7684\u611f\u77e5\u548c\u63a7\u5236\u9650\u5236\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u975e\u5b8c\u6574\u7ea6\u675fAckermann\u8f66\u8f86\u7a84\u6b7b\u80e1\u540c\u9003\u9038\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u89c4\u5212\u5668\uff0c\u63d0\u4f9b\u4e86\u5f00\u6e90\u5b9e\u73b0\uff0c\u4e3a\u590d\u6742\u975e\u5b8c\u6574\u7ea6\u675f\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21706", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21706", "abs": "https://arxiv.org/abs/2511.21706", "authors": ["Hui Wang", "Fafa Zhang", "Xiaoyu Zhang", "Chaoxu Mu"], "title": "A General Highly Accurate Online Planning Method Integrating Large Language Models into Nested Rollout Policy Adaptation for Dialogue Tasks", "comment": null, "summary": "In goal-oriented dialogue tasks, the main challenge is to steer the interaction towards a given goal within a limited number of turns. Existing approaches either rely on elaborate prompt engineering, whose effectiveness is heavily dependent on human experience, or integrate policy networks and pre-trained policy models, which are usually difficult to adapt to new dialogue scenarios and costly to train. Therefore, in this paper, we present Nested Rollout Policy Adaptation for Goal-oriented Dialogue (NRPA-GD), a novel dialogue policy planning method that completely avoids specific model training by utilizing a Large Language Model (LLM) to simulate behaviors of user and system at the same time. Specifically, NRPA-GD constructs a complete evaluation mechanism for dialogue trajectories and employs an optimization framework of nested Monte Carlo simulation and policy self-adaptation to dynamically adjust policies during the dialogue process. The experimental results on four typical goal-oriented dialogue datasets show that NRPA-GD outperforms both existing prompt engineering and specifically pre-trained model-based methods. Impressively, NRPA-GD surpasses ChatGPT and pre-trained policy models with only a 0.6-billion-parameter LLM. The proposed approach further demonstrates the advantages and novelty of employing planning methods on LLMs to solve practical planning tasks.", "AI": {"tldr": "NRPA-GD\uff1a\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u514d\u8bad\u7ec3\u5bf9\u8bdd\u7b56\u7565\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u5d4c\u5957\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u7b56\u7565\u81ea\u9002\u5e94\u4f18\u5316\uff0c\u5728\u76ee\u6807\u5bfc\u5411\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u76ee\u6807\u5bfc\u5411\u5bf9\u8bdd\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4eba\u5de5\u7ecf\u9a8c\u9a71\u52a8\u7684\u63d0\u793a\u5de5\u7a0b\uff0c\u8981\u4e48\u9700\u8981\u8bad\u7ec3\u96be\u4ee5\u9002\u5e94\u65b0\u573a\u666f\u7684\u7279\u5b9a\u7b56\u7565\u6a21\u578b\uff0c\u5b58\u5728\u6548\u7387\u4f4e\u3001\u6210\u672c\u9ad8\u7684\u95ee\u9898", "method": "\u63d0\u51faNRPA-GD\u65b9\u6cd5\uff0c\u5229\u7528LLM\u540c\u65f6\u6a21\u62df\u7528\u6237\u548c\u7cfb\u7edf\u884c\u4e3a\uff0c\u6784\u5efa\u5bf9\u8bdd\u8f68\u8ff9\u5b8c\u6574\u8bc4\u4f30\u673a\u5236\uff0c\u91c7\u7528\u5d4c\u5957\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u7b56\u7565\u81ea\u9002\u5e94\u4f18\u5316\u6846\u67b6\u52a8\u6001\u8c03\u6574\u5bf9\u8bdd\u7b56\u7565", "result": "\u5728\u56db\u4e2a\u5178\u578b\u76ee\u6807\u5bfc\u5411\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\uff0cNRPA-GD\u8d85\u8d8a\u4e86\u73b0\u6709\u63d0\u793a\u5de5\u7a0b\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u65b9\u6cd5\uff0c\u4ec5\u75286\u4ebf\u53c2\u6570\u7684LLM\u5c31\u8d85\u8d8a\u4e86ChatGPT\u548c\u9884\u8bad\u7ec3\u7b56\u7565\u6a21\u578b", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728LLM\u4e0a\u5e94\u7528\u89c4\u5212\u65b9\u6cd5\u89e3\u51b3\u5b9e\u9645\u89c4\u5212\u4efb\u52a1\u7684\u4f18\u52bf\u548c\u65b0\u9896\u6027\uff0c\u4e3a\u514d\u8bad\u7ec3\u5bf9\u8bdd\u7b56\u7565\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84"}}
{"id": "2511.22354", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22354", "abs": "https://arxiv.org/abs/2511.22354", "authors": ["Suraj Borate", "Bhavish Rai B", "Vipul Pardeshi", "Madhu Vadali"], "title": "LLM-Based Generalizable Hierarchical Task Planning and Execution for Heterogeneous Robot Teams with Event-Driven Replanning", "comment": "submitted to ICRA 2026", "summary": "This paper introduces CoMuRoS (Collaborative Multi-Robot System), a generalizable hierarchical architecture for heterogeneous robot teams that unifies centralized deliberation with decentralized execution, and supports event-driven replanning. A Task Manager LLM interprets natural-language goals, classifies tasks, and allocates subtasks using static rules plus dynamic contexts (task, history, robot and task status, and events).Each robot runs a local LLM that composes executable Python code from primitive skills (ROS2 nodes, policies), while onboard perception (VLMs/image processing) continuously monitors events and classifies them into relevant or irrelevant to the task. Task failures or user intent changes trigger replanning, allowing robots to assist teammates, resume tasks, or request human help. Hardware studies demonstrate autonomous recovery from disruptive events, filtering of irrelevant distractions, and tightly coordinated transport with emergent human-robot cooperation (e.g., multirobot collaborative object recovery success rate: 9/10, coordinated transport: 8/8, human-assisted recovery: 5/5).Simulation studies show intention-aware replanning. A curated textual benchmark spanning 22 scenarios (3 tasks each, around 20 robots) evaluates task allocation, classification, IoU, executability, and correctness, with high average scores (e.g., correctness up to 0.91) across multiple LLMs, a separate replanning set (5 scenarios) achieves 1.0 correctness. Compared with prior LLM-based systems, CoMuRoS uniquely demonstrates runtime, event-driven replanning on physical robots, delivering robust, flexible multi-robot and human-robot collaboration.", "AI": {"tldr": "CoMuRoS\u662f\u4e00\u4e2a\u5206\u5c42\u67b6\u6784\uff0c\u7528\u4e8e\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\uff0c\u7ed3\u5408\u96c6\u4e2d\u5f0f\u89c4\u5212\u548c\u5206\u5e03\u5f0f\u6267\u884c\uff0c\u652f\u6301\u4e8b\u4ef6\u9a71\u52a8\u7684\u91cd\u89c4\u5212\uff0c\u901a\u8fc7LLM\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u89e3\u91ca\u548c\u4ee3\u7801\u751f\u6210\uff0c\u5728\u786c\u4ef6\u548c\u4eff\u771f\u4e2d\u5c55\u793a\u4e86\u9c81\u68d2\u7684\u591a\u673a\u5668\u4eba\u548c\u4eba\u673a\u534f\u4f5c\u3002", "motivation": "\u73b0\u6709LLM\u9a71\u52a8\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u7f3a\u4e4f\u8fd0\u884c\u65f6\u4e8b\u4ef6\u9a71\u52a8\u7684\u91cd\u89c4\u5212\u80fd\u529b\uff0c\u65e0\u6cd5\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u9c81\u68d2\u7684\u591a\u673a\u5668\u4eba\u548c\u4eba\u673a\u534f\u4f5c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u52a8\u6001\u73af\u5883\u53d8\u5316\u3001\u4efb\u52a1\u5931\u8d25\u548c\u7528\u6237\u610f\u56fe\u53d8\u66f4\u7684\u901a\u7528\u67b6\u6784\u3002", "method": "\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff1a\u4efb\u52a1\u7ba1\u7406\u5668LLM\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u3001\u5206\u7c7b\u4efb\u52a1\u3001\u5206\u914d\u5b50\u4efb\u52a1\uff1b\u6bcf\u4e2a\u673a\u5668\u4eba\u8fd0\u884c\u672c\u5730LLM\u5c06\u539f\u59cb\u6280\u80fd\u7ec4\u5408\u6210\u53ef\u6267\u884cPython\u4ee3\u7801\uff1b\u673a\u8f7d\u611f\u77e5\u6301\u7eed\u76d1\u63a7\u4e8b\u4ef6\u5e76\u5206\u7c7b\uff1b\u4efb\u52a1\u5931\u8d25\u6216\u7528\u6237\u610f\u56fe\u53d8\u5316\u89e6\u53d1\u91cd\u89c4\u5212\u3002", "result": "\u786c\u4ef6\u5b9e\u9a8c\uff1a\u81ea\u4e3b\u6062\u590d\u6210\u529f\u73879/10\uff0c\u534f\u8c03\u8fd0\u8f938/8\uff0c\u4eba\u8f85\u52a9\u6062\u590d5/5\uff1b\u4eff\u771f\u5c55\u793a\u610f\u56fe\u611f\u77e5\u91cd\u89c4\u5212\uff1b\u6587\u672c\u57fa\u51c6\u6d4b\u8bd5\uff0822\u4e2a\u573a\u666f\uff09\u5728\u4efb\u52a1\u5206\u914d\u3001\u5206\u7c7b\u3001IoU\u7b49\u65b9\u9762\u83b7\u5f97\u9ad8\u5206\uff08\u6b63\u786e\u7387\u6700\u9ad80.91\uff09\uff1b\u91cd\u89c4\u5212\u573a\u666f\u6b63\u786e\u7387\u8fbe1.0\u3002", "conclusion": "CoMuRoS\u9996\u6b21\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u8fd0\u884c\u65f6\u4e8b\u4ef6\u9a71\u52a8\u7684\u91cd\u89c4\u5212\uff0c\u63d0\u4f9b\u4e86\u9c81\u68d2\u3001\u7075\u6d3b\u7684\u591a\u673a\u5668\u4eba\u548c\u4eba\u673a\u534f\u4f5c\u80fd\u529b\uff0c\u76f8\u6bd4\u73b0\u6709LLM\u7cfb\u7edf\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2511.21708", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21708", "abs": "https://arxiv.org/abs/2511.21708", "authors": ["Matteo Spreafico", "Ludovica Tassini", "Camilla Sancricca", "Cinzia Cappiello"], "title": "Lost in the Pipeline: How Well Do Large Language Models Handle Data Preparation?", "comment": null, "summary": "Large language models have recently demonstrated their exceptional capabilities in supporting and automating various tasks. Among the tasks worth exploring for testing large language model capabilities, we considered data preparation, a critical yet often labor-intensive step in data-driven processes. This paper investigates whether large language models can effectively support users in selecting and automating data preparation tasks. To this aim, we considered both general-purpose and fine-tuned tabular large language models. We prompted these models with poor-quality datasets and measured their ability to perform tasks such as data profiling and cleaning. We also compare the support provided by large language models with that offered by traditional data preparation tools. To evaluate the capabilities of large language models, we developed a custom-designed quality model that has been validated through a user study to gain insights into practitioners' expectations.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u51c6\u5907\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5305\u62ec\u6570\u636e\u5206\u6790\u548c\u6e05\u6d17\uff0c\u5e76\u4e0e\u4f20\u7edf\u5de5\u5177\u5bf9\u6bd4", "motivation": "\u6570\u636e\u51c6\u5907\u662f\u6570\u636e\u9a71\u52a8\u6d41\u7a0b\u4e2d\u5173\u952e\u4f46\u52b3\u52a8\u5bc6\u96c6\u7684\u6b65\u9aa4\uff0c\u9700\u8981\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u6709\u6548\u652f\u6301\u7528\u6237\u9009\u62e9\u548c\u81ea\u52a8\u5316\u6570\u636e\u51c6\u5907\u4efb\u52a1", "method": "\u4f7f\u7528\u901a\u7528\u548c\u5fae\u8c03\u7684\u8868\u683c\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4f4e\u8d28\u91cf\u6570\u636e\u96c6\u8fdb\u884c\u63d0\u793a\uff0c\u6d4b\u91cf\u5176\u6267\u884c\u6570\u636e\u5206\u6790\u548c\u6e05\u6d17\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u5e76\u4e0e\u4f20\u7edf\u6570\u636e\u51c6\u5907\u5de5\u5177\u5bf9\u6bd4", "result": "\u5f00\u53d1\u4e86\u7ecf\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u7684\u81ea\u5b9a\u4e49\u8d28\u91cf\u6a21\u578b\uff0c\u4ee5\u4e86\u89e3\u4ece\u4e1a\u8005\u7684\u671f\u671b\uff0c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u51c6\u5907\u4efb\u52a1\u4e2d\u7684\u80fd\u529b", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u652f\u6301\u6570\u636e\u51c6\u5907\u4efb\u52a1\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u8bc4\u4f30\u5176\u4e0e\u4f20\u7edf\u5de5\u5177\u76f8\u6bd4\u7684\u5b9e\u9645\u6548\u679c"}}
{"id": "2511.22364", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22364", "abs": "https://arxiv.org/abs/2511.22364", "authors": ["Seongwon Cho", "Daechul Ahn", "Donghyun Shin", "Hyeonbeom Choi", "San Kim", "Jonghyun Choi"], "title": "BINDER: Instantly Adaptive Mobile Manipulation with Open-Vocabulary Commands", "comment": "12 pages, 8 figures", "summary": "Open-vocabulary mobile manipulation (OVMM) requires robots to follow language instructions, navigate, and manipulate while updating their world representation under dynamic environmental changes. However, most prior approaches update their world representation only at discrete update points such as navigation targets, waypoints, or the end of an action step, leaving robots blind between updates and causing cascading failures: overlooked objects, late error detection, and delayed replanning. To address this limitation, we propose BINDER (Bridging INstant and DEliberative Reasoning), a dual process framework that decouples strategic planning from continuous environment monitoring. Specifically, BINDER integrates a Deliberative Response Module (DRM, a multimodal LLM for task planning) with an Instant Response Module (IRM, a VideoLLM for continuous monitoring). The two modules play complementary roles: the DRM performs strategic planning with structured 3D scene updates and guides what the IRM attends to, while the IRM analyzes video streams to update memory, correct ongoing actions, and trigger replanning when necessary. Through this bidirectional coordination, the modules address the trade off between maintaining awareness and avoiding costly updates, enabling robust adaptation under dynamic conditions. Evaluated in three real world environments with dynamic object placement, BINDER achieves substantially higher success and efficiency than SoTA baselines, demonstrating its effectiveness for real world deployment.", "AI": {"tldr": "BINDER\u662f\u4e00\u4e2a\u53cc\u8fc7\u7a0b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u6218\u7565\u89c4\u5212\u548c\u8fde\u7eed\u73af\u5883\u76d1\u63a7\u6765\u89e3\u51b3\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\u4e2d\u7684\u52a8\u6001\u73af\u5883\u9002\u5e94\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53ea\u5728\u79bb\u6563\u66f4\u65b0\u70b9\u66f4\u65b0\u4e16\u754c\u8868\u793a\uff08\u5982\u5bfc\u822a\u76ee\u6807\u3001\u8def\u5f84\u70b9\uff09\uff0c\u5bfc\u81f4\u673a\u5668\u4eba\u5728\u66f4\u65b0\u95f4\u5904\u4e8e\"\u76f2\u533a\"\uff0c\u9020\u6210\u7ea7\u8054\u5931\u8d25\uff1a\u9057\u6f0f\u7269\u4f53\u3001\u9519\u8bef\u68c0\u6d4b\u5ef6\u8fdf\u3001\u91cd\u89c4\u5212\u6ede\u540e\u3002\u9700\u8981\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u4fdd\u6301\u611f\u77e5\u4e0e\u907f\u514d\u6602\u8d35\u66f4\u65b0\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51faBINDER\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u6a21\u5757\uff1a1) \u5ba1\u614e\u54cd\u5e94\u6a21\u5757\uff08DRM\uff09- \u591a\u6a21\u6001LLM\u8fdb\u884c\u6218\u7565\u89c4\u5212\u548c\u7ed3\u6784\u53163D\u573a\u666f\u66f4\u65b0\uff1b2) \u5373\u65f6\u54cd\u5e94\u6a21\u5757\uff08IRM\uff09- VideoLLM\u8fdb\u884c\u8fde\u7eed\u89c6\u9891\u6d41\u76d1\u63a7\u3002\u4e24\u6a21\u5757\u53cc\u5411\u534f\u8c03\uff1aDRM\u6307\u5bfcIRM\u5173\u6ce8\u5185\u5bb9\uff0cIRM\u66f4\u65b0\u8bb0\u5fc6\u3001\u7ea0\u6b63\u6b63\u5728\u6267\u884c\u7684\u52a8\u4f5c\u5e76\u5728\u5fc5\u8981\u65f6\u89e6\u53d1\u91cd\u89c4\u5212\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8fdb\u884c\u52a8\u6001\u7269\u4f53\u653e\u7f6e\u8bc4\u4f30\uff0cBINDER\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u663e\u8457\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u6548\u7387\uff0c\u5c55\u793a\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u6709\u6548\u6027\u3002", "conclusion": "BINDER\u901a\u8fc7\u5206\u79bb\u6218\u7565\u89c4\u5212\u548c\u8fde\u7eed\u76d1\u63a7\u7684\u53cc\u8fc7\u7a0b\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\u4e2d\u52a8\u6001\u73af\u5883\u9002\u5e94\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u9c81\u68d2\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2511.21709", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.21709", "abs": "https://arxiv.org/abs/2511.21709", "authors": ["Blessed Guda", "Lawrence Francis", "Gabrial Zencha Ashungafac", "Carlee Joe-Wong", "Moise Busogi"], "title": "Quantifying and Mitigating Selection Bias in LLMs: A Transferable LoRA Fine-Tuning and Efficient Majority Voting Approach", "comment": "Accepted into IJCNLP-AACL 2026", "summary": "Multiple Choice Question (MCQ) answering is a widely used method for evaluating the performance of Large Language Models (LLMs). However, LLMs often exhibit selection bias in MCQ tasks, where their choices are influenced by factors like answer position or option symbols rather than the content. This bias undermines the reliability of MCQ as an evaluation framework. Most existing selection bias metrics require answer labels and measure divergences between prediction and answer distributions, but do not fully capture the consistency of a model's predictions across different orderings of answer choices. Existing selection bias mitigation strategies have notable limitations: majority voting, though effective, is computationally prohibitive; calibration-based methods require validation sets and often fail to generalize across datasets. To address these gaps, we propose three key contributions: (1) a new unsupervised label-free Permutation Bias Metric (PBM) that directly quantifies inconsistencies in model predictions across answer permutations, providing a more precise measure of selection bias, (2) an efficient majority voting approach called Batch Question-Context KV caching (BaQCKV), to significantly reduce computational costs while preserving bias mitigation effectiveness, and (3) an unsupervised Low-Rank Adaptation (LoRA-1) fine-tuning strategy based on our proposed metric and the BaQCKV that mitigates selection bias, providing a computationally efficient alternative that maintains model generalizability. Experiments across multiple MCQ benchmarks demonstrate that our approaches reduce bias, increasing consistency in accuracy while minimizing computational costs.", "AI": {"tldr": "\u63d0\u51faPBM\u6307\u6807\u3001BaQCKV\u9ad8\u6548\u591a\u6570\u6295\u7968\u548cLoRA-1\u5fae\u8c03\u7b56\u7565\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u5730\u91cf\u5316\u548c\u7f13\u89e3LLM\u5728\u9009\u62e9\u9898\u4e2d\u7684\u9009\u62e9\u504f\u5dee\u3002", "motivation": "LLM\u5728\u9009\u62e9\u9898\u4efb\u52a1\u4e2d\u5b58\u5728\u9009\u62e9\u504f\u5dee\uff08\u53d7\u7b54\u6848\u4f4d\u7f6e\u6216\u9009\u9879\u7b26\u53f7\u5f71\u54cd\u800c\u975e\u5185\u5bb9\uff09\uff0c\u8fd9\u524a\u5f31\u4e86\u9009\u62e9\u9898\u4f5c\u4e3a\u8bc4\u4f30\u6846\u67b6\u7684\u53ef\u9760\u6027\u3002\u73b0\u6709\u504f\u5dee\u5ea6\u91cf\u9700\u8981\u7b54\u6848\u6807\u7b7e\u4e14\u65e0\u6cd5\u5145\u5206\u6355\u6349\u6a21\u578b\u5728\u4e0d\u540c\u9009\u9879\u6392\u5217\u4e0b\u9884\u6d4b\u7684\u4e00\u81f4\u6027\uff0c\u800c\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "1) \u63d0\u51fa\u65e0\u76d1\u7763\u65e0\u6807\u7b7e\u7684\u6392\u5217\u504f\u5dee\u5ea6\u91cf(PBM)\uff0c\u76f4\u63a5\u91cf\u5316\u6a21\u578b\u5728\u4e0d\u540c\u7b54\u6848\u6392\u5217\u4e0b\u9884\u6d4b\u7684\u4e0d\u4e00\u81f4\u6027\uff1b2) \u63d0\u51fa\u6279\u91cf\u95ee\u9898\u4e0a\u4e0b\u6587KV\u7f13\u5b58(BaQCKV)\u7684\u9ad8\u6548\u591a\u6570\u6295\u7968\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff1b3) \u57fa\u4e8ePBM\u548cBaQCKV\u63d0\u51fa\u65e0\u76d1\u7763\u4f4e\u79e9\u9002\u5e94(LoRA-1)\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u9009\u62e9\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u51cf\u5c11\u504f\u5dee\uff0c\u63d0\u9ad8\u51c6\u786e\u7387\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u63d0\u51fa\u7684PBM\u6307\u6807\u3001BaQCKV\u9ad8\u6548\u6295\u7968\u548cLoRA-1\u5fae\u8c03\u7b56\u7565\u4e3aLLM\u9009\u62e9\u9898\u8bc4\u4f30\u4e2d\u7684\u9009\u62e9\u504f\u5dee\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65e0\u76d1\u7763\u91cf\u5316\u548c\u7f13\u89e3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\u3002"}}
{"id": "2511.22445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22445", "abs": "https://arxiv.org/abs/2511.22445", "authors": ["Yikai Tang", "Haoran Geng", "Sheng Zang", "Pieter Abbeel", "Jitendra Malik"], "title": "Visual-Geometry Diffusion Policy: Robust Generalization via Complementarity-Aware Multimodal Fusion", "comment": null, "summary": "Imitation learning has emerged as a crucial ap proach for acquiring visuomotor skills from demonstrations, where designing effective observation encoders is essential for policy generalization. However, existing methods often struggle to generalize under spatial and visual randomizations, instead tending to overfit. To address this challenge, we propose Visual Geometry Diffusion Policy (VGDP), a multimodal imitation learning framework built around a Complementarity-Aware Fusion Module where modality-wise dropout enforces balanced use of RGB and point-cloud cues, with cross-attention serving only as a lightweight interaction layer. Our experiments show that the expressiveness of the fused latent space is largely induced by the enforced complementarity from modality-wise dropout, with cross-attention serving primarily as a lightweight interaction mechanism rather than the main source of robustness. Across a benchmark of 18 simulated tasks and 4 real-world tasks, VGDP outperforms seven baseline policies with an average performance improvement of 39.1%. More importantly, VGDP demonstrates strong robustness under visual and spatial per turbations, surpassing baselines with an average improvement of 41.5% in different visual conditions and 15.2% in different spatial settings.", "AI": {"tldr": "VGDP\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e92\u8865\u611f\u77e5\u878d\u5408\u6a21\u5757\u548c\u6a21\u6001\u7ea7dropout\u5e73\u8861RGB\u548c\u70b9\u4e91\u7279\u5f81\uff0c\u5728\u89c6\u89c9\u548c\u7a7a\u95f4\u6270\u52a8\u4e0b\u8868\u73b0\u51fa\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u7a7a\u95f4\u548c\u89c6\u89c9\u968f\u673a\u5316\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u5bb9\u6613\u8fc7\u62df\u5408\uff0c\u9700\u8981\u8bbe\u8ba1\u66f4\u6709\u6548\u7684\u89c2\u5bdf\u7f16\u7801\u5668\u6765\u63d0\u5347\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u51e0\u4f55\u6269\u6563\u7b56\u7565(VGDP)\uff0c\u91c7\u7528\u4e92\u8865\u611f\u77e5\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u6a21\u6001\u7ea7dropout\u5f3a\u5236\u5e73\u8861RGB\u548c\u70b9\u4e91\u7ebf\u7d22\u7684\u4f7f\u7528\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u4ec5\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u4ea4\u4e92\u5c42\u3002", "result": "\u572818\u4e2a\u6a21\u62df\u4efb\u52a1\u548c4\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0cVGDP\u5e73\u5747\u6027\u80fd\u63d0\u534739.1%\uff0c\u5728\u89c6\u89c9\u6270\u52a8\u4e0b\u5e73\u5747\u63d0\u534741.5%\uff0c\u5728\u7a7a\u95f4\u8bbe\u7f6e\u4e0b\u5e73\u5747\u63d0\u534715.2%\uff0c\u663e\u8457\u4f18\u4e8e7\u4e2a\u57fa\u7ebf\u7b56\u7565\u3002", "conclusion": "\u878d\u5408\u6f5c\u5728\u7a7a\u95f4\u7684\u8868\u8fbe\u80fd\u529b\u4e3b\u8981\u6765\u81ea\u6a21\u6001\u7ea7dropout\u5f3a\u5236\u5b9e\u73b0\u7684\u4e92\u8865\u6027\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u4e3b\u8981\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u4ea4\u4e92\u673a\u5236\u800c\u975e\u9c81\u68d2\u6027\u7684\u4e3b\u8981\u6765\u6e90\u3002"}}
{"id": "2511.21711", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21711", "abs": "https://arxiv.org/abs/2511.21711", "authors": ["Fatima Kazi"], "title": "Addressing Stereotypes in Large Language Models: A Critical Examination and Mitigation", "comment": null, "summary": "Large Language models (LLMs), such as ChatGPT, have gained popularity in recent years with the advancement of Natural Language Processing (NLP), with use cases spanning many disciplines and daily lives as well. LLMs inherit explicit and implicit biases from the datasets they were trained on; these biases can include social, ethical, cultural, religious, and other prejudices and stereotypes. It is important to comprehensively examine such shortcomings by identifying the existence and extent of such biases, recognizing the origin, and attempting to mitigate such biased outputs to ensure fair outputs to reduce harmful stereotypes and misinformation. This study inspects and highlights the need to address biases in LLMs amid growing generative Artificial Intelligence (AI). We utilize bias-specific benchmarks such StereoSet and CrowSPairs to evaluate the existence of various biases in many different generative models such as BERT, GPT 3.5, and ADA. To detect both explicit and implicit biases, we adopt a three-pronged approach for thorough and inclusive analysis. Results indicate fine-tuned models struggle with gender biases but excel at identifying and avoiding racial biases. Our findings also illustrated that despite some cases of success, LLMs often over-rely on keywords in prompts and its outputs. This demonstrates the incapability of LLMs to attempt to truly understand the accuracy and authenticity of its outputs. Finally, in an attempt to bolster model performance, we applied an enhancement learning strategy involving fine-tuning, models using different prompting techniques, and data augmentation of the bias benchmarks. We found fine-tuned models to exhibit promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u95ee\u9898\uff0c\u4f7f\u7528StereoSet\u548cCrowSPairs\u57fa\u51c6\u6d4b\u8bd5BERT\u3001GPT-3.5\u7b49\u6a21\u578b\uff0c\u53d1\u73b0\u5fae\u8c03\u6a21\u578b\u5728\u6027\u522b\u504f\u89c1\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u4f46\u5728\u79cd\u65cf\u504f\u89c1\u8bc6\u522b\u4e0a\u8f83\u597d\uff0c\u901a\u8fc7\u589e\u5f3a\u5b66\u4e60\u7b56\u7565\u53ef\u5c06\u9690\u5f0f\u504f\u89c1\u68c0\u6d4b\u6027\u80fd\u63d0\u5347\u8fbe20%\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\u53ca\u5176\u5728\u5404\u5b66\u79d1\u548c\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u8fd9\u4e9b\u6a21\u578b\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u7ee7\u627f\u4e86\u663e\u6027\u548c\u9690\u6027\u7684\u504f\u89c1\uff0c\u5305\u62ec\u793e\u4f1a\u3001\u4f26\u7406\u3001\u6587\u5316\u3001\u5b97\u6559\u7b49\u65b9\u9762\u7684\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u3002\u9700\u8981\u5168\u9762\u68c0\u67e5\u8fd9\u4e9b\u7f3a\u9677\uff0c\u8bc6\u522b\u504f\u89c1\u7684\u5b58\u5728\u548c\u7a0b\u5ea6\uff0c\u4e86\u89e3\u5176\u6765\u6e90\uff0c\u5e76\u5c1d\u8bd5\u51cf\u8f7b\u8fd9\u4e9b\u6709\u504f\u89c1\u7684\u8f93\u51fa\uff0c\u4ee5\u786e\u4fdd\u516c\u5e73\u8f93\u51fa\uff0c\u51cf\u5c11\u6709\u5bb3\u7684\u523b\u677f\u5370\u8c61\u548c\u9519\u8bef\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u4e09\u7ba1\u9f50\u4e0b\u7684\u65b9\u6cd5\u8fdb\u884c\u5f7b\u5e95\u548c\u5305\u5bb9\u6027\u5206\u6790\uff1a1) \u4f7f\u7528StereoSet\u548cCrowSPairs\u7b49\u504f\u89c1\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30BERT\u3001GPT-3.5\u3001ADA\u7b49\u591a\u79cd\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5404\u79cd\u504f\u89c1\uff1b2) \u68c0\u6d4b\u663e\u6027\u548c\u9690\u6027\u504f\u89c1\uff1b3) \u5e94\u7528\u589e\u5f3a\u5b66\u4e60\u7b56\u7565\uff0c\u5305\u62ec\u5fae\u8c03\u6a21\u578b\u3001\u4f7f\u7528\u4e0d\u540c\u7684\u63d0\u793a\u6280\u672f\u548c\u504f\u89c1\u57fa\u51c6\u7684\u6570\u636e\u589e\u5f3a\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a1) \u5fae\u8c03\u6a21\u578b\u5728\u6027\u522b\u504f\u89c1\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u5728\u8bc6\u522b\u548c\u907f\u514d\u79cd\u65cf\u504f\u89c1\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff1b2) \u5c3d\u7ba1\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6210\u529f\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5f80\u5f80\u8fc7\u5ea6\u4f9d\u8d56\u63d0\u793a\u4e2d\u7684\u5173\u952e\u8bcd\uff0c\u65e0\u6cd5\u771f\u6b63\u7406\u89e3\u5176\u8f93\u51fa\u7684\u51c6\u786e\u6027\u548c\u771f\u5b9e\u6027\uff1b3) \u589e\u5f3a\u5b66\u4e60\u7b56\u7565\u4f7f\u5fae\u8c03\u6a21\u578b\u5728\u8de8\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6709\u5e0c\u671b\u7684\u9002\u5e94\u6027\uff0c\u5728\u9690\u6027\u504f\u89c1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u6027\u80fd\u589e\u76ca\u9ad8\u8fbe20%\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5feb\u901f\u53d1\u5c55\u80cc\u666f\u4e0b\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u504f\u89c1\u95ee\u9898\u7684\u91cd\u8981\u6027\u3002\u867d\u7136\u5fae\u8c03\u6a21\u578b\u5728\u7279\u5b9a\u504f\u89c1\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u52bf\uff0c\u4f46\u6a21\u578b\u4ecd\u5b58\u5728\u8fc7\u5ea6\u4f9d\u8d56\u5173\u952e\u8bcd\u548c\u65e0\u6cd5\u771f\u6b63\u7406\u89e3\u8f93\u51fa\u7684\u5c40\u9650\u6027\u3002\u589e\u5f3a\u5b66\u4e60\u7b56\u7565\u4e3a\u6539\u5584\u6a21\u578b\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u7279\u522b\u662f\u5728\u9690\u6027\u504f\u89c1\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u8868\u660e\u901a\u8fc7\u9002\u5f53\u7684\u6280\u672f\u5e72\u9884\u53ef\u4ee5\u51cf\u8f7b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u95ee\u9898\u3002"}}
{"id": "2511.22505", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22505", "abs": "https://arxiv.org/abs/2511.22505", "authors": ["Xiujian Liang", "Jiacheng Liu", "Mingyang Sun", "Qichen He", "Cewu Lu", "Jianhua Sun"], "title": "RealD$^2$iff: Bridging Real-World Gap in Robot Manipulation via Depth Diffusion", "comment": null, "summary": "Robot manipulation in the real world is fundamentally constrained by the visual sim2real gap, where depth observations collected in simulation fail to reflect the complex noise patterns inherent to real sensors. In this work, inspired by the denoising capability of diffusion models, we invert the conventional perspective and propose a clean-to-noisy paradigm that learns to synthesize noisy depth, thereby bridging the visual sim2real gap through purely simulation-driven robotic learning. Building on this idea, we introduce RealD$^2$iff, a hierarchical coarse-to-fine diffusion framework that decomposes depth noise into global structural distortions and fine-grained local perturbations. To enable progressive learning of these components, we further develop two complementary strategies: Frequency-Guided Supervision (FGS) for global structure modeling and Discrepancy-Guided Optimization (DGO) for localized refinement. To integrate RealD$^2$iff seamlessly into imitation learning, we construct a pipeline that spans six stages. We provide comprehensive empirical and experimental validation demonstrating the effectiveness of this paradigm. RealD$^2$iff enables two key applications: (1) generating real-world-like depth to construct clean-noisy paired datasets without manual sensor data collection. (2) Achieving zero-shot sim2real robot manipulation, substantially improving real-world performance without additional fine-tuning.", "AI": {"tldr": "\u63d0\u51faRealD\u00b2iff\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b66\u4e60\u4ece\u5e72\u51c0\u6df1\u5ea6\u5408\u6210\u771f\u5b9e\u566a\u58f0\u6df1\u5ea6\uff0c\u5b9e\u73b0\u7eaf\u4eff\u771f\u9a71\u52a8\u7684\u673a\u5668\u4eba\u5b66\u4e60\uff0c\u65e0\u9700\u771f\u5b9e\u4f20\u611f\u5668\u6570\u636e\u6536\u96c6\u5373\u53ef\u6784\u5efa\u5e72\u51c0-\u566a\u58f0\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u5e76\u5b9e\u73b0\u96f6\u6837\u672csim2real\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u53d7\u89c6\u89c9sim2real\u5dee\u8ddd\u9650\u5236\uff0c\u4eff\u771f\u4e2d\u6536\u96c6\u7684\u6df1\u5ea6\u89c2\u6d4b\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4f20\u611f\u5668\u7684\u590d\u6742\u566a\u58f0\u6a21\u5f0f\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u771f\u5b9e\u6df1\u5ea6\u566a\u58f0\u3002", "method": "\u63d0\u51faRealD\u00b2iff\u5206\u5c42\u6269\u6563\u6846\u67b6\uff0c\u5c06\u6df1\u5ea6\u566a\u58f0\u5206\u89e3\u4e3a\u5168\u5c40\u7ed3\u6784\u626d\u66f2\u548c\u7ec6\u7c92\u5ea6\u5c40\u90e8\u6270\u52a8\u3002\u91c7\u7528\u9891\u7387\u5f15\u5bfc\u76d1\u7763(FGS)\u8fdb\u884c\u5168\u5c40\u7ed3\u6784\u5efa\u6a21\uff0c\u5dee\u5f02\u5f15\u5bfc\u4f18\u5316(DGO)\u8fdb\u884c\u5c40\u90e8\u7ec6\u5316\u3002\u6784\u5efa\u516d\u9636\u6bb5\u7ba1\u9053\u96c6\u6210\u5230\u6a21\u4eff\u5b66\u4e60\u4e2d\u3002", "result": "RealD\u00b2iff\u80fd\u591f\u751f\u6210\u771f\u5b9e\u4e16\u754c\u822c\u7684\u6df1\u5ea6\u6570\u636e\uff0c\u65e0\u9700\u624b\u52a8\u4f20\u611f\u5668\u6570\u636e\u6536\u96c6\u5373\u53ef\u6784\u5efa\u5e72\u51c0-\u566a\u58f0\u914d\u5bf9\u6570\u636e\u96c6\u3002\u5b9e\u73b0\u96f6\u6837\u672csim2real\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u9ad8\u771f\u5b9e\u4e16\u754c\u6027\u80fd\u800c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u3002", "conclusion": "\u901a\u8fc7clean-to-noisy\u8303\u5f0f\u6210\u529f\u5f25\u5408\u89c6\u89c9sim2real\u5dee\u8ddd\uff0c\u5b9e\u73b0\u7eaf\u4eff\u771f\u9a71\u52a8\u7684\u673a\u5668\u4eba\u5b66\u4e60\u3002RealD\u00b2iff\u6846\u67b6\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u6709\u6548\u7684\u89c6\u89c9\u57df\u9002\u5e94\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21712", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.21712", "abs": "https://arxiv.org/abs/2511.21712", "authors": ["Yi Ding", "Xushuo Tang", "Zhengyi Yang", "Wenqian Zhang", "Simin Wu", "Yuxin Huang", "Lingjing Lan", "Weiyuan Li", "Yin Chen", "Mingchen Ju", "Wenke Yang", "Thong Hoang", "Mykhailo Klymenko", "Xiwei Zu", "Wenjie Zhang"], "title": "EulerESG: Automating ESG Disclosure Analysis with LLMs", "comment": null, "summary": "Environmental, Social, and Governance (ESG) reports have become central to how companies communicate climate risk, social impact, and governance practices, yet they are still published primarily as long, heterogeneous PDF documents. This makes it difficult to systematically answer seemingly simple questions. Existing tools either rely on brittle rule-based extraction or treat ESG reports as generic text, without explicitly modelling the underlying reporting standards. We present \\textbf{EulerESG}, an LLM-powered system for automating ESG disclosure analysis with explicit awareness of ESG frameworks. EulerESG combines (i) dual-channel retrieval and LLM-driven disclosure analysis over ESG reports, and (ii) an interactive dashboard and chatbot for exploration, benchmarking, and explanation. Using four globally recognised companies and twelve SASB sub-industries, we show that EulerESG can automatically populate standard-aligned metric tables with high fidelity (up to 0.95 average accuracy) while remaining practical in end-to-end runtime, and we compare several recent LLM models in this setting. The full implementation, together with a demonstration video, is publicly available at https://github.com/UNSW-database/EulerESG.", "AI": {"tldr": "EulerESG\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316ESG\u62ab\u9732\u5206\u6790\uff0c\u901a\u8fc7\u53cc\u901a\u9053\u68c0\u7d22\u548cLLM\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u4ee5\u9ad8\u51c6\u786e\u7387\u586b\u5145\u6807\u51c6\u5bf9\u9f50\u7684\u6307\u6807\u8868\u3002", "motivation": "ESG\u62a5\u544a\u4e3b\u8981\u4ee5\u5197\u957f\u3001\u5f02\u6784\u7684PDF\u6587\u6863\u5f62\u5f0f\u53d1\u5e03\uff0c\u96be\u4ee5\u7cfb\u7edf\u56de\u7b54\u7b80\u5355\u95ee\u9898\u3002\u73b0\u6709\u5de5\u5177\u8981\u4e48\u4f9d\u8d56\u8106\u5f31\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u63d0\u53d6\uff0c\u8981\u4e48\u5c06ESG\u62a5\u544a\u89c6\u4e3a\u901a\u7528\u6587\u672c\uff0c\u6ca1\u6709\u660e\u786e\u5efa\u6a21\u5e95\u5c42\u62a5\u544a\u6807\u51c6\u3002", "method": "\u7ed3\u5408(i)\u53cc\u901a\u9053\u68c0\u7d22\u548cLLM\u9a71\u52a8\u7684ESG\u62a5\u544a\u62ab\u9732\u5206\u6790\uff0c\u4ee5\u53ca(ii)\u4ea4\u4e92\u5f0f\u4eea\u8868\u677f\u548c\u804a\u5929\u673a\u5668\u4eba\uff0c\u7528\u4e8e\u63a2\u7d22\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u89e3\u91ca\u3002\u7cfb\u7edf\u660e\u786e\u8003\u8651ESG\u6846\u67b6\uff0c\u4f7f\u7528LLM\u6a21\u578b\u8fdb\u884c\u81ea\u52a8\u5316\u5206\u6790\u3002", "result": "\u5728\u56db\u4e2a\u5168\u7403\u77e5\u540d\u516c\u53f8\u548c\u5341\u4e8c\u4e2aSASB\u5b50\u884c\u4e1a\u4e0a\u6d4b\u8bd5\uff0cEulerESG\u80fd\u591f\u4ee5\u9ad8\u4fdd\u771f\u5ea6\uff08\u6700\u9ad80.95\u5e73\u5747\u51c6\u786e\u7387\uff09\u81ea\u52a8\u586b\u5145\u6807\u51c6\u5bf9\u9f50\u7684\u6307\u6807\u8868\uff0c\u540c\u65f6\u4fdd\u6301\u7aef\u5230\u7aef\u8fd0\u884c\u65f6\u95f4\u7684\u5b9e\u7528\u6027\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u4e2a\u8fd1\u671fLLM\u6a21\u578b\u7684\u8868\u73b0\u3002", "conclusion": "EulerESG\u662f\u4e00\u4e2a\u5b9e\u7528\u7684LLM\u9a71\u52a8\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u81ea\u52a8\u5316ESG\u62ab\u9732\u5206\u6790\uff0c\u586b\u8865\u4e86\u73b0\u6709\u5de5\u5177\u7684\u4e0d\u8db3\uff0c\u4e3aESG\u62a5\u544a\u5206\u6790\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22541", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22541", "abs": "https://arxiv.org/abs/2511.22541", "authors": ["Jinyang Li", "Marcello Farina", "Luca Mozzarelli", "Luca Cattaneo", "Panita Rattamasanaprapai", "Eleonora A. Tagarelli", "Matteo Corno", "Paolo Perego", "Giuseppe Andreoni", "Emanuele Lettieri"], "title": "BUDD-e: an autonomous robotic guide for visually impaired users", "comment": "14 pages", "summary": "This paper describes the design and the realization of a prototype of the novel guide robot BUDD-e for visually impaired users. The robot has been tested in a real scenario with the help of visually disabled volunteers at ASST Grande Ospedale Metropolitano Niguarda, in Milan. The results of the experimental campaign are throughly described in the paper, displaying its remarkable performance and user-acceptance.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u540d\u4e3aBUDD-e\u7684\u65b0\u578b\u5bfc\u76f2\u673a\u5668\u4eba\u539f\u578b\uff0c\u5e76\u5728\u533b\u9662\u771f\u5b9e\u573a\u666f\u4e2d\u7531\u89c6\u969c\u5fd7\u613f\u8005\u6d4b\u8bd5\uff0c\u8868\u73b0\u51fa\u8272\u4e14\u7528\u6237\u63a5\u53d7\u5ea6\u9ad8\u3002", "motivation": "\u4e3a\u89c6\u969c\u7528\u6237\u5f00\u53d1\u5b9e\u7528\u7684\u5bfc\u76f2\u673a\u5668\u4eba\uff0c\u5e2e\u52a9\u4ed6\u4eec\u5728\u590d\u6742\u73af\u5883\u4e2d\u5bfc\u822a\uff0c\u63d0\u9ad8\u72ec\u7acb\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86BUDD-e\u5bfc\u76f2\u673a\u5668\u4eba\u539f\u578b\uff0c\u5728\u7c73\u5170Niguarda\u5927\u90fd\u4f1a\u533b\u9662\u7684\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u901a\u8fc7\u89c6\u969c\u5fd7\u613f\u8005\u8fdb\u884c\u5b9e\u5730\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u673a\u5668\u4eba\u6027\u80fd\u5353\u8d8a\uff0c\u7528\u6237\u63a5\u53d7\u5ea6\u9ad8\uff0c\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "BUDD-e\u5bfc\u76f2\u673a\u5668\u4eba\u539f\u578b\u5728\u771f\u5b9e\u573a\u666f\u6d4b\u8bd5\u4e2d\u8bc1\u660e\u6709\u6548\uff0c\u5177\u6709\u826f\u597d\u7684\u6027\u80fd\u548c\u7528\u6237\u63a5\u53d7\u5ea6\uff0c\u4e3a\u89c6\u969c\u4eba\u58eb\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5bfc\u822a\u8f85\u52a9\u3002"}}
{"id": "2511.21714", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21714", "abs": "https://arxiv.org/abs/2511.21714", "authors": ["Pawel Batorski", "Paul Swoboda"], "title": "GPS: General Per-Sample Prompter", "comment": null, "summary": "LLMs are sensitive to prompting, with task performance often hinging on subtle, sometimes imperceptible variations in phrasing. As a result, crafting effective prompts manually remains challenging and time-consuming. Recent automatic prompting methods mitigate this difficulty but face three key limitations: (i) for each new task, they require large datasets to train good prompts;(ii) they rely on costly optimization loops that may take hours; (iii)they typically produce a single task-level prompt that does not adapt to the individual input problem to be solved.\n  We propose GPS, the first general-purpose, per-sample prompting method. Without any task-specific tuning, GPS generates a tailored prompt for each unseen input, improving performance across diverse tasks. The prompter is trained with reinforcement learning on a suite of training tasks and includes a novel regularization for effectively adapting to per-sample prompting. Finally, we employ Minimum Bayes Risk decoding to stabilize inference.\n  Empirically, GPS demonstrates competitive performance: we attain second best results among baselines on text simplification, third best results on summarization and on-par results on classification, while not training on any of these tasks, in contrast to the baselines. For in-domain prompting, we obtain sota on GSM8K. Our work shows the potential of a novel and effective paradigm for automatic prompting: generating adaptive, input-specific prompts without extensive optimization and without access to a task-specific training set. Our code is available at https://github.com/Batorskq/GPS.", "AI": {"tldr": "GPS\u662f\u4e00\u79cd\u901a\u7528\u7684\u6309\u6837\u672c\u63d0\u793a\u65b9\u6cd5\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u8c03\u4f18\u5373\u53ef\u4e3a\u6bcf\u4e2a\u8f93\u5165\u751f\u6210\u5b9a\u5236\u63d0\u793a\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u7ade\u4e89\u6027\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u5173\u952e\u9650\u5236\uff1a\u9700\u8981\u5927\u91cf\u4efb\u52a1\u7279\u5b9a\u6570\u636e\u8bad\u7ec3\u3001\u4f9d\u8d56\u8017\u65f6\u7684\u4f18\u5316\u5faa\u73af\u3001\u53ea\u80fd\u751f\u6210\u5355\u4e00\u4efb\u52a1\u7ea7\u63d0\u793a\u800c\u65e0\u6cd5\u9002\u5e94\u4e2a\u4f53\u8f93\u5165\u3002\u624b\u52a8\u8bbe\u8ba1\u63d0\u793a\u65e2\u56f0\u96be\u53c8\u8017\u65f6\u3002", "method": "\u63d0\u51faGPS\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u5728\u8bad\u7ec3\u4efb\u52a1\u5957\u4ef6\u4e0a\u8bad\u7ec3\u63d0\u793a\u751f\u6210\u5668\uff1b2) \u5f15\u5165\u65b0\u9896\u7684\u6b63\u5219\u5316\u6280\u672f\u4ee5\u6709\u6548\u9002\u5e94\u6309\u6837\u672c\u63d0\u793a\uff1b3) \u91c7\u7528\u6700\u5c0f\u8d1d\u53f6\u65af\u98ce\u9669\u89e3\u7801\u6765\u7a33\u5b9a\u63a8\u7406\u3002", "result": "\u5728\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e2d\u8fbe\u5230\u57fa\u7ebf\u65b9\u6cd5\u7b2c\u4e8c\u597d\u7ed3\u679c\uff0c\u5728\u6458\u8981\u4efb\u52a1\u4e2d\u7b2c\u4e09\u597d\uff0c\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u76f8\u5f53\uff0c\u4e14\u672a\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u5728GSM8K\u4efb\u52a1\u4e0a\u83b7\u5f97\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "GPS\u5c55\u793a\u4e86\u4e00\u79cd\u65b0\u9896\u6709\u6548\u7684\u81ea\u52a8\u63d0\u793a\u8303\u5f0f\uff1a\u65e0\u9700\u5927\u91cf\u4f18\u5316\u548c\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u96c6\u5373\u53ef\u751f\u6210\u81ea\u9002\u5e94\u3001\u8f93\u5165\u7279\u5b9a\u7684\u63d0\u793a\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.22555", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22555", "abs": "https://arxiv.org/abs/2511.22555", "authors": ["Yanbo Mao", "Jianlong Fu", "Ruoxuan Zhang", "Hongxia Xie", "Meibao Yao"], "title": "Beyond Success: Refining Elegant Robot Manipulation from Mixed-Quality Data via Just-in-Time Intervention", "comment": null, "summary": "Vision-Language-Action (VLA) models have enabled notable progress in general-purpose robotic manipulation, yet their learned policies often exhibit variable execution quality. We attribute this variability to the mixed-quality nature of human demonstrations, where the implicit principles that govern how actions should be carried out are only partially satisfied. To address this challenge, we introduce the LIBERO-Elegant benchmark with explicit criteria for evaluating execution quality. Using these criteria, we develop a decoupled refinement framework that improves execution quality without modifying or retraining the base VLA policy. We formalize Elegant Execution as the satisfaction of Implicit Task Constraints (ITCs) and train an Elegance Critic via offline Calibrated Q-Learning to estimate the expected quality of candidate actions. At inference time, a Just-in-Time Intervention (JITI) mechanism monitors critic confidence and intervenes only at decision-critical moments, providing selective, on-demand refinement. Experiments on LIBERO-Elegant and real-world manipulation tasks show that the learned Elegance Critic substantially improves execution quality, even on unseen tasks. The proposed model enables robotic control that values not only whether tasks succeed, but also how they are performed.", "AI": {"tldr": "\u63d0\u51faLIBERO-Elegant\u57fa\u51c6\u548c\u4f18\u96c5\u6267\u884c\u6846\u67b6\uff0c\u901a\u8fc7\u79bb\u7ebf\u5b66\u4e60\u4f18\u96c5\u6279\u8bc4\u5668\u548c\u5373\u65f6\u5e72\u9884\u673a\u5236\uff0c\u5728\u4e0d\u4fee\u6539\u57fa\u7840VLA\u7b56\u7565\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u673a\u5668\u4eba\u6267\u884c\u8d28\u91cf", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u6267\u884c\u8d28\u91cf\u4e0d\u7a33\u5b9a\uff0c\u8fd9\u6e90\u4e8e\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u7684\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff0c\u5176\u4e2d\u9690\u542b\u7684\u4efb\u52a1\u7ea6\u675f\u53ea\u88ab\u90e8\u5206\u6ee1\u8db3", "method": "1) \u5b9a\u4e49\u4f18\u96c5\u6267\u884c\u4e3a\u6ee1\u8db3\u9690\u542b\u4efb\u52a1\u7ea6\u675f\uff1b2) \u901a\u8fc7\u79bb\u7ebf\u6821\u51c6Q\u5b66\u4e60\u8bad\u7ec3\u4f18\u96c5\u6279\u8bc4\u5668\u8bc4\u4f30\u52a8\u4f5c\u8d28\u91cf\uff1b3) \u63a8\u7406\u65f6\u4f7f\u7528\u5373\u65f6\u5e72\u9884\u673a\u5236\u5728\u5173\u952e\u65f6\u523b\u9009\u62e9\u6027\u5e72\u9884", "result": "\u5728LIBERO-Elegant\u57fa\u51c6\u548c\u771f\u5b9e\u4e16\u754c\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f18\u96c5\u6279\u8bc4\u5668\u663e\u8457\u63d0\u5347\u4e86\u6267\u884c\u8d28\u91cf\uff0c\u5373\u4f7f\u5bf9\u672a\u89c1\u4efb\u52a1\u4e5f\u6709\u6548", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u4e0d\u4ec5\u5173\u6ce8\u4efb\u52a1\u662f\u5426\u6210\u529f\uff0c\u66f4\u5173\u6ce8\u4efb\u52a1\u6267\u884c\u65b9\u5f0f\u7684\u673a\u5668\u4eba\u63a7\u5236\uff0c\u4e3a\u63d0\u5347VLA\u6a21\u578b\u6267\u884c\u8d28\u91cf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.21716", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21716", "abs": "https://arxiv.org/abs/2511.21716", "authors": ["Shabbir Anees", "Anshuman", "Ayush Chaurasia", "Prathmesh Bogar"], "title": "An Optimized Machine Learning Classifier for Detecting Fake Reviews Using Extracted Features", "comment": null, "summary": "It is well known that fraudulent reviews cast doubt on the legitimacy and dependability of online purchases. The most recent development that leads customers towards darkness is the appearance of human reviews in computer-generated (CG) ones. In this work, we present an advanced machine-learning-based system that analyses these reviews produced by AI with remarkable precision. Our method integrates advanced text preprocessing, multi-modal feature extraction, Harris Hawks Optimization (HHO) for feature selection, and a stacking ensemble classifier. We implemented this methodology on a public dataset of 40,432 Original (OR) and Computer-Generated (CG) reviews. From an initial set of 13,539 features, HHO selected the most applicable 1,368 features, achieving an 89.9% dimensionality reduction. Our final stacking model achieved 95.40% accuracy, 92.81% precision, 95.01% recall, and a 93.90% F1-Score, which demonstrates that the combination of ensemble learning and bio-inspired optimisation is an effective method for machine-generated text recognition. Because large-scale review analytics commonly run on cloud platforms, privacy-preserving techniques such as differential approaches and secure outsourcing are essential to protect user data in these systems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u68c0\u6d4bAI\u751f\u6210\u7684\u865a\u5047\u8bc4\u8bba\uff0c\u7ed3\u5408\u6587\u672c\u9884\u5904\u7406\u3001\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u3001\u54c8\u91cc\u65af\u9e70\u4f18\u5316\u7b97\u6cd5\u548c\u5806\u53e0\u96c6\u6210\u5206\u7c7b\u5668\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u523095.4%\u51c6\u786e\u7387", "motivation": "\u968f\u7740AI\u751f\u6210\u5185\u5bb9\u7684\u53d1\u5c55\uff0c\u8ba1\u7b97\u673a\u751f\u6210\u7684\u865a\u5047\u8bc4\u8bba\u6df7\u5165\u771f\u5b9e\u8bc4\u8bba\u4e2d\uff0c\u635f\u5bb3\u5728\u7ebf\u8d2d\u7269\u7684\u53ef\u4fe1\u5ea6\uff0c\u9700\u8981\u6709\u6548\u65b9\u6cd5\u8bc6\u522b\u8fd9\u4e9bAI\u751f\u6210\u7684\u865a\u5047\u5185\u5bb9", "method": "\u91c7\u7528\u6587\u672c\u9884\u5904\u7406\u3001\u591a\u6a21\u6001\u7279\u5f81\u63d0\u53d6\u3001\u54c8\u91cc\u65af\u9e70\u4f18\u5316\u7b97\u6cd5\u8fdb\u884c\u7279\u5f81\u9009\u62e9\uff0c\u4f7f\u7528\u5806\u53e0\u96c6\u6210\u5206\u7c7b\u5668\u8fdb\u884c\u68c0\u6d4b\uff0c\u5e76\u8003\u8651\u4e91\u5e73\u53f0\u4e0a\u7684\u9690\u79c1\u4fdd\u62a4\u6280\u672f", "result": "\u572840,432\u6761\u771f\u5b9e\u548cAI\u751f\u6210\u8bc4\u8bba\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u54c8\u91cc\u65af\u9e70\u4f18\u5316\u5c06\u7279\u5f81\u4ece13,539\u4e2a\u51cf\u5c11\u52301,368\u4e2a\uff08\u964d\u7ef489.9%\uff09\uff0c\u6700\u7ec8\u6a21\u578b\u8fbe\u523095.40%\u51c6\u786e\u7387\u300192.81%\u7cbe\u786e\u7387\u300195.01%\u53ec\u56de\u7387\u548c93.90% F1\u5206\u6570", "conclusion": "\u96c6\u6210\u5b66\u4e60\u4e0e\u751f\u7269\u542f\u53d1\u4f18\u5316\u76f8\u7ed3\u5408\u662f\u68c0\u6d4b\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5728\u4e91\u5e73\u53f0\u4e0a\u5e94\u7528\u65f6\u9700\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u548c\u5b89\u5168\u5916\u5305\u7b49\u9690\u79c1\u4fdd\u62a4\u6280\u672f"}}
{"id": "2511.22685", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22685", "abs": "https://arxiv.org/abs/2511.22685", "authors": ["Haoyi Wang", "Licheng Luo", "Yiannis Kantaros", "Bruno Sinopoli", "Mingyu Cai"], "title": "Deadlock-Free Hybrid RL-MAPF Framework for Zero-Shot Multi-Robot Navigation", "comment": null, "summary": "Multi-robot navigation in cluttered environments presents fundamental challenges in balancing reactive collision avoidance with long-range goal achievement. When navigating through narrow passages\n  or confined spaces, deadlocks frequently emerge that prevent agents from reaching their destinations, particularly when Reinforcement Learning (RL) control policies encounter novel configurations out of learning distribution. Existing RL-based approaches suffer from limited generalization capability in unseen environments. We propose a hybrid framework that seamlessly integrates RL-based reactive navigation with on-demand Multi-Agent Path Finding (MAPF) to explicitly resolve topological deadlocks. Our approach integrates a safety layer that monitors agent progress to detect deadlocks and, when detected, triggers a coordination controller for affected agents. The framework constructs globally feasible trajectories via MAPF and regulates waypoint progression to reduce inter-agent conflicts during navigation.\n  Extensive evaluation on dense multi-agent benchmarks shows that our method boosts task completion from marginal to near-universal success, markedly reducing deadlocks and collisions. When integrated with hierarchical task planning, it enables coordinated navigation for heterogeneous robots, demonstrating that coupling reactive RL navigation with selective MAPF intervention yields a robust, zero-shot performance.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u53cd\u5e94\u5f0f\u5bfc\u822a\u4e0e\u6309\u9700\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff0c\u89e3\u51b3\u5bc6\u96c6\u73af\u5883\u4e2d\u591a\u673a\u5668\u4eba\u5bfc\u822a\u7684\u6b7b\u9501\u95ee\u9898", "motivation": "\u591a\u673a\u5668\u4eba\u5728\u5bc6\u96c6\u73af\u5883\u5bfc\u822a\u65f6\uff0c\u9700\u8981\u5728\u53cd\u5e94\u5f0f\u907f\u78b0\u4e0e\u957f\u671f\u76ee\u6807\u8fbe\u6210\u95f4\u5e73\u8861\u3002\u72ed\u7a84\u901a\u9053\u6216\u53d7\u9650\u7a7a\u95f4\u4e2d\u5e38\u51fa\u73b0\u6b7b\u9501\uff0c\u7279\u522b\u662f\u5f53\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u7b56\u7565\u9047\u5230\u8d85\u51fa\u5b66\u4e60\u5206\u5e03\u7684\u65b0\u914d\u7f6e\u65f6\u3002\u73b0\u6709\u57fa\u4e8eRL\u7684\u65b9\u6cd5\u5728\u672a\u89c1\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6\uff0c\u65e0\u7f1d\u96c6\u6210\u57fa\u4e8eRL\u7684\u53cd\u5e94\u5f0f\u5bfc\u822a\u4e0e\u6309\u9700\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212(MAPF)\u3002\u5305\u542b\u5b89\u5168\u5c42\u76d1\u63a7\u667a\u80fd\u4f53\u8fdb\u5ea6\u4ee5\u68c0\u6d4b\u6b7b\u9501\uff0c\u89e6\u53d1\u65f6\u542f\u52a8\u53d7\u5f71\u54cd\u667a\u80fd\u4f53\u7684\u534f\u8c03\u63a7\u5236\u5668\u3002\u6846\u67b6\u901a\u8fc7MAPF\u6784\u5efa\u5168\u5c40\u53ef\u884c\u8f68\u8ff9\uff0c\u5e76\u8c03\u8282\u822a\u70b9\u8fdb\u5ea6\u4ee5\u51cf\u5c11\u5bfc\u822a\u4e2d\u7684\u667a\u80fd\u4f53\u95f4\u51b2\u7a81\u3002", "result": "\u5728\u5bc6\u96c6\u591a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u5c06\u4efb\u52a1\u5b8c\u6210\u7387\u4ece\u8fb9\u7f18\u63d0\u5347\u5230\u63a5\u8fd1\u666e\u904d\u6210\u529f\uff0c\u663e\u8457\u51cf\u5c11\u6b7b\u9501\u548c\u78b0\u649e\u3002\u4e0e\u5206\u5c42\u4efb\u52a1\u89c4\u5212\u7ed3\u5408\u65f6\uff0c\u652f\u6301\u5f02\u6784\u673a\u5668\u4eba\u7684\u534f\u8c03\u5bfc\u822a\uff0c\u8bc1\u660e\u53cd\u5e94\u5f0fRL\u5bfc\u822a\u4e0e\u9009\u62e9\u6027MAPF\u5e72\u9884\u7ed3\u5408\u80fd\u5b9e\u73b0\u9c81\u68d2\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "conclusion": "\u8026\u5408\u53cd\u5e94\u5f0fRL\u5bfc\u822a\u4e0e\u9009\u62e9\u6027MAPF\u5e72\u9884\u80fd\u4ea7\u751f\u9c81\u68d2\u3001\u96f6\u6837\u672c\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u6b7b\u9501\u95ee\u9898\uff0c\u63d0\u5347\u5728\u5bc6\u96c6\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6210\u529f\u7387\u3002"}}
{"id": "2511.21717", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21717", "abs": "https://arxiv.org/abs/2511.21717", "authors": ["Baoliang Tian", "Yuxuan Si", "Jilong Wang", "Lingyao Li", "Zhongyuan Bao", "Zineng Zhou", "Tao Wang", "Sixu Li", "Ziyao Xu", "Mingze Wang", "Zhouzhuo Zhang", "Zhihao Wang", "Yike Yun", "Ke Tian", "Ning Yang", "Minghui Qiu"], "title": "CrossCheck-Bench: Diagnosing Compositional Failures in Multimodal Conflict Resolution", "comment": "Accepted by AAAI 2026", "summary": "Multimodal Large Language Models are primarily trained and evaluated on aligned image-text pairs, which leaves their ability to detect and resolve real-world inconsistencies largely unexplored. In open-domain applications visual and textual cues often conflict, requiring models to perform structured reasoning beyond surface-level alignment. We introduce CrossCheck-Bench, a diagnostic benchmark for evaluating contradiction detection in multimodal inputs. The benchmark adopts a hierarchical task framework covering three levels of reasoning complexity and defines seven atomic capabilities essential for resolving cross-modal inconsistencies. CrossCheck-Bench includes 15k question-answer pairs sourced from real-world artifacts with synthetically injected contradictions. The dataset is constructed through a multi-stage annotation pipeline involving more than 450 expert hours to ensure semantic validity and calibrated difficulty across perception, integration, and reasoning. We evaluate 13 state-of-the-art vision-language models and observe a consistent performance drop as tasks shift from perceptual matching to logical contradiction detection. Most models perform well on isolated entity recognition but fail when multiple clues must be synthesized for conflict reasoning. Capability-level analysis further reveals uneven skill acquisition, especially in tasks requiring multi-step inference or rule-based validation. Additional probing shows that conventional prompting strategies such as Chain-of-Thought and Set-of-Mark yield only marginal gains. By contrast, methods that interleave symbolic reasoning with grounded visual processing achieve more stable improvements. These results highlight a persistent bottleneck in multimodal reasoning and suggest new directions for building models capable of robust cross-modal verification.", "AI": {"tldr": "CrossCheck-Bench\u662f\u4e00\u4e2a\u8bca\u65ad\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4b\u548c\u89e3\u51b3\u56fe\u50cf-\u6587\u672c\u77db\u76fe\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5305\u542b15k\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u8986\u76d6\u4e09\u4e2a\u63a8\u7406\u590d\u6742\u5ea6\u5c42\u6b21\u548c\u4e03\u79cd\u6838\u5fc3\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u5728\u5bf9\u9f50\u7684\u56fe\u50cf-\u6587\u672c\u5bf9\u4e0a\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u7f3a\u4e4f\u5bf9\u73b0\u5b9e\u4e16\u754c\u4e2d\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u548c\u89e3\u51b3\u80fd\u529b\u7684\u63a2\u7d22\u3002\u5728\u5f00\u653e\u57df\u5e94\u7528\u4e2d\uff0c\u89c6\u89c9\u548c\u6587\u672c\u7ebf\u7d22\u7ecf\u5e38\u51b2\u7a81\uff0c\u9700\u8981\u6a21\u578b\u8fdb\u884c\u8d85\u8d8a\u8868\u9762\u5bf9\u9f50\u7684\u7ed3\u6784\u5316\u63a8\u7406\u3002", "method": "\u6784\u5efaCrossCheck-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91c7\u7528\u5206\u5c42\u4efb\u52a1\u6846\u67b6\u8986\u76d6\u4e09\u4e2a\u63a8\u7406\u590d\u6742\u5ea6\u5c42\u6b21\uff0c\u5b9a\u4e49\u4e03\u79cd\u89e3\u51b3\u8de8\u6a21\u6001\u4e0d\u4e00\u81f4\u6027\u7684\u6838\u5fc3\u80fd\u529b\u3002\u6570\u636e\u96c6\u5305\u542b15k\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u6765\u81ea\u771f\u5b9e\u4e16\u754c\u7d20\u6750\u5e76\u6ce8\u5165\u5408\u6210\u77db\u76fe\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u6807\u6ce8\u6d41\u7a0b\uff08450+\u4e13\u5bb6\u5c0f\u65f6\uff09\u786e\u4fdd\u8bed\u4e49\u6709\u6548\u6027\u548c\u96be\u5ea6\u6821\u51c6\u3002", "result": "\u8bc4\u4f3013\u4e2a\u6700\u5148\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d1\u73b0\uff1a1) \u968f\u7740\u4efb\u52a1\u4ece\u611f\u77e5\u5339\u914d\u8f6c\u5411\u903b\u8f91\u77db\u76fe\u68c0\u6d4b\uff0c\u6027\u80fd\u4e00\u81f4\u4e0b\u964d\uff1b2) \u5927\u591a\u6570\u6a21\u578b\u5728\u5b64\u7acb\u5b9e\u4f53\u8bc6\u522b\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u7efc\u5408\u591a\u4e2a\u7ebf\u7d22\u8fdb\u884c\u51b2\u7a81\u63a8\u7406\u65f6\u5931\u8d25\uff1b3) \u80fd\u529b\u5206\u6790\u663e\u793a\u6280\u80fd\u83b7\u53d6\u4e0d\u5747\u8861\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u591a\u6b65\u63a8\u7406\u6216\u57fa\u4e8e\u89c4\u5219\u9a8c\u8bc1\u7684\u4efb\u52a1\u4e2d\uff1b4) \u4f20\u7edf\u63d0\u793a\u7b56\u7565\uff08\u5982\u601d\u7ef4\u94fe\u3001\u6807\u8bb0\u96c6\uff09\u4ec5\u5e26\u6765\u8fb9\u9645\u6539\u8fdb\uff0c\u800c\u5c06\u7b26\u53f7\u63a8\u7406\u4e0e\u57fa\u7840\u89c6\u89c9\u5904\u7406\u4ea4\u7ec7\u7684\u65b9\u6cd5\u83b7\u5f97\u66f4\u7a33\u5b9a\u63d0\u5347\u3002", "conclusion": "\u591a\u6a21\u6001\u63a8\u7406\u5b58\u5728\u6301\u7eed\u74f6\u9888\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u6784\u5efa\u80fd\u591f\u8fdb\u884c\u7a33\u5065\u8de8\u6a21\u6001\u9a8c\u8bc1\u7684\u6a21\u578b\u3002\u5c06\u7b26\u53f7\u63a8\u7406\u4e0e\u57fa\u7840\u89c6\u89c9\u5904\u7406\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.22697", "categories": ["cs.RO", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22697", "abs": "https://arxiv.org/abs/2511.22697", "authors": ["Chancharik Mitra", "Yusen Luo", "Raj Saravanan", "Dantong Niu", "Anirudh Pai", "Jesse Thomason", "Trevor Darrell", "Abrar Anwar", "Deva Ramanan", "Roei Herzig"], "title": "Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations", "comment": null, "summary": "Vision-Language Action (VLAs) models promise to extend the remarkable success of vision-language models (VLMs) to robotics. Yet, unlike VLMs in the vision-language domain, VLAs for robotics require finetuning to contend with varying physical factors like robot embodiment, environment characteristics, and spatial relationships of each task. Existing fine-tuning methods lack specificity, adapting the same set of parameters regardless of a task's visual, linguistic, and physical characteristics. Inspired by functional specificity in neuroscience, we hypothesize that it is more effective to finetune sparse model representations specific to a given task. In this work, we introduce Robotic Steering, a finetuning approach grounded in mechanistic interpretability that leverages few-shot demonstrations to identify and selectively finetune task-specific attention heads aligned with the physical, visual, and linguistic requirements of robotic tasks. Through comprehensive on-robot evaluations with a Franka Emika robot arm, we demonstrate that Robotic Steering outperforms LoRA while achieving superior robustness under task variation, reduced computational cost, and enhanced interpretability for adapting VLAs to diverse robotic tasks.", "AI": {"tldr": "Robotic Steering\uff1a\u57fa\u4e8e\u673a\u5236\u53ef\u89e3\u91ca\u6027\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u6837\u672c\u6f14\u793a\u8bc6\u522b\u5e76\u9009\u62e9\u6027\u5fae\u8c03\u4e0e\u673a\u5668\u4eba\u4efb\u52a1\u7269\u7406\u3001\u89c6\u89c9\u3001\u8bed\u8a00\u9700\u6c42\u5bf9\u9f50\u7684\u7279\u5b9a\u6ce8\u610f\u529b\u5934\uff0c\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u4f18\u4e8eLoRA\u65b9\u6cd5", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5fae\u8c03\u65b9\u6cd5\u7f3a\u4e4f\u7279\u5f02\u6027\uff0c\u65e0\u8bba\u4efb\u52a1\u7684\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u7269\u7406\u7279\u6027\u5982\u4f55\uff0c\u90fd\u8c03\u6574\u76f8\u540c\u7684\u53c2\u6570\u96c6\u3002\u53d7\u795e\u7ecf\u79d1\u5b66\u4e2d\u529f\u80fd\u7279\u5f02\u6027\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u8ba4\u4e3a\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u5fae\u8c03\u7a00\u758f\u6a21\u578b\u8868\u793a\u66f4\u6709\u6548", "method": "\u63d0\u51faRobotic Steering\u65b9\u6cd5\uff1a\u57fa\u4e8e\u673a\u5236\u53ef\u89e3\u91ca\u6027\uff0c\u5229\u7528\u5c11\u6837\u672c\u6f14\u793a\u8bc6\u522b\u5e76\u9009\u62e9\u6027\u5fae\u8c03\u4e0e\u673a\u5668\u4eba\u4efb\u52a1\u7269\u7406\u3001\u89c6\u89c9\u3001\u8bed\u8a00\u9700\u6c42\u5bf9\u9f50\u7684\u7279\u5b9a\u6ce8\u610f\u529b\u5934", "result": "\u5728Franka Emika\u673a\u68b0\u81c2\u4e0a\u7684\u5168\u9762\u673a\u5668\u4eba\u8bc4\u4f30\u8868\u660e\uff0cRobotic Steering\u5728\u4efb\u52a1\u53d8\u5316\u4e0b\u4f18\u4e8eLoRA\uff0c\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3001\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u548c\u66f4\u5f3a\u7684\u53ef\u89e3\u91ca\u6027", "conclusion": "Robotic Steering\u4e3a\u9002\u5e94\u591a\u6837\u5316\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u529f\u80fd\u7279\u5f02\u6027\u539f\u5219\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd"}}
{"id": "2511.21718", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21718", "abs": "https://arxiv.org/abs/2511.21718", "authors": ["Zhaoxin Zhang", "Borui Chen", "Yiming Hu", "Youyang Qu", "Tianqing Zhu", "Longxiang Gao"], "title": "When Harmless Words Harm: A New Threat to LLM Safety via Conceptual Triggers", "comment": null, "summary": "Recent research on large language model (LLM) jailbreaks has primarily focused on techniques that bypass safety mechanisms to elicit overtly harmful outputs. However, such efforts often overlook attacks that exploit the model's capacity for abstract generalization, creating a critical blind spot in current alignment strategies. This gap enables adversaries to induce objectionable content by subtly manipulating the implicit social values embedded in model outputs. In this paper, we introduce MICM, a novel, model-agnostic jailbreak method that targets the aggregate value structure reflected in LLM responses. Drawing on conceptual morphology theory, MICM encodes specific configurations of nuanced concepts into a fixed prompt template through a predefined set of phrases. These phrases act as conceptual triggers, steering model outputs toward a specific value stance without triggering conventional safety filters. We evaluate MICM across five advanced LLMs, including GPT-4o, Deepseek-R1, and Qwen3-8B. Experimental results show that MICM consistently outperforms state-of-the-art jailbreak techniques, achieving high success rates with minimal rejection. Our findings reveal a critical vulnerability in commercial LLMs: their safety mechanisms remain susceptible to covert manipulation of underlying value alignment.", "AI": {"tldr": "MICM\u662f\u4e00\u79cd\u65b0\u578b\u7684LLM\u8d8a\u72f1\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u5ff5\u5f62\u6001\u5b66\u7406\u8bba\u7f16\u7801\u5fae\u5999\u6982\u5ff5\u914d\u7f6e\uff0c\u64cd\u7eb5\u6a21\u578b\u9690\u542b\u7684\u793e\u4f1a\u4ef7\u503c\u89c2\uff0c\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\u751f\u6210\u4e0d\u5f53\u5185\u5bb9\uff0c\u5728\u591a\u4e2a\u5148\u8fdbLLM\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5f53\u524dLLM\u5b89\u5168\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u76f4\u63a5\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\u83b7\u53d6\u6709\u5bb3\u8f93\u51fa\u7684\u65b9\u6cd5\uff0c\u4f46\u5ffd\u89c6\u4e86\u653b\u51fb\u6a21\u578b\u62bd\u8c61\u6cdb\u5316\u80fd\u529b\u7684\u6f0f\u6d1e\u3002\u8fd9\u79cd\u76f2\u70b9\u4f7f\u5f97\u653b\u51fb\u8005\u53ef\u4ee5\u901a\u8fc7\u5fae\u5999\u64cd\u7eb5\u6a21\u578b\u8f93\u51fa\u4e2d\u9690\u542b\u7684\u793e\u4f1a\u4ef7\u503c\u89c2\u6765\u8bf1\u5bfc\u4e0d\u5f53\u5185\u5bb9\uff0c\u800c\u73b0\u6709\u5bf9\u9f50\u7b56\u7565\u5bf9\u6b64\u9632\u5fa1\u4e0d\u8db3\u3002", "method": "MICM\u57fa\u4e8e\u6982\u5ff5\u5f62\u6001\u5b66\u7406\u8bba\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u77ed\u8bed\u96c6\u5c06\u5fae\u5999\u6982\u5ff5\u7684\u5177\u4f53\u914d\u7f6e\u7f16\u7801\u5230\u56fa\u5b9a\u63d0\u793a\u6a21\u677f\u4e2d\u3002\u8fd9\u4e9b\u77ed\u8bed\u4f5c\u4e3a\u6982\u5ff5\u89e6\u53d1\u5668\uff0c\u5f15\u5bfc\u6a21\u578b\u8f93\u51fa\u671d\u5411\u7279\u5b9a\u4ef7\u503c\u7acb\u573a\uff0c\u540c\u65f6\u907f\u514d\u89e6\u53d1\u4f20\u7edf\u5b89\u5168\u8fc7\u6ee4\u5668\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\u3002", "result": "\u5728GPT-4o\u3001Deepseek-R1\u3001Qwen3-8B\u7b49\u4e94\u4e2a\u5148\u8fdbLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMICM\u5728\u4fdd\u6301\u4f4e\u62d2\u7edd\u7387\u7684\u540c\u65f6\uff0c\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u8d8a\u72f1\u6280\u672f\uff0c\u53d6\u5f97\u4e86\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u5546\u4e1aLLM\u7684\u5b89\u5168\u673a\u5236\u5bf9\u5e95\u5c42\u4ef7\u503c\u5bf9\u9f50\u7684\u9690\u853d\u64cd\u7eb5\u4ecd\u7136\u8106\u5f31\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5bf9\u9f50\u7b56\u7565\u7684\u5173\u952e\u6f0f\u6d1e\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u7ec6\u7684\u4ef7\u503c\u68c0\u6d4b\u548c\u9632\u5fa1\u673a\u5236\u3002"}}
{"id": "2511.22705", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.22705", "abs": "https://arxiv.org/abs/2511.22705", "authors": ["Ian Lalonde", "Jeff Denis", "Mathieu Lamy", "Camille Martin", "Karina Lebel", "Alexandre Girard"], "title": "A Two Degrees-of-Freedom Floor-Based Robot for Transfer and Rehabilitation Applications", "comment": "13 pages, 16 figures", "summary": "The ability to accomplish a sit-to-stand (STS) motion is key to increase functional mobility and reduce rehospitalization risks. While raising aid (transfer) devices and partial bodyweight support (rehabilitation) devices exist, both are unable to adjust the STS training to different mobility levels. Therefore, We have developed an STS training device that allows various configurations of impedance and vertical/forward forces to adapt to many training needs while maintaining commercial raising aid transfer capabilities. Experiments with healthy adults (both men and women) of various heights and weights show that the device 1) has a low impact on the natural STS kinematics, 2) can provide precise weight unloading at the patient's center of mass and 3) can add a forward virtual spring to assist the transfer of the bodyweight to the feet for seat-off, at the start of the STS motion.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u8c03\u8282\u963b\u6297\u548c\u5782\u76f4/\u524d\u5411\u529b\u7684\u5750\u7acb\u8bad\u7ec3\u8bbe\u5907\uff0c\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u6d3b\u52a8\u80fd\u529b\u6c34\u5e73\u7684\u8bad\u7ec3\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u5546\u4e1a\u5347\u964d\u8f85\u52a9\u8bbe\u5907\u7684\u8f6c\u79fb\u529f\u80fd\u3002", "motivation": "\u5750\u7acb\u52a8\u4f5c\u80fd\u529b\u5bf9\u63d0\u9ad8\u529f\u80fd\u6d3b\u52a8\u6027\u548c\u964d\u4f4e\u518d\u4f4f\u9662\u98ce\u9669\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u5347\u964d\u8f85\u52a9\u8bbe\u5907\u548c\u90e8\u5206\u4f53\u91cd\u652f\u6491\u5eb7\u590d\u8bbe\u5907\u65e0\u6cd5\u6839\u636e\u4e0d\u540c\u6d3b\u52a8\u80fd\u529b\u6c34\u5e73\u8c03\u6574\u5750\u7acb\u8bad\u7ec3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5750\u7acb\u8bad\u7ec3\u8bbe\u5907\uff0c\u5141\u8bb8\u914d\u7f6e\u4e0d\u540c\u7684\u963b\u6297\u548c\u5782\u76f4/\u524d\u5411\u529b\uff0c\u4ee5\u9002\u5e94\u591a\u79cd\u8bad\u7ec3\u9700\u6c42\uff0c\u540c\u65f6\u4fdd\u6301\u5546\u4e1a\u5347\u964d\u8f85\u52a9\u8bbe\u5907\u7684\u8f6c\u79fb\u80fd\u529b\u3002", "result": "\u5bf9\u5177\u6709\u4e0d\u540c\u8eab\u9ad8\u4f53\u91cd\u7684\u5065\u5eb7\u6210\u5e74\u4eba\uff08\u7537\u5973\u5747\u6709\uff09\u7684\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09\u8bbe\u5907\u5bf9\u81ea\u7136\u5750\u7acb\u8fd0\u52a8\u5b66\u5f71\u54cd\u5c0f\uff1b2\uff09\u80fd\u5728\u60a3\u8005\u8d28\u5fc3\u63d0\u4f9b\u7cbe\u786e\u7684\u51cf\u91cd\u652f\u6301\uff1b3\uff09\u53ef\u5728\u5750\u7acb\u52a8\u4f5c\u5f00\u59cb\u65f6\u589e\u52a0\u865a\u62df\u524d\u5411\u5f39\u7c27\uff0c\u8f85\u52a9\u4f53\u91cd\u8f6c\u79fb\u5230\u811a\u90e8\u4ee5\u5b9e\u73b0\u79bb\u5ea7\u3002", "conclusion": "\u8be5\u5750\u7acb\u8bad\u7ec3\u8bbe\u5907\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u8bad\u7ec3\u9700\u6c42\uff0c\u6709\u6548\u652f\u6301\u5750\u7acb\u52a8\u4f5c\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u5347\u964d\u8f85\u52a9\u529f\u80fd\u3002"}}
{"id": "2511.21721", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21721", "abs": "https://arxiv.org/abs/2511.21721", "authors": ["Gao Mo", "Naveen Raman", "Megan Chai", "Cindy Peng", "Shannon Pagdon", "Nev Jones", "Hong Shen", "Peggy Swarbrick", "Fei Fang"], "title": "PeerCoPilot: A Language Model-Powered Assistant for Behavioral Health Organizations", "comment": "Accepted at IAAI'26", "summary": "Behavioral health conditions, which include mental health and substance use disorders, are the leading disease burden in the United States. Peer-run behavioral health organizations (PROs) critically assist individuals facing these conditions by combining mental health services with assistance for needs such as income, employment, and housing. However, limited funds and staffing make it difficult for PROs to address all service user needs. To assist peer providers at PROs with their day-to-day tasks, we introduce PeerCoPilot, a large language model (LLM)-powered assistant that helps peer providers create wellness plans, construct step-by-step goals, and locate organizational resources to support these goals. PeerCoPilot ensures information reliability through a retrieval-augmented generation pipeline backed by a large database of over 1,300 vetted resources. We conducted human evaluations with 15 peer providers and 6 service users and found that over 90% of users supported using PeerCoPilot. Moreover, we demonstrated that PeerCoPilot provides more reliable and specific information than a baseline LLM. PeerCoPilot is now used by a group of 5-10 peer providers at CSPNJ, a large behavioral health organization serving over 10,000 service users, and we are actively expanding PeerCoPilot's use.", "AI": {"tldr": "PeerCoPilot\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u52a9\u624b\uff0c\u5e2e\u52a9\u540c\u4f34\u63d0\u4f9b\u8005\u5236\u5b9a\u5065\u5eb7\u8ba1\u5212\u3001\u8bbe\u5b9a\u76ee\u6807\u5e76\u67e5\u627e\u8d44\u6e90\uff0c\u5df2\u5728\u771f\u5b9e\u884c\u4e3a\u5065\u5eb7\u7ec4\u7ec7\u4e2d\u90e8\u7f72\u4f7f\u7528\u3002", "motivation": "\u884c\u4e3a\u5065\u5eb7\u95ee\u9898\u662f\u7f8e\u56fd\u6700\u4e3b\u8981\u7684\u75be\u75c5\u8d1f\u62c5\uff0c\u540c\u4f34\u8fd0\u8425\u7684\u884c\u4e3a\u5065\u5eb7\u7ec4\u7ec7(PROs)\u5728\u5e2e\u52a9\u60a3\u8005\u65b9\u9762\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8d44\u91d1\u548c\u4eba\u5458\u6709\u9650\uff0c\u96be\u4ee5\u6ee1\u8db3\u6240\u6709\u670d\u52a1\u9700\u6c42\u3002", "method": "\u5f00\u53d1PeerCoPilot\u7cfb\u7edf\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7ba1\u9053\uff0c\u57fa\u4e8e1300\u591a\u4e2a\u7ecf\u8fc7\u5ba1\u6838\u7684\u8d44\u6e90\u6570\u636e\u5e93\uff0c\u786e\u4fdd\u4fe1\u606f\u53ef\u9760\u6027\u3002\u7cfb\u7edf\u5e2e\u52a9\u540c\u4f34\u63d0\u4f9b\u8005\u521b\u5efa\u5065\u5eb7\u8ba1\u5212\u3001\u6784\u5efa\u5206\u6b65\u76ee\u6807\u5e76\u5b9a\u4f4d\u7ec4\u7ec7\u8d44\u6e90\u3002", "result": "\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\u8d85\u8fc790%\u7684\u7528\u6237\u652f\u6301\u4f7f\u7528PeerCoPilot\u3002\u7cfb\u7edf\u6bd4\u57fa\u7ebfLLM\u63d0\u4f9b\u66f4\u53ef\u9760\u548c\u5177\u4f53\u7684\u4fe1\u606f\u3002\u76ee\u524d\u5df2\u5728CSPNJ\u7ec4\u7ec7\u4e2d\u75315-10\u540d\u540c\u4f34\u63d0\u4f9b\u8005\u4f7f\u7528\uff0c\u670d\u52a1\u8d85\u8fc710,000\u540d\u7528\u6237\u3002", "conclusion": "PeerCoPilot\u662f\u4e00\u4e2a\u6709\u6548\u7684LLM\u52a9\u624b\uff0c\u80fd\u591f\u5e2e\u52a9\u540c\u4f34\u63d0\u4f9b\u8005\u66f4\u597d\u5730\u670d\u52a1\u884c\u4e3a\u5065\u5eb7\u60a3\u8005\uff0c\u7cfb\u7edf\u5df2\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u6210\u529f\u90e8\u7f72\u5e76\u8ba1\u5212\u8fdb\u4e00\u6b65\u6269\u5c55\u3002"}}
{"id": "2511.22744", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22744", "abs": "https://arxiv.org/abs/2511.22744", "authors": ["R\u00e9my Rahem", "Wael Suleiman"], "title": "Beyond Egocentric Limits: Multi-View Depth-Based Learning for Robust Quadrupedal Locomotion", "comment": "12 pages, 6 figures, code available at https://anonymous.4open.science/r/multiview-parkour-6FB8", "summary": "Recent progress in legged locomotion has allowed highly dynamic and parkour-like behaviors for robots, similar to their biological counterparts. Yet, these methods mostly rely on egocentric (first-person) perception, limiting their performance, especially when the viewpoint of the robot is occluded. A promising solution would be to enhance the robot's environmental awareness by using complementary viewpoints, such as multiple actors exchanging perceptual information. Inspired by this idea, this work proposes a multi-view depth-based locomotion framework that combines egocentric and exocentric observations to provide richer environmental context during agile locomotion. Using a teacher-student distillation approach, the student policy learns to fuse proprioception with dual depth streams while remaining robust to real-world sensing imperfections. To further improve robustness, we introduce extensive domain randomization, including stochastic remote-camera dropouts and 3D positional perturbations that emulate aerial-ground cooperative sensing. Simulation results show that multi-viewpoints policies outperform single-viewpoint baseline in gap crossing, step descent, and other dynamic maneuvers, while maintaining stability when the exocentric camera is partially or completely unavailable. Additional experiments show that moderate viewpoint misalignment is well tolerated when incorporated during training. This study demonstrates that heterogeneous visual feedback improves robustness and agility in quadrupedal locomotion. Furthermore, to support reproducibility, the implementation accompanying this work is publicly available at https://anonymous.4open.science/r/multiview-parkour-6FB8", "AI": {"tldr": "\u591a\u89c6\u89d2\u6df1\u5ea6\u611f\u77e5\u6846\u67b6\u7ed3\u5408\u672c\u4f53\u611f\u77e5\u4e0e\u53cc\u6df1\u5ea6\u6d41\uff0c\u901a\u8fc7\u5e08\u751f\u84b8\u998f\u65b9\u6cd5\u63d0\u5347\u56db\u8db3\u673a\u5668\u4eba\u5728\u52a8\u6001\u8fd0\u52a8\u4e2d\u7684\u73af\u5883\u611f\u77e5\u80fd\u529b\u548c\u9c81\u68d2\u6027", "motivation": "\u5f53\u524d\u817f\u5f0f\u673a\u5668\u4eba\u4e3b\u8981\u4f9d\u8d56\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u611f\u77e5\uff0c\u5728\u89c6\u91ce\u88ab\u906e\u6321\u65f6\u6027\u80fd\u53d7\u9650\u3002\u9700\u8981\u591a\u89c6\u89d2\u4e92\u8865\u611f\u77e5\u6765\u589e\u5f3a\u73af\u5883\u610f\u8bc6\uff0c\u5b9e\u73b0\u66f4\u654f\u6377\u3001\u9c81\u68d2\u7684\u52a8\u6001\u8fd0\u52a8", "method": "\u63d0\u51fa\u591a\u89c6\u89d2\u6df1\u5ea6\u611f\u77e5\u6846\u67b6\uff0c\u7ed3\u5408\u672c\u4f53\u611f\u77e5\u4e0e\u53cc\u6df1\u5ea6\u6d41\uff08\u7b2c\u4e00\u4eba\u79f0\u548c\u7b2c\u4e09\u4eba\u79f0\uff09\u3002\u91c7\u7528\u5e08\u751f\u84b8\u998f\u65b9\u6cd5\u8bad\u7ec3\u7b56\u7565\uff0c\u5f15\u5165\u5e7f\u6cdb\u7684\u9886\u57df\u968f\u673a\u5316\uff0c\u5305\u62ec\u968f\u673a\u8fdc\u7a0b\u76f8\u673a\u4e22\u5931\u548c3D\u4f4d\u7f6e\u6270\u52a8", "result": "\u591a\u89c6\u89d2\u7b56\u7565\u5728\u8de8\u8d8a\u95f4\u9699\u3001\u53f0\u9636\u4e0b\u964d\u7b49\u52a8\u6001\u52a8\u4f5c\u4e0a\u4f18\u4e8e\u5355\u89c6\u89d2\u57fa\u7ebf\uff0c\u5728\u5916\u89c6\u89d2\u76f8\u673a\u90e8\u5206\u6216\u5b8c\u5168\u4e0d\u53ef\u7528\u65f6\u4ecd\u80fd\u4fdd\u6301\u7a33\u5b9a\u6027\u3002\u9002\u5ea6\u7684\u89c6\u89d2\u504f\u5dee\u5728\u8bad\u7ec3\u4e2d\u88ab\u826f\u597d\u5bb9\u5fcd", "conclusion": "\u5f02\u6784\u89c6\u89c9\u53cd\u9988\u663e\u8457\u63d0\u9ad8\u4e86\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u9c81\u68d2\u6027\u548c\u654f\u6377\u6027\u3002\u591a\u89c6\u89d2\u611f\u77e5\u6846\u67b6\u4e3a\u52a8\u6001\u817f\u5f0f\u8fd0\u52a8\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.21722", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.21722", "abs": "https://arxiv.org/abs/2511.21722", "authors": ["Jens Rupprecht", "Leon Fr\u00f6hling", "Claudia Wagner", "Markus Strohmaier"], "title": "German General Personas: A Survey-Derived Persona Prompt Collection for Population-Aligned LLM Studies", "comment": "18 pages, 7 figures", "summary": "The use of Large Language Models (LLMs) for simulating human perspectives via persona prompting is gaining traction in computational social science. However, well-curated, empirically grounded persona collections remain scarce, limiting the accuracy and representativeness of such simulations. Here we introduce the German General Personas (GGP) collection, a comprehensive and representative persona prompt collection built from the German General Social Survey (ALLBUS). The GGP and its persona prompts are designed to be easily plugged into prompts for all types of LLMs and tasks, steering models to generate responses aligned with the underlying German population. We evaluate GGP by prompting various LLMs to simulate survey response distributions across diverse topics, demonstrating that GGP-guided LLMs outperform state-of-the-art classifiers, particularly under data scarcity. Furthermore, we analyze how the representativity and attribute selection within persona prompts affect alignment with population responses. Our findings suggest that GGP provides a potentially valuable resource for research on LLM-based social simulations that enables more systematic explorations of population-aligned persona prompting in NLP and social science research.", "AI": {"tldr": "GGP\u662f\u4e00\u4e2a\u57fa\u4e8e\u5fb7\u56fd\u7efc\u5408\u793e\u4f1a\u8c03\u67e5\u6784\u5efa\u7684\u5168\u9762\u3001\u6709\u4ee3\u8868\u6027\u7684\u4eba\u683c\u63d0\u793a\u96c6\u5408\uff0c\u7528\u4e8e\u5f15\u5bfcLLM\u751f\u6210\u4e0e\u5fb7\u56fd\u4eba\u53e3\u5bf9\u9f50\u7684\u54cd\u5e94\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u65f6\u4f18\u4e8e\u73b0\u6709\u5206\u7c7b\u5668\u3002", "motivation": "\u5f53\u524d\u4f7f\u7528LLM\u901a\u8fc7\u4eba\u683c\u63d0\u793a\u6a21\u62df\u4eba\u7c7b\u89c6\u89d2\u7684\u7814\u7a76\u7f3a\u4e4f\u7ecf\u8fc7\u7cbe\u5fc3\u7b56\u5212\u3001\u57fa\u4e8e\u5b9e\u8bc1\u7684\u4eba\u683c\u96c6\u5408\uff0c\u9650\u5236\u4e86\u6a21\u62df\u7684\u51c6\u786e\u6027\u548c\u4ee3\u8868\u6027\u3002", "method": "\u57fa\u4e8e\u5fb7\u56fd\u7efc\u5408\u793e\u4f1a\u8c03\u67e5(ALLBUS)\u6784\u5efa\u5fb7\u56fd\u901a\u7528\u4eba\u683c(GGP)\u96c6\u5408\uff0c\u8bbe\u8ba1\u6613\u4e8e\u96c6\u6210\u5230\u5404\u79cdLLM\u548c\u4efb\u52a1\u4e2d\u7684\u4eba\u683c\u63d0\u793a\uff0c\u8bc4\u4f30\u5176\u5728\u6a21\u62df\u8c03\u67e5\u54cd\u5e94\u5206\u5e03\u4e0a\u7684\u8868\u73b0\u3002", "result": "GGP\u5f15\u5bfc\u7684LLM\u5728\u6a21\u62df\u591a\u6837\u4e3b\u9898\u7684\u8c03\u67e5\u54cd\u5e94\u5206\u5e03\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u5206\u7c7b\u5668\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u3002\u4eba\u683c\u63d0\u793a\u7684\u4ee3\u8868\u6027\u548c\u5c5e\u6027\u9009\u62e9\u663e\u8457\u5f71\u54cd\u4e0e\u4eba\u53e3\u54cd\u5e94\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "conclusion": "GGP\u4e3a\u57fa\u4e8eLLM\u7684\u793e\u4f1a\u6a21\u62df\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u80fd\u591f\u4fc3\u8fdbNLP\u548c\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u4e2d\u66f4\u7cfb\u7edf\u5316\u7684\u4eba\u53e3\u5bf9\u9f50\u4eba\u683c\u63d0\u793a\u63a2\u7d22\u3002"}}
{"id": "2511.22773", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22773", "abs": "https://arxiv.org/abs/2511.22773", "authors": ["Rui Heng Yang", "Xuan Zhao", "Leo Maxime Brunswic", "Montgomery Alban", "Mateo Clemente", "Tongtong Cao", "Jun Jin", "Amir Rasouli"], "title": "CAPE: Context-Aware Diffusion Policy Via Proximal Mode Expansion for Collision Avoidance", "comment": "4 tables, 9 figures", "summary": "In robotics, diffusion models can capture multi-modal trajectories from demonstrations, making them a transformative approach in imitation learning. However, achieving optimal performance following this regiment requires a large-scale dataset, which is costly to obtain, especially for challenging tasks, such as collision avoidance. In those tasks, generalization at test time demands coverage of many obstacles types and their spatial configurations, which are impractical to acquire purely via data. To remedy this problem, we propose Context-Aware diffusion policy via Proximal mode Expansion (CAPE), a framework that expands trajectory distribution modes with context-aware prior and guidance at inference via a novel prior-seeded iterative guided refinement procedure. The framework generates an initial trajectory plan and executes a short prefix trajectory, and then the remaining trajectory segment is perturbed to an intermediate noise level, forming a trajectory prior. Such a prior is context-aware and preserves task intent. Repeating the process with context-aware guided denoising iteratively expands mode support to allow finding smoother, less collision-prone trajectories. For collision avoidance, CAPE expands trajectory distribution modes with collision-aware context, enabling the sampling of collision-free trajectories in previously unseen environments while maintaining goal consistency. We evaluate CAPE on diverse manipulation tasks in cluttered unseen simulated and real-world settings and show up to 26% and 80% higher success rates respectively compared to SOTA methods, demonstrating better generalization to unseen environments.", "AI": {"tldr": "CAPE\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6269\u6563\u7b56\u7565\u6846\u67b6\uff0c\u901a\u8fc7\u5148\u9a8c\u79cd\u5b50\u8fed\u4ee3\u5f15\u5bfc\u7cbe\u70bc\u8fc7\u7a0b\uff0c\u5728\u63a8\u7406\u65f6\u6269\u5c55\u8f68\u8ff9\u5206\u5e03\u6a21\u5f0f\uff0c\u4ee5\u89e3\u51b3\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u6570\u636e\u83b7\u53d6\u6210\u672c\u9ad8\u548c\u78b0\u649e\u907f\u514d\u4efb\u52a1\u6cdb\u5316\u96be\u7684\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\uff0c\u6269\u6563\u6a21\u578b\u867d\u7136\u80fd\u6355\u6349\u591a\u6a21\u6001\u8f68\u8ff9\uff0c\u4f46\u9700\u8981\u5927\u89c4\u6a21\u6570\u636e\u96c6\u624d\u80fd\u8fbe\u5230\u6700\u4f73\u6027\u80fd\uff0c\u800c\u83b7\u53d6\u8fd9\u4e9b\u6570\u636e\u6210\u672c\u9ad8\u6602\u3002\u7279\u522b\u662f\u5728\u78b0\u649e\u907f\u514d\u7b49\u6311\u6218\u6027\u4efb\u52a1\u4e2d\uff0c\u6cdb\u5316\u9700\u8981\u8986\u76d6\u591a\u79cd\u969c\u788d\u7269\u7c7b\u578b\u548c\u7a7a\u95f4\u914d\u7f6e\uff0c\u4ec5\u901a\u8fc7\u6570\u636e\u83b7\u53d6\u4e0d\u5207\u5b9e\u9645\u3002", "method": "CAPE\u6846\u67b6\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u5148\u9a8c\u548c\u5f15\u5bfc\u5728\u63a8\u7406\u65f6\u6269\u5c55\u8f68\u8ff9\u5206\u5e03\u6a21\u5f0f\u3002\u91c7\u7528\u5148\u9a8c\u79cd\u5b50\u8fed\u4ee3\u5f15\u5bfc\u7cbe\u70bc\u8fc7\u7a0b\uff1a\u9996\u5148\u751f\u6210\u521d\u59cb\u8f68\u8ff9\u8ba1\u5212\u5e76\u6267\u884c\u77ed\u524d\u7f00\u8f68\u8ff9\uff0c\u7136\u540e\u5c06\u5269\u4f59\u8f68\u8ff9\u6bb5\u6270\u52a8\u5230\u4e2d\u95f4\u566a\u58f0\u6c34\u5e73\u5f62\u6210\u8f68\u8ff9\u5148\u9a8c\uff08\u4fdd\u6301\u4efb\u52a1\u610f\u56fe\uff09\uff0c\u518d\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u5f15\u5bfc\u53bb\u566a\u8fed\u4ee3\u6269\u5c55\u6a21\u5f0f\u652f\u6301\uff0c\u5bfb\u627e\u66f4\u5e73\u6ed1\u3001\u78b0\u649e\u98ce\u9669\u66f4\u4f4e\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u6742\u4e71\u672a\u89c1\u8fc7\u7684\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e2d\u8bc4\u4f30CAPE\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5206\u522b\u5b9e\u73b0\u4e86\u9ad8\u8fbe26%\u548c80%\u7684\u6210\u529f\u7387\u63d0\u5347\uff0c\u5c55\u793a\u4e86\u5728\u672a\u89c1\u73af\u5883\u4e2d\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CAPE\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u5148\u9a8c\u548c\u8fed\u4ee3\u5f15\u5bfc\u7cbe\u70bc\uff0c\u6709\u6548\u6269\u5c55\u4e86\u8f68\u8ff9\u5206\u5e03\u6a21\u5f0f\uff0c\u80fd\u591f\u5728\u672a\u89c1\u73af\u5883\u4e2d\u91c7\u6837\u65e0\u78b0\u649e\u8f68\u8ff9\u5e76\u4fdd\u6301\u76ee\u6807\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u6570\u636e\u4f9d\u8d56\u548c\u6cdb\u5316\u6311\u6218\u7684\u95ee\u9898\u3002"}}
{"id": "2511.21724", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21724", "abs": "https://arxiv.org/abs/2511.21724", "authors": ["Zenan Sun", "Rashmie Abeysinghe", "Xiaojin Li", "Xinyue Hu", "Licong Cui", "Guo-Qiang Zhang", "Jiang Bian", "Cui Tao"], "title": "AD-CDO: A Lightweight Ontology for Representing Eligibility Criteria in Alzheimer's Disease Clinical Trials", "comment": null, "summary": "Objective\n  This study introduces the Alzheimer's Disease Common Data Element Ontology for Clinical Trials (AD-CDO), a lightweight, semantically enriched ontology designed to represent and standardize key eligibility criteria concepts in Alzheimer's disease (AD) clinical trials.\n  Materials and Methods\n  We extracted high-frequency concepts from more than 1,500 AD clinical trials on ClinicalTrials.gov and organized them into seven semantic categories: Disease, Medication, Diagnostic Test, Procedure, Social Determinants of Health, Rating Criteria, and Fertility. Each concept was annotated with standard biomedical vocabularies, including the UMLS, OMOP Standardized Vocabularies, DrugBank, NDC, and NLM VSAC value sets. To balance coverage and manageability, we applied the Jenks Natural Breaks method to identify an optimal set of representative concepts.\n  Results\n  The optimized AD-CDO achieved over 63% coverage of extracted trial concepts while maintaining interpretability and compactness. The ontology effectively captured the most frequent and clinically meaningful entities used in AD eligibility criteria. We demonstrated AD-CDO's practical utility through two use cases: (a) an ontology-driven trial simulation system for formal modeling and virtual execution of clinical trials, and (b) an entity normalization task mapping raw clinical text to ontology-aligned terms, enabling consistency and integration with EHR data.\n  Discussion\n  AD-CDO bridges the gap between broad biomedical ontologies and task-specific trial modeling needs. It supports multiple downstream applications, including phenotyping algorithm development, cohort identification, and structured data integration.\n  Conclusion\n  By harmonizing essential eligibility entities and aligning them with standardized vocabularies, AD-CDO provides a versatile foundation for ontology-driven AD clinical trial research.", "AI": {"tldr": "AD-CDO\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8bed\u4e49\u672c\u4f53\uff0c\u7528\u4e8e\u6807\u51c6\u5316\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u4e34\u5e8a\u8bd5\u9a8c\u7684\u8d44\u683c\u6807\u51c6\u6982\u5ff5\uff0c\u8986\u76d663%\u7684\u8bd5\u9a8c\u6982\u5ff5\uff0c\u652f\u6301\u8bd5\u9a8c\u6a21\u62df\u548c\u5b9e\u4f53\u89c4\u8303\u5316\u7b49\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u751f\u7269\u533b\u5b66\u672c\u4f53\u8fc7\u4e8e\u5bbd\u6cdb\u3001\u65e0\u6cd5\u6ee1\u8db3\u7279\u5b9a\u4e34\u5e8a\u8bd5\u9a8c\u5efa\u6a21\u9700\u6c42\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u4e34\u5e8a\u8bd5\u9a8c\u8d44\u683c\u6807\u51c6\u7684\u8f7b\u91cf\u7ea7\u8bed\u4e49\u672c\u4f53\u3002", "method": "\u4ece1500\u591a\u4e2aAD\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u63d0\u53d6\u9ad8\u9891\u6982\u5ff5\uff0c\u5206\u4e3a7\u4e2a\u8bed\u4e49\u7c7b\u522b\uff0c\u4f7f\u7528UMLS\u3001OMOP\u7b49\u6807\u51c6\u8bcd\u6c47\u8fdb\u884c\u6807\u6ce8\uff0c\u5e94\u7528Jenks\u81ea\u7136\u65ad\u70b9\u6cd5\u4f18\u5316\u6982\u5ff5\u96c6\u5e73\u8861\u8986\u76d6\u7387\u548c\u53ef\u7ba1\u7406\u6027\u3002", "result": "\u4f18\u5316\u540e\u7684AD-CDO\u8986\u76d6\u4e86\u8d85\u8fc763%\u7684\u63d0\u53d6\u8bd5\u9a8c\u6982\u5ff5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u7d27\u51d1\u6027\u3002\u901a\u8fc7\u4e24\u4e2a\u7528\u4f8b\u5c55\u793a\u4e86\u5b9e\u7528\u6027\uff1a\u8bd5\u9a8c\u6a21\u62df\u7cfb\u7edf\u548c\u5b9e\u4f53\u89c4\u8303\u5316\u4efb\u52a1\u3002", "conclusion": "AD-CDO\u901a\u8fc7\u534f\u8c03\u5173\u952e\u8d44\u683c\u5b9e\u4f53\u5e76\u4e0e\u6807\u51c6\u5316\u8bcd\u6c47\u5bf9\u9f50\uff0c\u4e3a\u57fa\u4e8e\u672c\u4f53\u7684AD\u4e34\u5e8a\u8bd5\u9a8c\u7814\u7a76\u63d0\u4f9b\u4e86\u591a\u529f\u80fd\u57fa\u7840\uff0c\u652f\u6301\u8868\u578b\u7b97\u6cd5\u5f00\u53d1\u3001\u961f\u5217\u8bc6\u522b\u548c\u7ed3\u6784\u5316\u6570\u636e\u96c6\u6210\u7b49\u5e94\u7528\u3002"}}
{"id": "2511.22777", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22777", "abs": "https://arxiv.org/abs/2511.22777", "authors": ["Sajjad Pakdamansavoji", "Mozhgan Pourkeshavarz", "Adam Sigal", "Zhiyuan Li", "Rui Heng Yang", "Amir Rasouli"], "title": "Improving Robotic Manipulation Robustness via NICE Scene Surgery", "comment": "11 figures, 3 tables", "summary": "Learning robust visuomotor policies for robotic manipulation remains a challenge in real-world settings, where visual distractors can significantly degrade performance and safety. In this work, we propose an effective and scalable framework, Naturalistic Inpainting for Context Enhancement (NICE). Our method minimizes out-of-distribution (OOD) gap in imitation learning by increasing visual diversity through construction of new experiences using existing demonstrations. By utilizing image generative frameworks and large language models, NICE performs three editing operations, object replacement, restyling, and removal of distracting (non-target) objects. These changes preserve spatial relationships without obstructing target objects and maintain action-label consistency. Unlike previous approaches, NICE requires no additional robot data collection, simulator access, or custom model training, making it readily applicable to existing robotic datasets.\n  Using real-world scenes, we showcase the capability of our framework in producing photo-realistic scene enhancement. For downstream tasks, we use NICE data to finetune a vision-language model (VLM) for spatial affordance prediction and a vision-language-action (VLA) policy for object manipulation. Our evaluations show that NICE successfully minimizes OOD gaps, resulting in over 20% improvement in accuracy for affordance prediction in highly cluttered scenes. For manipulation tasks, success rate increases on average by 11% when testing in environments populated with distractors in different quantities. Furthermore, we show that our method improves visual robustness, lowering target confusion by 6%, and enhances safety by reducing collision rate by 7%.", "AI": {"tldr": "NICE\u6846\u67b6\u901a\u8fc7\u81ea\u7136\u56fe\u50cf\u4fee\u590d\u589e\u5f3a\u89c6\u89c9\u591a\u6837\u6027\uff0c\u51cf\u5c11\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u5916\u5dee\u8ddd\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6536\u96c6\u6216\u6a21\u578b\u8bad\u7ec3\uff0c\u5728\u6742\u4e71\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u89c6\u89c9\u5e72\u6270\u7269\u4f1a\u663e\u8457\u964d\u4f4e\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6027\u80fd\u548c\u5b89\u5168\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u989d\u5916\u6570\u636e\u6536\u96c6\u6216\u6a21\u62df\u5668\u8bbf\u95ee\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5229\u7528\u56fe\u50cf\u751f\u6210\u6846\u67b6\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e09\u79cd\u7f16\u8f91\u64cd\u4f5c\uff1a\u7269\u4f53\u66ff\u6362\u3001\u91cd\u65b0\u98ce\u683c\u5316\u548c\u79fb\u9664\u5e72\u6270\u7269\uff0c\u4fdd\u6301\u7a7a\u95f4\u5173\u7cfb\u548c\u52a8\u4f5c\u6807\u7b7e\u4e00\u81f4\u6027\uff0c\u589e\u5f3a\u89c6\u89c9\u591a\u6837\u6027\u3002", "result": "\u5728\u9ad8\u5ea6\u6742\u4e71\u573a\u666f\u4e2d\uff0c\u7a7a\u95f4\u53ef\u4f9b\u6027\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u5347\u8d85\u8fc720%\uff1b\u5728\u542b\u4e0d\u540c\u6570\u91cf\u5e72\u6270\u7269\u7684\u73af\u5883\u4e2d\uff0c\u64cd\u4f5c\u4efb\u52a1\u6210\u529f\u7387\u5e73\u5747\u63d0\u9ad811%\uff1b\u76ee\u6807\u6df7\u6dc6\u964d\u4f4e6%\uff0c\u78b0\u649e\u7387\u51cf\u5c117%\u3002", "conclusion": "NICE\u6846\u67b6\u6709\u6548\u51cf\u5c11\u6a21\u4eff\u5b66\u4e60\u7684\u5206\u5e03\u5916\u5dee\u8ddd\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u89c6\u89c9\u9c81\u68d2\u6027\u548c\u5b89\u5168\u6027\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6570\u636e\u6536\u96c6\u6216\u6a21\u578b\u8bad\u7ec3\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u3002"}}
{"id": "2511.21725", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21725", "abs": "https://arxiv.org/abs/2511.21725", "authors": ["Yizhou Xu", "Janet Davis"], "title": "PromptTailor: Multi-turn Intent-Aligned Prompt Synthesis for Lightweight LLMs", "comment": "EMNLP 2025 Workshop PALS. Additional note: There is a citation error on Evoke. The paper we are referring to is \"Evoking critical thinking abilities in LLMs via reviewer-author prompt editing.\"", "summary": "Lightweight language models remain attractive for on-device and privacy-sensitive applications, but their responses are highly sensitive to prompt quality. For open-ended generation, non-expert users often lack the knowledge or time to consistently craft high-quality prompts, leading them to rely on prompt optimization tools. However, a key challenge is ensuring the optimized prompts genuinely align with users' original intents and preferences. We introduce PromptTailor, a system for controllable prompt generation for open-ended text that improves model output quality by intent-aligned prompt synthesis. PromptTailor expands minimal user instructions into rich, domain-aware prompts while preserving the user's stated preferences. The system is a quantized Llama3-8B model fine-tuned with a lightweight LoRA adapter on 12,300 prompt-refinement dialogues spanning 41 everyday domains, distilled from three stronger LLMs. The adapter attaches to any Llama3-8B base, enabling edge deployment. In human and LLM-judge evaluations across multiple target models and optimization baselines, PromptTailor yields higher preference rates than chain-of-thought prompting and matches or surpasses state-of-the-art prompt optimization methods while requiring fewer model calls (e.g., 3 vs. 9). These results show that a compact student, guided by powerful teachers, can learn effective prompt-generation strategies that enhance response quality while maintaining alignment with user intent.", "AI": {"tldr": "PromptTailor\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u63d0\u793a\u4f18\u5316\u7cfb\u7edf\uff0c\u4f7f\u7528\u91cf\u5316Llama3-8B\u6a21\u578b\u548cLoRA\u9002\u914d\u5668\uff0c\u901a\u8fc7\u4ece\u5f3aLLM\u84b8\u998f\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5c06\u7528\u6237\u7b80\u5355\u6307\u4ee4\u6269\u5c55\u4e3a\u4e30\u5bcc\u3001\u9886\u57df\u611f\u77e5\u7684\u63d0\u793a\uff0c\u540c\u65f6\u4fdd\u6301\u7528\u6237\u610f\u56fe\u5bf9\u9f50\u3002", "motivation": "\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u5728\u8bbe\u5907\u7aef\u548c\u9690\u79c1\u654f\u611f\u5e94\u7528\u4e2d\u5f88\u6709\u5438\u5f15\u529b\uff0c\u4f46\u5b83\u4eec\u7684\u54cd\u5e94\u8d28\u91cf\u5bf9\u63d0\u793a\u8d28\u91cf\u9ad8\u5ea6\u654f\u611f\u3002\u975e\u4e13\u4e1a\u7528\u6237\u901a\u5e38\u7f3a\u4e4f\u77e5\u8bc6\u6216\u65f6\u95f4\u6765\u5236\u4f5c\u9ad8\u8d28\u91cf\u63d0\u793a\uff0c\u800c\u73b0\u6709\u63d0\u793a\u4f18\u5316\u5de5\u5177\u96be\u4ee5\u786e\u4fdd\u4f18\u5316\u540e\u7684\u63d0\u793a\u4e0e\u7528\u6237\u539f\u59cb\u610f\u56fe\u548c\u504f\u597d\u4fdd\u6301\u4e00\u81f4\u3002", "method": "\u4f7f\u7528\u91cf\u5316Llama3-8B\u6a21\u578b\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7LoRA\u9002\u914d\u5668\u572812,300\u4e2a\u63d0\u793a\u4f18\u5316\u5bf9\u8bdd\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u8fd9\u4e9b\u5bf9\u8bdd\u6db5\u76d641\u4e2a\u65e5\u5e38\u9886\u57df\uff0c\u4ece\u4e09\u4e2a\u66f4\u5f3a\u7684LLM\u84b8\u998f\u800c\u6765\u3002\u7cfb\u7edf\u53ef\u4ee5\u5c06\u6700\u5c0f\u7528\u6237\u6307\u4ee4\u6269\u5c55\u4e3a\u4e30\u5bcc\u3001\u9886\u57df\u611f\u77e5\u7684\u63d0\u793a\u3002", "result": "\u5728\u4eba\u7c7b\u548cLLM\u8bc4\u4f30\u4e2d\uff0cPromptTailor\u5728\u591a\u4e2a\u76ee\u6807\u6a21\u578b\u548c\u4f18\u5316\u57fa\u51c6\u4e0a\uff0c\u6bd4\u601d\u7ef4\u94fe\u63d0\u793a\u83b7\u5f97\u66f4\u9ad8\u7684\u504f\u597d\u7387\uff0c\u5339\u914d\u6216\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff0c\u540c\u65f6\u9700\u8981\u66f4\u5c11\u7684\u6a21\u578b\u8c03\u7528\uff08\u59823\u6b21 vs 9\u6b21\uff09\u3002", "conclusion": "\u7d27\u51d1\u7684\u5b66\u751f\u6a21\u578b\u5728\u5f3a\u5927\u6559\u5e08\u6a21\u578b\u7684\u6307\u5bfc\u4e0b\uff0c\u53ef\u4ee5\u5b66\u4e60\u6709\u6548\u7684\u63d0\u793a\u751f\u6210\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u4e0e\u7528\u6237\u610f\u56fe\u5bf9\u9f50\u7684\u540c\u65f6\u63d0\u9ad8\u54cd\u5e94\u8d28\u91cf\uff0c\u9002\u5408\u8fb9\u7f18\u90e8\u7f72\u3002"}}
{"id": "2511.22780", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22780", "abs": "https://arxiv.org/abs/2511.22780", "authors": ["Amir Rasouli", "Montgomery Alban", "Sajjad Pakdamansavoji", "Zhiyuan Li", "Zhanguang Zhang", "Aaron Wu", "Xuan Zhao"], "title": "Distracted Robot: How Visual Clutter Undermine Robotic Manipulation", "comment": "12 figures, 2 tables", "summary": "In this work, we propose an evaluation protocol for examining the performance of robotic manipulation policies in cluttered scenes. Contrary to prior works, we approach evaluation from a psychophysical perspective, therefore we use a unified measure of clutter that accounts for environmental factors as well as the distractors quantity, characteristics, and arrangement. Using this measure, we systematically construct evaluation scenarios in both hyper-realistic simulation and real-world and conduct extensive experimentation on manipulation policies, in particular vision-language-action (VLA) models. Our experiments highlight the significant impact of scene clutter, lowering the performance of the policies, by as much as 34% and show that despite achieving similar average performance across the tasks, different VLA policies have unique vulnerabilities and a relatively low agreement on success scenarios. We further show that our clutter measure is an effective indicator of performance degradation and analyze the impact of distractors in terms of their quantity and occluding influence. At the end, we show that finetuning on enhanced data, although effective, does not equally remedy all negative impacts of clutter on performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5fc3\u7406\u7269\u7406\u5b66\u89d2\u5ea6\u8bc4\u4f30\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5728\u6742\u4e71\u573a\u666f\u4e2d\u6027\u80fd\u7684\u534f\u8bae\uff0c\u4f7f\u7528\u7edf\u4e00\u7684\u6742\u4e71\u5ea6\u91cf\u6765\u7cfb\u7edf\u6784\u5efa\u8bc4\u4f30\u573a\u666f\uff0c\u53d1\u73b0\u6742\u4e71\u4f1a\u663e\u8457\u964d\u4f4eVLA\u6a21\u578b\u6027\u80fd\u8fbe34%\uff0c\u4e0d\u540c\u7b56\u7565\u6709\u72ec\u7279\u5f31\u70b9\uff0c\u5fae\u8c03\u4e0d\u80fd\u5b8c\u5168\u89e3\u51b3\u6742\u4e71\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u6742\u4e71\u573a\u666f\u7684\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u9700\u8981\u4ece\u5fc3\u7406\u7269\u7406\u5b66\u89d2\u5ea6\u5efa\u7acb\u7edf\u4e00\u7684\u6742\u4e71\u5ea6\u91cf\u6807\u51c6\uff0c\u4ee5\u5168\u9762\u8bc4\u4f30\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5728\u771f\u5b9e\u590d\u6742\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u7684\u6742\u4e71\u5ea6\u91cf\u65b9\u6cd5\uff0c\u8003\u8651\u73af\u5883\u56e0\u7d20\u3001\u5e72\u6270\u7269\u6570\u91cf\u3001\u7279\u5f81\u548c\u6392\u5217\uff1b\u5728\u8d85\u771f\u5b9e\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u7cfb\u7edf\u6784\u5efa\u8bc4\u4f30\u573a\u666f\uff1b\u5bf9\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff1b\u5206\u6790\u6742\u4e71\u5ea6\u91cf\u4e0e\u6027\u80fd\u4e0b\u964d\u7684\u5173\u7cfb\u3002", "result": "\u573a\u666f\u6742\u4e71\u663e\u8457\u964d\u4f4e\u7b56\u7565\u6027\u80fd\u8fbe34%\uff1b\u4e0d\u540cVLA\u7b56\u7565\u867d\u7136\u5e73\u5747\u6027\u80fd\u76f8\u4f3c\uff0c\u4f46\u6709\u72ec\u7279\u8106\u5f31\u6027\u548c\u8f83\u4f4e\u7684\u6210\u529f\u573a\u666f\u4e00\u81f4\u6027\uff1b\u6742\u4e71\u5ea6\u91cf\u662f\u6027\u80fd\u4e0b\u964d\u7684\u6709\u6548\u6307\u6807\uff1b\u5fae\u8c03\u5bf9\u6742\u4e71\u5f71\u54cd\u7684\u6539\u5584\u6548\u679c\u4e0d\u5747\u3002", "conclusion": "\u9700\u8981\u66f4\u7cfb\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\u6765\u5206\u6790\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5728\u6742\u4e71\u573a\u666f\u4e2d\u7684\u6027\u80fd\uff0c\u5f53\u524dVLA\u6a21\u578b\u5bf9\u6742\u4e71\u654f\u611f\u4e14\u5b58\u5728\u72ec\u7279\u5f31\u70b9\uff0c\u9700\u8981\u9488\u5bf9\u6027\u7684\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2511.21726", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21726", "abs": "https://arxiv.org/abs/2511.21726", "authors": ["Yicong Zheng", "Kevin L. McKee", "Thomas Miconi", "Zacharie Bugaud", "Mick van Gelderen", "Jed McCaleb"], "title": "Goal-Directed Search Outperforms Goal-Agnostic Memory Compression in Long-Context Memory Tasks", "comment": null, "summary": "How to enable human-like long-term memory in large language models (LLMs) has been a central question for unlocking more general capabilities such as few-shot generalization. Existing memory frameworks and benchmarks focus on finding the optimal memory compression algorithm for higher performance in tasks that require recollection and sometimes further reasoning. However, such efforts have ended up building more human bias into the compression algorithm, through the search for the best prompts and memory architectures that suit specific benchmarks, rather than finding a general solution that would work on other data distributions. On the other hand, goal-directed search on uncompressed information could potentially exhibit superior performance because compression is lossy, and a predefined compression algorithm will not fit all raw data distributions. Here we present SUMER (Search in Uncompressed Memory via Experience Replay), an end-to-end reinforcement learning agent with verifiable reward (RLVR) that learns to use search tools to gather information and answer a target question. On the LoCoMo dataset for long-context conversation understanding, SUMER with Qwen2.5-7B-Instruct learned to use search tools and outperformed all other biased memory compression approaches and also the full-context baseline, reaching SOTA performance (43% gain over the prior best). We demonstrate that a simple search method applied to raw data outperforms goal-agnostic and biased compression algorithms in current long-context memory tasks, arguing for new paradigms and benchmarks that are more dynamic and autonomously scalable. Code for SUMER and all implemented baselines is publicly available at https://github.com/zycyc/SUMER.", "AI": {"tldr": "SUMER\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u641c\u7d22\u65b9\u6cd5\uff0c\u76f4\u63a5\u5728\u672a\u538b\u7f29\u7684\u539f\u59cb\u6570\u636e\u4e2d\u8fdb\u884c\u76ee\u6807\u5bfc\u5411\u641c\u7d22\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u4e8e\u538b\u7f29\u7684\u8bb0\u5fc6\u65b9\u6cd5\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u8bb0\u5fc6\u6846\u67b6\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5bfb\u627e\u6700\u4f18\u7684\u8bb0\u5fc6\u538b\u7f29\u7b97\u6cd5\uff0c\u4f46\u8fd9\u5f15\u5165\u4e86\u4eba\u4e3a\u504f\u89c1\uff0c\u4e14\u538b\u7f29\u662f\u4fe1\u606f\u6709\u635f\u7684\uff0c\u9884\u5b9a\u4e49\u7684\u538b\u7f29\u7b97\u6cd5\u65e0\u6cd5\u9002\u5e94\u6240\u6709\u539f\u59cb\u6570\u636e\u5206\u5e03\u3002\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSUMER\uff08Search in Uncompressed Memory via Experience Replay\uff09\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u5b66\u4e60\u4f7f\u7528\u641c\u7d22\u5de5\u5177\u6536\u96c6\u4fe1\u606f\u5e76\u56de\u7b54\u76ee\u6807\u95ee\u9898\uff0c\u91c7\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u3002", "result": "\u5728LoCoMo\u957f\u4e0a\u4e0b\u6587\u5bf9\u8bdd\u7406\u89e3\u6570\u636e\u96c6\u4e0a\uff0cSUMER\u4f7f\u7528Qwen2.5-7B-Instruct\u6a21\u578b\u5b66\u4e60\u4f7f\u7528\u641c\u7d22\u5de5\u5177\uff0c\u8d85\u8d8a\u4e86\u6240\u6709\u57fa\u4e8e\u538b\u7f29\u7684\u8bb0\u5fc6\u65b9\u6cd5\u548c\u5b8c\u6574\u4e0a\u4e0b\u6587\u57fa\u7ebf\uff0c\u8fbe\u5230SOTA\u6027\u80fd\uff08\u6bd4\u4e4b\u524d\u6700\u4f73\u63d0\u534743%\uff09\u3002", "conclusion": "\u7b80\u5355\u7684\u641c\u7d22\u65b9\u6cd5\u5e94\u7528\u4e8e\u539f\u59cb\u6570\u636e\u4f18\u4e8e\u5f53\u524d\u957f\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u4efb\u52a1\u4e2d\u7684\u76ee\u6807\u65e0\u5173\u548c\u5e26\u6709\u504f\u89c1\u7684\u538b\u7f29\u7b97\u6cd5\uff0c\u9700\u8981\u66f4\u52a8\u6001\u548c\u81ea\u4e3b\u53ef\u6269\u5c55\u7684\u65b0\u8303\u5f0f\u4e0e\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2511.22829", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.22829", "abs": "https://arxiv.org/abs/2511.22829", "authors": ["Zhen Tian", "Zhihao Lin"], "title": "Safe Autonomous Lane Changing: Planning with Dynamic Risk Fields and Time-Varying Convex Space Generation", "comment": null, "summary": "This paper presents a novel trajectory planning pipeline for complex driving scenarios like autonomous lane changing, by integrating risk-aware planning with guaranteed collision avoidance into a unified optimization framework. We first construct a dynamic risk fields (DRF) that captures both the static and dynamic collision risks from surrounding vehicles. Then, we develop a rigorous strategy for generating time-varying convex feasible spaces that ensure kinematic feasibility and safety requirements. The trajectory planning problem is formulated as a finite-horizon optimal control problem and solved using a constrained iterative Linear Quadratic Regulator (iLQR) algorithm that jointly optimizes trajectory smoothness, control effort, and risk exposure while maintaining strict feasibility. Extensive simulations demonstrate that our method outperforms traditional approaches in terms of safety and efficiency, achieving collision-free trajectories with shorter lane-changing distances (28.59 m) and times (2.84 s) while maintaining smooth and comfortable acceleration patterns. In dense roundabout environments the planner further demonstrates robust adaptability, producing larger safety margins, lower jerk, and superior curvature smoothness compared with APF, MPC, and RRT based baselines. These results confirm that the integrated DRF with convex feasible space and constrained iLQR solver provides a balanced solution for safe, efficient, and comfortable trajectory generation in dynamic and interactive traffic scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6362\u9053\u7b49\u590d\u6742\u573a\u666f\u7684\u65b0\u578b\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u98ce\u9669\u611f\u77e5\u89c4\u5212\u4e0e\u4fdd\u8bc1\u78b0\u649e\u907f\u514d\u96c6\u6210\u5230\u7edf\u4e00\u4f18\u5316\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u5728\u590d\u6742\u52a8\u6001\u4ea4\u901a\u573a\u666f\u4e2d\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u8212\u9002\u6027\uff0c\u7279\u522b\u662f\u5728\u5bc6\u96c6\u4ea4\u4e92\u73af\u5883\u4e2d\u9700\u8981\u5e73\u8861\u98ce\u9669\u89c4\u907f\u4e0e\u8fd0\u52a8\u5e73\u6ed1\u6027\u3002", "method": "1. \u6784\u5efa\u52a8\u6001\u98ce\u9669\u573a(DRF)\u6355\u6349\u9759\u6001\u548c\u52a8\u6001\u78b0\u649e\u98ce\u9669\uff1b2. \u751f\u6210\u65f6\u53d8\u51f8\u53ef\u884c\u7a7a\u95f4\u786e\u4fdd\u8fd0\u52a8\u5b66\u53ef\u884c\u6027\u548c\u5b89\u5168\u8981\u6c42\uff1b3. \u5c06\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\u5efa\u6a21\u4e3a\u6709\u9650\u65f6\u57df\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u4f7f\u7528\u7ea6\u675f\u8fed\u4ee3\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668(iLQR)\u7b97\u6cd5\u8054\u5408\u4f18\u5316\u8f68\u8ff9\u5e73\u6ed1\u6027\u3001\u63a7\u5236\u52aa\u529b\u548c\u98ce\u9669\u66b4\u9732\u3002", "result": "\u5728\u6362\u9053\u573a\u666f\u4e2d\u5b9e\u73b0\u66f4\u77ed\u7684\u6362\u9053\u8ddd\u79bb(28.59\u7c73)\u548c\u65f6\u95f4(2.84\u79d2)\uff0c\u4fdd\u6301\u5e73\u6ed1\u8212\u9002\u7684\u52a0\u901f\u5ea6\u6a21\u5f0f\uff1b\u5728\u5bc6\u96c6\u73af\u5c9b\u73af\u5883\u4e2d\u76f8\u6bd4APF\u3001MPC\u548cRRT\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8868\u73b0\u51fa\u66f4\u5927\u7684\u5b89\u5168\u88d5\u5ea6\u3001\u66f4\u4f4e\u7684\u6025\u52a8\u5ea6\u548c\u66f4\u4f18\u7684\u66f2\u7387\u5e73\u6ed1\u6027\u3002", "conclusion": "\u96c6\u6210\u7684DRF\u3001\u51f8\u53ef\u884c\u7a7a\u95f4\u548c\u7ea6\u675fiLQR\u6c42\u89e3\u5668\u4e3a\u52a8\u6001\u4ea4\u4e92\u4ea4\u901a\u573a\u666f\u63d0\u4f9b\u4e86\u5b89\u5168\u3001\u9ad8\u6548\u548c\u8212\u9002\u7684\u8f68\u8ff9\u751f\u6210\u5e73\u8861\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21728", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21728", "abs": "https://arxiv.org/abs/2511.21728", "authors": ["Lin Yu", "Xiaofei Han", "Yifei Kang", "Chiung-Yi Tseng", "Danyang Zhang", "Ziqian Bi", "Zhimo Han"], "title": "Affective Multimodal Agents with Proactive Knowledge Grounding for Emotionally Aligned Marketing Dialogue", "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled fluent dialogue systems, but most remain reactive and struggle in emotionally rich, goal-oriented settings such as marketing conversations. To address this limitation, we propose AffectMind, a multimodal affective dialogue agent that performs proactive reasoning and dynamic knowledge grounding to sustain emotionally aligned and persuasive interactions. AffectMind combines three components: a Proactive Knowledge Grounding Network (PKGN) that continuously updates factual and affective context from text, vision, and prosody; an Emotion--Intent Alignment Model (EIAM) that jointly models user emotion and purchase intent to adapt persuasion strategies; and a Reinforced Discourse Loop (RDL) that optimizes emotional coherence and engagement via reinforcement signals from user responses. Experiments on two newly curated marketing dialogue datasets, MM-ConvMarket and AffectPromo, show that AffectMind outperforms strong LLM-based baselines in emotional consistency (+26\\%), persuasive success rate (+19\\%), and long-term user engagement (+23\\%), highlighting emotion-grounded proactivity as a key capability for commercial multimodal agents.", "AI": {"tldr": "AffectMind\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u60c5\u611f\u5bf9\u8bdd\u4ee3\u7406\uff0c\u901a\u8fc7\u4e3b\u52a8\u63a8\u7406\u548c\u52a8\u6001\u77e5\u8bc6\u57fa\u7840\uff0c\u5728\u8425\u9500\u5bf9\u8bdd\u4e2d\u5b9e\u73b0\u60c5\u611f\u5bf9\u9f50\u548c\u8bf4\u670d\u6027\u4ea4\u4e92\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709LLM\u57fa\u7ebf\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u4e30\u5bcc\u3001\u76ee\u6807\u5bfc\u5411\u7684\u8425\u9500\u5bf9\u8bdd\u4e2d\u8868\u73b0\u88ab\u52a8\uff0c\u96be\u4ee5\u7ef4\u6301\u60c5\u611f\u5bf9\u9f50\u548c\u8bf4\u670d\u6027\u4ea4\u4e92\uff0c\u9700\u8981\u66f4\u4e3b\u52a8\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faAffectMind\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4e3b\u52a8\u77e5\u8bc6\u57fa\u7840\u7f51\u7edc\uff08PKGN\uff09\u4ece\u6587\u672c\u3001\u89c6\u89c9\u548c\u97f5\u5f8b\u66f4\u65b0\u4e8b\u5b9e\u548c\u60c5\u611f\u4e0a\u4e0b\u6587\uff1b\u60c5\u611f-\u610f\u56fe\u5bf9\u9f50\u6a21\u578b\uff08EIAM\uff09\u8054\u5408\u5efa\u6a21\u7528\u6237\u60c5\u611f\u548c\u8d2d\u4e70\u610f\u56fe\uff1b\u5f3a\u5316\u8bdd\u8bed\u5faa\u73af\uff08RDL\uff09\u901a\u8fc7\u7528\u6237\u53cd\u9988\u4f18\u5316\u60c5\u611f\u8fde\u8d2f\u6027\u548c\u53c2\u4e0e\u5ea6\u3002", "result": "\u5728\u4e24\u4e2a\u65b0\u6784\u5efa\u7684\u8425\u9500\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\uff0cAffectMind\u5728\u60c5\u611f\u4e00\u81f4\u6027\uff08+26%\uff09\u3001\u8bf4\u670d\u6210\u529f\u7387\uff08+19%\uff09\u548c\u957f\u671f\u7528\u6237\u53c2\u4e0e\u5ea6\uff08+23%\uff09\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5f3aLLM\u57fa\u7ebf\u3002", "conclusion": "\u60c5\u611f\u57fa\u7840\u7684\u4e3b\u52a8\u6027\u662f\u591a\u6a21\u6001\u5546\u4e1a\u4ee3\u7406\u7684\u5173\u952e\u80fd\u529b\uff0cAffectMind\u901a\u8fc7\u4e3b\u52a8\u63a8\u7406\u548c\u52a8\u6001\u77e5\u8bc6\u57fa\u7840\uff0c\u5728\u60c5\u611f\u4e30\u5bcc\u7684\u76ee\u6807\u5bfc\u5411\u5bf9\u8bdd\u4e2d\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u4ea4\u4e92\u3002"}}
{"id": "2511.22847", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22847", "abs": "https://arxiv.org/abs/2511.22847", "authors": ["Yuying Zhang", "Na Fan", "Haowen Zheng", "Junning Liang", "Zongliang Pan", "Qifeng Chen", "Ximin Lyu"], "title": "Threat-Aware UAV Dodging of Human-Thrown Projectiles with an RGB-D Camera", "comment": null, "summary": "Uncrewed aerial vehicles (UAVs) performing tasks such as transportation and aerial photography are vulnerable to intentional projectile attacks from humans. Dodging such a sudden and fast projectile poses a significant challenge for UAVs, requiring ultra-low latency responses and agile maneuvers. Drawing inspiration from baseball, in which pitchers' body movements are analyzed to predict the ball's trajectory, we propose a novel real-time dodging system that leverages an RGB-D camera. Our approach integrates human pose estimation with depth information to predict the attacker's motion trajectory and the subsequent projectile trajectory. Additionally, we introduce an uncertainty-aware dodging strategy to enable the UAV to dodge incoming projectiles efficiently. Our perception system achieves high prediction accuracy and outperforms the baseline in effective distance and latency. The dodging strategy addresses temporal and spatial uncertainties to ensure UAV safety. Extensive real-world experiments demonstrate the framework's reliable dodging capabilities against sudden attacks and its outstanding robustness across diverse scenarios.", "AI": {"tldr": "\u57fa\u4e8eRGB-D\u76f8\u673a\u548c\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u65e0\u4eba\u673a\u5b9e\u65f6\u8eb2\u907f\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u6d4b\u653b\u51fb\u8005\u52a8\u4f5c\u548c\u629b\u5c04\u7269\u8f68\u8ff9\u6765\u8eb2\u907f\u7a81\u7136\u7684\u629b\u5c04\u7269\u653b\u51fb", "motivation": "\u6267\u884c\u8fd0\u8f93\u548c\u822a\u62cd\u7b49\u4efb\u52a1\u7684\u65e0\u4eba\u673a\u5bb9\u6613\u53d7\u5230\u4eba\u7c7b\u6545\u610f\u629b\u5c04\u7269\u653b\u51fb\uff0c\u8eb2\u907f\u8fd9\u79cd\u7a81\u7136\u5feb\u901f\u629b\u5c04\u7269\u9700\u8981\u8d85\u4f4e\u5ef6\u8fdf\u54cd\u5e94\u548c\u654f\u6377\u673a\u52a8\u80fd\u529b", "method": "\u53d7\u68d2\u7403\u8fd0\u52a8\u542f\u53d1\uff0c\u5229\u7528RGB-D\u76f8\u673a\u7ed3\u5408\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u4e0e\u6df1\u5ea6\u4fe1\u606f\uff0c\u9884\u6d4b\u653b\u51fb\u8005\u8fd0\u52a8\u8f68\u8ff9\u548c\u629b\u5c04\u7269\u8f68\u8ff9\uff0c\u5e76\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8eb2\u907f\u7b56\u7565", "result": "\u611f\u77e5\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5728\u6709\u6548\u8ddd\u79bb\u548c\u5ef6\u8fdf\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\uff1b\u8eb2\u907f\u7b56\u7565\u5904\u7406\u65f6\u7a7a\u4e0d\u786e\u5b9a\u6027\u786e\u4fdd\u65e0\u4eba\u673a\u5b89\u5168\uff1b\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\u7cfb\u7edf\u5728\u7a81\u53d1\u653b\u51fb\u4e0b\u5177\u6709\u53ef\u9760\u8eb2\u907f\u80fd\u529b\u548c\u51fa\u8272\u9c81\u68d2\u6027", "conclusion": "\u63d0\u51fa\u7684\u5b9e\u65f6\u8eb2\u907f\u7cfb\u7edf\u80fd\u6709\u6548\u5e94\u5bf9\u7a81\u7136\u629b\u5c04\u7269\u653b\u51fb\uff0c\u5728\u5404\u79cd\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u65e0\u4eba\u673a\u5b89\u5168\u9632\u62a4\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.21729", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21729", "abs": "https://arxiv.org/abs/2511.21729", "authors": ["Jithin Krishnan"], "title": "Beyond Component Strength: Synergistic Integration and Adaptive Calibration in Multi-Agent RAG Systems", "comment": "10 pages, 4 figures", "summary": "Building reliable retrieval-augmented generation (RAG) systems requires more than adding powerful components; it requires understanding how they interact. Using ablation studies on 50 queries (15 answerable, 10 edge cases, and 25 adversarial), we show that enhancements such as hybrid retrieval, ensemble verification, and adaptive thresholding provide almost no benefit when used in isolation, yet together achieve a 95% reduction in abstention (from 40% to 2%) without increasing hallucinations. We also identify a measurement challenge: different verification strategies can behave safely but assign inconsistent labels (for example, \"abstained\" versus \"unsupported\"), creating apparent hallucination rates that are actually artifacts of labeling. Our results show that synergistic integration matters more than the strength of any single component, that standardized metrics and labels are essential for correctly interpreting performance, and that adaptive calibration is needed to prevent overconfident over-answering even when retrieval quality is high.", "AI": {"tldr": "RAG\u7cfb\u7edf\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u5728\u4e8e\u7ec4\u4ef6\u95f4\u7684\u534f\u540c\u6574\u5408\uff0c\u800c\u975e\u5355\u4e2a\u7ec4\u4ef6\u7684\u5f3a\u5ea6\u3002\u901a\u8fc750\u4e2a\u67e5\u8be2\u7684\u6d88\u878d\u7814\u7a76\uff0c\u53d1\u73b0\u6df7\u5408\u68c0\u7d22\u3001\u96c6\u6210\u9a8c\u8bc1\u548c\u81ea\u9002\u5e94\u9608\u503c\u7b49\u589e\u5f3a\u63aa\u65bd\u5355\u72ec\u4f7f\u7528\u65f6\u51e0\u4e4e\u65e0\u76ca\uff0c\u4f46\u534f\u540c\u4f7f\u7528\u53ef\u5c06\u5f03\u7b54\u7387\u4ece40%\u964d\u81f32%\u4e14\u4e0d\u589e\u52a0\u5e7b\u89c9\u3002\u540c\u65f6\u63ed\u793a\u4e86\u6d4b\u91cf\u6311\u6218\uff1a\u4e0d\u540c\u9a8c\u8bc1\u7b56\u7565\u4f1a\u4ea7\u751f\u4e0d\u4e00\u81f4\u6807\u7b7e\uff0c\u5bfc\u81f4\u5e7b\u89c9\u7387\u8bc4\u4f30\u5931\u771f\u3002", "motivation": "\u6784\u5efa\u53ef\u9760\u7684RAG\u7cfb\u7edf\u9700\u8981\u7406\u89e3\u7ec4\u4ef6\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u6dfb\u52a0\u5f3a\u5927\u7ec4\u4ef6\u3002\u73b0\u6709\u7814\u7a76\u5f80\u5f80\u5173\u6ce8\u5355\u4e2a\u7ec4\u4ef6\u7684\u6539\u8fdb\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u7ec4\u4ef6\u534f\u540c\u6548\u5e94\u7684\u7cfb\u7edf\u5206\u6790\u3002\u540c\u65f6\uff0c\u4e0d\u540c\u9a8c\u8bc1\u7b56\u7565\u4ea7\u751f\u7684\u6807\u7b7e\u4e0d\u4e00\u81f4\u95ee\u9898\u4e5f\u5f71\u54cd\u4e86\u6027\u80fd\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u6d88\u878d\u7814\u7a76\u5206\u679050\u4e2a\u67e5\u8be2\uff0815\u4e2a\u53ef\u56de\u7b54\u300110\u4e2a\u8fb9\u7f18\u6848\u4f8b\u300125\u4e2a\u5bf9\u6297\u6027\u67e5\u8be2\uff09\uff0c\u8bc4\u4f30\u6df7\u5408\u68c0\u7d22\u3001\u96c6\u6210\u9a8c\u8bc1\u548c\u81ea\u9002\u5e94\u9608\u503c\u7b49\u589e\u5f3a\u63aa\u65bd\u7684\u6548\u679c\u3002\u7279\u522b\u5173\u6ce8\u4e0d\u540c\u9a8c\u8bc1\u7b56\u7565\u4ea7\u751f\u7684\u6807\u7b7e\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4ee5\u53ca\u7ec4\u4ef6\u95f4\u7684\u534f\u540c\u4f5c\u7528\u3002", "result": "\u589e\u5f3a\u63aa\u65bd\u5355\u72ec\u4f7f\u7528\u65f6\u51e0\u4e4e\u65e0\u76ca\uff0c\u4f46\u534f\u540c\u6574\u5408\u53ef\u5b9e\u73b095%\u7684\u5f03\u7b54\u7387\u964d\u4f4e\uff08\u4ece40%\u52302%\uff09\uff0c\u4e14\u4e0d\u589e\u52a0\u5e7b\u89c9\u3002\u53d1\u73b0\u6d4b\u91cf\u6311\u6218\uff1a\u4e0d\u540c\u9a8c\u8bc1\u7b56\u7565\u4f1a\u4ea7\u751f\u4e0d\u4e00\u81f4\u6807\u7b7e\uff08\u5982\"\u5f03\u7b54\"vs\"\u4e0d\u652f\u6301\"\uff09\uff0c\u5bfc\u81f4\u5e7b\u89c9\u7387\u8bc4\u4f30\u5931\u771f\u3002\u9700\u8981\u81ea\u9002\u5e94\u6821\u51c6\u6765\u9632\u6b62\u68c0\u7d22\u8d28\u91cf\u9ad8\u65f6\u7684\u8fc7\u5ea6\u81ea\u4fe1\u56de\u7b54\u3002", "conclusion": "RAG\u7cfb\u7edf\u7684\u6027\u80fd\u63d0\u5347\u5173\u952e\u5728\u4e8e\u7ec4\u4ef6\u7684\u534f\u540c\u6574\u5408\u800c\u975e\u5355\u4e2a\u7ec4\u4ef6\u5f3a\u5ea6\u3002\u9700\u8981\u6807\u51c6\u5316\u6307\u6807\u548c\u6807\u7b7e\u6765\u6b63\u786e\u89e3\u91ca\u6027\u80fd\uff0c\u81ea\u9002\u5e94\u6821\u51c6\u5bf9\u4e8e\u9632\u6b62\u8fc7\u5ea6\u81ea\u4fe1\u56de\u7b54\u81f3\u5173\u91cd\u8981\u3002\u7ec4\u4ef6\u95f4\u7684\u534f\u540c\u6548\u5e94\u6bd4\u4efb\u4f55\u5355\u4e2a\u7ec4\u4ef6\u7684\u5f3a\u5ea6\u66f4\u91cd\u8981\u3002"}}
{"id": "2511.22860", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22860", "abs": "https://arxiv.org/abs/2511.22860", "authors": ["Sacchin Sundar", "Atman Kikani", "Aaliya Alam", "Sumukh Shrote", "A. Nayeemulla Khan", "A. Shahina"], "title": "MARVO: Marine-Adaptive Radiance-aware Visual Odometry", "comment": "10 pages, 5 figures, 3 tables, Submitted to CVPR2026", "summary": "Underwater visual localization remains challenging due to wavelength-dependent attenuation, poor texture, and non-Gaussian sensor noise. We introduce MARVO, a physics-aware, learning-integrated odometry framework that fuses underwater image formation modeling, differentiable matching, and reinforcement-learning optimization. At the front-end, we extend transformer-based feature matcher with a Physics Aware Radiance Adapter that compensates for color channel attenuation and contrast loss, yielding geometrically consistent feature correspondences under turbidity. These semi dense matches are combined with inertial and pressure measurements inside a factor-graph backend, where we formulate a keyframe-based visual-inertial-barometric estimator using GTSAM library. Each keyframe introduces (i) Pre-integrated IMU motion factors, (ii) MARVO-derived visual pose factors, and (iii) barometric depth priors, giving a full-state MAP estimate in real time. Lastly, we introduce a Reinforcement-Learningbased Pose-Graph Optimizer that refines global trajectories beyond local minima of classical least-squares solvers by learning optimal retraction actions on SE(2).", "AI": {"tldr": "MARVO\uff1a\u4e00\u79cd\u878d\u5408\u6c34\u4e0b\u56fe\u50cf\u5f62\u6210\u5efa\u6a21\u3001\u53ef\u5fae\u5206\u5339\u914d\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u7269\u7406\u611f\u77e5\u5b66\u4e60\u96c6\u6210\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u6c34\u4e0b\u89c6\u89c9\u5b9a\u4f4d\u7684\u6311\u6218\u3002", "motivation": "\u6c34\u4e0b\u89c6\u89c9\u5b9a\u4f4d\u9762\u4e34\u6ce2\u957f\u76f8\u5173\u8870\u51cf\u3001\u7eb9\u7406\u8d2b\u4e4f\u548c\u975e\u9ad8\u65af\u4f20\u611f\u5668\u566a\u58f0\u7b49\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u6c34\u4e0b\u73af\u5883\u7684\u590d\u6742\u7269\u7406\u7279\u6027\u3002", "method": "1. \u524d\u7aef\uff1a\u6269\u5c55\u57fa\u4e8etransformer\u7684\u7279\u5f81\u5339\u914d\u5668\uff0c\u52a0\u5165\u7269\u7406\u611f\u77e5\u8f90\u5c04\u9002\u914d\u5668\u8865\u507f\u989c\u8272\u901a\u9053\u8870\u51cf\u548c\u5bf9\u6bd4\u5ea6\u635f\u5931\uff1b2. \u540e\u7aef\uff1a\u5c06\u534a\u5bc6\u96c6\u5339\u914d\u4e0e\u60ef\u6027\u548c\u538b\u529b\u6d4b\u91cf\u7ed3\u5408\u5728\u56e0\u5b50\u56fe\u4e2d\uff0c\u4f7f\u7528GTSAM\u5e93\u5b9e\u73b0\u5173\u952e\u5e27\u89c6\u89c9-\u60ef\u6027-\u6c14\u538b\u4f30\u8ba1\u5668\uff1b3. \u5168\u5c40\u4f18\u5316\uff1a\u5f15\u5165\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4f4d\u59ff\u56fe\u4f18\u5316\u5668\uff0c\u901a\u8fc7SE(2)\u4e0a\u7684\u6700\u4f18\u56de\u7f29\u52a8\u4f5c\u8d85\u8d8a\u7ecf\u5178\u6700\u5c0f\u4e8c\u4e58\u6c42\u89e3\u5668\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u3002", "result": "\u6846\u67b6\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5168\u72b6\u6001\u6700\u5927\u540e\u9a8c\u4f30\u8ba1\uff0c\u5728\u6d51\u6d4a\u6c34\u4e0b\u73af\u5883\u4e2d\u4ea7\u751f\u51e0\u4f55\u4e00\u81f4\u7684\u7279\u5f81\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u5668\u6539\u8fdb\u5168\u5c40\u8f68\u8ff9\u3002", "conclusion": "MARVO\u901a\u8fc7\u878d\u5408\u7269\u7406\u5efa\u6a21\u3001\u53ef\u5fae\u5206\u5339\u914d\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u4e3a\u6c34\u4e0b\u89c6\u89c9\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u6c34\u4e0b\u73af\u5883\u7684\u72ec\u7279\u6311\u6218\u3002"}}
{"id": "2511.21730", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21730", "abs": "https://arxiv.org/abs/2511.21730", "authors": ["Ishant Kohar", "Aswanth Krishnan"], "title": "A Benchmark for Procedural Memory Retrieval in Language Agents", "comment": null, "summary": "Current AI agents excel in familiar settings, but fail sharply when faced with novel tasks with unseen vocabularies -- a core limitation of procedural memory systems. We present the first benchmark that isolates procedural memory retrieval from task execution, evaluating whether agents can recognize functionally equivalent procedures that span different object instantiations. Using ALFWorld, we construct dual corpora of expert and LLM-generated trajectories and evaluate six retrieval methods using systematically stratified queries. Our results expose a clear generalization cliff: embedding-based methods perform strongly on familiar contexts, yet degrade considerably on novel ones, while LLM-generated procedural abstractions demonstrate reliable cross-context transfer. Controlled ablations show that although embeddings capture some lexical-level abstraction, they fundamentally treat procedures as unordered bags of words, discarding temporal structure necessary for cross-context transfer. Corpus scale delivers far larger gains than representation enrichment, revealing an architectural ceiling in current encoders. Our benchmark offers the first diagnostic framework separating genuine procedural understanding from surface-level memorization and gives tools for developing retrieval systems capable of dependable generalization. Resources available at our GitHub repository (https://github.com/qpiai/Proced_mem_bench).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u8bc4\u4f30AI\u4ee3\u7406\u7a0b\u5e8f\u8bb0\u5fc6\u68c0\u7d22\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u73b0\u6709\u5d4c\u5165\u65b9\u6cd5\u5728\u719f\u6089\u573a\u666f\u8868\u73b0\u826f\u597d\u4f46\u5728\u65b0\u573a\u666f\u6025\u5267\u4e0b\u964d\uff0c\u800cLLM\u751f\u6210\u7684\u7a0b\u5e8f\u62bd\u8c61\u80fd\u5b9e\u73b0\u53ef\u9760\u7684\u8de8\u4e0a\u4e0b\u6587\u8fc1\u79fb\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u5728\u719f\u6089\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9762\u5bf9\u5177\u6709\u672a\u89c1\u8bcd\u6c47\u7684\u65b0\u4efb\u52a1\u65f6\u8868\u73b0\u6025\u5267\u4e0b\u964d\u2014\u2014\u8fd9\u662f\u7a0b\u5e8f\u8bb0\u5fc6\u7cfb\u7edf\u7684\u6838\u5fc3\u9650\u5236\u3002\u9700\u8981\u8bc4\u4f30\u4ee3\u7406\u662f\u5426\u80fd\u8bc6\u522b\u8de8\u8d8a\u4e0d\u540c\u5bf9\u8c61\u5b9e\u4f8b\u7684\u529f\u80fd\u7b49\u4ef7\u7a0b\u5e8f\u3002", "method": "\u4f7f\u7528ALFWorld\u6784\u5efa\u4e13\u5bb6\u548cLLM\u751f\u6210\u8f68\u8ff9\u7684\u53cc\u91cd\u8bed\u6599\u5e93\uff0c\u8bc4\u4f30\u516d\u79cd\u68c0\u7d22\u65b9\u6cd5\uff0c\u4f7f\u7528\u7cfb\u7edf\u5206\u5c42\u67e5\u8be2\uff0c\u5e76\u8fdb\u884c\u53d7\u63a7\u6d88\u878d\u5b9e\u9a8c\u3002", "result": "\u53d1\u73b0\u660e\u663e\u7684\u6cdb\u5316\u60ac\u5d16\uff1a\u57fa\u4e8e\u5d4c\u5165\u7684\u65b9\u6cd5\u5728\u719f\u6089\u4e0a\u4e0b\u6587\u4e2d\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728\u65b0\u573a\u666f\u4e2d\u663e\u8457\u9000\u5316\uff1bLLM\u751f\u6210\u7684\u7a0b\u5e8f\u62bd\u8c61\u5c55\u793a\u4e86\u53ef\u9760\u7684\u8de8\u4e0a\u4e0b\u6587\u8fc1\u79fb\u80fd\u529b\u3002\u8bed\u6599\u5e93\u89c4\u6a21\u6bd4\u8868\u793a\u4e30\u5bcc\u5ea6\u5e26\u6765\u66f4\u5927\u6536\u76ca\u3002", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u9996\u4e2a\u8bca\u65ad\u6846\u67b6\uff0c\u80fd\u591f\u533a\u5206\u771f\u6b63\u7684\u7a0b\u5e8f\u7406\u89e3\u548c\u8868\u9762\u8bb0\u5fc6\uff0c\u5e76\u4e3a\u5f00\u53d1\u5177\u6709\u53ef\u9760\u6cdb\u5316\u80fd\u529b\u7684\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5de5\u5177\u3002"}}
{"id": "2511.22865", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22865", "abs": "https://arxiv.org/abs/2511.22865", "authors": ["Wonjeong Ryu", "Seungjun Yu", "Seokha Moon", "Hojun Choi", "Junsung Park", "Jinkyu Kim", "Hyunjung Shim"], "title": "SUPER-AD: Semantic Uncertainty-aware Planning for End-to-End Robust Autonomous Driving", "comment": null, "summary": "End-to-End (E2E) planning has become a powerful paradigm for autonomous driving, yet current systems remain fundamentally uncertainty-blind. They assume perception outputs are fully reliable, even in ambiguous or poorly observed scenes, leaving the planner without an explicit measure of uncertainty. To address this limitation, we propose a camera-only E2E framework that estimates aleatoric uncertainty directly in BEV space and incorporates it into planning. Our method produces a dense, uncertainty-aware drivability map that captures both semantic structure and geometric layout at pixel-level resolution. To further promote safe and rule-compliant behavior, we introduce a lane-following regularization that encodes lane structure and traffic norms. This prior stabilizes trajectory planning under normal conditions while preserving the flexibility needed for maneuvers such as overtaking or lane changes. Together, these components enable robust and interpretable trajectory planning, even under challenging uncertainty conditions. Evaluated on the NAVSIM benchmark, our method achieves state-of-the-art performance, delivering substantial gains on both the challenging NAVHARD and NAVSAFE subsets. These results demonstrate that our principled aleatoric uncertainty modeling combined with driving priors significantly advances the safety and reliability of camera-only E2E autonomous driving.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6444\u50cf\u5934\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6846\u67b6\uff0c\u901a\u8fc7\u4f30\u8ba1BEV\u7a7a\u95f4\u4e2d\u7684\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u5e76\u878d\u5165\u89c4\u5212\uff0c\u7ed3\u5408\u8f66\u9053\u8ddf\u968f\u6b63\u5219\u5316\uff0c\u5b9e\u73b0\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u8f68\u8ff9\u89c4\u5212\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff1a\u5b83\u4eec\u5047\u8bbe\u611f\u77e5\u8f93\u51fa\u5b8c\u5168\u53ef\u9760\uff0c\u5373\u4f7f\u5728\u6a21\u7cca\u6216\u89c2\u6d4b\u4e0d\u826f\u7684\u573a\u666f\u4e2d\u4e5f\u662f\u5982\u6b64\uff0c\u5bfc\u81f4\u89c4\u5212\u5668\u7f3a\u4e4f\u660e\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u9650\u5236\u4e86\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "1. \u63d0\u51fa\u6444\u50cf\u5934\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u76f4\u63a5\u5728BEV\u7a7a\u95f4\u4e2d\u4f30\u8ba1\u5076\u7136\u4e0d\u786e\u5b9a\u6027\uff1b2. \u751f\u6210\u5bc6\u96c6\u7684\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u53ef\u884c\u9a76\u6027\u5730\u56fe\uff0c\u6355\u6349\u8bed\u4e49\u7ed3\u6784\u548c\u51e0\u4f55\u5e03\u5c40\uff1b3. \u5f15\u5165\u8f66\u9053\u8ddf\u968f\u6b63\u5219\u5316\uff0c\u7f16\u7801\u8f66\u9053\u7ed3\u6784\u548c\u4ea4\u901a\u89c4\u8303\uff0c\u7a33\u5b9a\u8f68\u8ff9\u89c4\u5212\u540c\u65f6\u4fdd\u6301\u7075\u6d3b\u6027\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684NAVHARD\u548cNAVSAFE\u5b50\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4e0e\u9a7e\u9a76\u5148\u9a8c\u7ed3\u5408\u80fd\u663e\u8457\u63d0\u9ad8\u6444\u50cf\u5934\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4e0e\u9a7e\u9a76\u5148\u9a8c\u76f8\u7ed3\u5408\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63a8\u8fdb\u4e86\u6444\u50cf\u5934\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21731", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21731", "abs": "https://arxiv.org/abs/2511.21731", "authors": ["Diederik Aerts", "Jonito Aerts Argu\u00eblles", "Lester Beltran", "Suzette Geriente", "Roberto Leporini", "Massimiliano Sassoli de Bianchi", "Sandro Sozzo"], "title": "Identifying Quantum Structure in AI Language: Evidence for Evolutionary Convergence of Human and Artificial Cognition", "comment": null, "summary": "We present the results of cognitive tests on conceptual combinations, performed using specific Large Language Models (LLMs) as test subjects. In the first test, performed with ChatGPT and Gemini, we show that Bell's inequalities are significantly violated, which indicates the presence of 'quantum entanglement' in the tested concepts. In the second test, also performed using ChatGPT and Gemini, we instead identify the presence of 'Bose-Einstein statistics', rather than the intuitively expected 'Maxwell-Boltzmann statistics', in the distribution of the words contained in large-size texts. Interestingly, these findings mirror the results previously obtained in both cognitive tests with human participants and information retrieval tests on large corpora. Taken together, they point to the 'systematic emergence of quantum structures in conceptual-linguistic domains', regardless of whether the cognitive agent is human or artificial. Although LLMs are classified as neural networks for historical reasons, we believe that a more essential form of knowledge organization takes place in the distributive semantic structure of vector spaces built on top of the neural network. It is this meaning-bearing structure that lends itself to a phenomenon of evolutionary convergence between human cognition and language, slowly established through biological evolution, and LLM cognition and language, emerging much more rapidly as a result of self-learning and training. We analyze various aspects and examples that contain evidence supporting the above hypothesis. We also advance a unifying framework that explains the pervasive quantum organization of meaning that we identify.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8ba4\u77e5\u6d4b\u8bd5\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6982\u5ff5\u7ec4\u5408\u4e2d\u8868\u73b0\u51fa\u91cf\u5b50\u7ea0\u7f20\u7279\u5f81\uff0c\u5728\u6587\u672c\u8bcd\u5206\u5e03\u4e2d\u5448\u73b0\u73bb\u8272-\u7231\u56e0\u65af\u5766\u7edf\u8ba1\u800c\u975e\u9ea6\u514b\u65af\u97e6-\u73bb\u5c14\u5179\u66fc\u7edf\u8ba1\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\uff0c\u8868\u660e\u6982\u5ff5-\u8bed\u8a00\u9886\u57df\u5b58\u5728\u7cfb\u7edf\u6027\u7684\u91cf\u5b50\u7ed3\u6784\u6d8c\u73b0\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7a76\u4eba\u7c7b\u8ba4\u77e5\u4e0e\u4eba\u5de5\u667a\u80fd\u8bed\u8a00\u6a21\u578b\u5728\u6982\u5ff5\u7ec4\u7ec7\u548c\u8bed\u8a00\u5904\u7406\u65b9\u9762\u662f\u5426\u5b58\u5728\u5171\u540c\u7684\u5e95\u5c42\u7ed3\u6784\u3002\u4f5c\u8005\u5e0c\u671b\u9a8c\u8bc1\u91cf\u5b50\u8ba4\u77e5\u7406\u8bba\u4e2d\u63d0\u51fa\u7684\"\u6982\u5ff5-\u8bed\u8a00\u9886\u57df\u7684\u91cf\u5b50\u7ed3\u6784\"\u662f\u5426\u4e0d\u4ec5\u5b58\u5728\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u4e2d\uff0c\u4e5f\u5b58\u5728\u4e8eLLM\u4e2d\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u8ba4\u77e5\u6d4b\u8bd5\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528ChatGPT\u548cGemini\u8fdb\u884c\u6982\u5ff5\u7ec4\u5408\u6d4b\u8bd5\uff0c\u6d4b\u91cf\u8d1d\u5c14\u4e0d\u7b49\u5f0f\u8fdd\u53cd\u60c5\u51b5\uff1b2\uff09\u4f7f\u7528\u76f8\u540cLLM\u5206\u6790\u5927\u578b\u6587\u672c\u4e2d\u7684\u8bcd\u5206\u5e03\u7edf\u8ba1\u7279\u6027\u3002\u5c06\u7ed3\u679c\u4e0e\u5148\u524d\u4eba\u7c7b\u8ba4\u77e5\u5b9e\u9a8c\u548c\u8bed\u6599\u5e93\u4fe1\u606f\u68c0\u7d22\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6d4b\u8bd5\u7ed3\u679c\u663e\u793a\uff1a1\uff09LLM\u5728\u6982\u5ff5\u7ec4\u5408\u4e2d\u663e\u8457\u8fdd\u53cd\u8d1d\u5c14\u4e0d\u7b49\u5f0f\uff0c\u8868\u660e\u5b58\u5728\"\u91cf\u5b50\u7ea0\u7f20\"\u73b0\u8c61\uff1b2\uff09\u6587\u672c\u8bcd\u5206\u5e03\u5448\u73b0\u73bb\u8272-\u7231\u56e0\u65af\u5766\u7edf\u8ba1\u800c\u975e\u9884\u671f\u7684\u9ea6\u514b\u65af\u97e6-\u73bb\u5c14\u5179\u66fc\u7edf\u8ba1\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5b9e\u9a8c\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u7ed3\u8bba\u8ba4\u4e3a\uff0c\u6982\u5ff5-\u8bed\u8a00\u9886\u57df\u5b58\u5728\u7cfb\u7edf\u6027\u7684\u91cf\u5b50\u7ed3\u6784\u6d8c\u73b0\uff0c\u8fd9\u79cd\u7ed3\u6784\u5728\u4eba\u7c7b\u8ba4\u77e5\u548cLLM\u4e2d\u90fd\u5b58\u5728\u3002\u4f5c\u8005\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u89e3\u91ca\u8fd9\u79cd\u666e\u904d\u5b58\u5728\u7684\u91cf\u5b50\u610f\u4e49\u7ec4\u7ec7\uff0c\u5e76\u8ba4\u4e3a\u5411\u91cf\u7a7a\u95f4\u7684\u5206\u5e03\u5f0f\u8bed\u4e49\u7ed3\u6784\u662f\u77e5\u8bc6\u7ec4\u7ec7\u7684\u672c\u8d28\u5f62\u5f0f\uff0c\u5bfc\u81f4\u4e86\u4eba\u7c7b\u8ba4\u77e5\u4e0eLLM\u8ba4\u77e5\u7684\u8fdb\u5316\u8d8b\u540c\u3002"}}
{"id": "2511.22928", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22928", "abs": "https://arxiv.org/abs/2511.22928", "authors": ["Jiaxin Liu", "Xiangyu Yan", "Liang Peng", "Lei Yang", "Lingjun Zhang", "Yuechen Luo", "Yueming Tao", "Ashton Yu Xuan Tan", "Mu Li", "Lei Zhang", "Ziqi Zhan", "Sai Guo", "Hong Wang", "Jun Li"], "title": "Seeing before Observable: Potential Risk Reasoning in Autonomous Driving via Vision Language Models", "comment": null, "summary": "Ensuring safety remains a key challenge for autonomous vehicles (AVs), especially in rare and complex scenarios. One critical but understudied aspect is the \\textbf{potential risk} situations, where the risk is \\textbf{not yet observable} but can be inferred from subtle precursors, such as anomalous behaviors or commonsense violations. Recognizing these precursors requires strong semantic understanding and reasoning capabilities, which are often absent in current AV systems due to the scarcity of such cases in existing driving or risk-centric datasets. Moreover, current autonomous driving accident datasets often lack annotations of the causal reasoning chains behind incidents, which are essential for identifying potential risks before they become observable. To address these gaps, we introduce PotentialRiskQA, a novel vision-language dataset designed for reasoning about potential risks prior to observation. Each sample is annotated with structured scene descriptions, semantic precursors, and inferred risk outcomes. Based on this dataset, we further propose PR-Reasoner, a vision-language-model-based framework tailored for onboard potential risk reasoning. Experimental results show that fine-tuning on PotentialRiskQA enables PR-Reasoner to significantly enhance its performance on the potential risk reasoning task compared to baseline VLMs. Together, our dataset and model provide a foundation for developing autonomous systems with improved foresight and proactive safety capabilities, moving toward more intelligent and resilient AVs.", "AI": {"tldr": "\u63d0\u51fa\u4e86PotentialRiskQA\u6570\u636e\u96c6\u548cPR-Reasoner\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u98ce\u9669\u5c1a\u672a\u663e\u73b0\u65f6\u901a\u8fc7\u8bed\u4e49\u63a8\u7406\u8bc6\u522b\u6f5c\u5728\u98ce\u9669", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u8bc6\u522b\u6f5c\u5728\u98ce\u9669\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u7f55\u89c1\u590d\u6742\u573a\u666f\u548c\u56e0\u679c\u63a8\u7406\u94fe\u6807\u6ce8\uff0c\u5bfc\u81f4\u7cfb\u7edf\u65e0\u6cd5\u901a\u8fc7\u7ec6\u5fae\u524d\u5146\u63a8\u65ad\u5c1a\u672a\u663e\u73b0\u7684\u98ce\u9669", "method": "\u521b\u5efaPotentialRiskQA\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ed3\u6784\u5316\u573a\u666f\u63cf\u8ff0\u3001\u8bed\u4e49\u524d\u5146\u548c\u63a8\u65ad\u98ce\u9669\u7ed3\u679c\uff1b\u63d0\u51faPR-Reasoner\u6846\u67b6\uff0c\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8f66\u8f7d\u6f5c\u5728\u98ce\u9669\u63a8\u7406", "result": "\u5728PotentialRiskQA\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684PR-Reasoner\u5728\u6f5c\u5728\u98ce\u9669\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u89c6\u89c9\u8bed\u8a00\u6a21\u578b", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e3a\u5f00\u53d1\u5177\u6709\u66f4\u5f3a\u9884\u89c1\u6027\u548c\u4e3b\u52a8\u5b89\u5168\u80fd\u529b\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63a8\u52a8\u66f4\u667a\u80fd\u3001\u66f4\u5177\u97e7\u6027\u7684\u81ea\u52a8\u9a7e\u9a76\u53d1\u5c55"}}
{"id": "2511.21732", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21732", "abs": "https://arxiv.org/abs/2511.21732", "authors": ["Jiajun Zhang", "Shijia Luo", "Ruikang Zhang", "Qi Su"], "title": "HUMORCHAIN: Theory-Guided Multi-Stage Reasoning for Interpretable Multimodal Humor Generation", "comment": null, "summary": "Humor, as both a creative human activity and a social binding mechanism, has long posed a major challenge for AI generation. Although producing humor requires complex cognitive reasoning and social understanding, theories of humor suggest that it follows learnable patterns and structures, making it theoretically possible for generative models to acquire them implicitly. In recent years, multimodal humor has become a prevalent form of online communication, especially among Gen Z, highlighting the need for AI systems capable of integrating visual understanding with humorous language generation. However, existing data-driven approaches lack explicit modeling or theoretical grounding of humor, often producing literal descriptions that fail to capture its underlying cognitive mechanisms, resulting in the generated image descriptions that are fluent but lack genuine humor or cognitive depth. To address this limitation, we propose HUMORCHAIN (HUmor-guided Multi-step Orchestrated Reasoning Chain for Image Captioning), a theory-guided multi-stage reasoning framework. It integrates visual semantic parsing, humor- and psychology-based reasoning, and a fine-tuned discriminator for humor evaluation, forming an interpretable and controllable cognitive reasoning chain. To the best of our knowledge, this is the first work to explicitly embed cognitive structures from humor theories into multimodal humor generation, enabling a structured reasoning process from visual understanding to humor creation. Experiments on Meme-Image-No-Text, Oogiri-GO, and OxfordTVG-HIC datasets show that HUMORCHAIN outperforms state-of-the-art baselines in human humor preference, Elo/BT scores, and semantic diversity, demonstrating that theory-driven structured reasoning enables large language models to generate humor aligned with human perception.", "AI": {"tldr": "HUMORCHAIN\uff1a\u9996\u4e2a\u5c06\u5e7d\u9ed8\u8ba4\u77e5\u7406\u8bba\u5d4c\u5165\u591a\u6a21\u6001\u5e7d\u9ed8\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u4e49\u89e3\u6790\u3001\u5e7d\u9ed8\u5fc3\u7406\u5b66\u63a8\u7406\u548c\u5e7d\u9ed8\u8bc4\u4f30\u5668\uff0c\u5b9e\u73b0\u4ece\u89c6\u89c9\u7406\u89e3\u5230\u5e7d\u9ed8\u521b\u4f5c\u7684\u7ed3\u6784\u5316\u63a8\u7406\u94fe\u3002", "motivation": "\u5e7d\u9ed8\u4f5c\u4e3a\u4eba\u7c7b\u521b\u9020\u6027\u6d3b\u52a8\u548c\u793e\u4f1a\u7ebd\u5e26\u673a\u5236\uff0c\u5bf9AI\u751f\u6210\u6784\u6210\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5e7d\u9ed8\u7684\u663e\u5f0f\u5efa\u6a21\u6216\u7406\u8bba\u57fa\u7840\uff0c\u751f\u6210\u7684\u56fe\u50cf\u63cf\u8ff0\u6d41\u7545\u4f46\u7f3a\u4e4f\u771f\u6b63\u7684\u5e7d\u9ed8\u6216\u8ba4\u77e5\u6df1\u5ea6\u3002\u591a\u6a21\u6001\u5e7d\u9ed8\u5df2\u6210\u4e3a\u5728\u7ebf\u4ea4\u6d41\u7684\u666e\u904d\u5f62\u5f0f\uff0c\u9700\u8981AI\u7cfb\u7edf\u80fd\u591f\u6574\u5408\u89c6\u89c9\u7406\u89e3\u4e0e\u5e7d\u9ed8\u8bed\u8a00\u751f\u6210\u3002", "method": "\u63d0\u51faHUMORCHAIN\u6846\u67b6\uff0c\u6574\u5408\u89c6\u89c9\u8bed\u4e49\u89e3\u6790\u3001\u57fa\u4e8e\u5e7d\u9ed8\u548c\u5fc3\u7406\u5b66\u7684\u63a8\u7406\uff0c\u4ee5\u53ca\u5fae\u8c03\u7684\u5e7d\u9ed8\u8bc4\u4f30\u5224\u522b\u5668\uff0c\u5f62\u6210\u53ef\u89e3\u91ca\u548c\u53ef\u63a7\u7684\u8ba4\u77e5\u63a8\u7406\u94fe\u3002\u8fd9\u662f\u9996\u4e2a\u5c06\u5e7d\u9ed8\u7406\u8bba\u4e2d\u7684\u8ba4\u77e5\u7ed3\u6784\u663e\u5f0f\u5d4c\u5165\u591a\u6a21\u6001\u5e7d\u9ed8\u751f\u6210\u7684\u5de5\u4f5c\u3002", "result": "\u5728Meme-Image-No-Text\u3001Oogiri-GO\u548cOxfordTVG-HIC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHUMORCHAIN\u5728\u4eba\u7c7b\u5e7d\u9ed8\u504f\u597d\u3001Elo/BT\u5206\u6570\u548c\u8bed\u4e49\u591a\u6837\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7406\u8bba\u9a71\u52a8\u7684\u7ed3\u6784\u5316\u63a8\u7406\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u611f\u77e5\u7684\u5e7d\u9ed8\uff0c\u8bc1\u660e\u4e86\u5c06\u8ba4\u77e5\u7406\u8bba\u5d4c\u5165\u591a\u6a21\u6001\u5e7d\u9ed8\u751f\u6210\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.22963", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22963", "abs": "https://arxiv.org/abs/2511.22963", "authors": ["Zhirui Liu", "Kaiyang Ji", "Ke Yang", "Jingyi Yu", "Ye Shi", "Jingya Wang"], "title": "Commanding Humanoid by Free-form Language: A Large Language Action Model with Unified Motion Vocabulary", "comment": "Project page: https://humanoidlla.github.io/", "summary": "Enabling humanoid robots to follow free-form language commands is critical for seamless human-robot interaction, collaborative task execution, and general-purpose embodied intelligence. While recent advances have improved low-level humanoid locomotion and robot manipulation, language-conditioned whole-body control remains a significant challenge. Existing methods are often limited to simple instructions and sacrifice either motion diversity or physical plausibility. To address this, we introduce Humanoid-LLA, a Large Language Action Model that maps expressive language commands to physically executable whole-body actions for humanoid robots. Our approach integrates three core components: a unified motion vocabulary that aligns human and humanoid motion primitives into a shared discrete space; a vocabulary-directed controller distilled from a privileged policy to ensure physical feasibility; and a physics-informed fine-tuning stage using reinforcement learning with dynamics-aware rewards to enhance robustness and stability. Extensive evaluations in simulation and on a real-world Unitree G1 humanoid show that Humanoid-LLA delivers strong language generalization while maintaining high physical fidelity, outperforming existing language-conditioned controllers in motion naturalness, stability, and execution success rate.", "AI": {"tldr": "Humanoid-LLA\uff1a\u57fa\u4e8e\u5927\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\uff0c\u5c06\u81ea\u7531\u5f62\u5f0f\u8bed\u8a00\u6307\u4ee4\u6620\u5c04\u4e3a\u7c7b\u4eba\u673a\u5668\u4eba\u53ef\u6267\u884c\u7684\u5168\u8eab\u52a8\u4f5c\uff0c\u5b9e\u73b0\u8bed\u8a00\u6cdb\u5316\u4e0e\u7269\u7406\u53ef\u884c\u6027\u7684\u5e73\u8861", "motivation": "\u7c7b\u4eba\u673a\u5668\u4eba\u9700\u8981\u7406\u89e3\u81ea\u7531\u5f62\u5f0f\u8bed\u8a00\u6307\u4ee4\u4ee5\u5b9e\u73b0\u4eba\u673a\u4ea4\u4e92\u3001\u534f\u4f5c\u4efb\u52a1\u548c\u901a\u7528\u5177\u8eab\u667a\u80fd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u8a00\u6761\u4ef6\u5316\u5168\u8eab\u63a7\u5236\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u8981\u4e48\u6307\u4ee4\u7b80\u5355\uff0c\u8981\u4e48\u727a\u7272\u52a8\u4f5c\u591a\u6837\u6027\u6216\u7269\u7406\u5408\u7406\u6027", "method": "\u63d0\u51faHumanoid-LLA\u6a21\u578b\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1)\u7edf\u4e00\u8fd0\u52a8\u8bcd\u6c47\u8868\uff0c\u5c06\u4eba\u7c7b\u548c\u7c7b\u4eba\u8fd0\u52a8\u57fa\u5143\u5bf9\u9f50\u5230\u5171\u4eab\u79bb\u6563\u7a7a\u95f4\uff1b2)\u4ece\u7279\u6743\u7b56\u7565\u84b8\u998f\u7684\u8bcd\u6c47\u5bfc\u5411\u63a7\u5236\u5668\uff0c\u786e\u4fdd\u7269\u7406\u53ef\u884c\u6027\uff1b3)\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7269\u7406\u611f\u77e5\u5fae\u8c03\u9636\u6bb5\uff0c\u4f7f\u7528\u52a8\u529b\u5b66\u611f\u77e5\u5956\u52b1\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9eUnitree G1\u7c7b\u4eba\u673a\u5668\u4eba\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cHumanoid-LLA\u5728\u4fdd\u6301\u9ad8\u7269\u7406\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u52a8\u4f5c\u81ea\u7136\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6267\u884c\u6210\u529f\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u8bed\u8a00\u6761\u4ef6\u5316\u63a7\u5236\u5668", "conclusion": "Humanoid-LLA\u6210\u529f\u89e3\u51b3\u4e86\u7c7b\u4eba\u673a\u5668\u4eba\u8bed\u8a00\u6761\u4ef6\u5316\u5168\u8eab\u63a7\u5236\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u8868\u8fbe\u6027\u8bed\u8a00\u6307\u4ee4\u5230\u7269\u7406\u53ef\u6267\u884c\u52a8\u4f5c\u7684\u6620\u5c04\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u548c\u901a\u7528\u5177\u8eab\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.21733", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21733", "abs": "https://arxiv.org/abs/2511.21733", "authors": ["Dayan Pan", "Jingyuan Wang", "Yilong Zhou", "Jiawei Cheng", "Pengyue Jia", "Xiangyu Zhao"], "title": "RoSA: Enhancing Parameter-Efficient Fine-Tuning via RoPE-aware Selective Adaptation in Large Language Models", "comment": "Accepted by AAAI' 26", "summary": "Fine-tuning large language models is essential for task-specific adaptation, yet it remains computationally prohibitive. Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a solution, but current approaches typically ignore the distinct roles of model components and the heterogeneous importance across layers, thereby limiting adaptation efficiency. Motivated by the observation that Rotary Position Embeddings (RoPE) induce critical activations in the low-frequency dimensions of attention states, we propose RoPE-aware Selective Adaptation (RoSA), a novel PEFT framework that allocates trainable parameters in a more targeted and effective manner. RoSA comprises a RoPE-aware Attention Enhancement (RoAE) module, which selectively enhances the low-frequency components of RoPE-influenced attention states, and a Dynamic Layer Selection (DLS) strategy that adaptively identifies and updates the most critical layers based on LayerNorm gradient norms. By combining dimension-wise enhancement with layer-wise adaptation, RoSA achieves more targeted and efficient fine-tuning. Extensive experiments on fifteen commonsense and arithmetic benchmarks demonstrate that RoSA outperforms existing mainstream PEFT methods under comparable trainable parameters. The code is available to ease reproducibility at https://github.com/Applied-Machine-Learning-Lab/RoSA.", "AI": {"tldr": "RoSA\u662f\u4e00\u79cd\u65b0\u578b\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7RoPE\u611f\u77e5\u7684\u6ce8\u610f\u529b\u589e\u5f3a\u548c\u52a8\u6001\u5c42\u9009\u62e9\uff0c\u5728\u53ef\u8bad\u7ec3\u53c2\u6570\u76f8\u8fd1\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709PEFT\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dPEFT\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u6a21\u578b\u7ec4\u4ef6\u7684\u4e0d\u540c\u89d2\u8272\u548c\u5404\u5c42\u7684\u5f02\u8d28\u6027\u91cd\u8981\u6027\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6548\u7387\u3002\u7814\u7a76\u53d1\u73b0RoPE\u5728\u6ce8\u610f\u529b\u72b6\u6001\u7684\u4f4e\u9891\u7ef4\u5ea6\u4e2d\u8bf1\u5bfc\u5173\u952e\u6fc0\u6d3b\uff0c\u8fd9\u542f\u53d1\u4e86\u66f4\u9488\u5bf9\u6027\u7684\u53c2\u6570\u5206\u914d\u7b56\u7565\u3002", "method": "RoSA\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) RoPE\u611f\u77e5\u6ce8\u610f\u529b\u589e\u5f3a\u6a21\u5757\uff0c\u9009\u62e9\u6027\u5730\u589e\u5f3aRoPE\u5f71\u54cd\u7684\u6ce8\u610f\u529b\u72b6\u6001\u7684\u4f4e\u9891\u5206\u91cf\uff1b2) \u52a8\u6001\u5c42\u9009\u62e9\u7b56\u7565\uff0c\u57fa\u4e8eLayerNorm\u68af\u5ea6\u8303\u6570\u81ea\u9002\u5e94\u8bc6\u522b\u548c\u66f4\u65b0\u6700\u5173\u952e\u5c42\u3002", "result": "\u572815\u4e2a\u5e38\u8bc6\u548c\u7b97\u672f\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cRoSA\u5728\u53ef\u8bad\u7ec3\u53c2\u6570\u76f8\u8fd1\u7684\u60c5\u51b5\u4e0b\u4f18\u4e8e\u73b0\u6709\u4e3b\u6d41PEFT\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u7ef4\u5ea6\u589e\u5f3a\u548c\u5c42\u9002\u5e94\uff0cRoSA\u5b9e\u73b0\u4e86\u66f4\u9488\u5bf9\u6027\u548c\u9ad8\u6548\u7684\u5fae\u8c03\uff0c\u4e3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2511.22996", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22996", "abs": "https://arxiv.org/abs/2511.22996", "authors": ["Ke Chen"], "title": "Analytical Inverse Kinematic Solution for \"Moz1\" NonSRS 7-DOF Robot arm with novel arm angle", "comment": null, "summary": "This paper presents an analytical solution to the inverse kinematic problem(IKP) for the seven degree-of-freedom (7-DOF) Moz1 Robot Arm with offsets on wrist. We provide closed-form solutions with the novel arm angle . it allow fully self-motion and solve the problem of algorithmic singularities within the workspace. It also provides information on how the redundancy is resolved in a new arm angle representation where traditional SEW angle faied to be defined and how singularities are handled. The solution is simple, fast and exact, providing full solution space (i.e. all 16 solutions) per pose.", "AI": {"tldr": "\u63d0\u51fa7\u81ea\u7531\u5ea6Moz1\u673a\u68b0\u81c2\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\u7684\u89e3\u6790\u89e3\uff0c\u4f7f\u7528\u65b0\u9896\u7684\u81c2\u89d2\u6982\u5ff5\uff0c\u63d0\u4f9b\u5c01\u95ed\u5f62\u5f0f\u89e3\uff0c\u89e3\u51b3\u7b97\u6cd5\u5947\u5f02\u6027\u95ee\u9898\uff0c\u5e76\u80fd\u83b7\u5f97\u6240\u670916\u4e2a\u89e3\u3002", "motivation": "\u4f20\u7edfSEW\u89d2\u5ea6\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u65e0\u6cd5\u5b9a\u4e49\uff0c\u5b58\u5728\u7b97\u6cd5\u5947\u5f02\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b37\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u7684\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\uff0c\u5b9e\u73b0\u5b8c\u5168\u81ea\u8fd0\u52a8\u5e76\u5904\u7406\u5de5\u4f5c\u7a7a\u95f4\u5185\u7684\u5947\u5f02\u6027\u3002", "method": "\u91c7\u7528\u65b0\u9896\u7684\u81c2\u89d2\u8868\u793a\u6cd5\uff0c\u4e3a\u5e26\u6709\u8155\u90e8\u504f\u79fb\u76847-DOF Moz1\u673a\u68b0\u81c2\u63d0\u4f9b\u5c01\u95ed\u5f62\u5f0f\u7684\u9006\u8fd0\u52a8\u5b66\u89e3\u3002\u8be5\u65b9\u6cd5\u80fd\u89e3\u6790\u5730\u89e3\u51b3\u5197\u4f59\u5ea6\u95ee\u9898\uff0c\u5904\u7406\u4f20\u7edfSEW\u89d2\u5ea6\u65e0\u6cd5\u5b9a\u4e49\u7684\u60c5\u51b5\u3002", "result": "\u8be5\u65b9\u6cd5\u7b80\u5355\u3001\u5feb\u901f\u4e14\u7cbe\u786e\uff0c\u80fd\u591f\u4e3a\u6bcf\u4e2a\u59ff\u6001\u63d0\u4f9b\u5b8c\u6574\u7684\u89e3\u7a7a\u95f4\uff08\u5373\u6240\u670916\u4e2a\u89e3\uff09\uff0c\u89e3\u51b3\u4e86\u5de5\u4f5c\u7a7a\u95f4\u5185\u7684\u7b97\u6cd5\u5947\u5f02\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b8c\u5168\u81ea\u8fd0\u52a8\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u65b0\u9896\u81c2\u89d2\u7684\u89e3\u6790\u89e3\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e867\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u7684\u9006\u8fd0\u52a8\u5b66\u95ee\u9898\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u673a\u68b0\u81c2\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2511.21734", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21734", "abs": "https://arxiv.org/abs/2511.21734", "authors": ["Shiguang Wu", "Quanming Yao"], "title": "Asking LLMs to Verify First is Almost Free Lunch", "comment": null, "summary": "To enhance the reasoning capabilities of Large Language Models (LLMs) without high costs of training, nor extensive test-time sampling, we introduce Verification-First (VF), a strategy that prompts models to verify a provided candidate answer, even a trivial or random one, before generating a solution. This approach triggers a \"reverse reasoning\" process that is cognitively easier and complementary to standard forward Chain-of-Thought (CoT), effectively invoking the model's critical thinking to reduce logical errors. We further generalize the VF strategy to Iter-VF, a sequential test-time scaling (TTS) method that iteratively cycles the verification-generation process using the model's previous answer. Extensive experiments across various benchmarks (from mathematical reasoning to coding and agentic tasks) and various LLMs (from open-source 1B to cutting-edge commercial ones) confirm that VF with random answer consistently outperforms standard CoT with minimal computational overhead, and Iter-VF outperforms existing TTS strategies.", "AI": {"tldr": "\u63d0\u51faVerification-First\u7b56\u7565\uff0c\u8ba9LLMs\u5148\u9a8c\u8bc1\u5019\u9009\u7b54\u6848\u518d\u751f\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u89e6\u53d1\"\u9006\u5411\u63a8\u7406\"\u8fc7\u7a0b\uff0c\u6bd4\u6807\u51c6CoT\u66f4\u6709\u6548\u4e14\u8ba1\u7b97\u5f00\u9500\u5c0f", "motivation": "\u5728\u4e0d\u589e\u52a0\u8bad\u7ec3\u6210\u672c\u6216\u5927\u91cf\u6d4b\u8bd5\u65f6\u91c7\u6837\u7684\u524d\u63d0\u4e0b\uff0c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u65b9\u6cd5\u5982Chain-of-Thought\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\uff0c\u800c\u9a8c\u8bc1\u4f18\u5148\u7b56\u7565\u80fd\u4ee5\u66f4\u4f4e\u7684\u6210\u672c\u5b9e\u73b0\u66f4\u597d\u7684\u63a8\u7406\u6548\u679c\u3002", "method": "\u63d0\u51faVerification-First\u7b56\u7565\uff1a\u5148\u8ba9\u6a21\u578b\u9a8c\u8bc1\u4e00\u4e2a\u5019\u9009\u7b54\u6848\uff08\u5373\u4f7f\u662f\u968f\u673a\u6216\u7b80\u5355\u7684\u7b54\u6848\uff09\uff0c\u7136\u540e\u518d\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002\u8fd9\u79cd\u65b9\u6cd5\u89e6\u53d1\"\u9006\u5411\u63a8\u7406\"\u8fc7\u7a0b\uff0c\u4e0e\u6807\u51c6\u524d\u5411CoT\u4e92\u8865\u3002\u8fdb\u4e00\u6b65\u6269\u5c55\u4e3aIter-VF\uff1a\u4e00\u79cd\u987a\u5e8f\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u5faa\u73af\u9a8c\u8bc1-\u751f\u6210\u8fc7\u7a0b\uff0c\u4f7f\u7528\u6a21\u578b\u4e4b\u524d\u7684\u7b54\u6848\u3002", "result": "\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\uff08\u4ece\u6570\u5b66\u63a8\u7406\u5230\u7f16\u7801\u548c\u4ee3\u7406\u4efb\u52a1\uff09\u548c\u591a\u79cdLLM\uff08\u4ece\u5f00\u6e901B\u5230\u5c16\u7aef\u5546\u4e1a\u6a21\u578b\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u5b9e\uff1aVF\u4f7f\u7528\u968f\u673a\u7b54\u6848\u59cb\u7ec8\u4f18\u4e8e\u6807\u51c6CoT\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\uff1bIter-VF\u4f18\u4e8e\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u3002", "conclusion": "Verification-First\u7b56\u7565\u901a\u8fc7\u89e6\u53d1\u9006\u5411\u63a8\u7406\u8fc7\u7a0b\uff0c\u6709\u6548\u63d0\u5347\u4e86LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u3002\u8be5\u65b9\u6cd5\u4e3a\u589e\u5f3a\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.23017", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.23017", "abs": "https://arxiv.org/abs/2511.23017", "authors": ["Elham Ahmadi", "Alireza Olama", "Petri V\u00e4lisuo", "Heidi Kuusniemi"], "title": "Adaptive Factor Graph-Based Tightly Coupled GNSS/IMU Fusion for Robust Positionin", "comment": null, "summary": "Reliable positioning in GNSS-challenged environments remains a critical challenge for navigation systems. Tightly coupled GNSS/IMU fusion improves robustness but remains vulnerable to non-Gaussian noise and outliers. We present a robust and adaptive factor graph-based fusion framework that directly integrates GNSS pseudorange measurements with IMU preintegration factors and incorporates the Barron loss, a general robust loss function that unifies several m-estimators through a single tunable parameter. By adaptively down weighting unreliable GNSS measurements, our approach improves resilience positioning. The method is implemented in an extended GTSAM framework and evaluated on the UrbanNav dataset. The proposed solution reduces positioning errors by up to 41% relative to standard FGO, and achieves even larger improvements over extended Kalman filter (EKF) baselines in urban canyon environments. These results highlight the benefits of Barron loss in enhancing the resilience of GNSS/IMU-based navigation in urban and signal-compromised environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u5b50\u56fe\u7684\u9c81\u68d2\u81ea\u9002\u5e94GNSS/IMU\u878d\u5408\u6846\u67b6\uff0c\u91c7\u7528Barron\u635f\u5931\u51fd\u6570\u5904\u7406\u975e\u9ad8\u65af\u566a\u58f0\u548c\u5f02\u5e38\u503c\uff0c\u5728GNSS\u53d7\u9650\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6", "motivation": "GNSS\u53d7\u9650\u73af\u5883\uff08\u5982\u57ce\u5e02\u5ce1\u8c37\uff09\u4e2d\u7684\u53ef\u9760\u5b9a\u4f4d\u662f\u5bfc\u822a\u7cfb\u7edf\u7684\u5173\u952e\u6311\u6218\u3002\u4f20\u7edf\u7684\u7d27\u8026\u5408GNSS/IMU\u878d\u5408\u867d\u7136\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\uff0c\u4f46\u4ecd\u5bb9\u6613\u53d7\u5230\u975e\u9ad8\u65af\u566a\u58f0\u548c\u5f02\u5e38\u503c\u7684\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51fa\u57fa\u4e8e\u56e0\u5b50\u56fe\u7684\u9c81\u68d2\u81ea\u9002\u5e94\u878d\u5408\u6846\u67b6\uff0c\u5c06GNSS\u4f2a\u8ddd\u6d4b\u91cf\u4e0eIMU\u9884\u79ef\u5206\u56e0\u5b50\u76f4\u63a5\u96c6\u6210\uff0c\u5e76\u5f15\u5165Barron\u635f\u5931\u51fd\u6570\uff08\u4e00\u79cd\u901a\u8fc7\u5355\u4e2a\u53ef\u8c03\u53c2\u6570\u7edf\u4e00\u591a\u4e2aM\u4f30\u8ba1\u5668\u7684\u901a\u7528\u9c81\u68d2\u635f\u5931\u51fd\u6570\uff09\uff0c\u81ea\u9002\u5e94\u964d\u4f4e\u4e0d\u53ef\u9760GNSS\u6d4b\u91cf\u7684\u6743\u91cd", "result": "\u5728UrbanNav\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u6807\u51c6\u56e0\u5b50\u56fe\u4f18\u5316(FGO)\u51cf\u5c11\u5b9a\u4f4d\u8bef\u5dee\u8fbe41%\uff0c\u76f8\u6bd4\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668(EKF)\u57fa\u7ebf\u5728\u57ce\u5e02\u573a\u666f\u4e2d\u63d0\u5347\u66f4\u663e\u8457", "conclusion": "Barron\u635f\u5931\u51fd\u6570\u80fd\u6709\u6548\u589e\u5f3aGNSS/IMU\u5bfc\u822a\u5728\u57ce\u5e02\u573a\u666f\u548c\u4fe1\u53f7\u53d7\u9650\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cfb\u7edf\u7684\u6297\u5e72\u6270\u80fd\u529b"}}
{"id": "2511.21735", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.21735", "abs": "https://arxiv.org/abs/2511.21735", "authors": ["Harshita Sharma", "Maxwell C. Reynolds", "Valentina Salvatelli", "Anne-Marie G. Sykes", "Kelly K. Horst", "Anton Schwaighofer", "Maximilian Ilse", "Olesya Melnichenko", "Sam Bond-Taylor", "Fernando P\u00e9rez-Garc\u00eda", "Vamshi K. Mugu", "Alex Chan", "Ceylan Colak", "Shelby A. Swartz", "Motassem B. Nashawaty", "Austin J. Gonzalez", "Heather A. Ouellette", "Selnur B. Erdal", "Beth A. Schueler", "Maria T. Wetscherek", "Noel Codella", "Mohit Jain", "Shruthi Bannur", "Kenza Bouzid", "Daniel C. Castro", "Stephanie Hyland", "Panos Korfiatis", "Ashish Khandelwal", "Javier Alvarez-Valle"], "title": "Closing the Performance Gap Between AI and Radiologists in Chest X-Ray Reporting", "comment": null, "summary": "AI-assisted report generation offers the opportunity to reduce radiologists' workload stemming from expanded screening guidelines, complex cases and workforce shortages, while maintaining diagnostic accuracy. In addition to describing pathological findings in chest X-ray reports, interpreting lines and tubes (L&T) is demanding and repetitive for radiologists, especially with high patient volumes. We introduce MAIRA-X, a clinically evaluated multimodal AI model for longitudinal chest X-ray (CXR) report generation, that encompasses both clinical findings and L&T reporting. Developed using a large-scale, multi-site, longitudinal dataset of 3.1 million studies (comprising 6 million images from 806k patients) from Mayo Clinic, MAIRA-X was evaluated on three holdout datasets and the public MIMIC-CXR dataset, where it significantly improved AI-generated reports over the state of the art on lexical quality, clinical correctness, and L&T-related elements. A novel L&T-specific metrics framework was developed to assess accuracy in reporting attributes such as type, longitudinal change and placement. A first-of-its-kind retrospective user evaluation study was conducted with nine radiologists of varying experience, who blindly reviewed 600 studies from distinct subjects. The user study found comparable rates of critical errors (3.0% for original vs. 4.6% for AI-generated reports) and a similar rate of acceptable sentences (97.8% for original vs. 97.4% for AI-generated reports), marking a significant improvement over prior user studies with larger gaps and higher error rates. Our results suggest that MAIRA-X can effectively assist radiologists, particularly in high-volume clinical settings.", "AI": {"tldr": "MAIRA-X\u662f\u4e00\u4e2a\u7528\u4e8e\u80f8\u90e8X\u5149\u62a5\u544a\u751f\u6210\u7684\u591a\u6a21\u6001AI\u6a21\u578b\uff0c\u5728\u4e34\u5e8a\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u662f\u5728\u5bfc\u7ba1\u548c\u7ba1\u8def\u62a5\u544a\u65b9\u9762\uff0c\u9519\u8bef\u7387\u4e0e\u539f\u59cb\u62a5\u544a\u76f8\u5f53\u3002", "motivation": "AI\u8f85\u52a9\u62a5\u544a\u751f\u6210\u53ef\u4ee5\u51cf\u8f7b\u653e\u5c04\u79d1\u533b\u751f\u7684\u5de5\u4f5c\u8d1f\u62c5\uff0c\u7279\u522b\u662f\u5728\u5bfc\u7ba1\u548c\u7ba1\u8def\u89e3\u91ca\u8fd9\u79cd\u91cd\u590d\u6027\u4efb\u52a1\u4e0a\uff0c\u540c\u65f6\u5e94\u5bf9\u7b5b\u67e5\u6307\u5357\u6269\u5c55\u3001\u590d\u6742\u75c5\u4f8b\u548c\u4eba\u5458\u77ed\u7f3a\u7684\u6311\u6218\u3002", "method": "\u4f7f\u7528\u6885\u5965\u8bca\u6240\u7684\u5927\u89c4\u6a21\u591a\u4e2d\u5fc3\u7eb5\u5411\u6570\u636e\u96c6\uff08310\u4e07\u7814\u7a76\uff0c600\u4e07\u56fe\u50cf\uff0c80.6\u4e07\u60a3\u8005\uff09\u5f00\u53d1MAIRA-X\u6a21\u578b\uff0c\u5e76\u5728\u4e09\u4e2a\u4fdd\u7559\u6570\u636e\u96c6\u548cMIMIC-CXR\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u3002\u5f00\u53d1\u4e86\u4e13\u95e8\u7684\u5bfc\u7ba1\u548c\u7ba1\u8def\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u8fdb\u884c\u4e86\u9996\u6b21\u56de\u987e\u6027\u7528\u6237\u8bc4\u4f30\u7814\u7a76\u3002", "result": "MAIRA-X\u5728\u8bcd\u6c47\u8d28\u91cf\u3001\u4e34\u5e8a\u6b63\u786e\u6027\u548c\u5bfc\u7ba1\u7ba1\u8def\u76f8\u5173\u5143\u7d20\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002\u7528\u6237\u7814\u7a76\u53d1\u73b0\u5173\u952e\u9519\u8bef\u7387\u76f8\u5f53\uff08\u539f\u59cb3.0% vs AI 4.6%\uff09\uff0c\u53ef\u63a5\u53d7\u53e5\u5b50\u7387\u76f8\u4f3c\uff08\u539f\u59cb97.8% vs AI 97.4%\uff09\uff0c\u76f8\u6bd4\u4e4b\u524d\u7814\u7a76\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "MAIRA-X\u53ef\u4ee5\u6709\u6548\u8f85\u52a9\u653e\u5c04\u79d1\u533b\u751f\uff0c\u7279\u522b\u662f\u5728\u9ad8\u6d41\u91cf\u4e34\u5e8a\u73af\u5883\u4e2d\uff0c\u4e3aAI\u8f85\u52a9\u80f8\u90e8X\u5149\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.23030", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.23030", "abs": "https://arxiv.org/abs/2511.23030", "authors": ["Casimir Feldmann", "Maximum Wilder-Smith", "Vaishakh Patil", "Michael Oechsle", "Michael Niemeyer", "Keisuke Tateno", "Marco Hutter"], "title": "DiskChunGS: Large-Scale 3D Gaussian SLAM Through Chunk-Based Memory Management", "comment": null, "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated impressive results for novel view synthesis with real-time rendering capabilities. However, integrating 3DGS with SLAM systems faces a fundamental scalability limitation: methods are constrained by GPU memory capacity, restricting reconstruction to small-scale environments. We present DiskChunGS, a scalable 3DGS SLAM system that overcomes this bottleneck through an out-of-core approach that partitions scenes into spatial chunks and maintains only active regions in GPU memory while storing inactive areas on disk. Our architecture integrates seamlessly with existing SLAM frameworks for pose estimation and loop closure, enabling globally consistent reconstruction at scale. We validate DiskChunGS on indoor scenes (Replica, TUM-RGBD), urban driving scenarios (KITTI), and resource-constrained Nvidia Jetson platforms. Our method uniquely completes all 11 KITTI sequences without memory failures while achieving superior visual quality, demonstrating that algorithmic innovation can overcome the memory constraints that have limited previous 3DGS SLAM methods.", "AI": {"tldr": "DiskChunGS\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u76843D\u9ad8\u65af\u6cfc\u6e85SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u5916\u5b58\u65b9\u6cd5\u5c06\u573a\u666f\u5206\u5272\u6210\u7a7a\u95f4\u5757\uff0c\u4ec5\u5c06\u6d3b\u8dc3\u533a\u57df\u4fdd\u7559\u5728GPU\u5185\u5b58\u4e2d\uff0c\u4ece\u800c\u514b\u670d\u4e86\u5185\u5b58\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u73af\u5883\u7684\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u76843DGS SLAM\u7cfb\u7edf\u53d7\u9650\u4e8eGPU\u5185\u5b58\u5bb9\u91cf\uff0c\u53ea\u80fd\u91cd\u5efa\u5c0f\u89c4\u6a21\u73af\u5883\uff0c\u65e0\u6cd5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u573a\u666f\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u514b\u670d\u5185\u5b58\u74f6\u9888\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5916\u5b58\u65b9\u6cd5\uff0c\u5c06\u573a\u666f\u5206\u5272\u6210\u7a7a\u95f4\u5757\uff0c\u4ec5\u5c06\u6d3b\u8dc3\u533a\u57df\u4fdd\u7559\u5728GPU\u5185\u5b58\u4e2d\uff0c\u4e0d\u6d3b\u8dc3\u533a\u57df\u5b58\u50a8\u5728\u78c1\u76d8\u4e0a\u3002\u4e0e\u73b0\u6709SLAM\u6846\u67b6\u65e0\u7f1d\u96c6\u6210\uff0c\u652f\u6301\u59ff\u6001\u4f30\u8ba1\u548c\u95ed\u73af\u68c0\u6d4b\u3002", "result": "\u5728\u5ba4\u5185\u573a\u666f\uff08Replica, TUM-RGBD\uff09\u3001\u57ce\u5e02\u9a7e\u9a76\u573a\u666f\uff08KITTI\uff09\u548c\u8d44\u6e90\u53d7\u9650\u7684Nvidia Jetson\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u3002\u6210\u529f\u5b8c\u6210\u6240\u670911\u4e2aKITTI\u5e8f\u5217\u800c\u4e0d\u4f1a\u51fa\u73b0\u5185\u5b58\u6545\u969c\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "DiskChunGS\u901a\u8fc7\u7b97\u6cd5\u521b\u65b0\u514b\u670d\u4e86\u5148\u524d3DGS SLAM\u65b9\u6cd5\u7684\u5185\u5b58\u9650\u5236\uff0c\u8bc1\u660e\u4e86\u53ef\u6269\u5c55\u76843DGS SLAM\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u80fd\u591f\u5b9e\u73b0\u5927\u89c4\u6a21\u73af\u5883\u7684\u5168\u5c40\u4e00\u81f4\u91cd\u5efa\u3002"}}
{"id": "2511.21736", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21736", "abs": "https://arxiv.org/abs/2511.21736", "authors": ["Jiayi Chen", "Jieqi Shi", "Jing Huo", "Chen Wu"], "title": "R2Q: Towards Robust 2-Bit Large Language Models via Residual Refinement Quantization", "comment": null, "summary": "The rapid progress of Large Language Models (LLMs) has brought substantial computational and memory demands, spurring the adoption of low-bit quantization. While 8-bit and 4-bit formats have become prevalent, extending quantization to 2 bits remains challenging due to severe accuracy degradation. To address this, we propose Residual Refinement Quantization (R2Q)-a novel 2-bit quantization framework that decomposes the process into two sequential 1-bit sub-quantizations, forming an adaptive quantization lattice. Extensive evaluations on Llama, OPT, and Qwen across diverse benchmarks-covering question answering, commonsense reasoning, and language modeling-demonstrate that R2Q consistently outperforms existing 2-bit quantization methods in both fine-grained and coarse-grained settings. By refining quantization through a residual learning mechanism, R2Q enhances performance, improves training stability, and accelerates convergence under extreme compression. Furthermore, its modular design enables seamless integration with existing quantization-aware training (QAT) frameworks.", "AI": {"tldr": "R2Q\u662f\u4e00\u79cd\u65b0\u9896\u76842\u4f4d\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c062\u4f4d\u91cf\u5316\u5206\u89e3\u4e3a\u4e24\u4e2a\u987a\u5e8f\u76841\u4f4d\u5b50\u91cf\u5316\uff0c\u5f62\u6210\u81ea\u9002\u5e94\u91cf\u5316\u7f51\u683c\uff0c\u663e\u8457\u63d0\u5347\u4e862\u4f4d\u91cf\u5316\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u5de8\u5927\uff0c\u4fc3\u4f7f\u4f4e\u6bd4\u7279\u91cf\u5316\u6210\u4e3a\u5fc5\u8981\u3002\u867d\u71368\u4f4d\u548c4\u4f4d\u91cf\u5316\u5df2\u666e\u904d\u5e94\u7528\uff0c\u4f462\u4f4d\u91cf\u5316\u7531\u4e8e\u4e25\u91cd\u7684\u7cbe\u5ea6\u4e0b\u964d\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6b8b\u5dee\u7cbe\u5316\u91cf\u5316(R2Q)\u6846\u67b6\uff0c\u5c062\u4f4d\u91cf\u5316\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e24\u4e2a\u987a\u5e8f\u76841\u4f4d\u5b50\u91cf\u5316\uff0c\u5f62\u6210\u81ea\u9002\u5e94\u91cf\u5316\u7f51\u683c\u3002\u901a\u8fc7\u6b8b\u5dee\u5b66\u4e60\u673a\u5236\u7cbe\u5316\u91cf\u5316\u8fc7\u7a0b\uff0c\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\u53ef\u4e0e\u73b0\u6709\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u6846\u67b6\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728Llama\u3001OPT\u548cQwen\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cR2Q\u5728\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u8bbe\u7f6e\u4e0b\u5747\u4f18\u4e8e\u73b0\u67092\u4f4d\u91cf\u5316\u65b9\u6cd5\uff0c\u6db5\u76d6\u95ee\u7b54\u3001\u5e38\u8bc6\u63a8\u7406\u548c\u8bed\u8a00\u5efa\u6a21\u7b49\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u3002\u540c\u65f6\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u5e76\u52a0\u901f\u4e86\u6536\u655b\u3002", "conclusion": "R2Q\u901a\u8fc7\u521b\u65b0\u7684\u6b8b\u5dee\u7cbe\u5316\u91cf\u5316\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e862\u4f4d\u91cf\u5316\u7684\u7cbe\u5ea6\u4e0b\u964d\u95ee\u9898\uff0c\u5728\u6781\u7aef\u538b\u7f29\u4e0b\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u6536\u655b\u901f\u5ea6\uff0c\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u786e\u4fdd\u4e86\u4e0e\u73b0\u6709\u6846\u67b6\u7684\u517c\u5bb9\u6027\u3002"}}
{"id": "2511.23034", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.23034", "abs": "https://arxiv.org/abs/2511.23034", "authors": ["Zuolei Li", "Xingyu Gao", "Xiaofan Wang", "Jianlong Fu"], "title": "LatBot: Distilling Universal Latent Actions for Vision-Language-Action Models", "comment": "Project Page: https://mm-robot.github.io/distill_latent_action/", "summary": "Learning transferable latent actions from large-scale object manipulation videos can significantly enhance generalization in downstream robotics tasks, as such representations are agnostic to different robot embodiments. Existing approaches primarily rely on visual reconstruction objectives while neglecting physical priors, leading to sub-optimal performance in learning universal representations. To address these challenges, we propose a Universal Latent Action Learning framework that takes task instructions and multiple frames as inputs, and optimizes both future frame reconstruction and action sequence prediction. Unlike prior works, incorporating action predictions (e.g., gripper or hand trajectories and orientations) allows the model to capture richer physical priors such as real-world distances and orientations, thereby enabling seamless transferability to downstream tasks. We further decompose the latent actions into learnable motion and scene tokens to distinguish the robot's active movements from environmental changes, thus filtering out irrelevant dynamics. By distilling the learned latent actions into the latest VLA models, we achieve strong performance across both simulated (SIMPLER and LIBERO) and real-world robot settings. Notably, with only 10 real-world trajectories per task collected on a Franka robot, our approach successfully completes all five challenging tasks, demonstrating strong few-shot transferability in robotic manipulation.", "AI": {"tldr": "\u63d0\u51fa\u901a\u7528\u6f5c\u5728\u52a8\u4f5c\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u672a\u6765\u5e27\u91cd\u5efa\u548c\u52a8\u4f5c\u5e8f\u5217\u9884\u6d4b\uff0c\u4ece\u5927\u89c4\u6a21\u7269\u4f53\u64cd\u4f5c\u89c6\u9891\u4e2d\u5b66\u4e60\u53ef\u8f6c\u79fb\u7684\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u4efb\u52a1\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u91cd\u5efa\u76ee\u6807\u800c\u5ffd\u7565\u7269\u7406\u5148\u9a8c\uff0c\u5bfc\u81f4\u5b66\u4e60\u901a\u7528\u8868\u793a\u7684\u6027\u80fd\u4e0d\u4f73\u3002\u9700\u8981\u4ece\u5927\u89c4\u6a21\u7269\u4f53\u64cd\u4f5c\u89c6\u9891\u4e2d\u5b66\u4e60\u53ef\u8f6c\u79fb\u7684\u6f5c\u5728\u52a8\u4f5c\uff0c\u4ee5\u589e\u5f3a\u4e0b\u6e38\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u901a\u7528\u6f5c\u5728\u52a8\u4f5c\u5b66\u4e60\u6846\u67b6\uff0c\u8f93\u5165\u4efb\u52a1\u6307\u4ee4\u548c\u591a\u5e27\u56fe\u50cf\uff0c\u540c\u65f6\u4f18\u5316\u672a\u6765\u5e27\u91cd\u5efa\u548c\u52a8\u4f5c\u5e8f\u5217\u9884\u6d4b\u3002\u5c06\u6f5c\u5728\u52a8\u4f5c\u5206\u89e3\u4e3a\u53ef\u5b66\u4e60\u7684\u8fd0\u52a8\u548c\u573a\u666ftoken\uff0c\u533a\u5206\u673a\u5668\u4eba\u4e3b\u52a8\u8fd0\u52a8\u548c\u73af\u5883\u53d8\u5316\u3002\u5c06\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u52a8\u4f5c\u84b8\u998f\u5230\u6700\u65b0\u7684VLA\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\uff08SIMPLER\u548cLIBERO\uff09\u548c\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u8bbe\u7f6e\u4e2d\u5747\u53d6\u5f97\u5f3a\u6027\u80fd\u3002\u4ec5\u9700\u6bcf\u4e2a\u4efb\u52a110\u6761\u771f\u5b9e\u4e16\u754c\u8f68\u8ff9\uff0c\u5c31\u80fd\u6210\u529f\u5b8c\u6210\u6240\u6709\u4e94\u4e2a\u6311\u6218\u6027\u4efb\u52a1\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u5c11\u6837\u672c\u8fc1\u79fb\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u91cd\u5efa\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u80fd\u591f\u5b66\u4e60\u5305\u542b\u4e30\u5bcc\u7269\u7406\u5148\u9a8c\u7684\u901a\u7528\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u5b9e\u73b0\u4ece\u5927\u89c4\u6a21\u89c6\u9891\u5230\u4e0b\u6e38\u673a\u5668\u4eba\u4efb\u52a1\u7684\u65e0\u7f1d\u8fc1\u79fb\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.21737", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21737", "abs": "https://arxiv.org/abs/2511.21737", "authors": ["Sabrina Sadiekh", "Elena Ericheva", "Chirag Agarwal"], "title": "Polarity-Aware Probing for Quantifying Latent Alignment in Language Models", "comment": "7 pages", "summary": "Advances in unsupervised probes such as Contrast-Consistent Search (CCS), which reveal latent beliefs without relying on token outputs, raise the question of whether these methods can reliably assess model alignment. We investigate this by examining the sensitivity of CCS to harmful vs. safe statements and by introducing Polarity-Aware CCS (PA-CCS), a method for evaluating whether a model's internal representations remain consistent under polarity inversion. We propose two alignment-oriented metrics, Polar-Consistency and the Contradiction Index, to quantify the semantic robustness of a model's latent knowledge. To validate PA-CCS, we curate two main datasets and one control dataset containing matched harmful-safe sentence pairs constructed using different methodologies (concurrent and antagonistic statements). We apply PA-CCS to 16 language models. Our results show that PA-CCS identifies both architectural and layer-specific differences in the encoding of latent harmful knowledge. Notably, replacing the negation token with a meaningless marker degrades PA-CCS scores for models with well-aligned internal representations, while models lacking robust internal calibration do not exhibit this degradation. Our findings highlight the potential of unsupervised probing for alignment evaluation and emphasize the need to incorporate structural robustness checks into interpretability benchmarks. Code and datasets are available at: https://github.com/SadSabrina/polarity-probing. WARNING: This paper contains potentially sensitive, harmful, and offensive content.", "AI": {"tldr": "PA-CCS\u65b9\u6cd5\u901a\u8fc7\u6781\u6027\u53cd\u8f6c\u8bc4\u4f30\u6a21\u578b\u5185\u90e8\u8868\u5f81\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u4e24\u4e2a\u5bf9\u9f50\u6307\u6807\uff0c\u53d1\u73b0\u80fd\u8bc6\u522b\u6709\u5bb3\u77e5\u8bc6\u7684\u67b6\u6784\u548c\u5c42\u7ea7\u5dee\u5f02\uff0c\u4e3a\u65e0\u76d1\u7763\u63a2\u6d4b\u63d0\u4f9b\u7ed3\u6784\u9c81\u68d2\u6027\u68c0\u67e5\u3002", "motivation": "\u7814\u7a76\u65e0\u76d1\u7763\u63a2\u6d4b\u65b9\u6cd5\uff08\u5982CCS\uff09\u662f\u5426\u80fd\u53ef\u9760\u8bc4\u4f30\u6a21\u578b\u5bf9\u9f50\uff0c\u7279\u522b\u662f\u5bf9\u6709\u5bb3\u4e0e\u5b89\u5168\u9648\u8ff0\u7684\u654f\u611f\u6027\uff0c\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u5185\u90e8\u8868\u5f81\u5728\u6781\u6027\u53cd\u8f6c\u4e0b\u7684\u8bed\u4e49\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u6781\u6027\u611f\u77e5CCS\uff08PA-CCS\uff09\uff0c\u5f15\u5165\u6781\u6027\u4e00\u81f4\u6027\u548c\u77db\u76fe\u6307\u6570\u4e24\u4e2a\u5bf9\u9f50\u6307\u6807\uff0c\u6784\u5efa\u4e09\u4e2a\u6570\u636e\u96c6\uff08\u4e24\u4e2a\u4e3b\u8981\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u63a7\u5236\u96c6\uff09\uff0c\u5305\u542b\u5339\u914d\u7684\u6709\u5bb3-\u5b89\u5168\u53e5\u5b50\u5bf9\uff0c\u5e94\u7528\u4e8e16\u4e2a\u8bed\u8a00\u6a21\u578b\u3002", "result": "PA-CCS\u80fd\u8bc6\u522b\u6709\u5bb3\u77e5\u8bc6\u7f16\u7801\u7684\u67b6\u6784\u548c\u5c42\u7ea7\u5dee\u5f02\uff1b\u5bf9\u9f50\u826f\u597d\u7684\u6a21\u578b\u5728\u5426\u5b9a\u6807\u8bb0\u66ff\u6362\u4e3a\u65e0\u610f\u4e49\u6807\u8bb0\u65f6PA-CCS\u5206\u6570\u4e0b\u964d\uff0c\u800c\u7f3a\u4e4f\u5185\u90e8\u6821\u51c6\u7684\u6a21\u578b\u5219\u65e0\u6b64\u9000\u5316\u3002", "conclusion": "\u65e0\u76d1\u7763\u63a2\u6d4b\u5728\u6a21\u578b\u5bf9\u9f50\u8bc4\u4f30\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u5c06\u7ed3\u6784\u9c81\u68d2\u6027\u68c0\u67e5\u7eb3\u5165\u53ef\u89e3\u91ca\u6027\u57fa\u51c6\uff0cPA-CCS\u4e3a\u8bc4\u4f30\u6a21\u578b\u5185\u90e8\u8868\u5f81\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2511.23143", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23143", "abs": "https://arxiv.org/abs/2511.23143", "authors": ["Enrico Saccon", "Davide De Martini", "Matteo Saveriano", "Edoardo Lamon", "Luigi Palopoli", "Marco Roveri"], "title": "Automated Generation of MDPs Using Logic Programming and LLMs for Robotic Applications", "comment": "9 pages, 11 figures, 2 tables, 2 algorithms, accepted for publication in IEEE Robotics and Automation Letters", "summary": "We present a novel framework that integrates Large Language Models (LLMs) with automated planning and formal verification to streamline the creation and use of Markov Decision Processes (MDP). Our system leverages LLMs to extract structured knowledge in the form of a Prolog knowledge base from natural language (NL) descriptions. It then automatically constructs an MDP through reachability analysis, and synthesises optimal policies using the Storm model checker. The resulting policy is exported as a state-action table for execution. We validate the framework in three human-robot interaction scenarios, demonstrating its ability to produce executable policies with minimal manual effort. This work highlights the potential of combining language models with formal methods to enable more accessible and scalable probabilistic planning in robotics.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u3001\u81ea\u52a8\u89c4\u5212\u548c\u5f62\u5f0f\u9a8c\u8bc1\u7684\u6846\u67b6\uff0c\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u81ea\u52a8\u6784\u5efa\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5e76\u751f\u6210\u6700\u4f18\u7b56\u7565", "motivation": "\u4f20\u7edf\u6982\u7387\u89c4\u5212\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u624b\u52a8\u5efa\u6a21\u5de5\u4f5c\uff0c\u9650\u5236\u4e86\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u548c\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u7b80\u5316MDP\u7684\u521b\u5efa\u548c\u4f7f\u7528\u8fc7\u7a0b\u3002", "method": "1) \u4f7f\u7528LLM\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u6784\u5efaProlog\u77e5\u8bc6\u5e93\uff1b2) \u901a\u8fc7\u53ef\u8fbe\u6027\u5206\u6790\u81ea\u52a8\u6784\u5efaMDP\uff1b3) \u4f7f\u7528Storm\u6a21\u578b\u68c0\u67e5\u5668\u5408\u6210\u6700\u4f18\u7b56\u7565\uff1b4) \u5c06\u7b56\u7565\u5bfc\u51fa\u4e3a\u72b6\u6001-\u52a8\u4f5c\u8868\u4f9b\u6267\u884c", "result": "\u5728\u4e09\u4e2a\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u4eba\u5de5\u52aa\u529b\u751f\u6210\u53ef\u6267\u884c\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027", "conclusion": "\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u4e0e\u5f62\u5f0f\u5316\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u66f4\u6613\u8bbf\u95ee\u548c\u53ef\u6269\u5c55\u7684\u6982\u7387\u89c4\u5212\uff0c\u4e3a\u673a\u5668\u4eba\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.21740", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21740", "abs": "https://arxiv.org/abs/2511.21740", "authors": ["Yizi Zhang", "Linyang He", "Chaofei Fan", "Tingkai Liu", "Han Yu", "Trung Le", "Jingyuan Li", "Scott Linderman", "Lea Duncker", "Francis R Willett", "Nima Mesgarani", "Liam Paninski"], "title": "Decoding inner speech with an end-to-end brain-to-text neural interface", "comment": null, "summary": "Speech brain-computer interfaces (BCIs) aim to restore communication for people with paralysis by translating neural activity into text. Most systems use cascaded frameworks that decode phonemes before assembling sentences with an n-gram language model (LM), preventing joint optimization of all stages simultaneously. Here, we introduce an end-to-end Brain-to-Text (BIT) framework that translates neural activity into coherent sentences using a single differentiable neural network. Central to our approach is a cross-task, cross-species pretrained neural encoder, whose representations transfer to both attempted and imagined speech. In a cascaded setting with an n-gram LM, the pretrained encoder establishes a new state-of-the-art (SOTA) on the Brain-to-Text '24 and '25 benchmarks. Integrated end-to-end with audio large language models (LLMs) and trained with contrastive learning for cross-modal alignment, BIT reduces the word error rate (WER) of the prior end-to-end method from 24.69% to 10.22%. Notably, we find that small-scale audio LLMs markedly improve end-to-end decoding. Beyond record-setting performance, BIT aligns attempted and imagined speech embeddings to enable cross-task generalization. Altogether, our approach advances the integration of large, diverse neural datasets, paving the way for an end-to-end decoding framework that supports seamless, differentiable optimization.", "AI": {"tldr": "BIT\u6846\u67b6\u901a\u8fc7\u7aef\u5230\u7aef\u795e\u7ecf\u7f51\u7edc\u5c06\u795e\u7ecf\u6d3b\u52a8\u76f4\u63a5\u7ffb\u8bd1\u4e3a\u8fde\u8d2f\u53e5\u5b50\uff0c\u4f7f\u7528\u8de8\u4efb\u52a1\u8de8\u7269\u79cd\u9884\u8bad\u7ec3\u795e\u7ecf\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u8bcd\u9519\u8bef\u7387\uff0c\u5b9e\u73b0\u5c1d\u8bd5\u548c\u60f3\u8c61\u8bed\u97f3\u7684\u8de8\u4efb\u52a1\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u8111\u673a\u63a5\u53e3\u5927\u591a\u91c7\u7528\u7ea7\u8054\u6846\u67b6\uff0c\u5148\u89e3\u7801\u97f3\u7d20\u518d\u7528n-gram\u8bed\u8a00\u6a21\u578b\u7ec4\u53e5\uff0c\u65e0\u6cd5\u540c\u65f6\u4f18\u5316\u6240\u6709\u9636\u6bb5\u3002\u9700\u8981\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u6846\u67b6\u6765\u63d0\u5347\u6027\u80fd\u5e76\u652f\u6301\u8de8\u4efb\u52a1\u6cdb\u5316\u3002", "method": "\u63d0\u51faBrain-to-Text (BIT)\u7aef\u5230\u7aef\u6846\u67b6\uff1a1) \u4f7f\u7528\u8de8\u4efb\u52a1\u8de8\u7269\u79cd\u9884\u8bad\u7ec3\u795e\u7ecf\u7f16\u7801\u5668\uff0c\u53ef\u8fc1\u79fb\u5230\u5c1d\u8bd5\u548c\u60f3\u8c61\u8bed\u97f3\uff1b2) \u4e0e\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u7aef\u5230\u7aef\u8bad\u7ec3\uff1b3) \u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u884c\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "1) \u5728\u7ea7\u8054\u8bbe\u7f6e\u4e0b\uff0c\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u5728Brain-to-Text '24\u548c'25\u57fa\u51c6\u4e0a\u8fbe\u5230\u65b0SOTA\uff1b2) \u7aef\u5230\u7aef\u96c6\u6210\u5c06\u8bcd\u9519\u8bef\u7387\u4ece24.69%\u964d\u81f310.22%\uff1b3) \u53d1\u73b0\u5c0f\u89c4\u6a21\u97f3\u9891LLMs\u663e\u8457\u6539\u5584\u7aef\u5230\u7aef\u89e3\u7801\uff1b4) \u5b9e\u73b0\u5c1d\u8bd5\u548c\u60f3\u8c61\u8bed\u97f3\u5d4c\u5165\u5bf9\u9f50\uff0c\u652f\u6301\u8de8\u4efb\u52a1\u6cdb\u5316\u3002", "conclusion": "BIT\u6846\u67b6\u901a\u8fc7\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u4f18\u5316\uff0c\u6574\u5408\u5927\u89c4\u6a21\u591a\u6837\u5316\u795e\u7ecf\u6570\u636e\u96c6\uff0c\u4e3a\u652f\u6301\u65e0\u7f1d\u4f18\u5316\u7684\u7aef\u5230\u7aef\u89e3\u7801\u6846\u67b6\u94fa\u5e73\u9053\u8def\uff0c\u663e\u8457\u63a8\u8fdb\u4e86\u8bed\u97f3\u8111\u673a\u63a5\u53e3\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.23186", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.23186", "abs": "https://arxiv.org/abs/2511.23186", "authors": ["Runyu Jiao", "Matteo Bortolon", "Francesco Giuliari", "Alice Fasoli", "Sergio Povoli", "Guofeng Mei", "Yiming Wang", "Fabio Poiesi"], "title": "Obstruction reasoning for robotic grasping", "comment": null, "summary": "Successful robotic grasping in cluttered environments not only requires a model to visually ground a target object but also to reason about obstructions that must be cleared beforehand. While current vision-language embodied reasoning models show emergent spatial understanding, they remain limited in terms of obstruction reasoning and accessibility planning. To bridge this gap, we present UNOGrasp, a learning-based vision-language model capable of performing visually-grounded obstruction reasoning to infer the sequence of actions needed to unobstruct the path and grasp the target object. We devise a novel multi-step reasoning process based on obstruction paths originated by the target object. We anchor each reasoning step with obstruction-aware visual cues to incentivize reasoning capability. UNOGrasp combines supervised and reinforcement finetuning through verifiable reasoning rewards. Moreover, we construct UNOBench, a large-scale dataset for both training and benchmarking, based on MetaGraspNetV2, with over 100k obstruction paths annotated by humans with obstruction ratios, contact points, and natural-language instructions. Extensive experiments and real-robot evaluations show that UNOGrasp significantly improves obstruction reasoning and grasp success across both synthetic and real-world environments, outperforming generalist and proprietary alternatives. Project website: https://tev-fbk.github.io/UnoGrasp/.", "AI": {"tldr": "UNOGrasp\u662f\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u673a\u5668\u4eba\u6293\u53d6\u4e2d\u7684\u969c\u788d\u7269\u63a8\u7406\u548c\u53ef\u8bbf\u95ee\u6027\u89c4\u5212\uff0c\u901a\u8fc7\u591a\u6b65\u63a8\u7406\u548c\u89c6\u89c9\u7ebf\u7d22\u589e\u5f3a\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u6293\u53d6\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u5177\u8eab\u63a8\u7406\u6a21\u578b\u5728\u7a7a\u95f4\u7406\u89e3\u65b9\u9762\u6709\u6240\u8fdb\u5c55\uff0c\u4f46\u5728\u969c\u788d\u7269\u63a8\u7406\u548c\u53ef\u8bbf\u95ee\u6027\u89c4\u5212\u65b9\u9762\u4ecd\u6709\u9650\u5236\u3002\u673a\u5668\u4eba\u8981\u5728\u6742\u4e71\u73af\u5883\u4e2d\u6210\u529f\u6293\u53d6\u76ee\u6807\u7269\u4f53\uff0c\u4e0d\u4ec5\u9700\u8981\u89c6\u89c9\u5b9a\u4f4d\u76ee\u6807\uff0c\u8fd8\u9700\u8981\u63a8\u7406\u54ea\u4e9b\u969c\u788d\u7269\u9700\u8981\u5148\u88ab\u6e05\u9664\u3002", "method": "\u63d0\u51fa\u4e86UNOGrasp\u6a21\u578b\uff0c\u91c7\u7528\u57fa\u4e8e\u76ee\u6807\u7269\u4f53\u969c\u788d\u8def\u5f84\u7684\u591a\u6b65\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc7\u969c\u788d\u611f\u77e5\u7684\u89c6\u89c9\u7ebf\u7d22\u951a\u5b9a\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u3002\u7ed3\u5408\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u4f7f\u7528\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u5956\u52b1\u3002\u540c\u65f6\u6784\u5efa\u4e86UNOBench\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u57fa\u4e8eMetaGraspNetV2\uff0c\u5305\u542b\u8d85\u8fc710\u4e07\u6761\u4eba\u5de5\u6807\u6ce8\u7684\u969c\u788d\u8def\u5f84\uff0c\u5305\u542b\u969c\u788d\u6bd4\u4f8b\u3001\u63a5\u89e6\u70b9\u548c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3002", "result": "\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u771f\u5b9e\u673a\u5668\u4eba\u8bc4\u4f30\u8868\u660e\uff0cUNOGrasp\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u663e\u8457\u6539\u5584\u4e86\u969c\u788d\u7269\u63a8\u7406\u548c\u6293\u53d6\u6210\u529f\u7387\uff0c\u8d85\u8d8a\u4e86\u901a\u7528\u6a21\u578b\u548c\u4e13\u6709\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "UNOGrasp\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u969c\u788d\u7269\u63a8\u7406\u80fd\u529b\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6742\u4e71\u73af\u5883\u4e2d\u673a\u5668\u4eba\u6293\u53d6\u7684\u969c\u788d\u7269\u6e05\u9664\u548c\u53ef\u8bbf\u95ee\u6027\u89c4\u5212\u95ee\u9898\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u529b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21741", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2511.21741", "abs": "https://arxiv.org/abs/2511.21741", "authors": ["Conrad D. Hougen", "Karl T. Pazdernik", "Alfred O. Hero"], "title": "A Multiscale Geometric Method for Capturing Relational Topic Alignment", "comment": "5 pages, 3 figures, 2025 IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing", "summary": "Interpretable topic modeling is essential for tracking how research interests evolve within co-author communities. In scientific corpora, where novelty is prized, identifying underrepresented niche topics is particularly important. However, contemporary models built from dense transformer embeddings tend to miss rare topics and therefore also fail to capture smooth temporal alignment. We propose a geometric method that integrates multimodal text and co-author network data, using Hellinger distances and Ward's linkage to construct a hierarchical topic dendrogram. This approach captures both local and global structure, supporting multiscale learning across semantic and temporal dimensions. Our method effectively identifies rare-topic structure and visualizes smooth topic drift over time. Experiments highlight the strength of interpretable bag-of-words models when paired with principled geometric alignment.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u51e0\u4f55\u65b9\u6cd5\u6574\u5408\u591a\u6a21\u6001\u6587\u672c\u548c\u5408\u8457\u7f51\u7edc\u6570\u636e\uff0c\u4f7f\u7528Hellinger\u8ddd\u79bb\u548cWard\u8fde\u63a5\u6784\u5efa\u5c42\u6b21\u4e3b\u9898\u6811\u72b6\u56fe\uff0c\u6709\u6548\u8bc6\u522b\u7f55\u89c1\u4e3b\u9898\u5e76\u53ef\u89c6\u5316\u5e73\u6ed1\u4e3b\u9898\u6f02\u79fb\u3002", "motivation": "\u5728\u79d1\u5b66\u6587\u732e\u4e2d\uff0c\u8ffd\u8e2a\u5408\u8457\u793e\u533a\u7814\u7a76\u5174\u8da3\u7684\u6f14\u53d8\u9700\u8981\u53ef\u89e3\u91ca\u7684\u4e3b\u9898\u5efa\u6a21\u3002\u5f53\u524d\u57fa\u4e8e\u5bc6\u96c6Transformer\u5d4c\u5165\u7684\u6a21\u578b\u5bb9\u6613\u9057\u6f0f\u7f55\u89c1\u4e3b\u9898\uff0c\u5e76\u4e14\u65e0\u6cd5\u6355\u6349\u5e73\u6ed1\u7684\u65f6\u95f4\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u65b9\u6cd5\u6574\u5408\u591a\u6a21\u6001\u6587\u672c\u548c\u5408\u8457\u7f51\u7edc\u6570\u636e\uff0c\u4f7f\u7528Hellinger\u8ddd\u79bb\u548cWard\u8fde\u63a5\u6784\u5efa\u5c42\u6b21\u4e3b\u9898\u6811\u72b6\u56fe\uff0c\u652f\u6301\u8bed\u4e49\u548c\u65f6\u95f4\u7ef4\u5ea6\u7684\u591a\u5c3a\u5ea6\u5b66\u4e60\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u8bc6\u522b\u7f55\u89c1\u4e3b\u9898\u7ed3\u6784\uff0c\u53ef\u89c6\u5316\u5e73\u6ed1\u7684\u4e3b\u9898\u6f02\u79fb\uff0c\u5b9e\u9a8c\u663e\u793a\u53ef\u89e3\u91ca\u7684\u8bcd\u888b\u6a21\u578b\u4e0e\u51e0\u4f55\u5bf9\u9f50\u7ed3\u5408\u7684\u4f18\u52bf\u3002", "conclusion": "\u51e0\u4f55\u65b9\u6cd5\u80fd\u591f\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u7ed3\u6784\uff0c\u652f\u6301\u591a\u5c3a\u5ea6\u5b66\u4e60\uff0c\u5728\u8bc6\u522b\u7f55\u89c1\u4e3b\u9898\u548c\u53ef\u89c6\u5316\u4e3b\u9898\u6f14\u53d8\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8eTransformer\u5d4c\u5165\u7684\u5f53\u4ee3\u6a21\u578b\u3002"}}
{"id": "2511.23193", "categories": ["cs.RO", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.23193", "abs": "https://arxiv.org/abs/2511.23193", "authors": ["Yuchen Shi", "Huaxin Pei", "Yi Zhang", "Danya Yao"], "title": "Fault-Tolerant MARL for CAVs under Observation Perturbations for Highway On-Ramp Merging", "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) holds significant promise for enabling cooperative driving among Connected and Automated Vehicles (CAVs). However, its practical application is hindered by a critical limitation, i.e., insufficient fault tolerance against observational faults. Such faults, which appear as perturbations in the vehicles' perceived data, can substantially compromise the performance of MARL-based driving systems. Addressing this problem presents two primary challenges. One is to generate adversarial perturbations that effectively stress the policy during training, and the other is to equip vehicles with the capability to mitigate the impact of corrupted observations. To overcome the challenges, we propose a fault-tolerant MARL method for cooperative on-ramp vehicles incorporating two key agents. First, an adversarial fault injection agent is co-trained to generate perturbations that actively challenge and harden the vehicle policies. Second, we design a novel fault-tolerant vehicle agent equipped with a self-diagnosis capability, which leverages the inherent spatio-temporal correlations in vehicle state sequences to detect faults and reconstruct credible observations, thereby shielding the policy from misleading inputs. Experiments in a simulated highway merging scenario demonstrate that our method significantly outperforms baseline MARL approaches, achieving near-fault-free levels of safety and efficiency under various observation fault patterns.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u534f\u540c\u9a7e\u9a76\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u6545\u969c\u6ce8\u5165\u548c\u81ea\u8bca\u65ad\u673a\u5236\u589e\u5f3a\u7cfb\u7edf\u5bf9\u89c2\u6d4b\u6545\u969c\u7684\u5bb9\u9519\u80fd\u529b\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u534f\u540c\u9a7e\u9a76\u4e2d\u5e94\u7528\u53d7\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u5bf9\u89c2\u6d4b\u6545\u969c\u7684\u5bb9\u9519\u80fd\u529b\u4e0d\u8db3\u3002\u89c2\u6d4b\u6545\u969c\uff08\u611f\u77e5\u6570\u636e\u6270\u52a8\uff09\u4f1a\u4e25\u91cd\u635f\u5bb3\u57fa\u4e8eMARL\u7684\u9a7e\u9a76\u7cfb\u7edf\u6027\u80fd\uff0c\u9700\u8981\u89e3\u51b3\u5bf9\u6297\u6027\u6270\u52a8\u751f\u6210\u548c\u6545\u969c\u7f13\u89e3\u4e24\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u5305\u542b\u4e24\u4e2a\u5173\u952e\u667a\u80fd\u4f53\u7684\u5bb9\u9519MARL\u65b9\u6cd5\uff1a1\uff09\u5bf9\u6297\u6027\u6545\u969c\u6ce8\u5165\u667a\u80fd\u4f53\uff0c\u534f\u540c\u8bad\u7ec3\u751f\u6210\u6270\u52a8\u4ee5\u6311\u6218\u548c\u5f3a\u5316\u8f66\u8f86\u7b56\u7565\uff1b2\uff09\u5bb9\u9519\u8f66\u8f86\u667a\u80fd\u4f53\uff0c\u5177\u5907\u81ea\u8bca\u65ad\u80fd\u529b\uff0c\u5229\u7528\u8f66\u8f86\u72b6\u6001\u5e8f\u5217\u7684\u65f6\u7a7a\u76f8\u5173\u6027\u68c0\u6d4b\u6545\u969c\u5e76\u91cd\u5efa\u53ef\u4fe1\u89c2\u6d4b\u3002", "result": "\u5728\u6a21\u62df\u9ad8\u901f\u516c\u8def\u6c47\u5165\u573a\u666f\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfMARL\u65b9\u6cd5\uff0c\u5728\u5404\u79cd\u89c2\u6d4b\u6545\u969c\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u65e0\u6545\u969c\u6c34\u5e73\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5bf9\u6297\u6027\u6545\u969c\u6ce8\u5165\u548c\u81ea\u8bca\u65ad\u673a\u5236\uff0c\u63d0\u51fa\u7684\u5bb9\u9519MARL\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u534f\u540c\u9a7e\u9a76\u7cfb\u7edf\u5bf9\u89c2\u6d4b\u6545\u969c\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u4fdd\u969c\u3002"}}
{"id": "2511.21742", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21742", "abs": "https://arxiv.org/abs/2511.21742", "authors": ["Meenakshi Mittal", "Rishi Khare", "Mihran Miroyan", "Chancharik Mitra", "Narges Norouzi"], "title": "EduMod-LLM: A Modular Approach for Designing Flexible and Transparent Educational Assistants", "comment": "Proceedings of the AAAI Conference on Artificial Intelligence", "summary": "With the growing use of Large Language Model (LLM)-based Question-Answering (QA) systems in education, it is critical to evaluate their performance across individual pipeline components. In this work, we introduce {\\model}, a modular function-calling LLM pipeline, and present a comprehensive evaluation along three key axes: function calling strategies, retrieval methods, and generative language models. Our framework enables fine-grained analysis by isolating and assessing each component. We benchmark function-calling performance across LLMs, compare our novel structure-aware retrieval method to vector-based and LLM-scoring baselines, and evaluate various LLMs for response synthesis. This modular approach reveals specific failure modes and performance patterns, supporting the development of interpretable and effective educational QA systems. Our findings demonstrate the value of modular function calling in improving system transparency and pedagogical alignment. Website and Supplementary Material: https://chancharikmitra.github.io/EduMod-LLM-website/", "AI": {"tldr": "EduMod-LLM\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u51fd\u6570\u8c03\u7528LLM\u7ba1\u9053\uff0c\u7528\u4e8e\u6559\u80b2\u95ee\u7b54\u7cfb\u7edf\u8bc4\u4f30\uff0c\u91cd\u70b9\u5173\u6ce8\u51fd\u6570\u8c03\u7528\u7b56\u7565\u3001\u68c0\u7d22\u65b9\u6cd5\u548c\u751f\u6210\u6a21\u578b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\u3002", "motivation": "\u968f\u7740\u57fa\u4e8eLLM\u7684\u6559\u80b2\u95ee\u7b54\u7cfb\u7edf\u65e5\u76ca\u666e\u53ca\uff0c\u9700\u8981\u5bf9\u5176\u5404\u4e2a\u7ba1\u9053\u7ec4\u4ef6\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u4ee5\u4e86\u89e3\u7cfb\u7edf\u6027\u80fd\u3001\u53d1\u73b0\u6545\u969c\u6a21\u5f0f\uff0c\u5e76\u5f00\u53d1\u66f4\u900f\u660e\u3001\u6559\u5b66\u5bf9\u9f50\u7684\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u6a21\u5757\u5316\u7684\u51fd\u6570\u8c03\u7528LLM\u7ba1\u9053EduMod-LLM\uff0c\u901a\u8fc7\u9694\u79bb\u548c\u8bc4\u4f30\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u51fd\u6570\u8c03\u7528\u7b56\u7565\u3001\u68c0\u7d22\u65b9\u6cd5\uff08\u5305\u62ec\u65b0\u9896\u7684\u7ed3\u6784\u611f\u77e5\u68c0\u7d22\uff09\u548c\u751f\u6210\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5206\u6790\u3002", "result": "\u5bf9LLM\u7684\u51fd\u6570\u8c03\u7528\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6bd4\u8f83\u4e86\u7ed3\u6784\u611f\u77e5\u68c0\u7d22\u4e0e\u57fa\u4e8e\u5411\u91cf\u548cLLM\u8bc4\u5206\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc4\u4f30\u4e86\u5404\u79cdLLM\u7684\u54cd\u5e94\u5408\u6210\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5177\u4f53\u7684\u6545\u969c\u6a21\u5f0f\u548c\u6027\u80fd\u6a21\u5f0f\u3002", "conclusion": "\u6a21\u5757\u5316\u7684\u51fd\u6570\u8c03\u7528\u65b9\u6cd5\u80fd\u63d0\u9ad8\u6559\u80b2\u95ee\u7b54\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u6559\u5b66\u5bf9\u9f50\u6027\uff0c\u652f\u6301\u5f00\u53d1\u53ef\u89e3\u91ca\u4e14\u6709\u6548\u7684\u6559\u80b2\u7cfb\u7edf\u3002"}}
{"id": "2511.23215", "categories": ["cs.RO", "cond-mat.other", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2511.23215", "abs": "https://arxiv.org/abs/2511.23215", "authors": ["Eduardo Sergio Oliveros-Mata", "Oleksandr V. Pylypovskyi", "Eleonora Raimondo", "Rico Illing", "Yevhen Zabila", "Lin Guo", "Guannan Mu", "M\u00f3nica Navarro L\u00f3pez", "Xu Wang", "Georgios Tzortzinis", "Angelos Filippatos", "Gilbert Santiago Ca\u00f1\u00f3n Berm\u00fadez", "Francesca Garesc\u00ec", "Giovanni Finocchio", "Denys Makarov"], "title": "Field-programmable dynamics in a soft magnetic actuator enabling true random number generation and reservoir computing", "comment": null, "summary": "Complex and even chaotic dynamics, though prevalent in many natural and engineered systems, has been largely avoided in the design of electromechanical systems due to concerns about wear and controlability. Here, we demonstrate that complex dynamics might be particularly advantageous in soft robotics, offering new functionalities beyond motion not easily achievable with traditional actuation methods. We designed and realized resilient magnetic soft actuators capable of operating in a tunable dynamic regime for tens of thousands cycles without fatigue. We experimentally demonstrated the application of these actuators for true random number generation and stochastic computing. {W}e validate soft robots as physical reservoirs capable of performing Mackey--Glass time series prediction. These findings show that exploring the complex dynamics in soft robotics would extend the application scenarios in soft computing, human-robot interaction and collaborative robots as we demonstrate with biomimetic blinking and randomized voice modulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u590d\u6742\u52a8\u529b\u5b66\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u4e2d\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u8bbe\u8ba1\u53ef\u8c03\u52a8\u6001\u78c1\u81f4\u52a8\u5668\uff0c\u5b9e\u73b0\u4e86\u771f\u968f\u673a\u6570\u751f\u6210\u3001\u968f\u673a\u8ba1\u7b97\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7b49\u65b0\u529f\u80fd\u3002", "motivation": "\u4f20\u7edf\u673a\u7535\u7cfb\u7edf\u901a\u5e38\u907f\u514d\u590d\u6742\u548c\u6df7\u6c8c\u52a8\u529b\u5b66\uff0c\u62c5\u5fc3\u78e8\u635f\u548c\u53ef\u63a7\u6027\u95ee\u9898\u3002\u4f46\u4f5c\u8005\u8ba4\u4e3a\u590d\u6742\u52a8\u529b\u5b66\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u4e2d\u53ef\u80fd\u7279\u522b\u6709\u5229\uff0c\u80fd\u63d0\u4f9b\u4f20\u7edf\u9a71\u52a8\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u7684\u65b0\u529f\u80fd\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u5177\u6709\u53ef\u8c03\u52a8\u6001\u673a\u5236\u7684\u5f39\u6027\u78c1\u81f4\u52a8\u5668\uff0c\u80fd\u591f\u5728\u6570\u4e07\u6b21\u5faa\u73af\u4e2d\u65e0\u75b2\u52b3\u8fd0\u884c\u3002\u5229\u7528\u8fd9\u4e9b\u81f4\u52a8\u5668\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u5c55\u793a\u4e86\u81f4\u52a8\u5668\u5728\u771f\u968f\u673a\u6570\u751f\u6210\u548c\u968f\u673a\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\uff0c\u9a8c\u8bc1\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u4f5c\u4e3a\u7269\u7406\u50a8\u5c42\u6267\u884cMackey-Glass\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u80fd\u529b\u3002\u8fd8\u5c55\u793a\u4e86\u4eff\u751f\u7728\u773c\u548c\u968f\u673a\u5316\u8bed\u97f3\u8c03\u5236\u7b49\u5e94\u7528\u3002", "conclusion": "\u63a2\u7d22\u8f6f\u4f53\u673a\u5668\u4eba\u4e2d\u7684\u590d\u6742\u52a8\u529b\u5b66\u5c06\u6269\u5c55\u5176\u5728\u8f6f\u8ba1\u7b97\u3001\u4eba\u673a\u4ea4\u4e92\u548c\u534f\u4f5c\u673a\u5668\u4eba\u7b49\u9886\u57df\u7684\u5e94\u7528\u573a\u666f\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u5f00\u8f9f\u65b0\u7684\u529f\u80fd\u53ef\u80fd\u6027\u3002"}}
{"id": "2511.21743", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21743", "abs": "https://arxiv.org/abs/2511.21743", "authors": ["Mukul Singh", "Ananya Singha", "Arjun Radhakrishna", "Sumit Gulwani"], "title": "Scaling Competence, Shrinking Reasoning: Cognitive Signatures in Language Model Learning", "comment": null, "summary": "We analyze reasoning in language models during task-specific fine-tuning and draws parallel between reasoning tokens--intermediate steps generated while solving problem and the human working memory. Drawing from cognitive science, we align training dynamics with the Four Stages of Competence: models initially produce incorrect outputs without reasoning, then begin reasoning (but still fail), eventually reason effectively, and finally solve tasks without explicit reasoning. We find that reasoning token length expands as performance improves, peaks at the stage of conscious competence, then declines as the model internalizes the task. Notably, after training, models retain performance even when reasoning is removed--suggesting it scaffolded learning but is no longer needed. This progression offers actionable insights: reasoning token dynamics can serve as a signal for diagnosing training stage, identifying convergence, and guiding early stopping. We propose metrics to track this trajectory and argue that reasoning behavior is valuable for understanding and optimizing reasoning model training.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u52a1\u5fae\u8c03\u4e2d\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c06\u63a8\u7406\u6807\u8bb0\uff08\u89e3\u51b3\u95ee\u9898\u7684\u4e2d\u95f4\u6b65\u9aa4\uff09\u4e0e\u4eba\u7c7b\u5de5\u4f5c\u8bb0\u5fc6\u7c7b\u6bd4\uff0c\u53d1\u73b0\u63a8\u7406\u884c\u4e3a\u9075\u5faa\u4ece\u65e0\u610f\u8bc6\u5230\u6709\u610f\u8bc6\u518d\u5230\u81ea\u52a8\u5316\u7684\u56db\u9636\u6bb5\u53d1\u5c55\u6a21\u5f0f\uff0c\u53ef\u4f5c\u4e3a\u8bad\u7ec3\u8bca\u65ad\u548c\u4f18\u5316\u7684\u4fe1\u53f7\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u5728\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u5c06\u6a21\u578b\u7684\u63a8\u7406\u6807\u8bb0\u4e0e\u4eba\u7c7b\u5de5\u4f5c\u8bb0\u5fc6\u673a\u5236\u8fdb\u884c\u7c7b\u6bd4\uff0c\u63a2\u7d22\u63a8\u7406\u5728\u6a21\u578b\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u53ca\u5176\u52a8\u6001\u53d8\u5316\u89c4\u5f8b\u3002", "method": "\u65b9\u6cd5\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u7684\"\u56db\u9636\u6bb5\u80fd\u529b\u6a21\u578b\"\uff1a\u4ece\u65e0\u610f\u8bc6\u65e0\u80fd\u3001\u6709\u610f\u8bc6\u65e0\u80fd\u3001\u6709\u610f\u8bc6\u6709\u80fd\u5230\u65e0\u610f\u8bc6\u6709\u80fd\u3002\u901a\u8fc7\u5206\u6790\u63a8\u7406\u6807\u8bb0\u7684\u957f\u5ea6\u548c\u6a21\u5f0f\u53d8\u5316\uff0c\u8ffd\u8e2a\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u80fd\u529b\u53d1\u5c55\u9636\u6bb5\uff0c\u5e76\u63d0\u51fa\u76f8\u5e94\u7684\u5ea6\u91cf\u6307\u6807\u6765\u8ddf\u8e2a\u8fd9\u4e00\u8f68\u8ff9\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u6807\u8bb0\u957f\u5ea6\u968f\u6027\u80fd\u63d0\u5347\u800c\u589e\u52a0\uff0c\u5728\"\u6709\u610f\u8bc6\u6709\u80fd\"\u9636\u6bb5\u8fbe\u5230\u5cf0\u503c\uff0c\u7136\u540e\u968f\u7740\u4efb\u52a1\u5185\u5316\u800c\u51cf\u5c11\u3002\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u5373\u4f7f\u79fb\u9664\u63a8\u7406\u6807\u8bb0\uff0c\u6a21\u578b\u4ecd\u80fd\u4fdd\u6301\u6027\u80fd\uff0c\u8868\u660e\u63a8\u7406\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u8d77\u5230\u811a\u624b\u67b6\u4f5c\u7528\u4f46\u975e\u6700\u7ec8\u5fc5\u9700\u3002", "conclusion": "\u7ed3\u8bba\u662f\u63a8\u7406\u884c\u4e3a\u52a8\u6001\u53ef\u4f5c\u4e3a\u8bca\u65ad\u8bad\u7ec3\u9636\u6bb5\u3001\u8bc6\u522b\u6536\u655b\u548c\u6307\u5bfc\u65e9\u505c\u7684\u4fe1\u53f7\u3002\u63a8\u7406\u884c\u4e3a\u5bf9\u4e8e\u7406\u89e3\u548c\u4f18\u5316\u63a8\u7406\u6a21\u578b\u8bad\u7ec3\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u63d0\u51fa\u7684\u5ea6\u91cf\u6307\u6807\u80fd\u6709\u6548\u8ddf\u8e2a\u6a21\u578b\u80fd\u529b\u53d1\u5c55\u8f68\u8ff9\u3002"}}
{"id": "2511.23236", "categories": ["cs.RO", "cs.ET"], "pdf": "https://arxiv.org/pdf/2511.23236", "abs": "https://arxiv.org/abs/2511.23236", "authors": ["Alex Richardson", "Azhar Hasan", "Gabor Karsai", "Jonathan Sprinkle"], "title": "Incorporating Ephemeral Traffic Waves in A Data-Driven Framework for Microsimulation in CARLA", "comment": "Submitted to IEEE IV 2026", "summary": "This paper introduces a data-driven traffic microsimulation framework in CARLA that reconstructs real-world wave dynamics using high-fidelity time-space data from the I-24 MOTION testbed. Calibration of road networks in microsimulators to reproduce ephemeral phenomena such as traffic waves for large-scale simulation is a process that is fraught with challenges. This work reconsiders the existence of the traffic state data as boundary conditions on an ego vehicle moving through previously recorded traffic data, rather than reproducing those traffic phenomena in a calibrated microsim. Our approach is to autogenerate a 1 mile highway segment corresponding to I-24, and use the I-24 data to power a cosimulation module that injects traffic information into the simulation. The CARLA and cosimulation simulations are centered around an ego vehicle sampled from the empirical data, with autogeneration of \"visible\" traffic within the longitudinal range of the ego vehicle. Boundary control beyond these visible ranges is achieved using ghost cells behind (upstream) and ahead (downstream) of the ego vehicle. Unlike prior simulation work that focuses on local car-following behavior or abstract geometries, our framework targets full time-space diagram fidelity as the validation objective. Leveraging CARLA's rich sensor suite and configurable vehicle dynamics, we simulate wave formation and dissipation in both low-congestion and high-congestion scenarios for qualitative analysis. The resulting emergent behavior closely mirrors that of real traffic, providing a novel cosimulation framework for evaluating traffic control strategies, perception-driven autonomy, and future deployment of wave mitigation solutions. Our work bridges microscopic modeling with physical experimental data, enabling the first perceptually realistic, boundary-driven simulation of empirical traffic wave phenomena in CARLA.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCARLA\u7684\u6570\u636e\u9a71\u52a8\u4ea4\u901a\u5fae\u89c2\u4eff\u771f\u6846\u67b6\uff0c\u5229\u7528I-24 MOTION\u771f\u5b9e\u65f6\u7a7a\u6570\u636e\u91cd\u5efa\u4ea4\u901a\u6ce2\u52a8\u6001\uff0c\u901a\u8fc7\u5171\u4eff\u771f\u6a21\u5757\u6ce8\u5165\u771f\u5b9e\u4ea4\u901a\u4fe1\u606f\uff0c\u5b9e\u73b0\u611f\u77e5\u771f\u5b9e\u7684\u8fb9\u754c\u9a71\u52a8\u4eff\u771f\u3002", "motivation": "\u4f20\u7edf\u5fae\u89c2\u4eff\u771f\u5668\u6821\u51c6\u96be\u4ee5\u91cd\u73b0\u5927\u89c4\u6a21\u4ea4\u901a\u6ce2\u7b49\u77ed\u6682\u73b0\u8c61\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u5229\u7528\u9ad8\u4fdd\u771f\u771f\u5b9e\u4e16\u754c\u6570\u636e\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u4ea4\u901a\u52a8\u6001\u6a21\u62df\u3002", "method": "1) \u5728CARLA\u4e2d\u81ea\u52a8\u751f\u62101\u82f1\u91cc\u9ad8\u901f\u516c\u8def\u6bb5\u5bf9\u5e94I-24\uff1b2) \u4f7f\u7528I-24\u6570\u636e\u9a71\u52a8\u5171\u4eff\u771f\u6a21\u5757\u5411\u4eff\u771f\u6ce8\u5165\u4ea4\u901a\u4fe1\u606f\uff1b3) \u4ee5\u7ecf\u9a8c\u6570\u636e\u4e2d\u7684\u81ea\u8f66\u4e3a\u4e2d\u5fc3\uff0c\u5728\u7eb5\u5411\u8303\u56f4\u5185\u81ea\u52a8\u751f\u6210\"\u53ef\u89c1\"\u4ea4\u901a\uff1b4) \u8d85\u51fa\u8303\u56f4\u4f7f\u7528\u4e0a\u6e38\u548c\u4e0b\u6e38\u7684\u5e7d\u7075\u5355\u5143\u8fdb\u884c\u8fb9\u754c\u63a7\u5236\u3002", "result": "\u4eff\u771f\u4ea7\u751f\u7684\u6d8c\u73b0\u884c\u4e3a\u4e0e\u771f\u5b9e\u4ea4\u901a\u9ad8\u5ea6\u4e00\u81f4\uff0c\u80fd\u591f\u5728\u4f4e\u62e5\u5835\u548c\u9ad8\u62e5\u5835\u573a\u666f\u4e2d\u6a21\u62df\u6ce2\u7684\u5f62\u6210\u548c\u6d88\u6563\uff0c\u4e3a\u4ea4\u901a\u63a7\u5236\u7b56\u7565\u3001\u611f\u77e5\u9a71\u52a8\u81ea\u4e3b\u6027\u548c\u6ce2\u7f13\u89e3\u89e3\u51b3\u65b9\u6848\u8bc4\u4f30\u63d0\u4f9b\u65b0\u6846\u67b6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c06\u5fae\u89c2\u5efa\u6a21\u4e0e\u7269\u7406\u5b9e\u9a8c\u6570\u636e\u76f8\u7ed3\u5408\uff0c\u9996\u6b21\u5728CARLA\u4e2d\u5b9e\u73b0\u4e86\u611f\u77e5\u771f\u5b9e\u3001\u8fb9\u754c\u9a71\u52a8\u7684\u7ecf\u9a8c\u4ea4\u901a\u6ce2\u73b0\u8c61\u4eff\u771f\uff0c\u4e3a\u4ea4\u901a\u7814\u7a76\u63d0\u4f9b\u4e86\u521b\u65b0\u7684\u5171\u4eff\u771f\u6846\u67b6\u3002"}}
{"id": "2511.21744", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21744", "abs": "https://arxiv.org/abs/2511.21744", "authors": ["Sergey K. Aityan", "William Claster", "Karthik Sai Emani", "Sohni Rais", "Thy Tran"], "title": "A Lightweight Approach to Detection of AI-Generated Texts Using Stylometric Features", "comment": "19 pages, 6 figures, 3 tables", "summary": "A growing number of AI-generated texts raise serious concerns. Most existing approaches to AI-generated text detection rely on fine-tuning large transformer models or building ensembles, which are computationally expensive and often provide limited generalization across domains. Existing lightweight alternatives achieved significantly lower accuracy on large datasets. We introduce NEULIF, a lightweight approach that achieves best performance in the lightweight detector class, that does not require extensive computational power and provides high detection accuracy. In our approach, a text is first decomposed into stylometric and readability features which are then used for classification by a compact Convolutional Neural Network (CNN) or Random Forest (RF). Evaluated and tested on the Kaggle AI vs. Human corpus, our models achieve 97% accuracy (~ 0.95 F1) for CNN and 95% accuracy (~ 0.94 F1) for the Random Forest, demonstrating high precision and recall, with ROC-AUC scores of 99.5% and 95%, respectively. The CNN (~ 25 MB) and Random Forest (~ 10.6 MB) models are orders of magnitude smaller than transformer-based ensembles and can be run efficiently on standard CPU devices, without sacrificing accuracy.This study also highlights the potential of such models for broader applications across languages, domains, and streaming contexts, showing that simplicity, when guided by structural insights, can rival complexity in AI-generated content detection.", "AI": {"tldr": "NEULIF\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u98ce\u683c\u8ba1\u91cf\u548c\u53ef\u8bfb\u6027\u7279\u5f81\u7ed3\u5408CNN\u6216\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\uff0c\u5728Kaggle\u6570\u636e\u96c6\u4e0a\u8fbe\u523097%\u51c6\u786e\u7387\uff0c\u6a21\u578b\u5927\u5c0f\u4ec525MB\u548c10.6MB\uff0c\u8fdc\u5c0f\u4e8e\u57fa\u4e8etransformer\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u6709AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5fae\u8c03\u5927\u578btransformer\u6a21\u578b\u6216\u6784\u5efa\u96c6\u6210\u6a21\u578b\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u5728\u5927\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u663e\u8457\u8f83\u4f4e\u3002", "method": "\u9996\u5148\u5c06\u6587\u672c\u5206\u89e3\u4e3a\u98ce\u683c\u8ba1\u91cf\u548c\u53ef\u8bfb\u6027\u7279\u5f81\uff0c\u7136\u540e\u4f7f\u7528\u7d27\u51d1\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc(CNN)\u6216\u968f\u673a\u68ee\u6797(RF)\u8fdb\u884c\u5206\u7c7b\u3002\u6a21\u578b\u8bbe\u8ba1\u8f7b\u91cf\uff0c\u53ef\u5728\u6807\u51c6CPU\u8bbe\u5907\u4e0a\u9ad8\u6548\u8fd0\u884c\u3002", "result": "\u5728Kaggle AI vs. Human\u8bed\u6599\u5e93\u4e0a\uff0cCNN\u6a21\u578b\u8fbe\u523097%\u51c6\u786e\u7387(~0.95 F1)\uff0c\u968f\u673a\u68ee\u6797\u8fbe\u523095%\u51c6\u786e\u7387(~0.94 F1)\u3002CNN\u6a21\u578b\u7ea625MB\uff0c\u968f\u673a\u68ee\u6797\u7ea610.6MB\uff0c\u6bd4\u57fa\u4e8etransformer\u7684\u96c6\u6210\u6a21\u578b\u5c0f\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "NEULIF\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u5728AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u5728\u7ed3\u6784\u6d1e\u5bdf\u6307\u5bfc\u4e0b\uff0c\u7b80\u5355\u6027\u53ef\u4ee5\u4e0e\u590d\u6742\u6027\u76f8\u5ab2\u7f8e\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u8de8\u8bed\u8a00\u3001\u9886\u57df\u548c\u6d41\u5f0f\u4e0a\u4e0b\u6587\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.23300", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.23300", "abs": "https://arxiv.org/abs/2511.23300", "authors": ["Yara Mahmoud", "Jeffrin Sam", "Nguyen Khang", "Marcelino Fernando", "Issatay Tokmurziyev", "Miguel Altamirano Cabrera", "Muhammad Haris Khan", "Artem Lykov", "Dzmitry Tsetserukou"], "title": "SafeHumanoid: VLM-RAG-driven Control of Upper Body Impedance for Humanoid Robot", "comment": null, "summary": "Safe and trustworthy Human Robot Interaction (HRI) requires robots not only to complete tasks but also to regulate impedance and speed according to scene context and human proximity. We present SafeHumanoid, an egocentric vision pipeline that links Vision Language Models (VLMs) with Retrieval-Augmented Generation (RAG) to schedule impedance and velocity parameters for a humanoid robot. Egocentric frames are processed by a structured VLM prompt, embedded and matched against a curated database of validated scenarios, and mapped to joint-level impedance commands via inverse kinematics. We evaluate the system on tabletop manipulation tasks with and without human presence, including wiping, object handovers, and liquid pouring. The results show that the pipeline adapts stiffness, damping, and speed profiles in a context-aware manner, maintaining task success while improving safety. Although current inference latency (up to 1.4 s) limits responsiveness in highly dynamic settings, SafeHumanoid demonstrates that semantic grounding of impedance control is a viable path toward safer, standard-compliant humanoid collaboration.", "AI": {"tldr": "SafeHumanoid\uff1a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u4eff\u4eba\u673a\u5668\u4eba\u963b\u6297\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u7406\u89e3\u5b9e\u73b0\u573a\u666f\u611f\u77e5\u7684\u5b89\u5168\u4eba\u673a\u4ea4\u4e92", "motivation": "\u5b89\u5168\u53ef\u4fe1\u7684\u4eba\u673a\u4ea4\u4e92\u9700\u8981\u673a\u5668\u4eba\u4e0d\u4ec5\u80fd\u5b8c\u6210\u4efb\u52a1\uff0c\u8fd8\u80fd\u6839\u636e\u573a\u666f\u4e0a\u4e0b\u6587\u548c\u4eba\u7c7b\u63a5\u8fd1\u7a0b\u5ea6\u8c03\u8282\u963b\u6297\u548c\u901f\u5ea6\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u96be\u4ee5\u5b9e\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5b89\u5168\u63a7\u5236\u3002", "method": "\u63d0\u51faSafeHumanoid\u6846\u67b6\uff1a1) \u4f7f\u7528\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u56fe\u50cf\u8f93\u5165\uff1b2) \u901a\u8fc7\u7ed3\u6784\u5316VLM\u63d0\u793a\u5904\u7406\u56fe\u50cf\uff1b3) \u5d4c\u5165\u5e76\u5339\u914d\u9a8c\u8bc1\u573a\u666f\u6570\u636e\u5e93\uff1b4) \u901a\u8fc7\u9006\u8fd0\u52a8\u5b66\u6620\u5c04\u5230\u5173\u8282\u7ea7\u963b\u6297\u547d\u4ee4\uff1b5) \u7ed3\u5408RAG\u6280\u672f\u8c03\u5ea6\u963b\u6297\u548c\u901f\u5ea6\u53c2\u6570\u3002", "result": "\u5728\u684c\u9762\u64cd\u4f5c\u4efb\u52a1\uff08\u64e6\u62ed\u3001\u7269\u4f53\u4ea4\u63a5\u3001\u6db2\u4f53\u503e\u5012\uff09\u4e2d\uff0c\u7cfb\u7edf\u80fd\u6839\u636e\u6709\u65e0\u4eba\u7c7b\u5728\u573a\u81ea\u9002\u5e94\u8c03\u6574\u521a\u5ea6\u3001\u963b\u5c3c\u548c\u901f\u5ea6\u914d\u7f6e\uff0c\u4fdd\u6301\u4efb\u52a1\u6210\u529f\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u5b89\u5168\u6027\u3002\u5f53\u524d\u63a8\u7406\u5ef6\u8fdf\u7ea61.4\u79d2\uff0c\u9650\u5236\u4e86\u9ad8\u52a8\u6001\u73af\u5883\u7684\u54cd\u5e94\u6027\u3002", "conclusion": "\u8bed\u4e49\u63a5\u5730\u7684\u963b\u6297\u63a7\u5236\u662f\u5b9e\u73b0\u66f4\u5b89\u5168\u3001\u7b26\u5408\u6807\u51c6\u7684\u4eff\u4eba\u673a\u5668\u4eba\u534f\u4f5c\u7684\u53ef\u884c\u8def\u5f84\u3002\u867d\u7136\u5f53\u524d\u5ef6\u8fdf\u9650\u5236\u4e86\u9ad8\u52a8\u6001\u573a\u666f\u7684\u54cd\u5e94\uff0c\u4f46SafeHumanoid\u5c55\u793a\u4e86\u8bed\u4e49\u7406\u89e3\u5728\u5b89\u5168\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.21746", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21746", "abs": "https://arxiv.org/abs/2511.21746", "authors": ["Mingyu Jeon", "Hyobin Kim"], "title": "DELTA: Language Diffusion-based EEG-to-Text Architecture", "comment": null, "summary": "Electroencephalogram (EEG)-to-text remains challenging due to high-dimensional noise, subject variability, and error accumulation in autoregressive decoding. We introduce DELTA, which pairs a Residual Vector Quantization (RVQ) EEG tokenizer with a masked language diffusion model (LLaDA). RVQ discretizes continuous EEG into multi-layer tokens to reduce noise and individual differences, while LLaDA reconstructs sentences via non-sequential denoising. On ZuCo, DELTA improves semantic alignment by up to 5.37 points over autoregressive baselines, achieving BLEU-1 21.9 and ROUGE-1 F 17.2 under word-level conditions. These results enable reliable text generation from small EEG-text datasets and point toward scalable multimodal EEG-language models.", "AI": {"tldr": "DELTA\uff1a\u4f7f\u7528\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\uff08RVQ\uff09EEG\u5206\u8bcd\u5668\u548c\u63a9\u7801\u8bed\u8a00\u6269\u6563\u6a21\u578b\uff08LLaDA\uff09\uff0c\u901a\u8fc7\u975e\u987a\u5e8f\u53bb\u566a\u4eceEEG\u4fe1\u53f7\u751f\u6210\u6587\u672c\uff0c\u663e\u8457\u63d0\u5347\u8bed\u4e49\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "EEG\u5230\u6587\u672c\u8f6c\u6362\u9762\u4e34\u9ad8\u7ef4\u566a\u58f0\u3001\u53d7\u8bd5\u8005\u53d8\u5f02\u6027\u4ee5\u53ca\u81ea\u56de\u5f52\u89e3\u7801\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u7b49\u6311\u6218\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u751f\u6210\u65b9\u6cd5\u3002", "method": "1. \u4f7f\u7528\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\uff08RVQ\uff09\u5c06\u8fde\u7eedEEG\u4fe1\u53f7\u79bb\u6563\u5316\u4e3a\u591a\u5c42token\u4ee5\u51cf\u5c11\u566a\u58f0\u548c\u4e2a\u4f53\u5dee\u5f02\uff1b2. \u91c7\u7528\u63a9\u7801\u8bed\u8a00\u6269\u6563\u6a21\u578b\uff08LLaDA\uff09\u901a\u8fc7\u975e\u987a\u5e8f\u53bb\u566a\u91cd\u5efa\u53e5\u5b50\u3002", "result": "\u5728ZuCo\u6570\u636e\u96c6\u4e0a\uff0cDELTA\u6bd4\u81ea\u56de\u5f52\u57fa\u7ebf\u63d0\u5347\u8bed\u4e49\u5bf9\u9f50\u8fbe5.37\u5206\uff0c\u5728\u8bcd\u7ea7\u6761\u4ef6\u4e0b\u8fbe\u5230BLEU-1 21.9\u548cROUGE-1 F 17.2\uff0c\u80fd\u591f\u4ece\u5c0f\u89c4\u6a21EEG-\u6587\u672c\u6570\u636e\u96c6\u4e2d\u53ef\u9760\u751f\u6210\u6587\u672c\u3002", "conclusion": "DELTA\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4eceEEG\u4fe1\u53f7\u5230\u6587\u672c\u7684\u53ef\u9760\u751f\u6210\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001EEG-\u8bed\u8a00\u6a21\u578b\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2511.23372", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.23372", "abs": "https://arxiv.org/abs/2511.23372", "authors": ["Kanhaiya Lal Chaurasiya", "Ruchira Kumar Pradhan", "Yashaswi Sinha", "Shivam Gupta", "Ujjain Kumar Bidila", "Digambar Killedar", "Kapil Das Sahu", "Bishakh Bhattacharya"], "title": "Design, modelling and experimental validation of bipenniform shape memory alloy-based linear actuator integrable with hydraulic stroke amplification mechanism", "comment": null, "summary": "The increasing industrial demand for alternative actuators over conventional electromagnetism-based systems having limited efficiency, bulky size, complex design due to in-built gear-train mechanisms, and high production and amortization costs necessitates the innovation in new actuator development. Integrating bio-inspired design principles into linear actuators could bring forth the next generation of adaptive and energy efficient smart material-based actuation systems. The present study amalgamates the advantages of bipenniform architecture, which generates high force in the given physiological region and a high power-to-weight ratio of shape memory alloy (SMA), into a novel bio-inspired SMA-based linear actuator. A mathematical model of a multi-layered bipenniform configuration-based SMA actuator was developed and validated experimentally. The current research also caters to the incorporation of failure mitigation strategies using design failure mode and effects analysis along with the experimental assessment of the performance of the developed actuator. The system has been benchmarked against an industry-developed stepper motor-driven actuator. It has shown promising results generating an actuation force of 257 N with 15 V input voltage, meeting the acceptable range for actuation operation. It further exhibits about 67% reduction in the weight of the drive mechanism, with 80% lesser component, 32% cost reduction, and 19% energy savings and similar envelope dimensions for assembly compatibility with dampers and louvers for easy onsite deployment. The study introduces SMA coil-based actuator as an advanced design that can be deployed for high force-high stroke applications. The bio-inspired SMA-based linear actuator has applications ranging from building automation controls to lightweight actuation systems for space robotics and medical prosthesis.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f62\u72b6\u8bb0\u5fc6\u5408\u91d1\uff08SMA\uff09\u548c\u53cc\u7fbd\u72b6\u7ed3\u6784\u751f\u7269\u542f\u53d1\u7684\u7ebf\u6027\u81f4\u52a8\u5668\uff0c\u76f8\u6bd4\u4f20\u7edf\u6b65\u8fdb\u7535\u673a\u81f4\u52a8\u5668\uff0c\u91cd\u91cf\u51cf\u5c1167%\uff0c\u90e8\u4ef6\u51cf\u5c1180%\uff0c\u6210\u672c\u964d\u4f4e32%\uff0c\u8282\u80fd19%\uff0c\u4ea7\u751f257N\u81f4\u52a8\u529b\u3002", "motivation": "\u5de5\u4e1a\u5bf9\u66ff\u4ee3\u4f20\u7edf\u7535\u78c1\u81f4\u52a8\u5668\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f20\u7edf\u7cfb\u7edf\u5b58\u5728\u6548\u7387\u6709\u9650\u3001\u4f53\u79ef\u5e9e\u5927\u3001\u8bbe\u8ba1\u590d\u6742\uff08\u5185\u7f6e\u9f7f\u8f6e\u673a\u6784\uff09\u3001\u751f\u4ea7\u548c\u644a\u9500\u6210\u672c\u9ad8\u7b49\u95ee\u9898\u3002\u5c06\u751f\u7269\u542f\u53d1\u8bbe\u8ba1\u539f\u5219\u878d\u5165\u7ebf\u6027\u81f4\u52a8\u5668\u53ef\u63a8\u52a8\u4e0b\u4e00\u4ee3\u81ea\u9002\u5e94\u3001\u9ad8\u80fd\u6548\u7684\u667a\u80fd\u6750\u6599\u81f4\u52a8\u7cfb\u7edf\u53d1\u5c55\u3002", "method": "\u7ed3\u5408\u53cc\u7fbd\u72b6\u7ed3\u6784\uff08\u5728\u7ed9\u5b9a\u751f\u7406\u533a\u57df\u4ea7\u751f\u9ad8\u529b\uff09\u548c\u5f62\u72b6\u8bb0\u5fc6\u5408\u91d1\uff08\u9ad8\u529f\u7387\u91cd\u91cf\u6bd4\uff09\u7684\u4f18\u52bf\uff0c\u5f00\u53d1\u4e86\u591a\u5c42\u53cc\u7fbd\u72b6\u914d\u7f6e\u7684SMA\u81f4\u52a8\u5668\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u3002\u91c7\u7528\u8bbe\u8ba1\u5931\u6548\u6a21\u5f0f\u4e0e\u5f71\u54cd\u5206\u6790\u8fdb\u884c\u5931\u6548\u7f13\u89e3\u7b56\u7565\uff0c\u5e76\u5b9e\u9a8c\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u4e0e\u5de5\u4e1a\u5f00\u53d1\u7684\u6b65\u8fdb\u7535\u673a\u81f4\u52a8\u5668\u76f8\u6bd4\uff0c\u8be5\u81f4\u52a8\u5668\u572815V\u8f93\u5165\u7535\u538b\u4e0b\u4ea7\u751f257N\u81f4\u52a8\u529b\uff0c\u6ee1\u8db3\u64cd\u4f5c\u8981\u6c42\u3002\u91cd\u91cf\u51cf\u5c1167%\uff0c\u90e8\u4ef6\u51cf\u5c1180%\uff0c\u6210\u672c\u964d\u4f4e32%\uff0c\u8282\u80fd19%\uff0c\u4e14\u5916\u5f62\u5c3a\u5bf8\u76f8\u4f3c\uff0c\u4fbf\u4e8e\u4e0e\u963b\u5c3c\u5668\u548c\u767e\u53f6\u7a97\u7ec4\u88c5\u90e8\u7f72\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSMA\u7ebf\u5708\u7684\u5148\u8fdb\u81f4\u52a8\u5668\u8bbe\u8ba1\uff0c\u9002\u7528\u4e8e\u9ad8\u529b-\u9ad8\u884c\u7a0b\u5e94\u7528\u3002\u8fd9\u79cd\u751f\u7269\u542f\u53d1\u7684SMA\u7ebf\u6027\u81f4\u52a8\u5668\u53ef\u5e94\u7528\u4e8e\u5efa\u7b51\u81ea\u52a8\u5316\u63a7\u5236\u3001\u7a7a\u95f4\u673a\u5668\u4eba\u8f7b\u91cf\u5316\u81f4\u52a8\u7cfb\u7edf\u548c\u533b\u7597\u5047\u4f53\u7b49\u9886\u57df\u3002"}}
{"id": "2511.21748", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21748", "abs": "https://arxiv.org/abs/2511.21748", "authors": ["Aman Kumar", "Ekant Muljibhai Amin", "Xian Yeow Lee", "Lasitha Vidyaratne", "Ahmed K. Farahat", "Dipanjan D. Ghosh", "Yuta Koreeda", "Chetan Gupta"], "title": "Building Domain-Specific Small Language Models via Guided Data Generation", "comment": "Accepted at Thirty-Eighth Annual Conference on Innovative Applications of Artificial Intelligence (IAAI-26)", "summary": "Large Language Models (LLMs) have shown remarkable success in supporting a wide range of knowledge-intensive tasks. In specialized domains, there is growing interest in leveraging LLMs to assist subject matter experts with domain-specific challenges. However, deploying LLMs as SaaS solutions raises data privacy concerns, while many open-source models demand significant computational resources for effective domain adaptation and deployment. A promising alternative is to develop smaller, domain-specialized LLMs, though this approach is often constrained by the lack of high-quality domain-specific training data. In this work, we address these limitations by presenting a cost-efficient and scalable training pipeline that combines guided synthetic data generation from a small seed corpus with bottom-up domain data curation. Our pipeline integrates Domain-Adaptive Pretraining (DAPT), Domain-specific Supervised Fine-tuning (DSFT), and Direct Preference Optimization (DPO) to train effective small-scale models for specialized use cases. We demonstrate this approach through DiagnosticSLM, a 3B-parameter domain-specific model tailored for fault diagnosis, root cause analysis, and repair recommendation in industrial settings. To evaluate model performance, we introduce four domain-specific benchmarks: multiple-choice questions (DiagnosticMCQ), question answering (DiagnosticQA), sentence completion (DiagnosticComp), and summarization (DiagnosticSum). DiagnosticSLM achieves up to 25% accuracy improvement over open-source models of comparable or larger size (2B-9B) on the MCQ task, while also outperforming or matching them in other tasks, demonstrating effective domain-specific reasoning and generalization capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6210\u672c\u6548\u76ca\u9ad8\u3001\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u6d41\u7a0b\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u9886\u57df\u6570\u636e\u6574\u7406\uff0c\u7ed3\u5408DAPT\u3001DSFT\u548cDPO\u8bad\u7ec3\u5c0f\u578b\u9886\u57df\u4e13\u7528LLM\uff0c\u5e76\u5728\u5de5\u4e1a\u6545\u969c\u8bca\u65ad\u9886\u57df\u9a8c\u8bc1\u6548\u679c\u3002", "motivation": "\u5728\u4e13\u4e1a\u9886\u57df\u4e2d\u90e8\u7f72LLM\u9762\u4e34\u6570\u636e\u9690\u79c1\u3001\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u5927\u3001\u9ad8\u8d28\u91cf\u9886\u57df\u6570\u636e\u7f3a\u4e4f\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u5c0f\u578b\u3001\u9886\u57df\u4e13\u7528\u7684LLM\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u6210\u672c\u6548\u76ca\u9ad8\u3001\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u6d41\u7a0b\uff1a1) \u4ece\u5c0f\u89c4\u6a21\u79cd\u5b50\u8bed\u6599\u5e93\u8fdb\u884c\u5f15\u5bfc\u5f0f\u5408\u6210\u6570\u636e\u751f\u6210\uff1b2) \u81ea\u4e0b\u800c\u4e0a\u7684\u9886\u57df\u6570\u636e\u6574\u7406\uff1b3) \u7ed3\u5408\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3(DAPT)\u3001\u9886\u57df\u7279\u5b9a\u76d1\u7763\u5fae\u8c03(DSFT)\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\u3002", "result": "\u5f00\u53d1\u4e86DiagnosticSLM\uff083B\u53c2\u6570\uff09\uff0c\u5728\u5de5\u4e1a\u6545\u969c\u8bca\u65ad\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff1a\u5728\u591a\u9879\u9009\u62e9\u9898\u4efb\u52a1\u4e0a\u6bd4\u5f00\u6e90\u6a21\u578b\uff082B-9B\uff09\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe25%\uff0c\u5728\u5176\u4ed6\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u76f8\u5f53\u6216\u66f4\u597d\u3002", "conclusion": "\u8be5\u8bad\u7ec3\u6d41\u7a0b\u80fd\u591f\u6709\u6548\u5f00\u53d1\u5c0f\u578b\u9886\u57df\u4e13\u7528LLM\uff0c\u89e3\u51b3\u6570\u636e\u9690\u79c1\u3001\u8ba1\u7b97\u8d44\u6e90\u548c\u9886\u57df\u6570\u636e\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u5728\u4e13\u4e1a\u9886\u57df\u5e94\u7528\u4e2d\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.23407", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.23407", "abs": "https://arxiv.org/abs/2511.23407", "authors": ["Jan Baumg\u00e4rtner", "Malte Hansjosten", "David Hald", "Adrian Hauptmannl", "Alexander Puchta", "J\u00fcrgen Fleischer"], "title": "From CAD to POMDP: Probabilistic Planning for Robotic Disassembly of End-of-Life Products", "comment": null, "summary": "To support the circular economy, robotic systems must not only assemble new products but also disassemble end-of-life (EOL) ones for reuse, recycling, or safe disposal. Existing approaches to disassembly sequence planning often assume deterministic and fully observable product models, yet real EOL products frequently deviate from their initial designs due to wear, corrosion, or undocumented repairs. We argue that disassembly should therefore be formulated as a Partially Observable Markov Decision Process (POMDP), which naturally captures uncertainty about the product's internal state. We present a mathematical formulation of disassembly as a POMDP, in which hidden variables represent uncertain structural or physical properties. Building on this formulation, we propose a task and motion planning framework that automatically derives specific POMDP models from CAD data, robot capabilities, and inspection results. To obtain tractable policies, we approximate this formulation with a reinforcement-learning approach that operates on stochastic action outcomes informed by inspection priors, while a Bayesian filter continuously maintains beliefs over latent EOL conditions during execution. Using three products on two robotic systems, we demonstrate that this probabilistic planning framework outperforms deterministic baselines in terms of average disassembly time and variance, generalizes across different robot setups, and successfully adapts to deviations from the CAD model, such as missing or stuck parts.", "AI": {"tldr": "\u63d0\u51fa\u5c06\u673a\u5668\u4eba\u62c6\u89e3\u89c4\u5212\u5efa\u6a21\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(POMDP)\uff0c\u4ee5\u5904\u7406\u5e9f\u65e7\u4ea7\u54c1\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u81ea\u9002\u5e94\u62c6\u89e3", "motivation": "\u652f\u6301\u5faa\u73af\u7ecf\u6d4e\u9700\u8981\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u62c6\u89e3\u5e9f\u65e7\u4ea7\u54c1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u4ea7\u54c1\u6a21\u578b\u786e\u5b9a\u4e14\u5b8c\u5168\u53ef\u89c2\u6d4b\uff0c\u800c\u5b9e\u9645\u5e9f\u65e7\u4ea7\u54c1\u5e38\u56e0\u78e8\u635f\u3001\u8150\u8680\u6216\u672a\u8bb0\u5f55\u7684\u7ef4\u4fee\u800c\u504f\u79bb\u539f\u59cb\u8bbe\u8ba1\uff0c\u5b58\u5728\u4e0d\u786e\u5b9a\u6027", "method": "\u5c06\u62c6\u89e3\u89c4\u5212\u5efa\u6a21\u4e3aPOMDP\uff0c\u9690\u85cf\u53d8\u91cf\u8868\u793a\u4e0d\u786e\u5b9a\u7684\u7ed3\u6784\u6216\u7269\u7406\u5c5e\u6027\uff1b\u63d0\u51fa\u4efb\u52a1\u4e0e\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u4eceCAD\u6570\u636e\u3001\u673a\u5668\u4eba\u80fd\u529b\u548c\u68c0\u6d4b\u7ed3\u679c\u81ea\u52a8\u751f\u6210POMDP\u6a21\u578b\uff1b\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8fd1\u4f3c\u6c42\u89e3\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u6ee4\u6ce2\u5668\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u6301\u7eed\u66f4\u65b0\u5bf9\u6f5c\u5728\u5e9f\u65e7\u72b6\u6001\u7684\u4fe1\u5ff5", "result": "\u5728\u4e24\u79cd\u673a\u5668\u4eba\u7cfb\u7edf\u4e0a\u6d4b\u8bd5\u4e09\u79cd\u4ea7\u54c1\uff0c\u8bc1\u660e\u8be5\u6982\u7387\u89c4\u5212\u6846\u67b6\u5728\u5e73\u5747\u62c6\u89e3\u65f6\u95f4\u548c\u65b9\u5dee\u65b9\u9762\u4f18\u4e8e\u786e\u5b9a\u6027\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u8de8\u4e0d\u540c\u673a\u5668\u4eba\u8bbe\u7f6e\u6cdb\u5316\uff0c\u5e76\u6210\u529f\u9002\u5e94CAD\u6a21\u578b\u7684\u504f\u5dee\uff08\u5982\u7f3a\u5931\u6216\u5361\u4f4f\u96f6\u4ef6\uff09", "conclusion": "\u5c06\u62c6\u89e3\u89c4\u5212\u5efa\u6a21\u4e3aPOMDP\u5e76\u91c7\u7528\u6982\u7387\u89c4\u5212\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5e9f\u65e7\u4ea7\u54c1\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u62c6\u89e3\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u5faa\u73af\u7ecf\u6d4e\u4e2d\u7684\u81ea\u52a8\u5316\u62c6\u89e3\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2511.21749", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21749", "abs": "https://arxiv.org/abs/2511.21749", "authors": ["Svitlana Volkova", "Will Dupree", "Hsien-Te Kao", "Peter Bautista", "Gabe Ganberg", "Jeff Beaubien", "Laura Cassani"], "title": "Proactive Defense: Compound AI for Detecting Persuasion Attacks and Measuring Inoculation Effectiveness", "comment": null, "summary": "This paper introduces BRIES, a novel compound AI architecture designed to detect and measure the effectiveness of persuasion attacks across information environments. We present a system with specialized agents: a Twister that generates adversarial content employing targeted persuasion tactics, a Detector that identifies attack types with configurable parameters, a Defender that creates resilient content through content inoculation, and an Assessor that employs causal inference to evaluate inoculation effectiveness. Experimenting with the SemEval 2023 Task 3 taxonomy across the synthetic persuasion dataset, we demonstrate significant variations in detection performance across language agents. Our comparative analysis reveals significant performance disparities with GPT-4 achieving superior detection accuracy on complex persuasion techniques, while open-source models like Llama3 and Mistral demonstrated notable weaknesses in identifying subtle rhetorical, suggesting that different architectures encode and process persuasive language patterns in fundamentally different ways. We show that prompt engineering dramatically affects detection efficacy, with temperature settings and confidence scoring producing model-specific variations; Gemma and GPT-4 perform optimally at lower temperatures while Llama3 and Mistral show improved capabilities at higher temperatures. Our causal analysis provides novel insights into socio-emotional-cognitive signatures of persuasion attacks, revealing that different attack types target specific cognitive dimensions. This research advances generative AI safety and cognitive security by quantifying LLM-specific vulnerabilities to persuasion attacks and delivers a framework for enhancing human cognitive resilience through structured interventions before exposure to harmful content.", "AI": {"tldr": "BRIES\u662f\u4e00\u4e2a\u590d\u5408AI\u67b6\u6784\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u8bc4\u4f30\u4fe1\u606f\u73af\u5883\u4e2d\u7684\u8bf4\u670d\u653b\u51fb\u6548\u679c\uff0c\u5305\u542b\u751f\u6210\u653b\u51fb\u5185\u5bb9\u3001\u68c0\u6d4b\u653b\u51fb\u7c7b\u578b\u3001\u521b\u5efa\u9632\u5fa1\u5185\u5bb9\u548c\u8bc4\u4f30\u9632\u5fa1\u6548\u679c\u7684\u4e13\u95e8\u4ee3\u7406\uff0c\u5728\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u4e0a\u5c55\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u5f0fAI\u9762\u4e34\u8bf4\u670d\u653b\u51fb\u7684\u5b89\u5168\u98ce\u9669\uff0c\u9700\u8981\u91cf\u5316\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u5bf9\u8bf4\u670d\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u5e76\u5f00\u53d1\u589e\u5f3a\u4eba\u7c7b\u8ba4\u77e5\u97e7\u6027\u7684\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u751f\u6210AI\u5b89\u5168\u6027\u548c\u8ba4\u77e5\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51faBRIES\u590d\u5408AI\u67b6\u6784\uff0c\u5305\u542b\u56db\u4e2a\u4e13\u95e8\u4ee3\u7406\uff1aTwister\uff08\u751f\u6210\u5bf9\u6297\u6027\u5185\u5bb9\uff09\u3001Detector\uff08\u8bc6\u522b\u653b\u51fb\u7c7b\u578b\uff09\u3001Defender\uff08\u521b\u5efa\u9632\u5fa1\u5185\u5bb9\uff09\u3001Assessor\uff08\u8bc4\u4f30\u9632\u5fa1\u6548\u679c\uff09\u3002\u4f7f\u7528SemEval 2023 Task 3\u5206\u7c7b\u6cd5\u5728\u5408\u6210\u8bf4\u670d\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\uff08GPT-4\u3001Llama3\u3001Mistral\u3001Gemma\uff09\u7684\u6027\u80fd\uff0c\u5206\u6790\u63d0\u793a\u5de5\u7a0b\u548c\u6e29\u5ea6\u8bbe\u7f6e\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u884c\u56e0\u679c\u5206\u6790\u3002", "result": "GPT-4\u5728\u590d\u6742\u8bf4\u670d\u6280\u672f\u68c0\u6d4b\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u5f00\u6e90\u6a21\u578b\u5982Llama3\u548cMistral\u5728\u8bc6\u522b\u5fae\u5999\u4fee\u8f9e\u65b9\u9762\u5b58\u5728\u660e\u663e\u5f31\u70b9\u3002\u63d0\u793a\u5de5\u7a0b\u663e\u8457\u5f71\u54cd\u68c0\u6d4b\u6548\u679c\uff0c\u4e0d\u540c\u6a21\u578b\u5bf9\u6e29\u5ea6\u8bbe\u7f6e\u6709\u7279\u5b9a\u504f\u597d\u3002\u56e0\u679c\u5206\u6790\u63ed\u793a\u4e86\u4e0d\u540c\u653b\u51fb\u7c7b\u578b\u9488\u5bf9\u7279\u5b9a\u8ba4\u77e5\u7ef4\u5ea6\u7684\u7279\u5f81\u3002", "conclusion": "BRIES\u67b6\u6784\u901a\u8fc7\u91cf\u5316LLM\u5bf9\u8bf4\u670d\u653b\u51fb\u7684\u7279\u5b9a\u8106\u5f31\u6027\uff0c\u63a8\u8fdb\u4e86\u751f\u6210AI\u5b89\u5168\u548c\u8ba4\u77e5\u5b89\u5168\u7814\u7a76\uff0c\u4e3a\u5728\u63a5\u89e6\u6709\u5bb3\u5185\u5bb9\u524d\u901a\u8fc7\u7ed3\u6784\u5316\u5e72\u9884\u589e\u5f3a\u4eba\u7c7b\u8ba4\u77e5\u97e7\u6027\u63d0\u4f9b\u4e86\u6846\u67b6\u3002"}}
{"id": "2511.22155", "categories": ["cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2511.22155", "abs": "https://arxiv.org/abs/2511.22155", "authors": ["Bernd J. Kr\u00f6ger"], "title": "Lips-Jaw and Tongue-Jaw Articulatory Tradeoff in DYNARTmo", "comment": "12 pages, 3 figures, supplementary material: python code", "summary": "This paper investigates how the dynamic articulatory model DYNARTmo accounts for articulatory tradeoffs between primary and secondary articulators, with a focus on lips-jaw and tongue-jaw coordination. While DYNARTmo does not implement full task-dynamic second-order biomechanics, it adopts first-order task-space gesture specifications comparable to those used in articulatory phonology and integrates a simplified mechanism for distributing articulatory effort across multiple articulators. We first outline the conceptual relationship between task dynamics and DYNARTmo, emphasizing the distinction between high-level task-space trajectories and their low-level articulatory execution. We then present simulation results for a set of CV syllables that illustrate how jaw displacement varies as a function of both place of articulation (labial, apical, dorsal) and vowel context (/a/, /i/, /u/). The model reproduces empirically attested patterns of articulatory synergy, including jaw-supported apical closures, lower-lip elevation in bilabial stops, tongue-jaw co-movement, and saturation effects in labial constrictions. These results demonstrate that even with computationally simplified assumptions, DYNARTmo can generate realistic spatio-temporal movement patterns that capture key aspects of articulatory tradeoff and synergy across a range of consonant-vowel combinations.", "AI": {"tldr": "DYNARTmo\u6a21\u578b\u7814\u7a76\u53d1\u97f3\u5668\u5b98\u95f4\u7684\u534f\u8c03\u4e0e\u6743\u8861\uff0c\u7279\u522b\u662f\u5507-\u988c\u548c\u820c-\u988c\u534f\u8c03\uff0c\u901a\u8fc7\u7b80\u5316\u7684\u4e00\u9636\u4efb\u52a1\u7a7a\u95f4\u89c4\u8303\u6a21\u62df\u53d1\u97f3\u534f\u540c\u6548\u5e94\u3002", "motivation": "\u7814\u7a76\u52a8\u6001\u53d1\u97f3\u6a21\u578bDYNARTmo\u5982\u4f55\u89e3\u91ca\u4e3b\u8981\u548c\u6b21\u8981\u53d1\u97f3\u5668\u5b98\u4e4b\u95f4\u7684\u534f\u8c03\u5173\u7cfb\uff0c\u7279\u522b\u662f\u5507-\u988c\u548c\u820c-\u988c\u534f\u8c03\uff0c\u63a2\u7d22\u7b80\u5316\u6a21\u578b\u80fd\u5426\u6355\u6349\u771f\u5b9e\u53d1\u97f3\u6a21\u5f0f\u3002", "method": "\u91c7\u7528\u4e00\u9636\u4efb\u52a1\u7a7a\u95f4\u624b\u52bf\u89c4\u8303\uff08\u7c7b\u4f3c\u53d1\u97f3\u97f3\u7cfb\u5b66\uff09\uff0c\u96c6\u6210\u7b80\u5316\u673a\u5236\u5728\u591a\u53d1\u97f3\u5668\u5b98\u95f4\u5206\u914d\u53d1\u97f3\u52aa\u529b\uff0c\u901a\u8fc7CV\u97f3\u8282\u6a21\u62df\u5206\u6790\u988c\u4f4d\u79fb\u968f\u53d1\u97f3\u90e8\u4f4d\u548c\u5143\u97f3\u73af\u5883\u7684\u53d8\u5316\u3002", "result": "\u6a21\u578b\u6210\u529f\u590d\u73b0\u4e86\u7ecf\u9a8c\u8bc1\u5b9e\u7684\u53d1\u97f3\u534f\u540c\u6a21\u5f0f\uff1a\u988c\u652f\u6301\u7684\u820c\u5c16\u95ed\u5408\u3001\u53cc\u5507\u585e\u97f3\u7684\u4e0b\u5507\u62ac\u9ad8\u3001\u820c-\u988c\u534f\u540c\u8fd0\u52a8\u3001\u5507\u90e8\u6536\u7f29\u7684\u9971\u548c\u6548\u5e94\uff0c\u5c55\u793a\u4e86\u7b80\u5316\u5047\u8bbe\u4e0b\u4ecd\u80fd\u751f\u6210\u73b0\u5b9e\u7684\u65f6\u7a7a\u8fd0\u52a8\u6a21\u5f0f\u3002", "conclusion": "\u5373\u4f7f\u91c7\u7528\u8ba1\u7b97\u7b80\u5316\u7684\u5047\u8bbe\uff0cDYNARTmo\u6a21\u578b\u4ecd\u80fd\u751f\u6210\u6355\u6349\u5173\u952e\u53d1\u97f3\u6743\u8861\u548c\u534f\u540c\u65b9\u9762\u7684\u73b0\u5b9e\u65f6\u7a7a\u8fd0\u52a8\u6a21\u5f0f\uff0c\u4e3a\u7406\u89e3\u53d1\u97f3\u534f\u8c03\u673a\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2511.21752", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21752", "abs": "https://arxiv.org/abs/2511.21752", "authors": ["Yanxi Li", "Ruocheng Shan"], "title": "Semantics as a Shield: Label Disguise Defense (LDD) against Prompt Injection in LLM Sentiment Classification", "comment": null, "summary": "Large language models are increasingly used for text classification tasks such as sentiment analysis, yet their reliance on natural language prompts exposes them to prompt injection attacks. In particular, class-directive injections exploit knowledge of the model's label set (e.g., positive vs. negative) to override its intended behavior through adversarial instructions. Existing defenses, such as detection-based filters, instruction hierarchies, and signed prompts, either require model retraining or remain vulnerable to obfuscation. This paper introduces Label Disguise Defense (LDD), a lightweight and model-agnostic strategy that conceals true labels by replacing them with semantically transformed or unrelated alias labels(e.g., blue vs. yellow). The model learns these new label mappings implicitly through few-shot demonstrations, preventing direct correspondence between injected directives and decision outputs. We evaluate LDD across nine state-of-the-art models, including GPT-5, GPT-4o, LLaMA3.2, Gemma3, and Mistral variants, under varying few-shot and an adversarial setting. Our results show that the ability of LDD to recover performance lost to the adversarial attack varies across models and alias choices. For every model evaluated, LDD is able to restore a portion of the accuracy degradation caused by the attack. Moreover, for the vast majority of models, we can identify more than one alias pair that achieves higher accuracy than the under-attack baseline, in which the model relies solely on few-shot learning without any defensive mechanism. A linguistic analysis further reveals that semantically aligned alias labels(e.g., good vs. bad) yield stronger robustness than unaligned symbols(e.g., blue vs. yellow). Overall, this study demonstrates that label semantics can serve as an effective defense layer, transforming meaning itself into a shield against prompt injection.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLabel Disguise Defense (LDD)\uff0c\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6a21\u578b\u65e0\u5173\u7684\u9632\u5fa1\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u771f\u5b9e\u6807\u7b7e\u66ff\u6362\u4e3a\u8bed\u4e49\u8f6c\u6362\u6216\u65e0\u5173\u7684\u522b\u540d\u6807\u7b7e\u6765\u62b5\u5fa1\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u4f9d\u8d56\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u7684\u7279\u6027\u4f7f\u5176\u5bb9\u6613\u53d7\u5230\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u3002\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u6a21\u578b\u91cd\u65b0\u8bad\u7ec3\uff0c\u8981\u4e48\u5bb9\u6613\u53d7\u5230\u6df7\u6dc6\u653b\u51fb\u7684\u89c4\u907f\u3002", "method": "LDD\u901a\u8fc7\u9690\u85cf\u771f\u5b9e\u6807\u7b7e\uff0c\u4f7f\u7528\u8bed\u4e49\u8f6c\u6362\u6216\u65e0\u5173\u7684\u522b\u540d\u6807\u7b7e\uff08\u5982\"\u84dd\u8272vs\u9ec4\u8272\"\u4ee3\u66ff\"\u6b63\u9762vs\u8d1f\u9762\"\uff09\uff0c\u6a21\u578b\u901a\u8fc7\u5c11\u91cf\u793a\u4f8b\u9690\u5f0f\u5b66\u4e60\u8fd9\u4e9b\u65b0\u6807\u7b7e\u6620\u5c04\uff0c\u9632\u6b62\u6ce8\u5165\u6307\u4ee4\u4e0e\u51b3\u7b56\u8f93\u51fa\u4e4b\u95f4\u7684\u76f4\u63a5\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u57289\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cLDD\u80fd\u591f\u6062\u590d\u56e0\u5bf9\u6297\u653b\u51fb\u800c\u635f\u5931\u7684\u90e8\u5206\u6027\u80fd\u3002\u5bf9\u4e8e\u7edd\u5927\u591a\u6570\u6a21\u578b\uff0c\u53ef\u4ee5\u627e\u5230\u591a\u4e2a\u522b\u540d\u5bf9\uff0c\u5176\u51c6\u786e\u7387\u9ad8\u4e8e\u4ec5\u4f9d\u8d56\u5c11\u91cf\u5b66\u4e60\u800c\u65e0\u9632\u5fa1\u673a\u5236\u7684\u57fa\u51c6\u3002\u8bed\u4e49\u5bf9\u9f50\u7684\u522b\u540d\u6807\u7b7e\u6bd4\u672a\u5bf9\u9f50\u7684\u7b26\u53f7\u5177\u6709\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6807\u7b7e\u8bed\u4e49\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u9632\u5fa1\u5c42\uff0c\u5c06\u610f\u4e49\u672c\u8eab\u8f6c\u5316\u4e3a\u62b5\u5fa1\u63d0\u793a\u6ce8\u5165\u7684\u76fe\u724c\u3002LDD\u5c55\u793a\u4e86\u901a\u8fc7\u8bed\u4e49\u8f6c\u6362\u4fdd\u62a4\u6a21\u578b\u514d\u53d7\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.21753", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21753", "abs": "https://arxiv.org/abs/2511.21753", "authors": ["Sameeah Noreen Hameed", "Surangika Ranathunga", "Raj Prasanna", "Kristin Stock", "Christopher B. Jones"], "title": "Extracting Disaster Impacts and Impact Related Locations in Social Media Posts Using Large Language Models", "comment": null, "summary": "Large-scale disasters can often result in catastrophic consequences on people and infrastructure. Situation awareness about such disaster impacts generated by authoritative data from in-situ sensors, remote sensing imagery, and/or geographic data is often limited due to atmospheric opacity, satellite revisits, and time limitations. This often results in geo-temporal information gaps. In contrast, impact-related social media posts can act as \"geo-sensors\" during a disaster, where people describe specific impacts and locations. However, not all locations mentioned in disaster-related social media posts relate to an impact. Only the impacted locations are critical for directing resources effectively. e.g., \"The death toll from a fire which ripped through the Greek coastal town of #Mati stood at 80, with dozens of people unaccounted for as forensic experts tried to identify victims who were burned alive #Greecefires #AthensFires #Athens #Greece.\" contains impacted location \"Mati\" and non-impacted locations \"Greece\" and \"Athens\". This research uses Large Language Models (LLMs) to identify all locations, impacts and impacted locations mentioned in disaster-related social media posts. In the process, LLMs are fine-tuned to identify only impacts and impacted locations (as distinct from other, non-impacted locations), including locations mentioned in informal expressions, abbreviations, and short forms. Our fine-tuned model demonstrates efficacy, achieving an F1-score of 0.69 for impact and 0.74 for impacted location extraction, substantially outperforming the pre-trained baseline. These robust results confirm the potential of fine-tuned language models to offer a scalable solution for timely decision-making in resource allocation, situational awareness, and post-disaster recovery planning for responders.", "AI": {"tldr": "\u5229\u7528\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u707e\u5bb3\u76f8\u5173\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u4e2d\u63d0\u53d6\u53d7\u5f71\u54cd\u4f4d\u7f6e\u548c\u5f71\u54cd\u4fe1\u606f\uff0c\u4ee5\u586b\u8865\u5730\u7406\u65f6\u7a7a\u4fe1\u606f\u7a7a\u767d\uff0c\u652f\u6301\u5e94\u6025\u54cd\u5e94\u51b3\u7b56", "motivation": "\u5927\u89c4\u6a21\u707e\u5bb3\u5f80\u5f80\u9020\u6210\u4e25\u91cd\u540e\u679c\uff0c\u4f46\u4f20\u7edf\u6743\u5a01\u6570\u636e\uff08\u73b0\u573a\u4f20\u611f\u5668\u3001\u9065\u611f\u5f71\u50cf\u7b49\uff09\u5b58\u5728\u5927\u6c14\u4e0d\u900f\u660e\u3001\u536b\u661f\u91cd\u8bbf\u5468\u671f\u957f\u3001\u65f6\u95f4\u9650\u5236\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u5730\u7406\u65f6\u7a7a\u4fe1\u606f\u7a7a\u767d\u3002\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u53ef\u4f5c\u4e3a\"\u5730\u7406\u4f20\u611f\u5668\"\u63d0\u4f9b\u5b9e\u65f6\u5f71\u54cd\u4fe1\u606f\uff0c\u4f46\u5e76\u975e\u6240\u6709\u63d0\u53ca\u7684\u4f4d\u7f6e\u90fd\u5b9e\u9645\u53d7\u707e\uff0c\u9700\u8981\u533a\u5206\u53d7\u5f71\u54cd\u4f4d\u7f6e\u548c\u975e\u53d7\u5f71\u54cd\u4f4d\u7f6e", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bc6\u522b\u707e\u5bb3\u76f8\u5173\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u4e2d\u7684\u6240\u6709\u4f4d\u7f6e\u3001\u5f71\u54cd\u548c\u53d7\u5f71\u54cd\u4f4d\u7f6e\u3002\u901a\u8fc7\u5fae\u8c03LLMs\u4e13\u95e8\u8bc6\u522b\u5f71\u54cd\u548c\u53d7\u5f71\u54cd\u4f4d\u7f6e\uff08\u533a\u522b\u4e8e\u975e\u53d7\u5f71\u54cd\u4f4d\u7f6e\uff09\uff0c\u5305\u62ec\u975e\u6b63\u5f0f\u8868\u8fbe\u3001\u7f29\u5199\u548c\u7b80\u5199\u5f62\u5f0f\u4e2d\u63d0\u53ca\u7684\u4f4d\u7f6e", "result": "\u5fae\u8c03\u6a21\u578b\u5728\u5f71\u54cd\u63d0\u53d6\u4e0a\u8fbe\u5230F1\u5206\u65700.69\uff0c\u53d7\u5f71\u54cd\u4f4d\u7f6e\u63d0\u53d6\u8fbe\u52300.74\uff0c\u663e\u8457\u4f18\u4e8e\u9884\u8bad\u7ec3\u57fa\u7ebf\u3002\u6a21\u578b\u80fd\u6709\u6548\u5904\u7406\u975e\u6b63\u5f0f\u8868\u8fbe\u3001\u7f29\u5199\u548c\u7b80\u5199\u5f62\u5f0f\u7684\u4f4d\u7f6e\u4fe1\u606f", "conclusion": "\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u4e3a\u8d44\u6e90\u5206\u914d\u3001\u6001\u52bf\u611f\u77e5\u548c\u707e\u540e\u6062\u590d\u89c4\u5212\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u53ca\u65f6\u51b3\u7b56\u652f\u6301\u65b9\u6848\uff0c\u8bc1\u5b9e\u4e86\u5176\u5728\u707e\u5bb3\u54cd\u5e94\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2511.21756", "categories": ["cs.CL", "cs.CE"], "pdf": "https://arxiv.org/pdf/2511.21756", "abs": "https://arxiv.org/abs/2511.21756", "authors": ["Soham Mirajkar"], "title": "Dissecting the Ledger: Locating and Suppressing \"Liar Circuits\" in Financial Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes financial domains, yet they suffer from specific, reproducible hallucinations when performing arithmetic operations. Current mitigation strategies often treat the model as a black box. In this work, we propose a mechanistic approach to intrinsic hallucination detection. By applying Causal Tracing to the GPT-2 XL architecture on the ConvFinQA benchmark, we identify a dual-stage mechanism for arithmetic reasoning: a distributed computational scratchpad in middle layers (L12-L30) and a decisive aggregation circuit in late layers (specifically Layer 46). We verify this mechanism via an ablation study, demonstrating that suppressing Layer 46 reduces the model's confidence in hallucinatory outputs by 81.8%. Furthermore, we demonstrate that a linear probe trained on this layer generalizes to unseen financial topics with 98% accuracy, suggesting a universal geometry of arithmetic deception.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5236\u6027\u65b9\u6cd5\u6765\u68c0\u6d4bLLM\u5728\u91d1\u878d\u7b97\u672f\u63a8\u7406\u4e2d\u7684\u5185\u5728\u5e7b\u89c9\uff0c\u901a\u8fc7\u56e0\u679c\u8ffd\u8e2a\u5728GPT-2 XL\u4e0a\u53d1\u73b0\u4e86\u7b97\u672f\u63a8\u7406\u7684\u53cc\u9636\u6bb5\u673a\u5236\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u8fd9\u4e00\u53d1\u73b0\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u5e7b\u89c9\u68c0\u6d4b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u91d1\u878d\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u90e8\u7f72\u65f6\uff0c\u5b58\u5728\u7279\u5b9a\u4e14\u53ef\u590d\u73b0\u7684\u7b97\u672f\u8fd0\u7b97\u5e7b\u89c9\u95ee\u9898\u3002\u5f53\u524d\u7f13\u89e3\u7b56\u7565\u901a\u5e38\u5c06\u6a21\u578b\u89c6\u4e3a\u9ed1\u76d2\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7406\u89e3\u5e7b\u89c9\u7684\u5185\u90e8\u673a\u5236\u3002", "method": "\u91c7\u7528\u673a\u5236\u6027\u65b9\u6cd5\u8fdb\u884c\u5185\u5728\u5e7b\u89c9\u68c0\u6d4b\uff0c\u5bf9GPT-2 XL\u67b6\u6784\u5728ConvFinQA\u57fa\u51c6\u4e0a\u5e94\u7528\u56e0\u679c\u8ffd\u8e2a\u6280\u672f\uff0c\u8bc6\u522b\u7b97\u672f\u63a8\u7406\u7684\u53cc\u9636\u6bb5\u673a\u5236\uff1a\u4e2d\u95f4\u5c42\uff08L12-L30\uff09\u7684\u5206\u5e03\u5f0f\u8ba1\u7b97\u8349\u7a3f\u548c\u665a\u671f\u5c42\uff08\u7279\u522b\u662f\u7b2c46\u5c42\uff09\u7684\u51b3\u7b56\u805a\u5408\u7535\u8def\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6291\u5236\u7b2c46\u5c42\u53ef\u5c06\u6a21\u578b\u5bf9\u5e7b\u89c9\u8f93\u51fa\u7684\u7f6e\u4fe1\u5ea6\u964d\u4f4e81.8%\u3002\u5728\u8be5\u5c42\u8bad\u7ec3\u7684\u7ebf\u6027\u63a2\u9488\u80fd\u591f\u4ee598%\u7684\u51c6\u786e\u7387\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u91d1\u878d\u4e3b\u9898\uff0c\u8868\u660e\u7b97\u672f\u6b3a\u9a97\u5b58\u5728\u901a\u7528\u51e0\u4f55\u7ed3\u6784\u3002", "conclusion": "\u901a\u8fc7\u673a\u5236\u6027\u5206\u6790\u63ed\u793a\u4e86LLM\u7b97\u672f\u5e7b\u89c9\u7684\u5185\u90e8\u5de5\u4f5c\u673a\u5236\uff0c\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u5e7b\u89c9\u68c0\u6d4b\u548c\u7f13\u89e3\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u91d1\u878d\u5e94\u7528\u573a\u666f\u4e2d\u3002"}}
{"id": "2511.21759", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21759", "abs": "https://arxiv.org/abs/2511.21759", "authors": ["Linye Wei", "Wenjue Chen", "Pingzhi Tang", "Xiaotian Guo", "Le Ye", "Runsheng Wang", "Meng Li"], "title": "Orchestrating Dual-Boundaries: An Arithmetic Intensity Inspired Acceleration Framework for Diffusion Language Models", "comment": null, "summary": "Diffusion-based large language models (dLLMs) have recently gained significant attention for their exceptional performance and inherent potential for parallel decoding. Existing frameworks further enhance its inference efficiency by enabling KV caching. However, its bidirectional attention mechanism necessitates periodic cache refreshes that interleave prefill and decoding phases, both contributing substantial inference cost and constraining achievable speedup. Inspired by the heterogeneous arithmetic intensity of the prefill and decoding phases, we propose ODB-dLLM, a framework that orchestrates dual-boundaries to accelerate dLLM inference. In the prefill phase, we find that the predefined fixed response length introduces heavy yet redundant computational overhead, which affects efficiency. To alleviate this, ODB-dLLM incorporates an adaptive length prediction mechanism that progressively reduces prefill overhead and unnecessary computation. In the decoding phase, we analyze the computational characteristics of dLLMs and propose a dLLM-specific jump-share speculative decoding method to enhance efficiency by reducing the number of decoding iterations. Experimental results demonstrate that ODB-dLLM achieves 46-162x and 2.63-6.30x speedups over the baseline dLLM and Fast-dLLM, respectively, while simultaneously mitigating the accuracy degradation in existing acceleration frameworks.", "AI": {"tldr": "ODB-dLLM\u901a\u8fc7\u81ea\u9002\u5e94\u957f\u5ea6\u9884\u6d4b\u548c\u8df3\u8f6c\u5171\u4eab\u63a8\u6d4b\u89e3\u7801\uff0c\u52a0\u901f\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\uff0c\u76f8\u6bd4\u57fa\u7ebf\u5b9e\u73b046-162\u500d\u52a0\u901f", "motivation": "\u73b0\u6709\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b(dLLM)\u7684\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u9700\u8981\u5468\u671f\u6027\u7f13\u5b58\u5237\u65b0\uff0c\u5bfc\u81f4\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u4ea4\u9519\uff0c\u9020\u6210\u5927\u91cf\u63a8\u7406\u6210\u672c\u5e76\u9650\u5236\u52a0\u901f\u6f5c\u529b\u3002\u9884\u586b\u5145\u9636\u6bb5\u56fa\u5b9a\u54cd\u5e94\u957f\u5ea6\u5f15\u5165\u5197\u4f59\u8ba1\u7b97\uff0c\u89e3\u7801\u9636\u6bb5\u8fed\u4ee3\u6b21\u6570\u591a\u5f71\u54cd\u6548\u7387\u3002", "method": "\u63d0\u51faODB-dLLM\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u8fb9\u754c\u534f\u8c03\u7b56\u7565\uff1a1) \u9884\u586b\u5145\u9636\u6bb5\u5f15\u5165\u81ea\u9002\u5e94\u957f\u5ea6\u9884\u6d4b\u673a\u5236\uff0c\u6e10\u8fdb\u51cf\u5c11\u9884\u586b\u5145\u5f00\u9500\u548c\u4e0d\u5fc5\u8981\u8ba1\u7b97\uff1b2) \u89e3\u7801\u9636\u6bb5\u63d0\u51fadLLM\u7279\u5b9a\u7684\u8df3\u8f6c\u5171\u4eab\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\uff0c\u51cf\u5c11\u89e3\u7801\u8fed\u4ee3\u6b21\u6570\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cODB-dLLM\u76f8\u6bd4\u57fa\u7ebfdLLM\u5b9e\u73b046-162\u500d\u52a0\u901f\uff0c\u76f8\u6bd4Fast-dLLM\u5b9e\u73b02.63-6.30\u500d\u52a0\u901f\uff0c\u540c\u65f6\u7f13\u89e3\u4e86\u73b0\u6709\u52a0\u901f\u6846\u67b6\u7684\u51c6\u786e\u7387\u4e0b\u964d\u95ee\u9898\u3002", "conclusion": "ODB-dLLM\u901a\u8fc7\u534f\u8c03\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u7684\u53cc\u8fb9\u754c\u7b56\u7565\uff0c\u6709\u6548\u52a0\u901fdLLM\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.21760", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21760", "abs": "https://arxiv.org/abs/2511.21760", "authors": ["Yuxiang Wei", "Yanteng Zhang", "Xi Xiao", "Chengxuan Qian", "Tianyang Wang", "Vince D. Calhoun"], "title": "fMRI-LM: Towards a Universal Foundation Model for Language-Aligned fMRI Understanding", "comment": null, "summary": "Recent advances in multimodal large language models (LLMs) have enabled unified reasoning across images, audio, and video, but extending such capability to brain imaging remains largely unexplored. Bridging this gap is essential to link neural activity with semantic cognition and to develop cross-modal brain representations. To this end, we present fMRI-LM, a foundational model that bridges functional MRI (fMRI) and language through a three-stage framework. In Stage 1, we learn a neural tokenizer that maps fMRI into discrete tokens embedded in a language-consistent space. In Stage 2, a pretrained LLM is adapted to jointly model fMRI tokens and text, treating brain activity as a sequence that can be temporally predicted and linguistically described. To overcome the lack of natural fMRI-text pairs, we construct a large descriptive corpus that translates diverse imaging-based features into structured textual descriptors, capturing the low-level organization of fMRI signals. In Stage 3, we perform multi-task, multi-paradigm instruction tuning to endow fMRI-LM with high-level semantic understanding, supporting diverse downstream applications. Across various benchmarks, fMRI-LM achieves strong zero-shot and few-shot performance, and adapts efficiently with parameter-efficient tuning (LoRA), establishing a scalable pathway toward a language-aligned, universal model for structural and semantic understanding of fMRI.", "AI": {"tldr": "fMRI-LM\uff1a\u901a\u8fc7\u4e09\u9636\u6bb5\u6846\u67b6\u5c06\u529f\u80fd\u78c1\u5171\u632f\u6210\u50cf\uff08fMRI\uff09\u4e0e\u8bed\u8a00\u5bf9\u9f50\u7684\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u795e\u7ecf\u6d3b\u52a8\u4e0e\u8bed\u4e49\u8ba4\u77e5\u7684\u6865\u6881", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u5728\u56fe\u50cf\u3001\u97f3\u9891\u3001\u89c6\u9891\u7b49\u9886\u57df\u5b9e\u73b0\u7edf\u4e00\u63a8\u7406\uff0c\u4f46\u8111\u6210\u50cf\u9886\u57df\u5c1a\u672a\u63a2\u7d22\u3002\u9700\u8981\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u4ee5\u8fde\u63a5\u795e\u7ecf\u6d3b\u52a8\u4e0e\u8bed\u4e49\u8ba4\u77e5\uff0c\u5e76\u5f00\u53d1\u8de8\u6a21\u6001\u8111\u8868\u5f81\u3002", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u5b66\u4e60\u795e\u7ecf\u5206\u8bcd\u5668\uff0c\u5c06fMRI\u6620\u5c04\u5230\u8bed\u8a00\u4e00\u81f4\u7a7a\u95f4\u4e2d\u7684\u79bb\u6563\u6807\u8bb0\uff1b2\uff09\u9002\u914d\u9884\u8bad\u7ec3LLM\uff0c\u8054\u5408\u5efa\u6a21fMRI\u6807\u8bb0\u548c\u6587\u672c\uff0c\u5c06\u8111\u6d3b\u52a8\u89c6\u4e3a\u53ef\u9884\u6d4b\u548c\u63cf\u8ff0\u7684\u5e8f\u5217\uff1b3\uff09\u591a\u4efb\u52a1\u3001\u591a\u8303\u5f0f\u6307\u4ee4\u5fae\u8c03\uff0c\u8d4b\u4e88\u6a21\u578b\u9ad8\u7ea7\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002", "result": "fMRI-LM\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u8c03\u4f18\uff08LoRA\uff09\u9ad8\u6548\u9002\u5e94\uff0c\u4e3afMRI\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u7406\u89e3\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u8bed\u8a00\u5bf9\u9f50\u901a\u7528\u6a21\u578b\u8def\u5f84\u3002", "conclusion": "fMRI-LM\u6210\u529f\u6784\u5efa\u4e86fMRI\u4e0e\u8bed\u8a00\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4e3a\u795e\u7ecf\u6d3b\u52a8\u4e0e\u8bed\u4e49\u8ba4\u77e5\u7684\u8fde\u63a5\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2511.21761", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.21761", "abs": "https://arxiv.org/abs/2511.21761", "authors": ["Tabia Tanzin Prama", "Christopher M. Danforth", "Peter Sheridan Dodds"], "title": "LLMs for Low-Resource Dialect Translation Using Context-Aware Prompting: A Case Study on Sylheti", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong translation abilities through prompting, even without task-specific training. However, their effectiveness in dialectal and low-resource contexts remains underexplored. This study presents the first systematic investigation of LLM-based machine translation (MT) for Sylheti, a dialect of Bangla that is itself low-resource. We evaluate five advanced LLMs (GPT-4.1, GPT-4.1, LLaMA 4, Grok 3, and DeepSeek V3.2) across both translation directions (Bangla $\\Leftrightarrow$ Sylheti), and find that these models struggle with dialect-specific vocabulary. To address this, we introduce Sylheti-CAP (Context-Aware Prompting), a three-step framework that embeds a linguistic rulebook, a dictionary (2{,}260 core vocabulary items and idioms), and an authenticity check directly into prompts. Extensive experiments show that Sylheti-CAP consistently improves translation quality across models and prompting strategies. Both automatic metrics and human evaluations confirm its effectiveness, while qualitative analysis reveals notable reductions in hallucinations, ambiguities, and awkward phrasing, establishing Sylheti-CAP as a scalable solution for dialectal and low-resource MT. Dataset link: \\href{https://github.com/TabiaTanzin/LLMs-for-Low-Resource-Dialect-Translation-Using-Context-Aware-Prompting-A-Case-Study-on-Sylheti.git}{https://github.com/TabiaTanzin/LLMs-for-Low-Resource-Dialect-Translation-Using-Context-Aware-Prompting-A-Case-Study-on-Sylheti.git}", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSylheti-CAP\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\u6539\u8fdbLLM\u5728\u4f4e\u8d44\u6e90\u65b9\u8a00\u7ffb\u8bd1\u4e2d\u7684\u8868\u73b0\uff0c\u663e\u8457\u63d0\u5347Sylheti-Bangla\u7ffb\u8bd1\u8d28\u91cf\u3002", "motivation": "LLM\u5728\u6807\u51c6\u8bed\u8a00\u7ffb\u8bd1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u65b9\u8a00\u548c\u4f4e\u8d44\u6e90\u8bed\u5883\u4e0b\u7684\u7ffb\u8bd1\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002Sylheti\u4f5c\u4e3a\u5b5f\u52a0\u62c9\u8bed\u7684\u4f4e\u8d44\u6e90\u65b9\u8a00\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u673a\u5668\u7ffb\u8bd1\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faSylheti-CAP\u4e09\u6b65\u9aa4\u6846\u67b6\uff1a1)\u5d4c\u5165\u8bed\u8a00\u89c4\u5219\u624b\u518c\uff1b2)\u96c6\u6210\u5305\u542b2260\u4e2a\u6838\u5fc3\u8bcd\u6c47\u548c\u4e60\u8bed\u7684\u8bcd\u5178\uff1b3)\u771f\u5b9e\u6027\u68c0\u67e5\u3002\u5728\u4e94\u4e2a\u5148\u8fdbLLM\u4e0a\u8bc4\u4f30\uff0c\u5bf9\u6bd4\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u3002", "result": "Sylheti-CAP\u5728\u6240\u6709\u6a21\u578b\u548c\u63d0\u793a\u7b56\u7565\u4e2d\u4e00\u81f4\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u3002\u81ea\u52a8\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u5747\u8bc1\u5b9e\u5176\u6709\u6548\u6027\uff0c\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u3001\u6b67\u4e49\u548c\u751f\u786c\u8868\u8fbe\u3002", "conclusion": "Sylheti-CAP\u4e3a\u65b9\u8a00\u548c\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u65b9\u8a00\u7279\u5b9a\u8bcd\u6c47\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2511.21762", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21762", "abs": "https://arxiv.org/abs/2511.21762", "authors": ["Gabriele Cesar Iwashima", "Claudia Susie Rodrigues", "Claudio Dipolitto", "Geraldo Xex\u00e9o"], "title": "Factors That Support Grounded Responses in LLM Conversations: A Rapid Review", "comment": "28 pages, 1 figure, 3 tables", "summary": "Large language models (LLMs) may generate outputs that are misaligned with user intent, lack contextual grounding, or exhibit hallucinations during conversation, which compromises the reliability of LLM-based applications. This review aimed to identify and analyze techniques that align LLM responses with conversational goals, ensure grounding, and reduce hallucination and topic drift. We conducted a Rapid Review guided by the PRISMA framework and the PICO strategy to structure the search, filtering, and selection processes. The alignment strategies identified were categorized according to the LLM lifecycle phase in which they operate: inference-time, post-training, and reinforcement learning-based methods. Among these, inference-time approaches emerged as particularly efficient, aligning outputs without retraining while supporting user intent, contextual grounding, and hallucination mitigation. The reviewed techniques provided structured mechanisms for improving the quality and reliability of LLM responses across key alignment objectives.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u56de\u987e\u4e86\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u8bdd\u8d28\u91cf\u7684\u6280\u672f\uff0c\u91cd\u70b9\u89e3\u51b3\u8f93\u51fa\u4e0e\u7528\u6237\u610f\u56fe\u4e0d\u4e00\u81f4\u3001\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u57fa\u7840\u3001\u5e7b\u89c9\u548c\u8bdd\u9898\u6f02\u79fb\u7b49\u95ee\u9898\uff0c\u901a\u8fc7PRISMA\u6846\u67b6\u548cPICO\u7b56\u7565\u8bc6\u522b\u5e76\u5206\u7c7b\u4e86\u4e0d\u540c\u9636\u6bb5\u7684\u6821\u51c6\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u4e2d\u53ef\u80fd\u4ea7\u751f\u4e0e\u7528\u6237\u610f\u56fe\u4e0d\u4e00\u81f4\u3001\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u57fa\u7840\u6216\u51fa\u73b0\u5e7b\u89c9\u7684\u8f93\u51fa\uff0c\u8fd9\u4e9b\u95ee\u9898\u4f1a\u635f\u5bb3\u57fa\u4e8eLLM\u5e94\u7528\u7684\u53ef\u9760\u6027\uff0c\u56e0\u6b64\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc6\u522b\u548c\u5206\u6790\u63d0\u5347\u5bf9\u8bdd\u8d28\u91cf\u7684\u6821\u51c6\u6280\u672f\u3002", "method": "\u91c7\u7528PRISMA\u6846\u67b6\u548cPICO\u7b56\u7565\u6307\u5bfc\u7684\u5feb\u901f\u7efc\u8ff0\u65b9\u6cd5\uff0c\u7cfb\u7edf\u6027\u5730\u8fdb\u884c\u6587\u732e\u68c0\u7d22\u3001\u7b5b\u9009\u548c\u9009\u62e9\uff0c\u5c06\u8bc6\u522b\u51fa\u7684\u6821\u51c6\u7b56\u7565\u6309\u7167LLM\u751f\u547d\u5468\u671f\u9636\u6bb5\u5206\u7c7b\uff1a\u63a8\u7406\u65f6\u65b9\u6cd5\u3001\u540e\u8bad\u7ec3\u65b9\u6cd5\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u65f6\u65b9\u6cd5\u7279\u522b\u9ad8\u6548\uff0c\u80fd\u591f\u5728\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6821\u51c6\u8f93\u51fa\uff0c\u540c\u65f6\u652f\u6301\u7528\u6237\u610f\u56fe\u5bf9\u9f50\u3001\u4e0a\u4e0b\u6587\u57fa\u7840\u548c\u5e7b\u89c9\u7f13\u89e3\uff0c\u4e3a\u63d0\u5347LLM\u54cd\u5e94\u8d28\u91cf\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u673a\u5236\u3002", "conclusion": "\u7efc\u8ff0\u7684\u6280\u672f\u4e3a\u6539\u5584LLM\u54cd\u5e94\u8d28\u91cf\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6846\u67b6\uff0c\u7279\u522b\u662f\u63a8\u7406\u65f6\u65b9\u6cd5\u5728\u5e73\u8861\u6548\u7387\u548c\u6548\u679c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5b9e\u73b0LLM\u4e0e\u5bf9\u8bdd\u76ee\u6807\u5bf9\u9f50\u3001\u786e\u4fdd\u4e0a\u4e0b\u6587\u57fa\u7840\u548c\u51cf\u5c11\u5e7b\u89c9\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2511.21843", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21843", "abs": "https://arxiv.org/abs/2511.21843", "authors": ["Sarina Xi", "Vishisht Rao", "Justin Payan", "Nihar B. Shah"], "title": "FLAWS: A Benchmark for Error Identification and Localization in Scientific Papers", "comment": "30 pages, 12 tables, 2 figures", "summary": "The identification and localization of errors is a core task in peer review, yet the exponential growth of scientific output has made it increasingly difficult for human reviewers to reliably detect errors given the limited pool of experts. Recent advances in Large Language Models (LLMs) have sparked interest in their potential to support such evaluation tasks, from academic peer review to automated scientific assessment. However, despite the growing use of LLMs in review systems, their capabilities to pinpoint errors remain underexplored. In this work, we introduce Fault Localization Across Writing in Science (FLAWS), an automated benchmark consisting of 713 paper-error pairs designed to evaluate how effectively LLMs detect errors that undermine key claims in research papers. We construct the benchmark by systematically inserting claim-invalidating errors into peer-reviewed papers using LLMs, paired with an automated evaluation metric that measures whether models can identify and localize these errors. Developing such a benchmark presents unique challenges that we overcome: ensuring that the inserted errors are well-defined, challenging, and relevant to the content of the paper, avoiding artifacts that would make identification trivial, and designing a scalable, automated evaluation metric. On the resulting benchmark, we evaluate five frontier LLMs: Claude Sonnet 4.5, DeepSeek Reasoner v3.1, Gemini 2.5 Pro, GPT 5, and Grok 4. Among these, GPT 5 is the top-performing model, achieving 39.1% identification accuracy when k=10, where k is the number of top-ranked error text candidates generated by the LLM.", "AI": {"tldr": "FLAWS\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u79d1\u7814\u8bba\u6587\u4e2d\u68c0\u6d4b\u548c\u5b9a\u4f4d\u9519\u8bef\u80fd\u529b\u7684\u81ea\u52a8\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b713\u4e2a\u8bba\u6587-\u9519\u8bef\u5bf9\uff0cGPT-5\u5728\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f73\uff0839.1%\u51c6\u786e\u7387\uff09\u3002", "motivation": "\u968f\u7740\u79d1\u5b66\u4ea7\u51fa\u7684\u6307\u6570\u7ea7\u589e\u957f\uff0c\u4eba\u5de5\u5ba1\u7a3f\u8d8a\u6765\u8d8a\u96be\u4ee5\u53ef\u9760\u5730\u68c0\u6d4b\u9519\u8bef\uff0c\u800cLLM\u5728\u9519\u8bef\u68c0\u6d4b\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u901a\u8fc7LLM\u7cfb\u7edf\u6027\u5730\u5728\u540c\u884c\u8bc4\u5ba1\u8bba\u6587\u4e2d\u63d2\u5165\u7834\u574f\u6027\u9519\u8bef\uff0c\u6784\u5efa\u5305\u542b713\u4e2a\u8bba\u6587-\u9519\u8bef\u5bf9\u7684FLAWS\u57fa\u51c6\uff0c\u5e76\u8bbe\u8ba1\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u6765\u6d4b\u91cf\u6a21\u578b\u8bc6\u522b\u548c\u5b9a\u4f4d\u9519\u8bef\u7684\u80fd\u529b\u3002", "result": "\u5728\u8bc4\u4f30\u7684\u4e94\u4e2a\u524d\u6cbfLLM\uff08Claude Sonnet 4.5\u3001DeepSeek Reasoner v3.1\u3001Gemini 2.5 Pro\u3001GPT 5\u3001Grok 4\uff09\u4e2d\uff0cGPT 5\u8868\u73b0\u6700\u4f73\uff0c\u5728k=10\u65f6\u8fbe\u523039.1%\u7684\u8bc6\u522b\u51c6\u786e\u7387\u3002", "conclusion": "FLAWS\u57fa\u51c6\u4e3a\u8bc4\u4f30LLM\u5728\u79d1\u7814\u9519\u8bef\u68c0\u6d4b\u65b9\u9762\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0cGPT-5\u5728\u5f53\u524d\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u4f46\u6574\u4f53\u51c6\u786e\u7387\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u8868\u660e\u8be5\u9886\u57df\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2511.21860", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.21860", "abs": "https://arxiv.org/abs/2511.21860", "authors": ["Paulo Cavalin", "Cassia Sanctos", "Marcelo Grave", "Claudio Pinhanez", "Yago Primerano"], "title": "Improving Score Reliability of Multiple Choice Benchmarks with Consistency Evaluation and Altered Answer Choices", "comment": null, "summary": "In this work we present the Consistency-Rebalanced Accuracy (CoRA) metric, improving the reliability of Large Language Model (LLM) scores computed on multiple choice (MC) benchmarks. Our metric explores the response consistency of the LLMs, taking advantage of synthetically-generated questions with altered answer choices. With two intermediate scores, i.e. Bare-Minimum-Consistency Accuracy (BMCA) and Consistency Index (CI), CoRA is computed by adjusting the multiple-choice question answering (MCQA) scores to better reflect the level of consistency of the LLM. We present evaluations in different benchmarks using diverse LLMs, and not only demonstrate that LLMs can present low response consistency even when they present high MCQA scores, but also that CoRA can successfully scale down the scores of inconsistent models.", "AI": {"tldr": "\u63d0\u51faCoRA\u6307\u6807\uff0c\u901a\u8fc7\u5206\u6790LLM\u5728\u5408\u6210\u95ee\u9898\u4e0a\u7684\u56de\u7b54\u4e00\u81f4\u6027\u6765\u8c03\u6574MCQA\u5206\u6570\uff0c\u63d0\u9ad8\u8bc4\u4f30\u53ef\u9760\u6027", "motivation": "\u73b0\u6709MCQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLM\u53ef\u80fd\u83b7\u5f97\u9ad8\u5206\u4f46\u56de\u7b54\u4e00\u81f4\u6027\u4f4e\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u8bc4\u4f30\u6307\u6807\u6765\u53cd\u6620\u6a21\u578b\u771f\u5b9e\u80fd\u529b", "method": "\u4f7f\u7528\u5408\u6210\u751f\u6210\u7684\u95ee\u9898\uff08\u6539\u53d8\u7b54\u6848\u9009\u9879\uff09\u8bc4\u4f30LLM\u56de\u7b54\u4e00\u81f4\u6027\uff0c\u901a\u8fc7BMCA\u548cCI\u4e24\u4e2a\u4e2d\u95f4\u5206\u6570\u8ba1\u7b97CoRA\uff0c\u8c03\u6574\u539f\u59cbMCQA\u5206\u6570", "result": "\u5b9e\u9a8c\u663e\u793aLLM\u5728MCQA\u4e2d\u53ef\u80fd\u9ad8\u5206\u4f46\u4e00\u81f4\u6027\u4f4e\uff0cCoRA\u80fd\u6210\u529f\u964d\u4f4e\u4e0d\u4e00\u81f4\u6a21\u578b\u7684\u5206\u6570\uff0c\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u8bc4\u4f30", "conclusion": "CoRA\u6307\u6807\u901a\u8fc7\u7ed3\u5408\u4e00\u81f4\u6027\u8bc4\u4f30\u6539\u8fdb\u4e86LLM\u5728MC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8bc4\u5206\u53ef\u9760\u6027\uff0c\u6709\u52a9\u4e8e\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u6a21\u578b\u80fd\u529b"}}
{"id": "2511.21909", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21909", "abs": "https://arxiv.org/abs/2511.21909", "authors": ["Sofie Labat", "Thomas Demeester", "V\u00e9ronique Hoste"], "title": "A Customer Journey in the Land of Oz: Leveraging the Wizard of Oz Technique to Model Emotions in Customer Service Interactions", "comment": null, "summary": "Emotion-aware customer service needs in-domain conversational data, rich annotations, and predictive capabilities, but existing resources for emotion recognition are often out-of-domain, narrowly labeled, and focused on post-hoc detection. To address this, we conducted a controlled Wizard of Oz (WOZ) experiment to elicit interactions with targeted affective trajectories. The resulting corpus, EmoWOZ-CS, contains 2,148 bilingual (Dutch-English) written dialogues from 179 participants across commercial aviation, e-commerce, online travel agencies, and telecommunication scenarios. Our contributions are threefold: (1) Evaluate WOZ-based operator-steered valence trajectories as a design for emotion research; (2) Quantify human annotation performance and variation, including divergences between self-reports and third-party judgments; (3) Benchmark detection and forward-looking emotion inference in real-time support. Findings show neutral dominates participant messages; desire and gratitude are the most frequent non-neutral emotions. Agreement is moderate for multilabel emotions and valence, lower for arousal and dominance; self-reports diverge notably from third-party labels, aligning most for neutral, gratitude, and anger. Objective strategies often elicit neutrality or gratitude, while suboptimal strategies increase anger, annoyance, disappointment, desire, and confusion. Some affective strategies (cheerfulness, gratitude) foster positive reciprocity, whereas others (apology, empathy) can also leave desire, anger, or annoyance. Temporal analysis confirms successful conversation-level steering toward prescribed trajectories, most distinctly for negative targets; positive and neutral targets yield similar final valence distributions. Benchmarks highlight the difficulty of forward-looking emotion inference from prior turns, underscoring the complexity of proactive emotion-aware support.", "AI": {"tldr": "EmoWOZ-CS\uff1a\u4e00\u4e2a\u7528\u4e8e\u60c5\u611f\u611f\u77e5\u5ba2\u670d\u7684\u53cc\u8bed\u5bf9\u8bdd\u8bed\u6599\u5e93\uff0c\u901a\u8fc7Wizard of Oz\u5b9e\u9a8c\u6536\u96c6\uff0c\u5305\u542b2,148\u4e2a\u5bf9\u8bdd\uff0c\u7528\u4e8e\u8bc4\u4f30\u60c5\u611f\u8f68\u8ff9\u8bbe\u8ba1\u3001\u6807\u6ce8\u5dee\u5f02\u548c\u5b9e\u65f6\u60c5\u611f\u63a8\u7406", "motivation": "\u73b0\u6709\u60c5\u611f\u8bc6\u522b\u8d44\u6e90\u5f80\u5f80\u9886\u57df\u4e0d\u5339\u914d\u3001\u6807\u6ce8\u6709\u9650\u4e14\u4fa7\u91cd\u4e8e\u4e8b\u540e\u68c0\u6d4b\uff0c\u800c\u60c5\u611f\u611f\u77e5\u5ba2\u670d\u9700\u8981\u9886\u57df\u5185\u5bf9\u8bdd\u6570\u636e\u3001\u4e30\u5bcc\u6807\u6ce8\u548c\u9884\u6d4b\u80fd\u529b", "method": "\u91c7\u7528\u53d7\u63a7\u7684Wizard of Oz\u5b9e\u9a8c\uff0c\u5f15\u5bfc\u5177\u6709\u76ee\u6807\u60c5\u611f\u8f68\u8ff9\u7684\u4ea4\u4e92\uff0c\u6536\u96c6\u4e862,148\u4e2a\u53cc\u8bed\uff08\u8377\u5170\u8bed-\u82f1\u8bed\uff09\u4e66\u9762\u5bf9\u8bdd\uff0c\u6db5\u76d6\u822a\u7a7a\u3001\u7535\u5546\u3001\u5728\u7ebf\u65c5\u6e38\u548c\u7535\u4fe1\u56db\u4e2a\u573a\u666f", "result": "\u4e2d\u6027\u60c5\u7eea\u5360\u4e3b\u5bfc\uff1b\u613f\u671b\u548c\u611f\u6fc0\u662f\u6700\u5e38\u89c1\u7684\u975e\u4e2d\u6027\u60c5\u7eea\uff1b\u591a\u6807\u7b7e\u60c5\u611f\u548c\u6548\u4ef7\u6807\u6ce8\u4e00\u81f4\u6027\u4e2d\u7b49\uff0c\u5524\u9192\u5ea6\u548c\u652f\u914d\u5ea6\u8f83\u4f4e\uff1b\u81ea\u6211\u62a5\u544a\u4e0e\u7b2c\u4e09\u65b9\u6807\u6ce8\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1b\u5ba2\u89c2\u7b56\u7565\u5e38\u5f15\u53d1\u4e2d\u6027\u6216\u611f\u6fc0\uff0c\u6b21\u4f18\u7b56\u7565\u589e\u52a0\u6124\u6012\u7b49\u8d1f\u9762\u60c5\u7eea", "conclusion": "WOZ\u5b9e\u9a8c\u80fd\u6210\u529f\u5f15\u5bfc\u5bf9\u8bdd\u7ea7\u60c5\u611f\u8f68\u8ff9\uff0c\u7279\u522b\u662f\u8d1f\u9762\u76ee\u6807\uff1b\u524d\u77bb\u6027\u60c5\u611f\u63a8\u7406\u5177\u6709\u6311\u6218\u6027\uff0c\u7a81\u663e\u4e86\u4e3b\u52a8\u60c5\u611f\u611f\u77e5\u652f\u6301\u7684\u590d\u6742\u6027"}}
{"id": "2511.21912", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21912", "abs": "https://arxiv.org/abs/2511.21912", "authors": ["Karin de Langis", "William Walker", "Khanh Chi Le", "Dongyeop Kang"], "title": "Tracing How Annotators Think: Augmenting Preference Judgments with Reading Processes", "comment": null, "summary": "We propose an annotation approach that captures not only labels but also the reading process underlying annotators' decisions, e.g., what parts of the text they focus on, re-read or skim. Using this framework, we conduct a case study on the preference annotation task, creating a dataset PreferRead that contains fine-grained annotator reading behaviors obtained from mouse tracking. PreferRead enables detailed analysis of how annotators navigate between a prompt and two candidate responses before selecting their preference. We find that annotators re-read a response in roughly half of all trials, most often revisiting the option they ultimately choose, and rarely revisit the prompt. Reading behaviors are also significantly related to annotation outcomes: re-reading is associated with higher inter-annotator agreement, whereas long reading paths and times are associated with lower agreement. These results demonstrate that reading processes provide a complementary cognitive dimension for understanding annotator reliability, decision-making and disagreement in complex, subjective NLP tasks. Our code and data are publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6355\u83b7\u6807\u6ce8\u8005\u9605\u8bfb\u8fc7\u7a0b\u7684\u6807\u6ce8\u65b9\u6cd5\uff0c\u901a\u8fc7\u9f20\u6807\u8ffd\u8e2a\u521b\u5efa\u4e86\u5305\u542b\u7ec6\u7c92\u5ea6\u9605\u8bfb\u884c\u4e3a\u7684PreferRead\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5206\u6790\u504f\u597d\u6807\u6ce8\u4efb\u52a1\u4e2d\u6807\u6ce8\u8005\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "motivation": "\u73b0\u6709\u6807\u6ce8\u65b9\u6cd5\u53ea\u5173\u6ce8\u6700\u7ec8\u6807\u7b7e\uff0c\u5ffd\u7565\u4e86\u6807\u6ce8\u8005\u7684\u8ba4\u77e5\u8fc7\u7a0b\u3002\u4e3a\u4e86\u66f4\u597d\u7406\u89e3\u590d\u6742\u4e3b\u89c2NLP\u4efb\u52a1\u4e2d\u7684\u6807\u6ce8\u8005\u53ef\u9760\u6027\u3001\u51b3\u7b56\u8fc7\u7a0b\u548c\u5206\u6b67\uff0c\u9700\u8981\u6355\u6349\u6807\u6ce8\u8005\u5728\u6807\u6ce8\u8fc7\u7a0b\u4e2d\u7684\u9605\u8bfb\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6807\u6ce8\u6846\u67b6\uff0c\u4e0d\u4ec5\u8bb0\u5f55\u6807\u7b7e\uff0c\u8fd8\u901a\u8fc7\u9f20\u6807\u8ffd\u8e2a\u6355\u83b7\u6807\u6ce8\u8005\u7684\u9605\u8bfb\u8fc7\u7a0b\uff08\u5982\u5173\u6ce8\u6587\u672c\u7684\u54ea\u4e9b\u90e8\u5206\u3001\u91cd\u8bfb\u6216\u7565\u8bfb\uff09\u3002\u5728\u504f\u597d\u6807\u6ce8\u4efb\u52a1\u4e0a\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\uff0c\u521b\u5efa\u4e86PreferRead\u6570\u636e\u96c6\u3002", "result": "\u6807\u6ce8\u8005\u5728\u5927\u7ea6\u4e00\u534a\u7684\u8bd5\u9a8c\u4e2d\u4f1a\u91cd\u8bfb\u56de\u7b54\uff0c\u6700\u5e38\u91cd\u8bfb\u6700\u7ec8\u9009\u62e9\u7684\u9009\u9879\uff0c\u5f88\u5c11\u91cd\u8bfb\u63d0\u793a\u3002\u9605\u8bfb\u884c\u4e3a\u4e0e\u6807\u6ce8\u7ed3\u679c\u663e\u8457\u76f8\u5173\uff1a\u91cd\u8bfb\u4e0e\u66f4\u9ad8\u7684\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u76f8\u5173\uff0c\u800c\u8f83\u957f\u7684\u9605\u8bfb\u8def\u5f84\u548c\u65f6\u95f4\u4e0e\u8f83\u4f4e\u7684\u4e00\u81f4\u6027\u76f8\u5173\u3002", "conclusion": "\u9605\u8bfb\u8fc7\u7a0b\u4e3a\u7406\u89e3\u590d\u6742\u4e3b\u89c2NLP\u4efb\u52a1\u4e2d\u6807\u6ce8\u8005\u7684\u53ef\u9760\u6027\u3001\u51b3\u7b56\u8fc7\u7a0b\u548c\u5206\u6b67\u63d0\u4f9b\u4e86\u8865\u5145\u7684\u8ba4\u77e5\u7ef4\u5ea6\u3002\u8be5\u65b9\u6cd5\u6709\u52a9\u4e8e\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u6807\u6ce8\u8d28\u91cf\u3002"}}
{"id": "2511.21930", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21930", "abs": "https://arxiv.org/abs/2511.21930", "authors": ["Yuxin Li", "Lorraine Xu", "Meng Fan Wang"], "title": "A Comparative Study of LLM Prompting and Fine-Tuning for Cross-genre Authorship Attribution on Chinese Lyrics", "comment": "8 pages, 6 figures", "summary": "We propose a novel study on authorship attribution for Chinese lyrics, a domain where clean, public datasets are sorely lacking. Our contributions are twofold: (1) we create a new, balanced dataset of Chinese lyrics spanning multiple genres, and (2) we develop and fine-tune a domain-specific model, comparing its performance against zero-shot inference using the DeepSeek LLM.\n  We test two central hypotheses. First, we hypothesize that a fine-tuned model will outperform a zero-shot LLM baseline. Second, we hypothesize that performance is genre-dependent. Our experiments strongly confirm Hypothesis 2: structured genres (e.g. Folklore & Tradition) yield significantly higher attribution accuracy than more abstract genres (e.g. Love & Romance). Hypothesis 1 receives only partial support: fine-tuning improves robustness and generalization in Test1 (real-world data and difficult genres), but offers limited or ambiguous gains in Test2, a smaller, synthetically-augmented set. We show that the design limitations of Test2 (e.g., label imbalance, shallow lexical differences, and narrow genre sampling) can obscure the true effectiveness of fine-tuning.\n  Our work establishes the first benchmark for cross-genre Chinese lyric attribution, highlights the importance of genre-sensitive evaluation, and provides a public dataset and analytical framework for future research. We conclude with recommendations: enlarge and diversify test sets, reduce reliance on token-level data augmentation, balance author representation across genres, and investigate domain-adaptive pretraining as a pathway for improved attribution performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u9996\u4e2a\u5e73\u8861\u7684\u4e2d\u6587\u6b4c\u8bcd\u4f5c\u8005\u5f52\u5c5e\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff0c\u53d1\u73b0\u7ed3\u6784\u5316\u6d41\u6d3e\u7684\u5f52\u5c5e\u51c6\u786e\u7387\u663e\u8457\u9ad8\u4e8e\u62bd\u8c61\u6d41\u6d3e\uff0c\u5fae\u8c03\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8868\u73b0\u66f4\u597d\u4f46\u53d7\u6d4b\u8bd5\u96c6\u8bbe\u8ba1\u5f71\u54cd\u3002", "motivation": "\u4e2d\u6587\u6b4c\u8bcd\u4f5c\u8005\u5f52\u5c5e\u9886\u57df\u7f3a\u4e4f\u5e72\u51c0\u3001\u516c\u5f00\u7684\u6570\u636e\u96c6\uff0c\u9700\u8981\u5efa\u7acb\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u521b\u5efa\u5e73\u8861\u7684\u4e2d\u6587\u6b4c\u8bcd\u591a\u6d41\u6d3e\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u9886\u57df\u7279\u5b9a\u6a21\u578b\u5e76\u8fdb\u884c\u5fae\u8c03\uff0c\u4e0eDeepSeek LLM\u7684\u96f6\u6837\u672c\u63a8\u7406\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5728\u4e24\u4e2a\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7ed3\u6784\u5316\u6d41\u6d3e\uff08\u5982\u6c11\u4fd7\u4f20\u7edf\uff09\u7684\u5f52\u5c5e\u51c6\u786e\u7387\u663e\u8457\u9ad8\u4e8e\u62bd\u8c61\u6d41\u6d3e\uff08\u5982\u7231\u60c5\u6d6a\u6f2b\uff09\uff1b\u5fae\u8c03\u5728\u771f\u5b9e\u6570\u636e\u6d4b\u8bd5\u96c6\u4e0a\u63d0\u5347\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u5408\u6210\u589e\u5f3a\u6d4b\u8bd5\u96c6\u4e0a\u589e\u76ca\u6709\u9650\uff1b\u6d4b\u8bd5\u96c6\u8bbe\u8ba1\u7f3a\u9677\u4f1a\u63a9\u76d6\u5fae\u8c03\u7684\u771f\u5b9e\u6548\u679c\u3002", "conclusion": "\u5efa\u7acb\u4e86\u9996\u4e2a\u8de8\u6d41\u6d3e\u4e2d\u6587\u6b4c\u8bcd\u4f5c\u8005\u5f52\u5c5e\u57fa\u51c6\uff0c\u5f3a\u8c03\u6d41\u6d3e\u654f\u611f\u8bc4\u4f30\u7684\u91cd\u8981\u6027\uff0c\u63d0\u4f9b\u516c\u5f00\u6570\u636e\u96c6\u548c\u5206\u6790\u6846\u67b6\uff0c\u5efa\u8bae\u6269\u5927\u6d4b\u8bd5\u96c6\u591a\u6837\u6027\u3001\u51cf\u5c11\u8bcd\u7ea7\u6570\u636e\u589e\u5f3a\u3001\u5e73\u8861\u4f5c\u8005\u6d41\u6d3e\u4ee3\u8868\u6027\u3001\u63a2\u7d22\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u3002"}}
{"id": "2511.21974", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.21974", "abs": "https://arxiv.org/abs/2511.21974", "authors": ["Pamela D. Rivi\u00e8re", "Sean Trott"], "title": "Start Making Sense(s): A Developmental Probe of Attention Specialization Using Lexical Ambiguity", "comment": "13 pages (main text), 5 figures (main text) 6 pages (appendix), 6 figures (appendix), journal submission to TACL (\"a\" decision: pre-MIT Press publication version)", "summary": "Despite an in-principle understanding of self-attention matrix operations in Transformer language models (LMs), it remains unclear precisely how these operations map onto interpretable computations or functions--and how or when individual attention heads develop specialized attention patterns. Here, we present a pipeline to systematically probe attention mechanisms, and we illustrate its value by leveraging lexical ambiguity--where a single word has multiple meanings--to isolate attention mechanisms that contribute to word sense disambiguation. We take a \"developmental\" approach: first, using publicly available Pythia LM checkpoints, we identify inflection points in disambiguation performance for each LM in the suite; in 14M and 410M, we identify heads whose attention to disambiguating words covaries with overall disambiguation performance across development. We then stress-test the robustness of these heads to stimulus perturbations: in 14M, we find limited robustness, but in 410M, we identify multiple heads with surprisingly generalizable behavior. Then, in a causal analysis, we find that ablating the target heads demonstrably impairs disambiguation performance, particularly in 14M. We additionally reproduce developmental analyses of 14M across all of its random seeds. Together, these results suggest: that disambiguation benefits from a constellation of mechanisms, some of which (especially in 14M) are highly sensitive to the position and part-of-speech of the disambiguating cue; and that larger models (410M) may contain heads with more robust disambiguation behavior. They also join a growing body of work that highlights the value of adopting a developmental perspective when probing LM mechanisms.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u7cfb\u7d71\u5316\u63a2\u6e2c\u6ce8\u610f\u529b\u6a5f\u5236\u7684\u6d41\u7a0b\uff0c\u5229\u7528\u8a5e\u5f59\u6b67\u7fa9\u4f86\u5206\u96e2\u8a5e\u7fa9\u6d88\u6b67\u7684\u6ce8\u610f\u529b\u6a5f\u5236\uff0c\u767c\u73fe\u4e0d\u540c\u898f\u6a21\u6a21\u578b\u6709\u4e0d\u540c\u7279\u5fb5\uff1a\u5c0f\u6a21\u578b\u6a5f\u5236\u8f03\u8106\u5f31\uff0c\u5927\u6a21\u578b\u6709\u66f4\u7a69\u5065\u7684\u6d88\u6b67\u982d\u3002", "motivation": "\u5118\u7ba1\u7406\u8ad6\u4e0a\u7406\u89e3Transformer\u8a9e\u8a00\u6a21\u578b\u4e2d\u7684\u81ea\u6ce8\u610f\u529b\u77e9\u9663\u64cd\u4f5c\uff0c\u4f46\u4ecd\u4e0d\u6e05\u695a\u9019\u4e9b\u64cd\u4f5c\u5982\u4f55\u6620\u5c04\u5230\u53ef\u89e3\u91cb\u7684\u8a08\u7b97\u6216\u529f\u80fd\uff0c\u4ee5\u53ca\u500b\u5225\u6ce8\u610f\u529b\u982d\u4f55\u6642\u767c\u5c55\u51fa\u5c08\u9580\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u3002\u9700\u8981\u7cfb\u7d71\u5316\u65b9\u6cd5\u4f86\u63a2\u6e2c\u6ce8\u610f\u529b\u6a5f\u5236\u3002", "method": "\u63a1\u7528\"\u767c\u5c55\u6027\"\u65b9\u6cd5\uff1a1) \u4f7f\u7528Pythia LM\u6aa2\u67e5\u9ede\u8b58\u5225\u6d88\u6b67\u6027\u80fd\u7684\u8f49\u6298\u9ede\uff1b2) \u8b58\u5225\u6ce8\u610f\u529b\u8207\u6574\u9ad4\u6d88\u6b67\u6027\u80fd\u5171\u8b8a\u7684\u982d\uff1b3) \u901a\u904e\u523a\u6fc0\u64fe\u52d5\u58d3\u529b\u6e2c\u8a66\u9019\u4e9b\u982d\u7684\u7a69\u5065\u6027\uff1b4) \u9032\u884c\u56e0\u679c\u5206\u6790\uff08\u6d88\u878d\u76ee\u6a19\u982d\uff09\uff1b5) \u572814M\u6a21\u578b\u7684\u6240\u6709\u96a8\u6a5f\u7a2e\u5b50\u4e0a\u91cd\u73fe\u767c\u5c55\u5206\u6790\u3002", "result": "1) \u572814M\u548c410M\u6a21\u578b\u4e2d\u8b58\u5225\u51fa\u6ce8\u610f\u529b\u8207\u6d88\u6b67\u6027\u80fd\u5171\u8b8a\u7684\u982d\uff1b2) 14M\u6a21\u578b\u7684\u7a69\u5065\u6027\u6709\u9650\uff0c410M\u6a21\u578b\u6709\u591a\u500b\u982d\u5c55\u73fe\u9a5a\u4eba\u7684\u6cdb\u5316\u884c\u70ba\uff1b3) \u6d88\u878d\u76ee\u6a19\u982d\u6703\u640d\u5bb3\u6d88\u6b67\u6027\u80fd\uff08\u5c24\u5176\u572814M\uff09\uff1b4) \u6d88\u6b67\u53d7\u76ca\u65bc\u591a\u7a2e\u6a5f\u5236\uff0c\u5c0f\u6a21\u578b\u6a5f\u5236\u5c0d\u6d88\u6b67\u7dda\u7d22\u7684\u4f4d\u7f6e\u548c\u8a5e\u6027\u9ad8\u5ea6\u654f\u611f\uff0c\u5927\u6a21\u578b\u5305\u542b\u66f4\u7a69\u5065\u7684\u6d88\u6b67\u982d\u3002", "conclusion": "\u8a5e\u7fa9\u6d88\u6b67\u4f9d\u8cf4\u591a\u7a2e\u6a5f\u5236\uff0c\u4e0d\u540c\u898f\u6a21\u6a21\u578b\u5c55\u73fe\u4e0d\u540c\u7279\u5fb5\uff1a\u5c0f\u6a21\u578b\u6a5f\u5236\u8106\u5f31\u4e14\u5c0d\u4e0a\u4e0b\u6587\u654f\u611f\uff0c\u5927\u6a21\u578b\u767c\u5c55\u51fa\u66f4\u7a69\u5065\u7684\u6d88\u6b67\u884c\u70ba\u3002\u7814\u7a76\u5f37\u8abf\u63a1\u7528\u767c\u5c55\u8996\u89d2\u63a2\u6e2c\u8a9e\u8a00\u6a21\u578b\u6a5f\u5236\u7684\u50f9\u503c\u3002"}}
{"id": "2511.22016", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22016", "abs": "https://arxiv.org/abs/2511.22016", "authors": ["Yann Le Beux", "Oluchi Audu", "Oche D. Ankeli", "Dhananjay Balakrishnan", "Melissah Weya", "Marie D. Ralaiarinosy", "Ignatius Ezeani"], "title": "AfriStereo: A Culturally Grounded Dataset for Evaluating Stereotypical Bias in Large Language Models", "comment": null, "summary": "Existing AI bias evaluation benchmarks largely reflect Western perspectives, leaving African contexts underrepresented and enabling harmful stereotypes in applications across various domains. To address this gap, we introduce AfriStereo, the first open-source African stereotype dataset and evaluation framework grounded in local socio-cultural contexts. Through community engaged efforts across Senegal, Kenya, and Nigeria, we collected 1,163 stereotypes spanning gender, ethnicity, religion, age, and profession. Using few-shot prompting with human-in-the-loop validation, we augmented the dataset to over 5,000 stereotype-antistereotype pairs. Entries were validated through semantic clustering and manual annotation by culturally informed reviewers. Preliminary evaluation of language models reveals that nine of eleven models exhibit statistically significant bias, with Bias Preference Ratios (BPR) ranging from 0.63 to 0.78 (p <= 0.05), indicating systematic preferences for stereotypes over antistereotypes, particularly across age, profession, and gender dimensions. Domain-specific models appeared to show weaker bias in our setup, suggesting task-specific training may mitigate some associations. Looking ahead, AfriStereo opens pathways for future research on culturally grounded bias evaluation and mitigation, offering key methodologies for the AI community on building more equitable, context-aware, and globally inclusive NLP technologies.", "AI": {"tldr": "\u9996\u4e2a\u5f00\u6e90\u975e\u6d32\u523b\u677f\u5370\u8c61\u6570\u636e\u96c6AfriStereo\uff0c\u5305\u542b5000+\u523b\u677f\u5370\u8c61-\u53cd\u523b\u677f\u5370\u8c61\u5bf9\uff0c\u8bc4\u4f30\u663e\u793a11\u4e2a\u6a21\u578b\u4e2d\u67099\u4e2a\u5b58\u5728\u663e\u8457\u504f\u89c1\u3002", "motivation": "\u73b0\u6709AI\u504f\u89c1\u8bc4\u4f30\u57fa\u51c6\u4e3b\u8981\u53cd\u6620\u897f\u65b9\u89c6\u89d2\uff0c\u975e\u6d32\u80cc\u666f\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5404\u79cd\u5e94\u7528\u4e2d\u51fa\u73b0\u6709\u5bb3\u523b\u677f\u5370\u8c61\u3002\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u5efa\u7acb\u57fa\u4e8e\u5f53\u5730\u793e\u4f1a\u6587\u5316\u80cc\u666f\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u585e\u5185\u52a0\u5c14\u3001\u80af\u5c3c\u4e9a\u548c\u5c3c\u65e5\u5229\u4e9a\u7684\u793e\u533a\u53c2\u4e0e\u6536\u96c61163\u4e2a\u523b\u677f\u5370\u8c61\uff1b\u4f7f\u7528\u5c11\u6837\u672c\u63d0\u793a\u548c\u4eba\u5728\u56de\u8def\u9a8c\u8bc1\u6269\u5c55\u81f35000+\u5bf9\uff1b\u901a\u8fc7\u8bed\u4e49\u805a\u7c7b\u548c\u6587\u5316\u77e5\u60c5\u8bc4\u5ba1\u5458\u624b\u52a8\u6807\u6ce8\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8bc4\u4f3011\u4e2a\u8bed\u8a00\u6a21\u578b\uff0c\u5176\u4e2d9\u4e2a\u663e\u793a\u7edf\u8ba1\u663e\u8457\u504f\u89c1\uff08BPR 0.63-0.78\uff0cp\u22640.05\uff09\uff0c\u7cfb\u7edf\u6027\u5730\u504f\u597d\u523b\u677f\u5370\u8c61\u800c\u975e\u53cd\u523b\u677f\u5370\u8c61\uff0c\u5c24\u5176\u5728\u5e74\u9f84\u3001\u804c\u4e1a\u548c\u6027\u522b\u7ef4\u5ea6\u3002\u9886\u57df\u7279\u5b9a\u6a21\u578b\u504f\u89c1\u8f83\u5f31\u3002", "conclusion": "AfriStereo\u4e3a\u6587\u5316\u57fa\u7840\u504f\u89c1\u8bc4\u4f30\u548c\u7f13\u89e3\u7814\u7a76\u5f00\u8f9f\u9053\u8def\uff0c\u4e3aAI\u793e\u533a\u63d0\u4f9b\u6784\u5efa\u66f4\u516c\u5e73\u3001\u60c5\u5883\u611f\u77e5\u548c\u5168\u7403\u5305\u5bb9NLP\u6280\u672f\u7684\u5173\u952e\u65b9\u6cd5\u3002"}}
{"id": "2511.22036", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22036", "abs": "https://arxiv.org/abs/2511.22036", "authors": ["Jingjun Xu", "Chongshan Lin", "Haofei Yu", "Tao Feng", "Jiaxuan You"], "title": "ResearchArcade: Graph Interface for Academic Tasks", "comment": null, "summary": "Academic research generates diverse data sources, and as researchers increasingly use machine learning to assist research tasks, a crucial question arises: Can we build a unified data interface to support the development of machine learning models for various academic tasks? Models trained on such a unified interface can better support human researchers throughout the research process, eventually accelerating knowledge discovery. In this work, we introduce ResearchArcade, a graph-based interface that connects multiple academic data sources, unifies task definitions, and supports a wide range of base models to address key academic challenges. ResearchArcade utilizes a coherent multi-table format with graph structures to organize data from different sources, including academic corpora from ArXiv and peer reviews from OpenReview, while capturing information with multiple modalities, such as text, figures, and tables. ResearchArcade also preserves temporal evolution at both the manuscript and community levels, supporting the study of paper revisions as well as broader research trends over time. Additionally, ResearchArcade unifies diverse academic task definitions and supports various models with distinct input requirements. Our experiments across six academic tasks demonstrate that combining cross-source and multi-modal information enables a broader range of tasks, while incorporating graph structures consistently improves performance over baseline methods. This highlights the effectiveness of ResearchArcade and its potential to advance research progress.", "AI": {"tldr": "ResearchArcade\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u7edf\u4e00\u6570\u636e\u63a5\u53e3\uff0c\u8fde\u63a5\u591a\u4e2a\u5b66\u672f\u6570\u636e\u6e90\uff0c\u652f\u6301\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u7528\u4e8e\u5f00\u53d1\u9762\u5411\u5404\u79cd\u5b66\u672f\u4efb\u52a1\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u5b66\u672f\u7814\u7a76\u4ea7\u751f\u591a\u6837\u5316\u7684\u6570\u636e\u6e90\uff0c\u968f\u7740\u7814\u7a76\u8005\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u8f85\u52a9\u7814\u7a76\u4efb\u52a1\uff0c\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u636e\u63a5\u53e3\u6765\u652f\u6301\u5f00\u53d1\u9762\u5411\u5404\u79cd\u5b66\u672f\u4efb\u52a1\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u4ece\u800c\u66f4\u597d\u5730\u652f\u6301\u6574\u4e2a\u7814\u7a76\u8fc7\u7a0b\uff0c\u52a0\u901f\u77e5\u8bc6\u53d1\u73b0\u3002", "method": "\u63d0\u51faResearchArcade\uff0c\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u63a5\u53e3\uff0c\u91c7\u7528\u8fde\u8d2f\u7684\u591a\u8868\u683c\u5f0f\u548c\u56fe\u7ed3\u6784\u6765\u7ec4\u7ec7\u4e0d\u540c\u6765\u6e90\u7684\u6570\u636e\uff08\u5982ArXiv\u5b66\u672f\u8bed\u6599\u548cOpenReview\u540c\u884c\u8bc4\u5ba1\uff09\uff0c\u652f\u6301\u6587\u672c\u3001\u56fe\u8868\u7b49\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u5e76\u4fdd\u7559\u624b\u7a3f\u548c\u793e\u533a\u5c42\u9762\u7684\u65f6\u95f4\u6f14\u5316\u3002\u7edf\u4e00\u4e86\u591a\u6837\u5316\u7684\u5b66\u672f\u4efb\u52a1\u5b9a\u4e49\uff0c\u652f\u6301\u5177\u6709\u4e0d\u540c\u8f93\u5165\u9700\u6c42\u7684\u5404\u79cd\u6a21\u578b\u3002", "result": "\u5728\u516d\u4e2a\u5b66\u672f\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408\u8de8\u6e90\u548c\u591a\u6a21\u6001\u4fe1\u606f\u80fd\u591f\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u4efb\u52a1\u8303\u56f4\uff0c\u800c\u878d\u5165\u56fe\u7ed3\u6784\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "ResearchArcade\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u5177\u6709\u63a8\u52a8\u7814\u7a76\u8fdb\u5c55\u7684\u6f5c\u529b\uff0c\u4e3a\u5b66\u672f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6570\u636e\u63a5\u53e3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22038", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22038", "abs": "https://arxiv.org/abs/2511.22038", "authors": ["Rochana Chaturvedi", "Yue Zhou", "Andrew Boyd", "Brian T. Layden", "Mudassir Rashid", "Lu Cheng", "Ali Cinar", "Barbara Di Eugenio"], "title": "Early Risk Prediction with Temporally and Contextually Grounded Clinical Language Processing", "comment": null, "summary": "Clinical notes in Electronic Health Records (EHRs) capture rich temporal information on events, clinician reasoning, and lifestyle factors often missing from structured data. Leveraging them for predictive modeling can be impactful for timely identification of chronic diseases. However, they present core natural language processing (NLP) challenges: long text, irregular event distribution, complex temporal dependencies, privacy constraints, and resource limitations. We present two complementary methods for temporally and contextually grounded risk prediction from longitudinal notes. First, we introduce HiTGNN, a hierarchical temporal graph neural network that integrates intra-note temporal event structures, inter-visit dynamics, and medical knowledge to model patient trajectories with fine-grained temporal granularity. Second, we propose ReVeAL, a lightweight, test-time framework that distills the reasoning of large language models into smaller verifier models. Applied to opportunistic screening for Type 2 Diabetes (T2D) using temporally realistic cohorts curated from private and public hospital corpora, HiTGNN achieves the highest predictive accuracy, especially for near-term risk, while preserving privacy and limiting reliance on large proprietary models. ReVeAL enhances sensitivity to true T2D cases and retains explanatory reasoning. Our ablations confirm the value of temporal structure and knowledge augmentation, and fairness analysis shows HiTGNN performs more equitably across subgroups.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u79cd\u4e92\u8865\u7684\u65b9\u6cd5\uff08HiTGNN\u548cReVeAL\uff09\u7528\u4e8e\u4ece\u7eb5\u5411\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u8fdb\u884c\u65f6\u95f4\u4e0a\u4e0b\u6587\u98ce\u9669\u9884\u6d4b\uff0c\u5e94\u7528\u4e8e2\u578b\u7cd6\u5c3f\u75c5\u7b5b\u67e5\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u4e34\u5e8a\u7b14\u8bb0\u5305\u542b\u4e30\u5bcc\u7684\u65f6\u95f4\u4fe1\u606f\uff0c\u4f46\u9762\u4e34\u957f\u6587\u672c\u3001\u4e0d\u89c4\u5219\u4e8b\u4ef6\u5206\u5e03\u3001\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\u3001\u9690\u79c1\u9650\u5236\u548c\u8d44\u6e90\u9650\u5236\u7b49NLP\u6311\u6218\uff0c\u9700\u8981\u6709\u6548\u65b9\u6cd5\u8fdb\u884c\u6162\u6027\u75c5\u98ce\u9669\u9884\u6d4b\u3002", "method": "1. HiTGNN\uff1a\u5206\u5c42\u65f6\u5e8f\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u6574\u5408\u7b14\u8bb0\u5185\u65f6\u95f4\u4e8b\u4ef6\u7ed3\u6784\u3001\u5c31\u8bca\u95f4\u52a8\u6001\u548c\u533b\u5b66\u77e5\u8bc6\uff0c\u4ee5\u7ec6\u7c92\u5ea6\u65f6\u95f4\u7c92\u5ea6\u5efa\u6a21\u60a3\u8005\u8f68\u8ff9\uff1b2. ReVeAL\uff1a\u8f7b\u91cf\u7ea7\u6d4b\u8bd5\u65f6\u6846\u67b6\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u84b8\u998f\u5230\u8f83\u5c0f\u7684\u9a8c\u8bc1\u5668\u6a21\u578b\u4e2d\u3002", "result": "\u57282\u578b\u7cd6\u5c3f\u75c5\u7b5b\u67e5\u5e94\u7528\u4e2d\uff0cHiTGNN\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff08\u7279\u522b\u662f\u8fd1\u671f\u98ce\u9669\uff09\uff0c\u540c\u65f6\u4fdd\u62a4\u9690\u79c1\u5e76\u51cf\u5c11\u5bf9\u5927\u578b\u4e13\u6709\u6a21\u578b\u7684\u4f9d\u8d56\uff1bReVeAL\u63d0\u9ad8\u4e86\u5bf9\u771f\u5b9e\u75c5\u4f8b\u7684\u654f\u611f\u6027\u5e76\u4fdd\u7559\u4e86\u53ef\u89e3\u91ca\u6027\u63a8\u7406\u3002", "conclusion": "\u4e24\u79cd\u65b9\u6cd5\u4e92\u8865\uff0c\u80fd\u6709\u6548\u5229\u7528\u4e34\u5e8a\u7b14\u8bb0\u7684\u65f6\u95f4\u7ed3\u6784\u8fdb\u884c\u98ce\u9669\u9884\u6d4b\uff0c\u6d88\u878d\u5b9e\u9a8c\u786e\u8ba4\u4e86\u65f6\u95f4\u7ed3\u6784\u548c\u77e5\u8bc6\u589e\u5f3a\u7684\u4ef7\u503c\uff0c\u516c\u5e73\u6027\u5206\u6790\u663e\u793aHiTGNN\u5728\u4e0d\u540c\u4e9a\u7ec4\u4e2d\u8868\u73b0\u66f4\u516c\u5e73\u3002"}}
{"id": "2511.22109", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22109", "abs": "https://arxiv.org/abs/2511.22109", "authors": ["Gia Bao Hoang", "Keith J Ransom", "Rachel Stephens", "Carolyn Semmler", "Nicolas Fay", "Lewis Mitchell"], "title": "A Hybrid Theory and Data-driven Approach to Persuasion Detection with Large Language Models", "comment": null, "summary": "Traditional psychological models of belief revision focus on face-to-face interactions, but with the rise of social media, more effective models are needed to capture belief revision at scale, in this rich text-based online discourse. Here, we use a hybrid approach, utilizing large language models (LLMs) to develop a model that predicts successful persuasion using features derived from psychological experiments.\n  Our approach leverages LLM generated ratings of features previously examined in the literature to build a random forest classification model that predicts whether a message will result in belief change. Of the eight features tested, \\textit{epistemic emotion} and \\textit{willingness to share} were the top-ranking predictors of belief change in the model. Our findings provide insights into the characteristics of persuasive messages and demonstrate how LLMs can enhance models of successful persuasion based on psychological theory. Given these insights, this work has broader applications in fields such as online influence detection and misinformation mitigation, as well as measuring the effectiveness of online narratives.", "AI": {"tldr": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u5fc3\u7406\u5b66\u7279\u5f81\u9884\u6d4b\u793e\u4ea4\u5a92\u4f53\u8bf4\u670d\u6548\u679c\uff0c\u53d1\u73b0\u8ba4\u77e5\u60c5\u7eea\u548c\u5206\u4eab\u610f\u613f\u662f\u4fe1\u5ff5\u6539\u53d8\u7684\u5173\u952e\u9884\u6d4b\u56e0\u5b50", "motivation": "\u4f20\u7edf\u5fc3\u7406\u5b66\u4fe1\u5ff5\u4fee\u6b63\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u9762\u5bf9\u9762\u4e92\u52a8\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u793e\u4ea4\u5a92\u4f53\u5927\u89c4\u6a21\u6587\u672c\u4ea4\u6d41\u4e2d\u7684\u8bf4\u670d\u8fc7\u7a0b\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u6a21\u578b\u6765\u9884\u6d4b\u5728\u7ebf\u8bf4\u670d\u6548\u679c", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5fc3\u7406\u5b66\u6587\u732e\u4e2d\u5df2\u7814\u7a76\u7684\u7279\u5f81\u8bc4\u5206\uff0c\u6784\u5efa\u968f\u673a\u68ee\u6797\u5206\u7c7b\u6a21\u578b\u9884\u6d4b\u4fe1\u606f\u662f\u5426\u4f1a\u5bfc\u81f4\u4fe1\u5ff5\u6539\u53d8", "result": "\u6d4b\u8bd5\u7684\u516b\u4e2a\u7279\u5f81\u4e2d\uff0c\u8ba4\u77e5\u60c5\u7eea\u548c\u5206\u4eab\u610f\u613f\u662f\u4fe1\u5ff5\u6539\u53d8\u7684\u6700\u5f3a\u9884\u6d4b\u56e0\u5b50\uff0c\u6a21\u578b\u6210\u529f\u8bc6\u522b\u4e86\u8bf4\u670d\u6027\u4fe1\u606f\u7684\u5173\u952e\u7279\u5f81", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u8bf4\u670d\u6027\u4fe1\u606f\u7684\u7279\u5f81\uff0c\u5c55\u793a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u589e\u5f3a\u57fa\u4e8e\u5fc3\u7406\u5b66\u7406\u8bba\u7684\u8bf4\u670d\u6a21\u578b\uff0c\u5728\u5728\u7ebf\u5f71\u54cd\u529b\u68c0\u6d4b\u3001\u9519\u8bef\u4fe1\u606f\u7f13\u89e3\u548c\u53d9\u4e8b\u6548\u679c\u8bc4\u4f30\u65b9\u9762\u6709\u5e7f\u6cdb\u5e94\u7528"}}
{"id": "2511.22141", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22141", "abs": "https://arxiv.org/abs/2511.22141", "authors": ["Shuhei Yamashita", "Daiki Shirafuji", "Tatsuhiko Saito"], "title": "Bridging the Modality Gap by Similarity Standardization with Pseudo-Positive Samples", "comment": "Accepted to PACLIC2025", "summary": "Advances in vision-language models (VLMs) have enabled effective cross-modality retrieval. However, when both text and images exist in the database, similarity scores would differ in scale by modality. This phenomenon, known as the modality gap, hinders accurate retrieval. Most existing studies address this issue with manually labeled data, e.g., by fine-tuning VLMs on them. In this work, we propose a similarity standardization approach with pseudo data construction. We first compute the mean and variance of the similarity scores between each query and its paired data in text or image modality. Using these modality-specific statistics, we standardize all similarity scores to compare on a common scale across modalities. These statistics are calculated from pseudo pairs, which are constructed by retrieving the text and image candidates with the highest cosine similarity to each query. We evaluate our method across seven VLMs using two multi-modal QA benchmarks (MMQA and WebQA), where each question requires retrieving either text or image data. Our experimental results show that our method significantly improves retrieval performance, achieving average Recall@20 gains of 64% on MMQA and 28% on WebQA when the query and the target data belong to different modalities. Compared to E5-V, which addresses the modality gap through image captioning, we confirm that our method more effectively bridges the modality gap.", "AI": {"tldr": "\u63d0\u51fa\u76f8\u4f3c\u5ea6\u6807\u51c6\u5316\u65b9\u6cd5\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6a21\u6001\u9e3f\u6c9f\u95ee\u9898\uff0c\u901a\u8fc7\u4f2a\u6570\u636e\u6784\u5efa\u8ba1\u7b97\u6a21\u6001\u7279\u5b9a\u7edf\u8ba1\u91cf\uff0c\u5c06\u4e0d\u540c\u6a21\u6001\u7684\u76f8\u4f3c\u5ea6\u5206\u6570\u6807\u51c6\u5316\u5230\u7edf\u4e00\u5c3a\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u8de8\u6a21\u6001\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u5f53\u6570\u636e\u5e93\u540c\u65f6\u5305\u542b\u6587\u672c\u548c\u56fe\u50cf\u65f6\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u7684\u76f8\u4f3c\u5ea6\u5206\u6570\u5728\u4e0d\u540c\u6a21\u6001\u95f4\u5b58\u5728\u5c3a\u5ea6\u5dee\u5f02\uff08\u6a21\u6001\u9e3f\u6c9f\uff09\uff0c\u8fd9\u963b\u788d\u4e86\u51c6\u786e\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u6210\u672c\u8f83\u9ad8\u3002", "method": "\u63d0\u51fa\u76f8\u4f3c\u5ea6\u6807\u51c6\u5316\u65b9\u6cd5\uff1a1\uff09\u4e3a\u6bcf\u4e2a\u67e5\u8be2\u6784\u5efa\u4f2a\u914d\u5bf9\u6570\u636e\uff08\u68c0\u7d22\u4f59\u5f26\u76f8\u4f3c\u5ea6\u6700\u9ad8\u7684\u6587\u672c\u548c\u56fe\u50cf\u5019\u9009\uff09\uff1b2\uff09\u8ba1\u7b97\u67e5\u8be2\u4e0e\u5176\u914d\u5bf9\u6570\u636e\u5728\u6587\u672c\u548c\u56fe\u50cf\u6a21\u6001\u4e0a\u7684\u76f8\u4f3c\u5ea6\u5747\u503c\u548c\u65b9\u5dee\uff1b3\uff09\u4f7f\u7528\u8fd9\u4e9b\u6a21\u6001\u7279\u5b9a\u7edf\u8ba1\u91cf\u5c06\u6240\u6709\u76f8\u4f3c\u5ea6\u5206\u6570\u6807\u51c6\u5316\u5230\u8de8\u6a21\u6001\u7684\u7edf\u4e00\u5c3a\u5ea6\u4e0a\u3002", "result": "\u57287\u4e2aVLM\u6a21\u578b\u548c\u4e24\u4e2a\u591a\u6a21\u6001QA\u57fa\u51c6\uff08MMQA\u548cWebQA\uff09\u4e0a\u8bc4\u4f30\uff0c\u5f53\u67e5\u8be2\u4e0e\u76ee\u6807\u6570\u636e\u5c5e\u4e8e\u4e0d\u540c\u6a21\u6001\u65f6\uff0c\u5e73\u5747Recall@20\u63d0\u534764%\uff08MMQA\uff09\u548c28%\uff08WebQA\uff09\u3002\u76f8\u6bd4\u901a\u8fc7\u56fe\u50cf\u63cf\u8ff0\u89e3\u51b3\u6a21\u6001\u9e3f\u6c9f\u7684E5-V\u65b9\u6cd5\uff0c\u672c\u65b9\u6cd5\u66f4\u6709\u6548\u5730\u5f25\u5408\u4e86\u6a21\u6001\u9e3f\u6c9f\u3002", "conclusion": "\u63d0\u51fa\u7684\u76f8\u4f3c\u5ea6\u6807\u51c6\u5316\u65b9\u6cd5\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6570\u636e\uff0c\u901a\u8fc7\u4f2a\u6570\u636e\u6784\u5efa\u548c\u7edf\u8ba1\u6807\u51c6\u5316\u6709\u6548\u89e3\u51b3\u4e86VLM\u4e2d\u7684\u6a21\u6001\u9e3f\u6c9f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u68c0\u7d22\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22146", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22146", "abs": "https://arxiv.org/abs/2511.22146", "authors": ["Kairong Han", "Nuanqiao Shan", "Ziyu Zhao", "Zijing Hu", "Xinpeng Dong", "Junjian Ye", "Lujia Pan", "Fei Wu", "Kun Kuang"], "title": "C$^2$DLM: Causal Concept-Guided Diffusion Large Language Models", "comment": null, "summary": "Autoregressive (AR) language models and Diffusion Language Models (DLMs) constitute the two principal paradigms of large language models. However, both paradigms suffer from insufficient reasoning capabilities. Human reasoning inherently relies on causal knowledge and thought, which are reflected in natural language. But in the AR paradigm, language is modeled as next token prediction (a strictly left-to-right, token-by-token order), whereas natural language itself exhibits more flexible causal structures. In the DLM paradigm, the attention mechanism is fully connected, which entirely disregards causal order. To fill this gap, we propose a \\underline{\\textbf{C}}ausal \\underline{\\textbf{C}}oncept-Guided \\underline{\\textbf{D}}iffusion \\underline{\\textbf{L}}anguage \\underline{\\textbf{M}}odel (C$^2$DLM). Starting from DLM's fully connected attention, C$^2$DLM first obtains a concept-level causal graph from the teacher model, and then explicitly guides attention to learn causal relationships between concepts. By focusing on causal relationships and avoiding interference from difficult subgoals involving causal inversion, C$^2$DLM improves 12\\% with about 3.2 times training speedup in the COT-OrderPerturb task, and achieves an average gain of 1.31\\% across six downstream reasoning tasks. More details in the repository ~\\href{https://github.com/Kairong-Han/C-2-DLM}{here}.", "AI": {"tldr": "\u63d0\u51faC\u00b2DLM\u6a21\u578b\uff0c\u901a\u8fc7\u6982\u5ff5\u7ea7\u56e0\u679c\u56fe\u5f15\u5bfc\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u56e0\u679c\u5173\u7cfb\uff0c\u63d0\u5347\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u81ea\u7136\u8bed\u8a00\u4e2d\u7684\u56e0\u679c\u7ed3\u6784\u3002\u81ea\u56de\u5f52\u6a21\u578b\u53d7\u9650\u4e8e\u4ece\u5de6\u5230\u53f3\u7684token\u9884\u6d4b\uff0c\u6269\u6563\u6a21\u578b\u5219\u5b8c\u5168\u5ffd\u7565\u56e0\u679c\u987a\u5e8f\uff0c\u800c\u4eba\u7c7b\u63a8\u7406\u4f9d\u8d56\u4e8e\u56e0\u679c\u77e5\u8bc6\u548c\u601d\u7ef4\u3002", "method": "\u4ece\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u5b8c\u5168\u8fde\u63a5\u6ce8\u610f\u529b\u673a\u5236\u51fa\u53d1\uff0c\u9996\u5148\u4ece\u6559\u5e08\u6a21\u578b\u83b7\u53d6\u6982\u5ff5\u7ea7\u56e0\u679c\u56fe\uff0c\u7136\u540e\u663e\u5f0f\u5f15\u5bfc\u6ce8\u610f\u529b\u5b66\u4e60\u6982\u5ff5\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002\u901a\u8fc7\u805a\u7126\u56e0\u679c\u5173\u7cfb\u5e76\u907f\u514d\u56e0\u679c\u53cd\u8f6c\u7684\u56f0\u96be\u5b50\u76ee\u6807\u5e72\u6270\u3002", "result": "\u5728COT-OrderPerturb\u4efb\u52a1\u4e2d\u63d0\u534712%\uff0c\u8bad\u7ec3\u901f\u5ea6\u52a0\u5feb\u7ea63.2\u500d\uff1b\u5728\u516d\u4e2a\u4e0b\u6e38\u63a8\u7406\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u53471.31%\u3002", "conclusion": "C\u00b2DLM\u901a\u8fc7\u5f15\u5165\u6982\u5ff5\u7ea7\u56e0\u679c\u6307\u5bfc\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u5efa\u6a21\u65b9\u9762\u7684\u7a7a\u767d\u3002"}}
{"id": "2511.22153", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22153", "abs": "https://arxiv.org/abs/2511.22153", "authors": ["Sepyan Purnama Kristanto", "Lutfi Hakim"], "title": "A Theoretically Grounded Hybrid Ensemble for Reliable Detection of LLM-Generated Text", "comment": "24 pages", "summary": "The rapid proliferation of Large Language Models (LLMs) has blurred the line between human and machine authorship, creating practical risks for academic integrity and information reliability. Existing text detectors typically rely on a single methodological paradigm and suffer from poor generalization and high false positive rates (FPR), especially on high-stakes academic text. We propose a theoretically grounded hybrid ensemble that systematically fuses three complementary detection paradigms: (i) a RoBERTa-based transformer classifier for deep semantic feature extraction, (ii) a GPT-2-based probabilistic detector using perturbation-induced likelihood curvature, and (iii) a statistical linguistic feature analyzer capturing stylometric patterns. The core novelty lies in an optimized weighted voting framework, where ensemble weights are learned on the probability simplex to maximize F1-score rather than set heuristically. We provide a bias-variance analysis and empirically demonstrate low inter-model correlation (rho ~ 0.35-0.42), a key condition for variance reduction. Evaluated on a large-scale, multigenerator corpus of 30,000 documents, our system achieves 94.2% accuracy and an AUC of 0.978, with a 35% relative reduction in false positives on academic text. This yields a more reliable and ethically responsible detector for real-world deployment in education and other high-stakes domains.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u878d\u5408\u4e09\u79cd\u4e92\u8865\u68c0\u6d4b\u8303\u5f0f\u7684\u6df7\u5408\u96c6\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4bLLM\u751f\u6210\u7684\u6587\u672c\uff0c\u5728\u5b66\u672f\u6587\u672c\u4e0a\u5b9e\u73b094.2%\u7684\u51c6\u786e\u7387\u548c35%\u7684\u5047\u9633\u6027\u964d\u4f4e\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u6269\u6563\u6a21\u7cca\u4e86\u4eba\u7c7b\u4e0e\u673a\u5668\u4f5c\u8005\u8eab\u4efd\u7684\u754c\u9650\uff0c\u5bf9\u5b66\u672f\u8bda\u4fe1\u548c\u4fe1\u606f\u53ef\u9760\u6027\u6784\u6210\u5b9e\u9645\u98ce\u9669\u3002\u73b0\u6709\u6587\u672c\u68c0\u6d4b\u5668\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u65b9\u6cd5\u8303\u5f0f\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u5047\u9633\u6027\u7387\u9ad8\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u7684\u5b66\u672f\u6587\u672c\u4e0a\u3002", "method": "\u63d0\u51fa\u7406\u8bba\u57fa\u7840\u7684\u6df7\u5408\u96c6\u6210\u65b9\u6cd5\uff0c\u7cfb\u7edf\u878d\u5408\u4e09\u79cd\u4e92\u8865\u68c0\u6d4b\u8303\u5f0f\uff1a1) \u57fa\u4e8eRoBERTa\u7684transformer\u5206\u7c7b\u5668\u7528\u4e8e\u6df1\u5ea6\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\uff1b2) \u57fa\u4e8eGPT-2\u7684\u6982\u7387\u68c0\u6d4b\u5668\u4f7f\u7528\u6270\u52a8\u8bf1\u5bfc\u4f3c\u7136\u66f2\u7387\uff1b3) \u7edf\u8ba1\u8bed\u8a00\u7279\u5f81\u5206\u6790\u5668\u6355\u6349\u6587\u4f53\u8ba1\u91cf\u6a21\u5f0f\u3002\u6838\u5fc3\u521b\u65b0\u5728\u4e8e\u4f18\u5316\u7684\u52a0\u6743\u6295\u7968\u6846\u67b6\uff0c\u5176\u4e2d\u96c6\u6210\u6743\u91cd\u5728\u6982\u7387\u5355\u7eaf\u5f62\u4e0a\u5b66\u4e60\u4ee5\u6700\u5927\u5316F1\u5206\u6570\u800c\u975e\u542f\u53d1\u5f0f\u8bbe\u7f6e\u3002", "result": "\u5728\u5305\u542b30,000\u4e2a\u6587\u6863\u7684\u5927\u89c4\u6a21\u591a\u751f\u6210\u5668\u8bed\u6599\u5e93\u4e0a\u8bc4\u4f30\uff0c\u7cfb\u7edf\u8fbe\u523094.2%\u7684\u51c6\u786e\u7387\u548c0.978\u7684AUC\uff0c\u5728\u5b66\u672f\u6587\u672c\u4e0a\u5047\u9633\u6027\u76f8\u5bf9\u964d\u4f4e35%\u3002\u6a21\u578b\u95f4\u76f8\u5173\u6027\u4f4e(rho ~ 0.35-0.42)\uff0c\u8fd9\u662f\u65b9\u5dee\u51cf\u5c11\u7684\u5173\u952e\u6761\u4ef6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6559\u80b2\u548c\u5176\u5b83\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u53ef\u9760\u548c\u9053\u5fb7\u8d1f\u8d23\u4efb\u7684\u68c0\u6d4b\u5668\uff0c\u901a\u8fc7\u7406\u8bba\u57fa\u7840\u7684\u6df7\u5408\u96c6\u6210\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u68c0\u6d4b\u5668\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.22173", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22173", "abs": "https://arxiv.org/abs/2511.22173", "authors": ["Young-Jun Lee", "Seungone Kim", "Byung-Kwan Lee", "Minkyeong Moon", "Yechan Hwang", "Jong Myoung Kim", "Graham Neubig", "Sean Welleck", "Ho-Jin Choi"], "title": "RefineBench: Evaluating Refinement Capability of Language Models via Checklists", "comment": "Project website: https://passing2961.github.io/refinebench-page/", "summary": "Can language models (LMs) self-refine their own responses? This question is increasingly relevant as a wide range of real-world user interactions involve refinement requests. However, prior studies have largely tested LMs' refinement abilities on verifiable tasks such as competition math or symbolic reasoning with simplified scaffolds, whereas users often pose open-ended queries and provide varying degrees of feedback on what they desire. The recent advent of reasoning models that exhibit self-reflection patterns in their chains-of-thought further motivates this question. To analyze this, we introduce RefineBench, a benchmark of 1,000 challenging problems across 11 domains paired with a checklist-based evaluation framework. We evaluate two refinement modes: (1) guided refinement, where an LM is provided natural language feedback, and (2) self-refinement, where LMs attempt to improve without guidance. In the self-refinement setting, even frontier LMs such as Gemini 2.5 Pro and GPT-5 achieve modest baseline scores of 31.3% and 29.1%, respectively, and most models fail to consistently improve across iterations (e.g., Gemini-2.5-Pro gains only +1.8%, while DeepSeek-R1 declines by -0.1%). By contrast, in guided refinement, both proprietary LMs and large open-weight LMs (>70B) can leverage targeted feedback to refine responses to near-perfect levels within five turns. These findings suggest that frontier LMs require breakthroughs to self-refine their incorrect responses, and that RefineBench provides a valuable testbed for tracking progress.", "AI": {"tldr": "\u8bed\u8a00\u6a21\u578b\u5728\u5f15\u5bfc\u5f0f\u7cbe\u70bc\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u81ea\u6211\u7cbe\u70bc\u65b9\u9762\u4ecd\u6709\u5c40\u9650\uff0c\u9700\u8981\u7a81\u7834\u6027\u8fdb\u5c55\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u81ea\u6211\u7cbe\u70bc\u5176\u54cd\u5e94\uff0c\u56e0\u4e3a\u73b0\u5b9e\u7528\u6237\u4ea4\u4e92\u5e38\u6d89\u53ca\u7cbe\u70bc\u8bf7\u6c42\uff0c\u800c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u53ef\u9a8c\u8bc1\u4efb\u52a1\u800c\u975e\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u5f15\u5165RefineBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b11\u4e2a\u9886\u57df\u76841000\u4e2a\u6311\u6218\u6027\u95ee\u9898\uff0c\u91c7\u7528\u68c0\u67e5\u8868\u8bc4\u4f30\u6846\u67b6\uff0c\u8bc4\u4f30\u5f15\u5bfc\u5f0f\u7cbe\u70bc\uff08\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u53cd\u9988\uff09\u548c\u81ea\u6211\u7cbe\u70bc\uff08\u65e0\u6307\u5bfc\uff09\u4e24\u79cd\u6a21\u5f0f\u3002", "result": "\u5728\u81ea\u6211\u7cbe\u70bc\u6a21\u5f0f\u4e0b\uff0c\u5373\u4f7f\u662f\u524d\u6cbf\u6a21\u578b\u5982Gemini 2.5 Pro\u548cGPT-5\u4e5f\u4ec5\u83b7\u5f9731.3%\u548c29.1%\u7684\u57fa\u51c6\u5206\u6570\uff0c\u591a\u6570\u6a21\u578b\u65e0\u6cd5\u5728\u8fed\u4ee3\u4e2d\u6301\u7eed\u6539\u8fdb\uff1b\u800c\u5728\u5f15\u5bfc\u5f0f\u7cbe\u70bc\u4e2d\uff0c\u5927\u578b\u6a21\u578b\u80fd\u5728\u4e94\u8f6e\u5185\u5c06\u54cd\u5e94\u7cbe\u70bc\u81f3\u63a5\u8fd1\u5b8c\u7f8e\u6c34\u5e73\u3002", "conclusion": "\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u9700\u8981\u7a81\u7834\u6027\u8fdb\u5c55\u624d\u80fd\u81ea\u6211\u7cbe\u70bc\u9519\u8bef\u54cd\u5e94\uff0cRefineBench\u4e3a\u8ddf\u8e2a\u8fdb\u5c55\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2511.22176", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22176", "abs": "https://arxiv.org/abs/2511.22176", "authors": ["Lukas Struppek", "Dominik Hintersdorf", "Hannah Struppek", "Daniel Neider", "Kristian Kersting"], "title": "Focused Chain-of-Thought: Efficient LLM Reasoning via Structured Input Information", "comment": null, "summary": "Recent large language models achieve strong reasoning performance by generating detailed chain-of-thought traces, but this often leads to excessive token use and high inference latency. Existing efficiency approaches typically focus on model-centric interventions, such as reinforcement learning or supervised fine-tuning, to reduce verbosity. In contrast, we propose a training-free, input-centric approach. Inspired by cognitive psychology, we introduce Focused Chain-of-Thought (F-CoT), which separates information extraction from the reasoning process. F-CoT first organizes the essential information from a query into a concise, structured context and then guides the model to reason exclusively over this context. By preventing attention to irrelevant details, F-CoT naturally produces shorter reasoning paths. On arithmetic word problems, F-CoT reduces generated tokens by 2-3x while maintaining accuracy comparable to standard zero-shot CoT. These results highlight structured input as a simple yet effective lever for more efficient LLM reasoning.", "AI": {"tldr": "F-CoT\u901a\u8fc7\u5c06\u4fe1\u606f\u63d0\u53d6\u4e0e\u63a8\u7406\u8fc7\u7a0b\u5206\u79bb\uff0c\u5148\u7ec4\u7ec7\u67e5\u8be2\u4e2d\u7684\u5173\u952e\u4fe1\u606f\u5230\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u4e2d\uff0c\u7136\u540e\u5f15\u5bfc\u6a21\u578b\u4ec5\u5728\u6b64\u4e0a\u4e0b\u6587\u4e2d\u63a8\u7406\uff0c\u4ece\u800c\u51cf\u5c11\u751f\u6210token\u6570\u91cf2-3\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6\u96f6\u6837\u672cCoT\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u751f\u6210\u8be6\u7ec6\u7684\u601d\u7ef4\u94fe\u5b9e\u73b0\u5f3a\u5927\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8fd9\u5bfc\u81f4\u8fc7\u591a\u7684token\u4f7f\u7528\u548c\u9ad8\u63a8\u7406\u5ef6\u8fdf\u3002\u73b0\u6709\u6548\u7387\u65b9\u6cd5\u901a\u5e38\u5173\u6ce8\u6a21\u578b\u4e2d\u5fc3\u5e72\u9884\uff08\u5982\u5f3a\u5316\u5b66\u4e60\u6216\u76d1\u7763\u5fae\u8c03\uff09\u6765\u51cf\u5c11\u5197\u4f59\uff0c\u4f46\u672c\u6587\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u3001\u8f93\u5165\u4e2d\u5fc3\u7684\u65b9\u6cd5\u3002", "method": "\u53d7\u8ba4\u77e5\u5fc3\u7406\u5b66\u542f\u53d1\uff0c\u63d0\u51faFocused Chain-of-Thought (F-CoT)\uff0c\u5c06\u4fe1\u606f\u63d0\u53d6\u4e0e\u63a8\u7406\u8fc7\u7a0b\u5206\u79bb\u3002\u9996\u5148\u5c06\u67e5\u8be2\u4e2d\u7684\u5173\u952e\u4fe1\u606f\u7ec4\u7ec7\u6210\u7b80\u6d01\u7684\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\uff0c\u7136\u540e\u5f15\u5bfc\u6a21\u578b\u4ec5\u5728\u6b64\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u63a8\u7406\uff0c\u907f\u514d\u5173\u6ce8\u65e0\u5173\u7ec6\u8282\u3002", "result": "\u5728\u7b97\u672f\u6587\u5b57\u95ee\u9898\u4e0a\uff0cF-CoT\u5c06\u751f\u6210\u7684token\u51cf\u5c112-3\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6\u96f6\u6837\u672cCoT\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002\u8fd9\u8bc1\u660e\u4e86\u7ed3\u6784\u5316\u8f93\u5165\u4f5c\u4e3a\u63d0\u9ad8LLM\u63a8\u7406\u6548\u7387\u7684\u7b80\u5355\u800c\u6709\u6548\u7684\u6760\u6746\u3002", "conclusion": "\u7ed3\u6784\u5316\u8f93\u5165\u662f\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6548\u7387\u7684\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0cF-CoT\u901a\u8fc7\u5206\u79bb\u4fe1\u606f\u63d0\u53d6\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u81ea\u7136\u4ea7\u751f\u66f4\u77ed\u7684\u63a8\u7406\u8def\u5f84\uff0c\u4e3a\u8bad\u7ec3\u81ea\u7531\u7684\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2511.22258", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22258", "abs": "https://arxiv.org/abs/2511.22258", "authors": ["Guifeng Wang", "Yuanfeng Song", "Meng Yang", "Tao Zhu", "Xiaoming Yin", "Xing Chen"], "title": "Beyond Query-Level Comparison: Fine-Grained Reinforcement Learning for Text-to-SQL with Automated Interpretable Critiques", "comment": null, "summary": "Text-to-SQL, a pivotal natural language processing (NLP) task that converts textual queries into executable SQL, has seen substantial progress in recent years. However, existing evaluation and reward mechanisms used to train and assess the text-to-SQL models remain a critical bottleneck. Current approaches heavily rely on manually annotated gold SQL queries, which are costly to produce and impractical for large-scale evaluation. More importantly, most reinforcement learning (RL) methods in text-to-SQL leverage only the final binary execution outcome as the reward signal, a coarse-grained supervision that overlooks detailed structural and semantic errors from the perspective of rubrics. To address these challenges, we propose RuCo-C, a novel generative judge model for fine-grained, query-specific automatic evaluation using interpretable critiques without human intervention. Our framework first automatically generates query-specific evaluation rubrics for human-free annotation, linking them to interpretable critiques. Subsequently, it integrates densified reward feedback through a \"progressive exploration\" strategy during the RL training process, which dynamically adjusts the rewards to enhance the model's performance. Comprehensive experiments demonstrate that RuCo-C outperforms existing methods in text-to-SQL evaluation, yielding significant performance gains.", "AI": {"tldr": "RuCo-C\u662f\u4e00\u4e2a\u7528\u4e8e\u6587\u672c\u5230SQL\u4efb\u52a1\u7684\u751f\u6210\u5f0f\u8bc4\u4f30\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u67e5\u8be2\u7279\u5b9a\u7684\u8bc4\u4f30\u6807\u51c6\u548c\u53ef\u89e3\u91ca\u7684\u6279\u8bc4\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u5e76\u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u901a\u8fc7\"\u6e10\u8fdb\u63a2\u7d22\"\u7b56\u7565\u6574\u5408\u5bc6\u96c6\u5956\u52b1\u53cd\u9988\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230SQL\u4efb\u52a1\u5b58\u5728\u8bc4\u4f30\u74f6\u9888\uff1a1) \u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u9ec4\u91d1SQL\u67e5\u8be2\uff1b2) \u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4ec5\u4f7f\u7528\u6700\u7ec8\u4e8c\u5143\u6267\u884c\u7ed3\u679c\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u8fd9\u79cd\u7c97\u7c92\u5ea6\u76d1\u7763\u5ffd\u7565\u4e86\u8be6\u7ec6\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u9519\u8bef\u3002", "method": "1) \u81ea\u52a8\u751f\u6210\u67e5\u8be2\u7279\u5b9a\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5e76\u5c06\u5176\u4e0e\u53ef\u89e3\u91ca\u7684\u6279\u8bc4\u76f8\u5173\u8054\uff1b2) \u5728\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u901a\u8fc7\"\u6e10\u8fdb\u63a2\u7d22\"\u7b56\u7565\u6574\u5408\u5bc6\u96c6\u5956\u52b1\u53cd\u9988\uff0c\u52a8\u6001\u8c03\u6574\u5956\u52b1\u4ee5\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cRuCo-C\u5728\u6587\u672c\u5230SQL\u8bc4\u4f30\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "RuCo-C\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6807\u51c6\u548c\u53ef\u89e3\u91ca\u6279\u8bc4\uff0c\u89e3\u51b3\u4e86\u6587\u672c\u5230SQL\u4efb\u52a1\u4e2d\u8bc4\u4f30\u548c\u5956\u52b1\u673a\u5236\u7684\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u76d1\u7763\u4fe1\u53f7\u3002"}}
{"id": "2511.22312", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22312", "abs": "https://arxiv.org/abs/2511.22312", "authors": ["Anjaneya Praharaj", "Jaykumar Kasundra"], "title": "Token-Level Marginalization for Multi-Label LLM Classifiers", "comment": null, "summary": "This paper addresses the critical challenge of deriving interpretable confidence scores from generative language models (LLMs) when applied to multi-label content safety classification. While models like LLaMA Guard are effective for identifying unsafe content and its categories, their generative architecture inherently lacks direct class-level probabilities, which hinders model confidence assessment and performance interpretation. This limitation complicates the setting of dynamic thresholds for content moderation and impedes fine-grained error analysis. This research proposes and evaluates three novel token-level probability estimation approaches to bridge this gap. The aim is to enhance model interpretability and accuracy, and evaluate the generalizability of this framework across different instruction-tuned models. Through extensive experimentation on a synthetically generated, rigorously annotated dataset, it is demonstrated that leveraging token logits significantly improves the interpretability and reliability of generative classifiers, enabling more nuanced content safety moderation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e09\u79cd\u65b0\u9896\u7684token\u7ea7\u6982\u7387\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4e3a\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6807\u7b7e\u5185\u5bb9\u5b89\u5168\u5206\u7c7b\u4e2d\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u89e3\u51b3\u4e86LLaMA Guard\u7b49\u6a21\u578b\u7f3a\u4e4f\u76f4\u63a5\u7c7b\u522b\u6982\u7387\u7684\u95ee\u9898\u3002", "motivation": "\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\uff08\u5982LLaMA Guard\uff09\u5728\u591a\u6807\u7b7e\u5185\u5bb9\u5b89\u5168\u5206\u7c7b\u4e2d\u867d\u7136\u6709\u6548\uff0c\u4f46\u5176\u751f\u6210\u5f0f\u67b6\u6784\u7f3a\u4e4f\u76f4\u63a5\u7684\u7c7b\u522b\u7ea7\u6982\u7387\uff0c\u8fd9\u963b\u788d\u4e86\u6a21\u578b\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u548c\u6027\u80fd\u89e3\u91ca\uff0c\u4f7f\u5f97\u5185\u5bb9\u5ba1\u6838\u7684\u52a8\u6001\u9608\u503c\u8bbe\u7f6e\u548c\u7ec6\u7c92\u5ea6\u9519\u8bef\u5206\u6790\u53d8\u5f97\u590d\u6742\u3002", "method": "\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u9896\u7684token\u7ea7\u6982\u7387\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528token logits\u6765\u6865\u63a5\u751f\u6210\u5f0f\u6a21\u578b\u4e0e\u6982\u7387\u8f93\u51fa\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u5728\u4e00\u4e2a\u7ecf\u8fc7\u4e25\u683c\u6807\u6ce8\u7684\u5408\u6210\u751f\u6210\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5229\u7528token logits\u80fd\u663e\u8457\u63d0\u9ad8\u751f\u6210\u5f0f\u5206\u7c7b\u5668\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\uff0c\u5b9e\u73b0\u66f4\u7ec6\u81f4\u7684\u5185\u5bb9\u5b89\u5168\u5ba1\u6838\uff0c\u5e76\u8bc4\u4f30\u4e86\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684token\u7ea7\u6982\u7387\u4f30\u8ba1\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6807\u7b7e\u5185\u5bb9\u5b89\u5168\u5206\u7c7b\u4e2d\u7f3a\u4e4f\u7f6e\u4fe1\u5ea6\u5206\u6570\u7684\u95ee\u9898\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u5185\u5bb9\u5b89\u5168\u5ba1\u6838\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.22313", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22313", "abs": "https://arxiv.org/abs/2511.22313", "authors": ["Zahri Aksa Dautd", "Aviv Yuniar Rahman"], "title": "Sentiment Analysis Of Shopee Product Reviews Using Distilbert", "comment": "6 pages, 11 figures", "summary": "The rapid growth of digital commerce has led to the accumulation of a massive number of consumer reviews on online platforms. Shopee, as one of the largest e-commerce platforms in Southeast Asia, receives millions of product reviews every day containing valuable information regarding customer satisfaction and preferences. Manual analysis of these reviews is inefficient, thus requiring a computational approach such as sentiment analysis. This study examines the use of DistilBERT, a lightweight transformer-based deep learning model, for sentiment classification on Shopee product reviews. The dataset used consists of approximately one million English-language reviews that have been preprocessed and trained using the distilbert-base-uncased model. Evaluation was conducted using accuracy, precision, recall, and F1-score metrics, and compared against benchmark models such as BERT and SVM. The results show that DistilBERT achieved an accuracy of 94.8%, slightly below BERT (95.3%) but significantly higher than SVM (90.2%), with computation time reduced by more than 55%. These findings demonstrate that DistilBERT provides an optimal balance between accuracy and efficiency, making it suitable for large scale sentiment analysis on e-commerce platforms. Keywords: Sentiment Analysis, DistilBERT, Shopee Reviews, Natural Language Processing, Deep Learning, Transformer Models.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u8f7b\u91cf\u7ea7Transformer\u6a21\u578bDistilBERT\u5bf9Shopee\u5e73\u53f0\u7ea6100\u4e07\u6761\u82f1\u6587\u4ea7\u54c1\u8bc4\u8bba\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\uff0894.8%\uff09\u7684\u540c\u65f6\uff0c\u8ba1\u7b97\u65f6\u95f4\u6bd4BERT\u51cf\u5c1155%\u4ee5\u4e0a\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u7387\u4e0e\u6548\u7387\u7684\u826f\u597d\u5e73\u8861\u3002", "motivation": "\u968f\u7740\u7535\u5b50\u5546\u52a1\u7684\u5feb\u901f\u53d1\u5c55\uff0cShopee\u7b49\u5e73\u53f0\u6bcf\u5929\u4ea7\u751f\u6d77\u91cf\u6d88\u8d39\u8005\u8bc4\u8bba\uff0c\u624b\u52a8\u5206\u6790\u6548\u7387\u4f4e\u4e0b\u3002\u9700\u8981\u91c7\u7528\u8ba1\u7b97\u65b9\u6cd5\u6765\u6316\u6398\u8fd9\u4e9b\u8bc4\u8bba\u4e2d\u7684\u5ba2\u6237\u6ee1\u610f\u5ea6\u548c\u504f\u597d\u4fe1\u606f\uff0c\u56e0\u6b64\u7814\u7a76\u9ad8\u6548\u7684\u60c5\u611f\u5206\u6790\u6280\u672f\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002", "method": "\u4f7f\u7528DistilBERT\uff08distilbert-base-uncased\u6a21\u578b\uff09\u5bf9\u7ea6100\u4e07\u6761\u82f1\u6587Shopee\u4ea7\u54c1\u8bc4\u8bba\u8fdb\u884c\u9884\u5904\u7406\u548c\u8bad\u7ec3\uff0c\u91c7\u7528\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u4e0eBERT\u548cSVM\u7b49\u57fa\u51c6\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "DistilBERT\u8fbe\u523094.8%\u7684\u51c6\u786e\u7387\uff0c\u7565\u4f4e\u4e8eBERT\uff0895.3%\uff09\uff0c\u4f46\u663e\u8457\u9ad8\u4e8eSVM\uff0890.2%\uff09\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u8ba1\u7b97\u65f6\u95f4\u6bd4BERT\u51cf\u5c11\u8d85\u8fc755%\uff0c\u5728\u51c6\u786e\u7387\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "conclusion": "DistilBERT\u4e3a\u5927\u89c4\u6a21\u7535\u5546\u5e73\u53f0\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u51c6\u786e\u7387\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6700\u4f73\u5e73\u8861\uff0c\u9002\u5408\u5904\u7406\u6d77\u91cf\u8bc4\u8bba\u6570\u636e\uff0c\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u7406\u60f3\u9009\u62e9\u3002"}}
{"id": "2511.22315", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22315", "abs": "https://arxiv.org/abs/2511.22315", "authors": ["Bakhtawar Abdalla", "Rebwar Mala Nabi", "Hassan Eshkiki", "Fabio Caraffini"], "title": "Named Entity Recognition for the Kurdish Sorani Language: Dataset Creation and Comparative Analysis", "comment": null, "summary": "This work contributes towards balancing the inclusivity and global applicability of natural language processing techniques by proposing the first 'name entity recognition' dataset for Kurdish Sorani, a low-resource and under-represented language, that consists of 64,563 annotated tokens. It also provides a tool for facilitating this task in this and many other languages and performs a thorough comparative analysis, including classic machine learning models and neural systems. The results obtained challenge established assumptions about the advantage of neural approaches within the context of NLP. Conventional methods, in particular CRF, obtain F1-scores of 0.825, outperforming the results of BiLSTM-based models (0.706) significantly. These findings indicate that simpler and more computationally efficient classical frameworks can outperform neural architectures in low-resource settings.", "AI": {"tldr": "\u4e3a\u5e93\u5c14\u5fb7\u7d22\u62c9\u5c3c\u8bed\u521b\u5efa\u9996\u4e2aNER\u6570\u636e\u96c6\uff0864,563\u6807\u6ce8\u8bcd\u5143\uff09\uff0c\u5f00\u53d1\u591a\u8bed\u8a00NER\u5de5\u5177\uff0c\u53d1\u73b0\u4f20\u7edfCRF\u65b9\u6cd5\uff08F1=0.825\uff09\u663e\u8457\u4f18\u4e8eBiLSTM\u6a21\u578b\uff08F1=0.706\uff09\uff0c\u6311\u6218\u4e86\u795e\u7ecf\u7f51\u7edc\u5728\u4f4e\u8d44\u6e90NLP\u4e2d\u7684\u4f18\u52bf\u5047\u8bbe\u3002", "motivation": "\u5e73\u8861NLP\u6280\u672f\u7684\u5305\u5bb9\u6027\u548c\u5168\u7403\u9002\u7528\u6027\uff0c\u4e3a\u4f4e\u8d44\u6e90\u3001\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u5e93\u5c14\u5fb7\u7d22\u62c9\u5c3c\u8bed\u521b\u5efa\u9996\u4e2a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6570\u636e\u96c6\uff0c\u4fc3\u8fdb\u8bed\u8a00\u591a\u6837\u6027\u3002", "method": "\u521b\u5efa\u5e93\u5c14\u5fb7\u7d22\u62c9\u5c3c\u8bedNER\u6570\u636e\u96c6\uff0864,563\u6807\u6ce8\u8bcd\u5143\uff09\uff0c\u5f00\u53d1\u591a\u8bed\u8a00NER\u5de5\u5177\uff0c\u8fdb\u884c\u7cfb\u7edf\u6bd4\u8f83\u5206\u6790\uff0c\u5305\u62ec\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5982CRF\uff09\u548c\u795e\u7ecf\u7f51\u7edc\u7cfb\u7edf\uff08\u5982BiLSTM\uff09\u3002", "result": "\u4f20\u7edfCRF\u65b9\u6cd5\u83b7\u5f970.825\u7684F1\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8eBiLSTM\u6a21\u578b\u76840.706\uff0c\u6311\u6218\u4e86\u795e\u7ecf\u7f51\u7edc\u5728NLP\u4e2d\u7684\u4f18\u52bf\u5047\u8bbe\uff0c\u8868\u660e\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\uff0c\u66f4\u7b80\u5355\u3001\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u7684\u7ecf\u5178\u6846\u67b6\u53ef\u4ee5\u8d85\u8d8a\u795e\u7ecf\u67b6\u6784\u3002", "conclusion": "\u5728\u4f4e\u8d44\u6e90NLP\u8bbe\u7f6e\u4e2d\uff0c\u7b80\u5355\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u7ecf\u5178\u65b9\u6cd5\uff08\u5982CRF\uff09\u53ef\u4ee5\u4f18\u4e8e\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u8fd9\u5bf9NLP\u7814\u7a76\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u548c\u65b9\u6cd5\u9009\u62e9\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.22402", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22402", "abs": "https://arxiv.org/abs/2511.22402", "authors": ["Srivarshinee Sridhar", "Raghav Kaushik Ravi", "Kripabandhu Ghosh"], "title": "Mapping Clinical Doubt: Locating Linguistic Uncertainty in LLMs", "comment": "Accepted to AAAI'26 SECURE-AI4H Workshop", "summary": "Large Language Models (LLMs) are increasingly used in clinical settings, where sensitivity to linguistic uncertainty can influence diagnostic interpretation and decision-making. Yet little is known about where such epistemic cues are internally represented within these models. Distinct from uncertainty quantification, which measures output confidence, this work examines input-side representational sensitivity to linguistic uncertainty in medical text. We curate a contrastive dataset of clinical statements varying in epistemic modality (e.g., 'is consistent with' vs. 'may be consistent with') and propose Model Sensitivity to Uncertainty (MSU), a layerwise probing metric that quantifies activation-level shifts induced by uncertainty cues. Our results show that LLMs exhibit structured, depth-dependent sensitivity to clinical uncertainty, suggesting that epistemic information is progressively encoded in deeper layers. These findings reveal how linguistic uncertainty is internally represented in LLMs, offering insight into their interpretability and epistemic reliability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4e34\u5e8a\u6587\u672c\u4e2d\u8bed\u8a00\u4e0d\u786e\u5b9a\u6027\u654f\u611f\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6570\u636e\u96c6\u548c\u5c42\u95f4\u63a2\u6d4b\u6307\u6807MSU\uff0c\u53d1\u73b0LLMs\u5bf9\u4e34\u5e8a\u4e0d\u786e\u5b9a\u6027\u5177\u6709\u7ed3\u6784\u5316\u3001\u6df1\u5ea6\u4f9d\u8d56\u7684\u654f\u611f\u6027\u3002", "motivation": "LLMs\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5bf9\u8bed\u8a00\u4e0d\u786e\u5b9a\u6027\u7684\u5185\u90e8\u8868\u5f81\u673a\u5236\u4e86\u89e3\u751a\u5c11\u3002\u4e0e\u8f93\u51fa\u7f6e\u4fe1\u5ea6\u4e0d\u540c\uff0c\u672c\u6587\u5173\u6ce8\u8f93\u5165\u4fa7\u5bf9\u8bed\u8a00\u4e0d\u786e\u5b9a\u6027\u7684\u8868\u5f81\u654f\u611f\u6027\uff0c\u8fd9\u5bf9\u4e34\u5e8a\u8bca\u65ad\u89e3\u91ca\u548c\u51b3\u7b56\u5236\u5b9a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e0d\u540c\u8ba4\u8bc6\u6a21\u6001\uff08\u5982\"is consistent with\" vs. \"may be consistent with\"\uff09\u7684\u5bf9\u6bd4\u6027\u4e34\u5e8a\u9648\u8ff0\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u654f\u611f\u6027\uff08MSU\uff09\u6307\u6807\uff0c\u901a\u8fc7\u5c42\u95f4\u63a2\u6d4b\u91cf\u5316\u6fc0\u6d3b\u6c34\u5e73\u53d8\u5316\u3002", "result": "LLMs\u5bf9\u4e34\u5e8a\u4e0d\u786e\u5b9a\u6027\u8868\u73b0\u51fa\u7ed3\u6784\u5316\u3001\u6df1\u5ea6\u4f9d\u8d56\u7684\u654f\u611f\u6027\uff0c\u8868\u660e\u8ba4\u8bc6\u4fe1\u606f\u5728\u66f4\u6df1\u5c42\u9010\u6e10\u7f16\u7801\u3002\u6a21\u578b\u5bf9\u4e0d\u786e\u5b9a\u6027\u7ebf\u7d22\u7684\u654f\u611f\u6027\u968f\u7f51\u7edc\u6df1\u5ea6\u53d8\u5316\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5185\u90e8\u5982\u4f55\u8868\u5f81\u8bed\u8a00\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u8ba4\u8bc6\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u4e34\u5e8a\u73af\u5883\u4e2dLLMs\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002"}}
{"id": "2511.22482", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.22482", "abs": "https://arxiv.org/abs/2511.22482", "authors": ["Isabel Gon\u00e7alves", "Paulo Cavalin", "Claudio Pinhanez"], "title": "Exploring Performance Variations in Finetuned Translators of Ultra-Low Resource Languages: Do Linguistic Differences Matter?", "comment": null, "summary": "Finetuning pre-trained language models with small amounts of data is a commonly-used method to create translators for ultra-low resource languages such as endangered Indigenous languages. However, previous works have reported substantially different performances with translators created using similar methodology and data. In this work we systematically explored possible causes of the performance difference, aiming to determine whether it was a product of different cleaning procedures, limitations of the pre-trained models, the size of the base model, or the size of the training dataset, studying both directions of translation. Our studies, using two Brazilian Indigenous languages, related but with significant structural linguistic characteristics, indicated none or very limited influence from those training factors, suggesting differences between languages may play a significant role in the ability to produce translators by fine-tuning pre-trained models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f7f\u7528\u76f8\u4f3c\u65b9\u6cd5\u548c\u6570\u636e\u521b\u5efa\u7684\u7ffb\u8bd1\u5668\u6027\u80fd\u5dee\u5f02\u4e3b\u8981\u6e90\u4e8e\u8bed\u8a00\u672c\u8eab\u5dee\u5f02\uff0c\u800c\u975e\u6570\u636e\u6e05\u6d17\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u9650\u5236\u3001\u57fa\u7840\u6a21\u578b\u5927\u5c0f\u6216\u8bad\u7ec3\u6570\u636e\u91cf\u7b49\u8bad\u7ec3\u56e0\u7d20\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u76f8\u4f3c\u65b9\u6cd5\u548c\u5c11\u91cf\u6570\u636e\u5fae\u8c03\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u521b\u5efa\u8d85\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u6fd2\u5371\u571f\u8457\u8bed\u8a00\uff09\u7ffb\u8bd1\u5668\u65f6\uff0c\u6027\u80fd\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u63a2\u7d22\u8fd9\u4e9b\u6027\u80fd\u5dee\u5f02\u7684\u53ef\u80fd\u539f\u56e0\u3002", "method": "\u7cfb\u7edf\u7814\u7a76\u53ef\u80fd\u5f71\u54cd\u6027\u80fd\u7684\u56e0\u7d20\uff1a\u6570\u636e\u6e05\u6d17\u7a0b\u5e8f\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u9650\u5236\u3001\u57fa\u7840\u6a21\u578b\u5927\u5c0f\u3001\u8bad\u7ec3\u6570\u636e\u96c6\u5927\u5c0f\uff0c\u5e76\u7814\u7a76\u53cc\u5411\u7ffb\u8bd1\u3002\u4f7f\u7528\u4e24\u79cd\u76f8\u5173\u4f46\u5177\u6709\u663e\u8457\u7ed3\u6784\u8bed\u8a00\u5b66\u7279\u5f81\u7684\u5df4\u897f\u571f\u8457\u8bed\u8a00\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u4e9b\u8bad\u7ec3\u56e0\u7d20\u5bf9\u7ffb\u8bd1\u5668\u6027\u80fd\u6ca1\u6709\u5f71\u54cd\u6216\u5f71\u54cd\u975e\u5e38\u6709\u9650\u3002\u8bed\u8a00\u4e4b\u95f4\u7684\u5dee\u5f02\u53ef\u80fd\u5728\u901a\u8fc7\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u521b\u5efa\u7ffb\u8bd1\u5668\u7684\u80fd\u529b\u4e2d\u8d77\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "\u7ffb\u8bd1\u5668\u6027\u80fd\u5dee\u5f02\u4e3b\u8981\u6e90\u4e8e\u8bed\u8a00\u672c\u8eab\u7684\u7ed3\u6784\u548c\u7279\u6027\u5dee\u5f02\uff0c\u800c\u975e\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u6280\u672f\u56e0\u7d20\u3002\u8fd9\u5bf9\u8d85\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ffb\u8bd1\u7814\u7a76\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2511.22503", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2511.22503", "abs": "https://arxiv.org/abs/2511.22503", "authors": ["Katia Vendrame", "Bolaji Yusuf", "Santosh Kesiraju", "\u0160imon Sedl\u00e1\u010dek", "Old\u0159ich Plchot", "Jan \u010cernock\u00fd"], "title": "Joint Speech and Text Training for LLM-Based End-to-End Spoken Dialogue State Tracking", "comment": "submitted to ICASSP 2026", "summary": "End-to-end spoken dialogue state tracking (DST) is made difficult by the tandem of having to handle speech input and data scarcity. Combining speech foundation encoders and large language models has been proposed in recent work as to alleviate some of this difficulty. Although this approach has been shown to result in strong spoken DST models, achieving state-of-the-art performance in realistic multi-turn DST, it struggles to generalize across domains and requires annotated spoken DST training data for each domain of interest. However, collecting such data for every target domain is both costly and difficult. Noting that textual DST data is more easily obtained for various domains, in this work, we propose jointly training on available spoken DST data and written textual data from other domains as a way to achieve cross-domain generalization. We conduct experiments which show the efficacy of our proposed method for getting good cross-domain DST performance without relying on spoken training data from the target domains.", "AI": {"tldr": "\u63d0\u51fa\u8054\u5408\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6709\u53e3\u8bed\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u6570\u636e\u548c\u8de8\u9886\u57df\u6587\u672c\u6570\u636e\uff0c\u5b9e\u73b0\u65e0\u9700\u76ee\u6807\u9886\u57df\u53e3\u8bed\u8bad\u7ec3\u6570\u636e\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b", "motivation": "\u7aef\u5230\u7aef\u53e3\u8bed\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u8bed\u97f3\u8f93\u5165\u5904\u7406\u548c\u6570\u636e\u7a00\u7f3a\u3002\u73b0\u6709\u65b9\u6cd5\u7ed3\u5408\u8bed\u97f3\u57fa\u7840\u7f16\u7801\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u5355\u9886\u57df\u8868\u73b0\u4f18\u79c0\u4f46\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u4e14\u9700\u8981\u4e3a\u6bcf\u4e2a\u76ee\u6807\u9886\u57df\u6536\u96c6\u6602\u8d35\u7684\u53e3\u8bed\u6807\u6ce8\u6570\u636e\u3002\u6587\u672c\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u6570\u636e\u76f8\u5bf9\u5bb9\u6613\u83b7\u53d6\uff0c\u56e0\u6b64\u63a2\u7d22\u5982\u4f55\u5229\u7528\u8de8\u9886\u57df\u6587\u672c\u6570\u636e\u63d0\u5347\u53e3\u8bed\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u7684\u6cdb\u5316\u80fd\u529b", "method": "\u63d0\u51fa\u8054\u5408\u8bad\u7ec3\u65b9\u6cd5\uff0c\u540c\u65f6\u4f7f\u7528\u53ef\u7528\u7684\u53e3\u8bed\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u6570\u636e\u548c\u5176\u4ed6\u9886\u57df\u7684\u4e66\u9762\u6587\u672c\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002\u901a\u8fc7\u591a\u9886\u57df\u6587\u672c\u6570\u636e\u589e\u5f3a\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u51cf\u5c11\u5bf9\u76ee\u6807\u9886\u57df\u53e3\u8bed\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u80fd\u591f\u5728\u4e0d\u9700\u8981\u76ee\u6807\u9886\u57df\u53e3\u8bed\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u83b7\u5f97\u826f\u597d\u7684\u8de8\u9886\u57df\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u6027\u80fd", "conclusion": "\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u53e3\u8bed\u548c\u8de8\u9886\u57df\u6587\u672c\u6570\u636e\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u53e3\u8bed\u5bf9\u8bdd\u72b6\u6001\u8ddf\u8e2a\u6a21\u578b\u7684\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u964d\u4f4e\u5bf9\u6602\u8d35\u53e3\u8bed\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.22582", "categories": ["cs.CL", "math.RA"], "pdf": "https://arxiv.org/pdf/2511.22582", "abs": "https://arxiv.org/abs/2511.22582", "authors": ["Matilde Marcolli", "Richard Larson", "Riny Huijbregts"], "title": "Extension Condition \"violations\" and Merge optimality constraints", "comment": "85 pages", "summary": "We analyze, using the mathematical formulation of Merge within the Strong Minimalist Thesis framework, a set of linguistic phenomena, including head-to-head movement, phrasal affixes and syntactic cliticization, verb-particle alternation, and operator-variable phenomena. These are often regarded as problematic, as violations of the Extension Condition. We show that, in fact, all of these phenomena can be explained without involving any EC violation. We first show that derivations using Sideward Merge are possible for all of these cases: these respect EC, though they involve some amount of optimality violations, with respect to Resource Restrictions cost functions, andthe amount of violation differs among these cases. We show that all the cases that involve large optimality violations can be derived in alternative ways involving neither EC nor the use of SM. The main remaining case (head-to-head movement) only involves SM with minimal violations of optimality (near equilibrium fluctuations). We analyze explicitly also the cases of multiple wh-fronting, clusters of clitics in Romance languages and possessor agreement construction in Korean, and how an explanation of these phenomena based on SM can be made compatible with the colored operad generators for phases and theta roles. We also show that the EC condition has a clear algebraic meaning in the mathematical formulation of Merge and is therefore an intrinsic structural algebraic constraint of the model, rather than an additional assumption. We also show that the minimal optimality violating SM plays a structural role in the Markovian properties of Merge, and we compare different optimality conditions coming from Minimal Search and from Resource Restriction in terms of their effect on the dynamics of the Hopf algebra Markov chain, in a simple explicit example.", "AI": {"tldr": "\u672c\u6587\u5728\u5f3a\u6700\u7b80\u8bba\u6846\u67b6\u4e0b\uff0c\u4f7f\u7528\u5408\u5e76\u7684\u6570\u5b66\u5f62\u5f0f\u5316\u5206\u6790\u4e86\u4e00\u7cfb\u5217\u8bed\u8a00\u73b0\u8c61\uff0c\u8bc1\u660e\u8fd9\u4e9b\u770b\u4f3c\u8fdd\u53cd\u6269\u5c55\u6761\u4ef6\u7684\u73b0\u8c61\u5b9e\u9645\u4e0a\u90fd\u80fd\u5728\u4e0d\u8fdd\u53cdEC\u7684\u60c5\u51b5\u4e0b\u5f97\u5230\u89e3\u91ca\u3002", "motivation": "\u8bb8\u591a\u8bed\u8a00\u73b0\u8c61\uff08\u5982\u4e2d\u5fc3\u8bed\u79fb\u52a8\u3001\u77ed\u8bed\u8bcd\u7f00\u3001\u53e5\u6cd5\u9644\u7740\u3001\u52a8\u8bcd-\u5c0f\u54c1\u8bcd\u4ea4\u66ff\u3001\u7b97\u5b50-\u53d8\u9879\u73b0\u8c61\uff09\u5e38\u88ab\u89c6\u4e3a\u8fdd\u53cd\u6269\u5c55\u6761\u4ef6\uff0c\u672c\u6587\u65e8\u5728\u8bc1\u660e\u8fd9\u4e9b\u73b0\u8c61\u5b9e\u9645\u4e0a\u90fd\u80fd\u5728\u4e0d\u8fdd\u53cdEC\u7684\u60c5\u51b5\u4e0b\u5f97\u5230\u5408\u7406\u89e3\u91ca\u3002", "method": "\u4f7f\u7528\u4fa7\u5411\u5408\u5e76\u63a8\u5bfc\u6240\u6709\u73b0\u8c61\uff0c\u5206\u6790\u4e0d\u540c\u60c5\u51b5\u4e0b\u7684\u6700\u4f18\u6027\u8fdd\u53cd\u7a0b\u5ea6\uff1b\u5bf9\u4e8e\u4e25\u91cd\u8fdd\u53cd\u6700\u4f18\u6027\u7684\u60c5\u51b5\uff0c\u63d0\u4f9b\u4e0d\u6d89\u53caEC\u548cSM\u7684\u66ff\u4ee3\u63a8\u5bfc\uff1b\u5206\u6790\u591a\u91cdwh-\u524d\u7f6e\u3001\u7f57\u66fc\u8bed\u9644\u7740\u8bed\u7c07\u3001\u97e9\u8bed\u9886\u5c5e\u4e00\u81f4\u7b49\u73b0\u8c61\u4e0eSM\u7684\u517c\u5bb9\u6027\u3002", "result": "\u8bc1\u660e\u6240\u6709\u73b0\u8c61\u90fd\u80fd\u5728\u4e0d\u8fdd\u53cdEC\u7684\u60c5\u51b5\u4e0b\u89e3\u91ca\uff1bEC\u5728\u5408\u5e76\u7684\u6570\u5b66\u5f62\u5f0f\u5316\u4e2d\u5177\u6709\u6e05\u6670\u7684\u4ee3\u6570\u610f\u4e49\uff0c\u662f\u6a21\u578b\u7684\u5185\u5728\u7ed3\u6784\u7ea6\u675f\u800c\u975e\u989d\u5916\u5047\u8bbe\uff1b\u6700\u5c0f\u6700\u4f18\u6027\u8fdd\u53cd\u7684SM\u5728\u5408\u5e76\u7684\u9a6c\u5c14\u53ef\u592b\u6027\u8d28\u4e2d\u8d77\u7ed3\u6784\u4f5c\u7528\u3002", "conclusion": "\u6269\u5c55\u6761\u4ef6\u662f\u5408\u5e76\u6570\u5b66\u5f62\u5f0f\u5316\u7684\u5185\u5728\u4ee3\u6570\u7ea6\u675f\uff0c\u770b\u4f3c\u8fdd\u53cdEC\u7684\u8bed\u8a00\u73b0\u8c61\u5b9e\u9645\u4e0a\u90fd\u80fd\u901a\u8fc7\u4fa7\u5411\u5408\u5e76\u7b49\u673a\u5236\u5f97\u5230\u5408\u7406\u89e3\u91ca\uff0c\u6700\u5c0f\u6700\u4f18\u6027\u8fdd\u53cd\u7684SM\u5728\u8bed\u8a00\u63a8\u5bfc\u7684\u52a8\u6001\u8fc7\u7a0b\u4e2d\u5177\u6709\u7ed3\u6784\u91cd\u8981\u6027\u3002"}}
{"id": "2511.22584", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22584", "abs": "https://arxiv.org/abs/2511.22584", "authors": ["Chao Feng", "Zihan Liu", "Siddhant Gupta", "Gongpei Cui", "Jan von der Assen", "Burkhard Stiller"], "title": "Smarter, not Bigger: Fine-Tuned RAG-Enhanced LLMs for Automotive HIL Testing", "comment": null, "summary": "Hardware-in-the-Loop (HIL) testing is essential for automotive validation but suffers from fragmented and underutilized test artifacts. This paper presents HIL-GPT, a retrieval-augmented generation (RAG) system integrating domain-adapted large language models (LLMs) with semantic retrieval. HIL-GPT leverages embedding fine-tuning using a domain-specific dataset constructed via heuristic mining and LLM-assisted synthesis, combined with vector indexing for scalable, traceable test case and requirement retrieval. Experiments show that fine-tuned compact models, such as \\texttt{bge-base-en-v1.5}, achieve a superior trade-off between accuracy, latency, and cost compared to larger models, challenging the notion that bigger is always better. An A/B user study further confirms that RAG-enhanced assistants improve perceived helpfulness, truthfulness, and satisfaction over general-purpose LLMs. These findings provide insights for deploying efficient, domain-aligned LLM-based assistants in industrial HIL environments.", "AI": {"tldr": "HIL-GPT\uff1a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u9886\u57df\u9002\u914d\u7684LLM\u548c\u8bed\u4e49\u68c0\u7d22\u6574\u5408\uff0c\u4f18\u5316\u6c7d\u8f66\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\u4e2d\u7684\u6d4b\u8bd5\u5de5\u4ef6\u5229\u7528\uff0c\u5728\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u548c\u6210\u672c\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\u3002", "motivation": "\u6c7d\u8f66\u786c\u4ef6\u5728\u73af\uff08HIL\uff09\u6d4b\u8bd5\u5b58\u5728\u6d4b\u8bd5\u5de5\u4ef6\u788e\u7247\u5316\u548c\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u8ffd\u6eaf\u7684\u6d4b\u8bd5\u7528\u4f8b\u548c\u9700\u6c42\u68c0\u7d22\u65b9\u6cd5\u3002", "method": "\u63d0\u51faHIL-GPT\u7cfb\u7edf\uff0c\u7ed3\u5408\u9886\u57df\u9002\u914d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u8bed\u4e49\u68c0\u7d22\uff0c\u901a\u8fc7\u542f\u53d1\u5f0f\u6316\u6398\u548cLLM\u8f85\u52a9\u5408\u6210\u6784\u5efa\u9886\u57df\u7279\u5b9a\u6570\u636e\u96c6\u8fdb\u884c\u5d4c\u5165\u5fae\u8c03\uff0c\u5e76\u4f7f\u7528\u5411\u91cf\u7d22\u5f15\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u68c0\u7d22\u3002", "result": "\u5fae\u8c03\u540e\u7684\u7d27\u51d1\u6a21\u578b\uff08\u5982bge-base-en-v1.5\uff09\u5728\u51c6\u786e\u6027\u3001\u5ef6\u8fdf\u548c\u6210\u672c\u65b9\u9762\u4f18\u4e8e\u5927\u578b\u6a21\u578b\uff1bA/B\u7528\u6237\u7814\u7a76\u8868\u660eRAG\u589e\u5f3a\u52a9\u624b\u5728\u5e2e\u52a9\u6027\u3001\u771f\u5b9e\u6027\u548c\u6ee1\u610f\u5ea6\u65b9\u9762\u4f18\u4e8e\u901a\u7528LLM\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5de5\u4e1aHIL\u73af\u5883\u4e2d\u90e8\u7f72\u9ad8\u6548\u3001\u9886\u57df\u5bf9\u9f50\u7684LLM\u52a9\u624b\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u6311\u6218\u4e86\"\u8d8a\u5927\u8d8a\u597d\"\u7684\u89c2\u5ff5\uff0c\u5c55\u793a\u4e86\u7d27\u51d1\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u7684\u4f18\u52bf\u3002"}}
{"id": "2511.22612", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22612", "abs": "https://arxiv.org/abs/2511.22612", "authors": ["Guilherme Sousa", "Rinaldo Lima", "Cassia Trojahn"], "title": "Improving LLM-based Ontology Matching with fine-tuning on synthetic data", "comment": null, "summary": "Large Language Models (LLMs) are increasingly being integrated into various components of Ontology Matching pipelines. This paper investigates the capability of LLMs to perform ontology matching directly on ontology modules and generate the corresponding alignments. Furthermore, it is explored how a dedicated fine-tuning strategy can enhance the model's matching performance in a zero-shot setting. The proposed method incorporates a search space reduction technique to select relevant subsets from both source and target ontologies, which are then used to automatically construct prompts. Recognizing the scarcity of reference alignments for training, a novel LLM-based approach is introduced for generating a synthetic dataset. This process creates a corpus of ontology submodule pairs and their corresponding reference alignments, specifically designed to fine-tune an LLM for the ontology matching task. The proposed approach was evaluated on the Conference, Geolink, Enslaved, Taxon, and Hydrography datasets from the OAEI complex track. The results demonstrate that the LLM fine-tuned on the synthetically generated data exhibits superior performance compared to the non-fine-tuned base model. The key contribution is a strategy that combines automatic dataset generation with fine-tuning to effectively adapt LLMs for ontology matching tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u52a8\u6570\u636e\u96c6\u751f\u6210\u548c\u5fae\u8c03\u7684\u7b56\u7565\uff0c\u4f7fLLM\u80fd\u591f\u6709\u6548\u6267\u884c\u672c\u4f53\u5339\u914d\u4efb\u52a1\uff0c\u901a\u8fc7\u641c\u7d22\u7a7a\u95f4\u7f29\u51cf\u548c\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u6765\u63d0\u5347\u96f6\u6837\u672c\u5339\u914d\u6027\u80fd\u3002", "motivation": "\u968f\u7740LLM\u8d8a\u6765\u8d8a\u591a\u5730\u96c6\u6210\u5230\u672c\u4f53\u5339\u914d\u6d41\u7a0b\u4e2d\uff0c\u9700\u8981\u63a2\u7d22LLM\u76f4\u63a5\u6267\u884c\u672c\u4f53\u5339\u914d\u7684\u80fd\u529b\uff0c\u5e76\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u5339\u914d\u6027\u80fd\u3002", "method": "1) \u91c7\u7528\u641c\u7d22\u7a7a\u95f4\u7f29\u51cf\u6280\u672f\u9009\u62e9\u76f8\u5173\u672c\u4f53\u5b50\u96c6\uff1b2) \u81ea\u52a8\u6784\u5efa\u63d0\u793a\uff1b3) \u5f15\u5165\u57fa\u4e8eLLM\u7684\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\uff0c\u521b\u5efa\u672c\u4f53\u5b50\u6a21\u5757\u5bf9\u53ca\u5176\u53c2\u8003\u5bf9\u9f50\u7684\u6570\u636e\u96c6\uff1b4) \u4f7f\u7528\u5408\u6210\u6570\u636e\u5bf9LLM\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728OAEI\u590d\u6742\u8ddf\u8e2a\u7684Conference\u3001Geolink\u3001Enslaved\u3001Taxon\u548cHydrography\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5728\u5408\u6210\u6570\u636e\u4e0a\u5fae\u8c03\u7684LLM\u76f8\u6bd4\u672a\u5fae\u8c03\u7684\u57fa\u7840\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ed3\u5408\u81ea\u52a8\u6570\u636e\u96c6\u751f\u6210\u548c\u5fae\u8c03\u7684\u7b56\u7565\u80fd\u591f\u6709\u6548\u9002\u914dLLM\u7528\u4e8e\u672c\u4f53\u5339\u914d\u4efb\u52a1\uff0c\u4e3a\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u63d0\u4f9b\u4e86\u521b\u65b0\u65b9\u6cd5\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2511.22769", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22769", "abs": "https://arxiv.org/abs/2511.22769", "authors": ["Kanchon Gharami", "Quazi Sarwar Muhtaseem", "Deepti Gupta", "Lavanya Elluri", "Shafika Showkat Moni"], "title": "Modeling Romanized Hindi and Bengali: Dataset Creation and Multilingual LLM Integration", "comment": "Proceedings of the 8th Workshop on Big Data for Cybersecurity (BigCyber)", "summary": "The development of robust transliteration techniques to enhance the effectiveness of transforming Romanized scripts into native scripts is crucial for Natural Language Processing tasks, including sentiment analysis, speech recognition, information retrieval, and intelligent personal assistants. Despite significant advancements, state-of-the-art multilingual models still face challenges in handling Romanized script, where the Roman alphabet is adopted to represent the phonetic structure of diverse languages. Within the South Asian context, where the use of Romanized script for Indo-Aryan languages is widespread across social media and digital communication platforms, such usage continues to pose significant challenges for cutting-edge multilingual models. While a limited number of transliteration datasets and models are available for Indo-Aryan languages, they generally lack sufficient diversity in pronunciation and spelling variations, adequate code-mixed data for large language model (LLM) training, and low-resource adaptation. To address this research gap, we introduce a novel transliteration dataset for two popular Indo-Aryan languages, Hindi and Bengali, which are ranked as the 3rd and 7th most spoken languages worldwide. Our dataset comprises nearly 1.8 million Hindi and 1 million Bengali transliteration pairs. In addition to that, we pre-train a custom multilingual seq2seq LLM based on Marian architecture using the developed dataset. Experimental results demonstrate significant improvements compared to existing relevant models in terms of BLEU and CER metrics.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5370\u5730\u8bed\u548c\u5b5f\u52a0\u62c9\u8bed\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b280\u4e07\u5bf9\u97f3\u8bd1\u6570\u636e\u7684\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8eMarian\u67b6\u6784\u9884\u8bad\u7ec3\u4e86\u4e00\u4e2a\u591a\u8bed\u8a00seq2seq\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7f57\u9a6c\u5316\u6587\u672c\u97f3\u8bd1\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u7f57\u9a6c\u5316\u811a\u672c\uff08\u7528\u7f57\u9a6c\u5b57\u6bcd\u8868\u793a\u5176\u4ed6\u8bed\u8a00\u8bed\u97f3\uff09\u65f6\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5357\u4e9a\u8bed\u5883\u4e2d\uff0c\u5370\u5730\u8bed\u548c\u5b5f\u52a0\u62c9\u8bed\u7b49\u5370\u6b27\u8bed\u7cfb\u8bed\u8a00\u5728\u793e\u4ea4\u5a92\u4f53\u4e0a\u5e7f\u6cdb\u4f7f\u7528\u7f57\u9a6c\u5316\u811a\u672c\u3002\u73b0\u6709\u97f3\u8bd1\u6570\u636e\u96c6\u7f3a\u4e4f\u53d1\u97f3\u548c\u62fc\u5199\u53d8\u4f53\u7684\u591a\u6837\u6027\u3001\u8db3\u591f\u7684\u4ee3\u7801\u6df7\u5408\u6570\u636e\u4ee5\u53ca\u4f4e\u8d44\u6e90\u9002\u5e94\u80fd\u529b\u3002", "method": "1) \u4e3a\u5370\u5730\u8bed\u548c\u5b5f\u52a0\u62c9\u8bed\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8fd1180\u4e07\u5bf9\u5370\u5730\u8bed\u97f3\u8bd1\u548c100\u4e07\u5bf9\u5b5f\u52a0\u62c9\u8bed\u97f3\u8bd1\u7684\u6570\u636e\u96c6\uff1b2) \u57fa\u4e8eMarian\u67b6\u6784\u9884\u8bad\u7ec3\u4e86\u4e00\u4e2a\u5b9a\u5236\u7684\u591a\u8bed\u8a00seq2seq\u5927\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u76f8\u5173\u6a21\u578b\u76f8\u6bd4\uff0c\u5728BLEU\u548cCER\u6307\u6807\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u96c6\u548c\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u5370\u6b27\u8bed\u7cfb\u8bed\u8a00\u7f57\u9a6c\u5316\u811a\u672c\u97f3\u8bd1\u7684\u6311\u6218\uff0c\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u652f\u6301\u3002"}}
{"id": "2511.22818", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22818", "abs": "https://arxiv.org/abs/2511.22818", "authors": ["Vivek Kumar", "Pushpraj Singh Rajawat", "Eirini Ntoutsi"], "title": "Mitigating Semantic Drift: Evaluating LLMs' Efficacy in Psychotherapy through MI Dialogue Summarization", "comment": null, "summary": "Recent advancements in large language models (LLMs) have shown their potential across both general and domain-specific tasks. However, there is a growing concern regarding their lack of sensitivity, factual incorrectness in responses, inconsistent expressions of empathy, bias, hallucinations, and overall inability to capture the depth and complexity of human understanding, especially in low-resource and sensitive domains such as psychology. To address these challenges, our study employs a mixed-methods approach to evaluate the efficacy of LLMs in psychotherapy. We use LLMs to generate precise summaries of motivational interviewing (MI) dialogues and design a two-stage annotation scheme based on key components of the Motivational Interviewing Treatment Integrity (MITI) framework, namely evocation, collaboration, autonomy, direction, empathy, and a non-judgmental attitude. Using expert-annotated MI dialogues as ground truth, we formulate multi-class classification tasks to assess model performance under progressive prompting techniques, incorporating one-shot and few-shot prompting. Our results offer insights into LLMs' capacity for understanding complex psychological constructs and highlight best practices to mitigate ``semantic drift\" in therapeutic settings. Our work contributes not only to the MI community by providing a high-quality annotated dataset to address data scarcity in low-resource domains but also critical insights for using LLMs for precise contextual interpretation in complex behavioral therapy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u6cbb\u7597\u4e2d\u7684\u6548\u80fd\uff0c\u901a\u8fc7\u751f\u6210\u52a8\u673a\u8bbf\u8c08\u5bf9\u8bdd\u6458\u8981\uff0c\u57fa\u4e8eMITI\u6846\u67b6\u8bbe\u8ba1\u6807\u6ce8\u65b9\u6848\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u590d\u6742\u5fc3\u7406\u6982\u5ff5\u7406\u89e3\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u548c\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u5728\u5fc3\u7406\u5b66\u7b49\u4f4e\u8d44\u6e90\u654f\u611f\u9886\u57df\u5b58\u5728\u654f\u611f\u6027\u4e0d\u8db3\u3001\u4e8b\u5b9e\u9519\u8bef\u3001\u5171\u60c5\u8868\u8fbe\u4e0d\u4e00\u81f4\u3001\u504f\u89c1\u3001\u5e7b\u89c9\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u6355\u6349\u4eba\u7c7b\u7406\u89e3\u7684\u6df1\u5ea6\u548c\u590d\u6742\u6027\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u8bc4\u4f30LLMs\u5728\u5fc3\u7406\u6cbb\u7597\u4e2d\u7684\u6548\u80fd\uff1a\u4f7f\u7528LLMs\u751f\u6210\u52a8\u673a\u8bbf\u8c08\u5bf9\u8bdd\u6458\u8981\uff0c\u57fa\u4e8eMITI\u6846\u67b6\uff08\u5524\u8d77\u3001\u5408\u4f5c\u3001\u81ea\u4e3b\u3001\u65b9\u5411\u3001\u5171\u60c5\u3001\u975e\u8bc4\u5224\u6001\u5ea6\uff09\u8bbe\u8ba1\u4e24\u9636\u6bb5\u6807\u6ce8\u65b9\u6848\uff0c\u4ee5\u4e13\u5bb6\u6807\u6ce8\u4e3a\u57fa\u51c6\uff0c\u901a\u8fc7\u6e10\u8fdb\u63d0\u793a\u6280\u672f\uff08\u5355\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\uff09\u6784\u5efa\u591a\u5206\u7c7b\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3aLLMs\u7406\u89e3\u590d\u6742\u5fc3\u7406\u6982\u5ff5\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u6cbb\u7597\u73af\u5883\u4e2d\u51cf\u8f7b\"\u8bed\u4e49\u6f02\u79fb\"\u7684\u6700\u4f73\u5b9e\u8df5\u3002\u4e3aMI\u793e\u533a\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u4ee5\u89e3\u51b3\u4f4e\u8d44\u6e90\u9886\u57df\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u5e76\u4e3a\u5728\u590d\u6742\u884c\u4e3a\u6cbb\u7597\u4e2d\u4f7f\u7528LLMs\u8fdb\u884c\u7cbe\u786e\u4e0a\u4e0b\u6587\u89e3\u91ca\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u901a\u8fc7\u63d0\u4f9b\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u9886\u57df\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u8fd8\u4e3a\u5728\u590d\u6742\u884c\u4e3a\u6cbb\u7597\u4e2d\u4f7f\u7528LLMs\u8fdb\u884c\u7cbe\u786e\u4e0a\u4e0b\u6587\u89e3\u91ca\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u63d0\u5347LLMs\u5728\u654f\u611f\u5fc3\u7406\u6cbb\u7597\u9886\u57df\u7684\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2511.22858", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.22858", "abs": "https://arxiv.org/abs/2511.22858", "authors": ["Yuya Ishihara", "Atsushi Keyaki", "Hiroaki Yamada", "Ryutaro Ohara", "Mihoko Sumida"], "title": "RAG System for Supporting Japanese Litigation Procedures: Faithful Response Generation Complying with Legal Norms", "comment": "This is a preprint version of a paper reviewed and accepted at BREV-RAG 2025: Beyond Relevance-based EValuation of RAG Systems, a SIGIR-AP 2025 workshop", "summary": "This study discusses the essential components that a Retrieval-Augmented Generation (RAG)-based LLM system should possess in order to support Japanese medical litigation procedures complying with legal norms. In litigation, expert commissioners, such as physicians, architects, accountants, and engineers, provide specialized knowledge to help judges clarify points of dispute. When considering the substitution of these expert roles with a RAG-based LLM system, the constraint of strict adherence to legal norms is imposed. Specifically, three requirements arise: (1) the retrieval module must retrieve appropriate external knowledge relevant to the disputed issues in accordance with the principle prohibiting the use of private knowledge, (2) the responses generated must originate from the context provided by the RAG and remain faithful to that context, and (3) the retrieval module must reference external knowledge with appropriate timestamps corresponding to the issues at hand. This paper discusses the design of a RAG-based LLM system that satisfies these requirements.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8eRAG\u7684LLM\u7cfb\u7edf\u5728\u652f\u6301\u65e5\u672c\u533b\u7597\u8bc9\u8bbc\u7a0b\u5e8f\u65f6\u9700\u8981\u6ee1\u8db3\u7684\u4e09\u4e2a\u6cd5\u5f8b\u89c4\u8303\u8981\u6c42\uff1a\u68c0\u7d22\u6a21\u5757\u5fc5\u987b\u9075\u5faa\u7981\u6b62\u4f7f\u7528\u79c1\u4eba\u77e5\u8bc6\u539f\u5219\u3001\u751f\u6210\u56de\u7b54\u5fc5\u987b\u5fe0\u5b9e\u4e8e\u68c0\u7d22\u4e0a\u4e0b\u6587\u3001\u68c0\u7d22\u5fc5\u987b\u5305\u542b\u9002\u5f53\u7684\u65f6\u95f4\u6233\u3002", "motivation": "\u5728\u8bc9\u8bbc\u4e2d\uff0c\u4e13\u5bb6\u59d4\u5458\uff08\u5982\u533b\u751f\u3001\u5efa\u7b51\u5e08\u3001\u4f1a\u8ba1\u5e08\u3001\u5de5\u7a0b\u5e08\uff09\u4e3a\u6cd5\u5b98\u63d0\u4f9b\u4e13\u4e1a\u77e5\u8bc6\u4ee5\u6f84\u6e05\u4e89\u8bae\u70b9\u3002\u8003\u8651\u7528\u57fa\u4e8eRAG\u7684LLM\u7cfb\u7edf\u66ff\u4ee3\u8fd9\u4e9b\u4e13\u5bb6\u89d2\u8272\u65f6\uff0c\u5fc5\u987b\u4e25\u683c\u9075\u5b88\u6cd5\u5f8b\u89c4\u8303\u7ea6\u675f\u3002", "method": "\u672c\u6587\u8ba8\u8bba\u4e86\u6ee1\u8db3\u4e09\u4e2a\u6cd5\u5f8b\u8981\u6c42\u7684RAG-based LLM\u7cfb\u7edf\u8bbe\u8ba1\uff1a1\uff09\u68c0\u7d22\u6a21\u5757\u5fc5\u987b\u6839\u636e\u7981\u6b62\u4f7f\u7528\u79c1\u4eba\u77e5\u8bc6\u539f\u5219\u68c0\u7d22\u4e0e\u4e89\u8bae\u95ee\u9898\u76f8\u5173\u7684\u5916\u90e8\u77e5\u8bc6\uff1b2\uff09\u751f\u6210\u56de\u7b54\u5fc5\u987b\u6e90\u81eaRAG\u63d0\u4f9b\u7684\u4e0a\u4e0b\u6587\u5e76\u5fe0\u5b9e\u4e8e\u8be5\u4e0a\u4e0b\u6587\uff1b3\uff09\u68c0\u7d22\u6a21\u5757\u5fc5\u987b\u5f15\u7528\u5177\u6709\u9002\u5f53\u65f6\u95f4\u6233\u7684\u5916\u90e8\u77e5\u8bc6\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7b26\u5408\u65e5\u672c\u533b\u7597\u8bc9\u8bbc\u6cd5\u5f8b\u89c4\u8303\u7684RAG-based LLM\u7cfb\u7edf\u8bbe\u8ba1\u65b9\u6848\uff0c\u786e\u4fdd\u7cfb\u7edf\u80fd\u591f\u66ff\u4ee3\u4e13\u5bb6\u59d4\u5458\u89d2\u8272\uff0c\u540c\u65f6\u6ee1\u8db3\u6cd5\u5f8b\u7a0b\u5e8f\u7684\u4e25\u683c\u8981\u6c42\u3002", "conclusion": "\u57fa\u4e8eRAG\u7684LLM\u7cfb\u7edf\u53ef\u4ee5\u652f\u6301\u65e5\u672c\u533b\u7597\u8bc9\u8bbc\u7a0b\u5e8f\uff0c\u4f46\u5fc5\u987b\u8bbe\u8ba1\u4e3a\u6ee1\u8db3\u7279\u5b9a\u7684\u6cd5\u5f8b\u89c4\u8303\u8981\u6c42\uff0c\u5305\u62ec\u68c0\u7d22\u7684\u9002\u5f53\u6027\u3001\u56de\u7b54\u7684\u5fe0\u5b9e\u6027\u4ee5\u53ca\u65f6\u95f4\u6233\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2511.22869", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22869", "abs": "https://arxiv.org/abs/2511.22869", "authors": ["Zhihan Cao", "Fumihito Nishino", "Hiroaki Yamada", "Nguyen Ha Thanh", "Yusuke Miyao", "Ken Satoh"], "title": "JBE-QA: Japanese Bar Exam QA Dataset for Assessing Legal Domain Knowledge", "comment": "Three tables and one figure", "summary": "We introduce JBE-QA, a Japanese Bar Exam Question-Answering dataset to evaluate large language models' legal knowledge. Derived from the multiple-choice (tanto-shiki) section of the Japanese bar exam (2015-2024), JBE-QA provides the first comprehensive benchmark for Japanese legal-domain evaluation of LLMs. It covers the Civil Code, the Penal Code, and the Constitution, extending beyond the Civil Code focus of prior Japanese resources. Each question is decomposed into independent true/false judgments with structured contextual fields. The dataset contains 3,464 items with balanced labels. We evaluate 26 LLMs, including proprietary, open-weight, Japanese-specialised, and reasoning models. Our results show that proprietary models with reasoning enabled perform best, and the Constitution questions are generally easier than the Civil Code or the Penal Code questions.", "AI": {"tldr": "JBE-QA\u662f\u4e00\u4e2a\u65e5\u672c\u53f8\u6cd5\u8003\u8bd5\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6cd5\u5f8b\u77e5\u8bc6\uff0c\u5305\u542b2015-2024\u5e74\u53f8\u6cd5\u8003\u8bd5\u9009\u62e9\u9898\uff0c\u6db5\u76d6\u6c11\u6cd5\u3001\u5211\u6cd5\u548c\u5baa\u6cd5\uff0c\u51713464\u4e2a\u5e73\u8861\u6807\u6ce8\u9879\u76ee\u3002", "motivation": "\u73b0\u6709\u65e5\u672c\u6cd5\u5f8b\u9886\u57df\u8bc4\u4f30\u8d44\u6e90\u4e3b\u8981\u5173\u6ce8\u6c11\u6cd5\uff0c\u7f3a\u4e4f\u5168\u9762\u7684\u6cd5\u5f8b\u77e5\u8bc6\u8bc4\u4f30\u57fa\u51c6\u3002\u9700\u8981\u521b\u5efa\u8986\u76d6\u6c11\u6cd5\u3001\u5211\u6cd5\u548c\u5baa\u6cd5\u7684\u7efc\u5408\u6570\u636e\u96c6\u6765\u8bc4\u4f30LLMs\u5728\u65e5\u672c\u6cd5\u5f8b\u9886\u57df\u7684\u8868\u73b0\u3002", "method": "\u4ece2015-2024\u5e74\u65e5\u672c\u53f8\u6cd5\u8003\u8bd5\u9009\u62e9\u9898\u4e2d\u63d0\u53d6\u95ee\u9898\uff0c\u5c06\u6bcf\u4e2a\u95ee\u9898\u5206\u89e3\u4e3a\u72ec\u7acb\u7684\u771f/\u5047\u5224\u65ad\uff0c\u5e76\u6dfb\u52a0\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u5b57\u6bb5\u3002\u6570\u636e\u96c6\u5305\u542b3464\u4e2a\u5e73\u8861\u6807\u6ce8\u9879\u76ee\u3002", "result": "\u8bc4\u4f30\u4e8626\u4e2aLLMs\uff08\u5305\u62ec\u4e13\u6709\u6a21\u578b\u3001\u5f00\u6e90\u6a21\u578b\u3001\u65e5\u672c\u4e13\u7528\u6a21\u578b\u548c\u63a8\u7406\u6a21\u578b\uff09\u3002\u7ed3\u679c\u663e\u793a\uff1a\u542f\u7528\u63a8\u7406\u7684\u4e13\u6709\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff1b\u5baa\u6cd5\u95ee\u9898\u901a\u5e38\u6bd4\u6c11\u6cd5\u6216\u5211\u6cd5\u95ee\u9898\u66f4\u5bb9\u6613\u3002", "conclusion": "JBE-QA\u662f\u9996\u4e2a\u5168\u9762\u7684\u65e5\u672c\u6cd5\u5f8b\u9886\u57df\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e3aLLMs\u5728\u65e5\u672c\u6cd5\u5f8b\u77e5\u8bc6\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u6cd5\u5f8b\u95ee\u9898\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\u3002"}}
{"id": "2511.22883", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22883", "abs": "https://arxiv.org/abs/2511.22883", "authors": ["Jingheng Ye", "Shen Wang", "Jiaqi Chen", "Hebin Wang", "Deqing Zou", "Yanyu Zhu", "Jiwei Tang", "Hai-Tao Zheng", "Ruitong Liu", "Haoyang Li", "Yanfeng Wang", "Qingsong Wen"], "title": "FEANEL: A Benchmark for Fine-Grained Error Analysis in K-12 English Writing", "comment": "19 pages, 7 figures, and 4 tables. The dataset is available at https://huggingface.co/datasets/Feanel/FEANEL", "summary": "Large Language Models (LLMs) have transformed artificial intelligence, offering profound opportunities for educational applications. However, their ability to provide fine-grained educational feedback for K-12 English writing remains underexplored. In this paper, we challenge the error analysis and pedagogical skills of LLMs by introducing the problem of Fine-grained Error Analysis for English Learners and present the Fine-grained Error ANalysis for English Learners (FEANEL) Benchmark. The benchmark comprises 1,000 essays written by elementary and secondary school students, and a well-developed English writing error taxonomy. Each error is annotated by language education experts and categorized by type, severity, and explanatory feedback, using a part-of-speech-based taxonomy they co-developed. We evaluate state-of-the-art LLMs on the FEANEL Benchmark to explore their error analysis and pedagogical abilities. Experimental results reveal significant gaps in current LLMs' ability to perform fine-grained error analysis, highlighting the need for advancements in particular methods for educational applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FEANEL\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728K-12\u82f1\u8bed\u5199\u4f5c\u4e2d\u7ec6\u7c92\u5ea6\u9519\u8bef\u5206\u6790\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5b58\u5728\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5b83\u4eec\u5728K-12\u82f1\u8bed\u5199\u4f5c\u4e2d\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u6559\u80b2\u53cd\u9988\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5176\u9519\u8bef\u5206\u6790\u548c\u6559\u5b66\u6280\u80fd\u3002", "method": "\u63d0\u51fa\u4e86FEANEL\u57fa\u51c6\uff0c\u5305\u542b1000\u7bc7\u4e2d\u5c0f\u5b66\u751f\u82f1\u8bed\u4f5c\u6587\uff0c\u57fa\u4e8e\u8bed\u8a00\u6559\u80b2\u4e13\u5bb6\u5171\u540c\u5f00\u53d1\u7684\u8bcd\u6027\u5206\u7c7b\u6cd5\u8fdb\u884c\u9519\u8bef\u6807\u6ce8\uff08\u5305\u62ec\u7c7b\u578b\u3001\u4e25\u91cd\u7a0b\u5ea6\u548c\u89e3\u91ca\u6027\u53cd\u9988\uff09\uff0c\u5e76\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u9519\u8bef\u5206\u6790\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u6559\u80b2\u5e94\u7528\u65b9\u9762\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u5e94\u7528\u4e2d\u7684\u7ec6\u7c92\u5ea6\u9519\u8bef\u5206\u6790\u80fd\u529b\uff0cFEANEL\u57fa\u51c6\u4e3a\u8fd9\u4e00\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2511.22904", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22904", "abs": "https://arxiv.org/abs/2511.22904", "authors": ["Anh Nguyen", "Stefan Lee"], "title": "Language-conditioned world model improves policy generalization by reading environmental descriptions", "comment": "NeuRIPS 2025. Workshop: LAW 2025: Bridging Language, Agent, and World Models", "summary": "To interact effectively with humans in the real world, it is important for agents to understand language that describes the dynamics of the environment--that is, how the environment behaves--rather than just task instructions specifying \"what to do\". Understanding this dynamics-descriptive language is important for human-agent interaction and agent behavior. Recent work address this problem using a model-based approach: language is incorporated into a world model, which is then used to learn a behavior policy. However, these existing methods either do not demonstrate policy generalization to unseen games or rely on limiting assumptions. For instance, assuming that the latency induced by inference-time planning is tolerable for the target task or expert demonstrations are available. Expanding on this line of research, we focus on improving policy generalization from a language-conditioned world model while dropping these assumptions. We propose a model-based reinforcement learning approach, where a language-conditioned world model is trained through interaction with the environment, and a policy is learned from this model--without planning or expert demonstrations. Our method proposes Language-aware Encoder for Dreamer World Model (LED-WM) built on top of DreamerV3. LED-WM features an observation encoder that uses an attention mechanism to explicitly ground language descriptions to entities in the observation. We show that policies trained with LED-WM generalize more effectively to unseen games described by novel dynamics and language compared to other baselines in several settings in two environments: MESSENGER and MESSENGER-WM.To highlight how the policy can leverage the trained world model before real-world deployment, we demonstrate the policy can be improved through fine-tuning on synthetic test trajectories generated by the world model.", "AI": {"tldr": "\u63d0\u51faLED-WM\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u8a00\u6761\u4ef6\u5316\u7684\u4e16\u754c\u6a21\u578b\u63d0\u5347\u7b56\u7565\u5728\u672a\u89c1\u6e38\u620f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u89c4\u5212\u6216\u4e13\u5bb6\u6f14\u793a", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u8bc1\u660e\u7b56\u7565\u5728\u672a\u89c1\u6e38\u620f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8981\u4e48\u4f9d\u8d56\u9650\u5236\u6027\u5047\u8bbe\uff08\u5982\u5bb9\u5fcd\u63a8\u7406\u65f6\u89c4\u5212\u5ef6\u8fdf\u6216\u9700\u8981\u4e13\u5bb6\u6f14\u793a\uff09\u3002\u9700\u8981\u6539\u8fdb\u8bed\u8a00\u6761\u4ef6\u5316\u4e16\u754c\u6a21\u578b\u7684\u7b56\u7565\u6cdb\u5316\u80fd\u529b\uff0c\u540c\u65f6\u53bb\u9664\u8fd9\u4e9b\u5047\u8bbe\u3002", "method": "\u57fa\u4e8eDreamerV3\u6784\u5efa\u8bed\u8a00\u611f\u77e5\u7f16\u7801\u5668\u4e16\u754c\u6a21\u578b\uff08LED-WM\uff09\uff0c\u4f7f\u7528\u6ce8\u610f\u529b\u673a\u5236\u5c06\u8bed\u8a00\u63cf\u8ff0\u663e\u5f0f\u5730\u5173\u8054\u5230\u89c2\u5bdf\u4e2d\u7684\u5b9e\u4f53\u3002\u901a\u8fc7\u4e0e\u73af\u5883\u4ea4\u4e92\u8bad\u7ec3\u8bed\u8a00\u6761\u4ef6\u5316\u4e16\u754c\u6a21\u578b\uff0c\u7136\u540e\u4ece\u8be5\u6a21\u578b\u5b66\u4e60\u7b56\u7565\uff0c\u65e0\u9700\u89c4\u5212\u6216\u4e13\u5bb6\u6f14\u793a\u3002", "result": "\u5728MESSENGER\u548cMESSENGER-WM\u4e24\u4e2a\u73af\u5883\u4e2d\uff0cLED-WM\u8bad\u7ec3\u7684\u7b56\u7565\u76f8\u6bd4\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u7531\u65b0\u52a8\u6001\u548c\u8bed\u8a00\u63cf\u8ff0\u7684\u672a\u89c1\u6e38\u620f\u4e2d\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8fd8\u5c55\u793a\u4e86\u7b56\u7565\u53ef\u4ee5\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6d4b\u8bd5\u8f68\u8ff9\u8fdb\u884c\u5fae\u8c03\u6765\u6539\u8fdb\u3002", "conclusion": "LED-WM\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u7b56\u7565\u5728\u672a\u89c1\u6e38\u620f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u4f9d\u8d56\u89c4\u5212\u6216\u4e13\u5bb6\u6f14\u793a\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u524d\u7684\u7b56\u7565\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2511.22943", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.22943", "abs": "https://arxiv.org/abs/2511.22943", "authors": ["Kelaiti Xiao", "Liang Yang", "Dongyu Zhang", "Paerhati Tulajiang", "Hongfei Lin"], "title": "Visual Puns from Idioms: An Iterative LLM-T2IM-MLLM Framework", "comment": "Submitted to ICASSP 2026 (under review)", "summary": "We study idiom-based visual puns--images that align an idiom's literal and figurative meanings--and present an iterative framework that coordinates a large language model (LLM), a text-to-image model (T2IM), and a multimodal LLM (MLLM) for automatic generation and evaluation. Given an idiom, the system iteratively (i) generates detailed visual prompts, (ii) synthesizes an image, (iii) infers the idiom from the image, and (iv) refines the prompt until recognition succeeds or a step limit is reached. Using 1,000 idioms as inputs, we synthesize a corresponding dataset of visual pun images with paired prompts, enabling benchmarking of both generation and understanding. Experiments across 10 LLMs, 10 MLLMs, and one T2IM (Qwen-Image) show that MLLM choice is the primary performance driver: GPT achieves the highest accuracies, Gemini follows, and the best open-source MLLM (Gemma) is competitive with some closed models. On the LLM side, Claude attains the strongest average performance for prompt generation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u8fed\u4ee3\u6846\u67b6\u7684\u89c6\u89c9\u53cc\u5173\u8bed\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u534f\u8c03LLM\u3001T2IM\u548cMLLM\u81ea\u52a8\u751f\u6210\u548c\u8bc4\u4f30\u6210\u8bed\u89c6\u89c9\u53cc\u5173\u56fe\u50cf\uff0c\u5e76\u521b\u5efa\u4e86\u5305\u542b1000\u4e2a\u6210\u8bed\u7684\u6570\u636e\u96c6\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u7814\u7a76\u6210\u8bed\u89c6\u89c9\u53cc\u5173\u8bed\uff08\u5c06\u6210\u8bed\u7684\u5b57\u9762\u610f\u4e49\u548c\u6bd4\u55bb\u610f\u4e49\u5bf9\u9f50\u7684\u56fe\u50cf\uff09\uff0c\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u751f\u6210\u548c\u8bc4\u4f30\u8fd9\u7c7b\u590d\u6742\u89c6\u89c9\u5185\u5bb9\u7684\u65b9\u6cd5\uff0c\u5e76\u5efa\u7acb\u76f8\u5e94\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u8fed\u4ee3\u6846\u67b6\uff1a\u7ed9\u5b9a\u6210\u8bed\u540e\uff0c\u7cfb\u7edf\u5faa\u73af\u6267\u884c\uff1a(1) LLM\u751f\u6210\u8be6\u7ec6\u89c6\u89c9\u63d0\u793a\uff0c(2) T2IM\u5408\u6210\u56fe\u50cf\uff0c(3) MLLM\u4ece\u56fe\u50cf\u63a8\u65ad\u6210\u8bed\uff0c(4) \u4f18\u5316\u63d0\u793a\u76f4\u5230\u8bc6\u522b\u6210\u529f\u6216\u8fbe\u5230\u6b65\u6570\u9650\u5236\u3002\u4f7f\u75281000\u4e2a\u6210\u8bed\u4f5c\u4e3a\u8f93\u5165\u521b\u5efa\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u4e8610\u4e2aLLM\u300110\u4e2aMLLM\u548c1\u4e2aT2IM\uff08Qwen-Image\uff09\u3002MLLM\u9009\u62e9\u662f\u4e3b\u8981\u6027\u80fd\u9a71\u52a8\u56e0\u7d20\uff1aGPT\u51c6\u786e\u7387\u6700\u9ad8\uff0cGemini\u6b21\u4e4b\uff0c\u6700\u4f73\u5f00\u6e90MLLM\uff08Gemma\uff09\u4e0e\u90e8\u5206\u95ed\u6e90\u6a21\u578b\u7ade\u4e89\u3002\u5728LLM\u65b9\u9762\uff0cClaude\u5728\u63d0\u793a\u751f\u6210\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u81ea\u52a8\u751f\u6210\u6210\u8bed\u89c6\u89c9\u53cc\u5173\u8bed\u7684\u8fed\u4ee3\u6846\u67b6\uff0c\u521b\u5efa\u4e86\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u6a21\u578b\u5728\u751f\u6210\u548c\u7406\u89e3\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u4e3a\u89c6\u89c9\u8bed\u8a00\u7406\u89e3\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u89c1\u89e3\u3002"}}
{"id": "2511.22972", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22972", "abs": "https://arxiv.org/abs/2511.22972", "authors": ["Jinze Li", "Yixing Xu", "Guanchen Li", "Shuo Yang", "Jinfeng Xu", "Xuanwu Yin", "Dong Li", "Edith C. H. Ngai", "Emad Barsoum"], "title": "Training-Free Loosely Speculative Decoding: Accepting Semantically Correct Drafts Beyond Exact Match", "comment": "Under review", "summary": "Large language models (LLMs) achieve strong performance across diverse tasks but suffer from high inference latency due to their autoregressive generation. Speculative Decoding (SPD) mitigates this issue by verifying candidate tokens in parallel from a smaller draft model, yet its strict exact-match verification discards many semantically valid continuations. Moreover, existing training-based SPD methods often suffer from performance degradation on out-of-distribution (OOD) tasks. To this end, we propose Training-Free Loosely Speculative Decoding (FLy), a novel method that loosens the rigid verification criterion by leveraging the target model's self-corrective behavior to judge whether a draft-target mismatch remains semantically valid. FLy introduces a two-tier mechanism: an entropy-level gate that identifies whether the current token allows multiple plausible alternatives or is nearly deterministic, and a token-level deferred window that distinguishes genuine errors from differently worded yet semantically correct variants. To further reduce latency, we design a multi-level acceleration strategy that accelerates not only the target model but also the drafter itself. Owing to its training-free design, FLy composes seamlessly with arbitrary draft-target pairs and generalizes across models and domains without hyperparameter re-tuning. Experiments show that FLy preserves more than 99% of the target model's accuracy while achieving an average 2.81x speedup on Llama-3.1-70B-Instruct and 5.07x speedup on the 405B variant. Notably, on out-of-domain datasets, our method remains highly effective and outperforms the training-based method EAGLE-3 by 1.62x.", "AI": {"tldr": "FLy\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u7801\u52a0\u901f\u65b9\u6cd5\uff0c\u901a\u8fc7\u653e\u5bbd\u9a8c\u8bc1\u6807\u51c6\u5e76\u5229\u7528\u76ee\u6807\u6a21\u578b\u7684\u81ea\u6821\u6b63\u80fd\u529b\uff0c\u5728\u4fdd\u630199%\u4ee5\u4e0a\u51c6\u786e\u7387\u7684\u540c\u65f6\u5b9e\u73b02.81-5.07\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4e25\u683c\u7684\u7cbe\u786e\u5339\u914d\u9a8c\u8bc1\u4e22\u5f03\u4e86\u8bb8\u591a\u8bed\u4e49\u6709\u6548\u7684\u5019\u9009\uff1b2\uff09\u57fa\u4e8e\u8bad\u7ec3\u7684\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u80fd\u6cdb\u5316\u5230\u4e0d\u540c\u6a21\u578b\u548c\u9886\u57df\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u8bad\u7ec3\u81ea\u7531\u7684\u5bbd\u677e\u63a8\u6d4b\u89e3\u7801\uff08FLy\uff09\uff0c\u91c7\u7528\u53cc\u5c42\u673a\u5236\uff1a1\uff09\u71b5\u7ea7\u95e8\u63a7\u8bc6\u522b\u5f53\u524dtoken\u662f\u5426\u5141\u8bb8\u591a\u4e2a\u5408\u7406\u66ff\u4ee3\uff1b2\uff09token\u7ea7\u5ef6\u8fdf\u7a97\u53e3\u533a\u5206\u771f\u6b63\u9519\u8bef\u4e0e\u8bed\u4e49\u6b63\u786e\u7684\u53d8\u4f53\u3002\u8fd8\u8bbe\u8ba1\u4e86\u591a\u7ea7\u52a0\u901f\u7b56\u7565\uff0c\u4e0d\u4ec5\u52a0\u901f\u76ee\u6807\u6a21\u578b\uff0c\u4e5f\u52a0\u901f\u8349\u7a3f\u6a21\u578b\u672c\u8eab\u3002", "result": "\u5728Llama-3.1-70B-Instruct\u4e0a\u5e73\u5747\u5b9e\u73b02.81\u500d\u52a0\u901f\uff0c\u5728405B\u53d8\u4f53\u4e0a\u5b9e\u73b05.07\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u76ee\u6807\u6a21\u578b99%\u4ee5\u4e0a\u7684\u51c6\u786e\u7387\u3002\u5728\u5206\u5e03\u5916\u6570\u636e\u96c6\u4e0a\uff0cFLy\u4ecd\u7136\u9ad8\u6548\uff0c\u6bd4\u57fa\u4e8e\u8bad\u7ec3\u7684EAGLE-3\u65b9\u6cd5\u5feb1.62\u500d\u3002", "conclusion": "FLy\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u7801\u52a0\u901f\u65b9\u6cd5\uff0c\u901a\u8fc7\u653e\u5bbd\u9a8c\u8bc1\u6807\u51c6\u548c\u591a\u7ea7\u52a0\u901f\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff0c\u4e14\u80fd\u65e0\u7f1d\u7ec4\u5408\u4efb\u610f\u8349\u7a3f-\u76ee\u6807\u6a21\u578b\u5bf9\uff0c\u65e0\u9700\u8d85\u53c2\u6570\u91cd\u65b0\u8c03\u6574\u5373\u53ef\u8de8\u6a21\u578b\u548c\u9886\u57df\u6cdb\u5316\u3002"}}
{"id": "2511.22977", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.22977", "abs": "https://arxiv.org/abs/2511.22977", "authors": ["Sumit Mamtani", "Abhijeet Bhure"], "title": "Pooling Attention: Evaluating Pretrained Transformer Embeddings for Deception Classification", "comment": "Accepted at the IEEE 7th Computing, Communications and IoT Applications Conference (ComComAp 2025), Madrid, Spain, December 2025. 6 pages", "summary": "This paper investigates fake news detection as a downstream evaluation of Transformer representations, benchmarking encoder-only and decoder-only pre-trained models (BERT, GPT-2, Transformer-XL) as frozen embedders paired with lightweight classifiers. Through controlled preprocessing comparing pooling versus padding and neural versus linear heads, results demonstrate that contextual self-attention encodings consistently transfer effectively. BERT embeddings combined with logistic regression outperform neural baselines on LIAR dataset splits, while analyses of sequence length and aggregation reveal robustness to truncation and advantages from simple max or average pooling. This work positions attention-based token encoders as robust, architecture-centric foundations for veracity tasks, isolating Transformer contributions from classifier complexity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5c06Transformer\u8868\u793a\u4f5c\u4e3a\u5047\u65b0\u95fb\u68c0\u6d4b\u7684\u4e0b\u6e38\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4e86\u7f16\u7801\u5668-\u89e3\u7801\u5668\u9884\u8bad\u7ec3\u6a21\u578b\u4f5c\u4e3a\u51bb\u7ed3\u5d4c\u5165\u5668\u4e0e\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u914d\u5bf9\u7684\u6027\u80fd\uff0c\u53d1\u73b0BERT\u5d4c\u5165\u7ed3\u5408\u903b\u8f91\u56de\u5f52\u5728LIAR\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7814\u7a76Transformer\u8868\u793a\u5728\u5047\u65b0\u95fb\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u901a\u8fc7\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u4f5c\u4e3a\u5d4c\u5165\u5668\u6765\u5206\u79bbTransformer\u67b6\u6784\u8d21\u732e\u4e0e\u5206\u7c7b\u5668\u590d\u6742\u6027\uff0c\u4e3a\u9a8c\u8bc1\u6027\u4efb\u52a1\u63d0\u4f9b\u67b6\u6784\u4e2d\u5fc3\u7684\u7a33\u5065\u57fa\u7840\u3002", "method": "\u4f7f\u7528\u7f16\u7801\u5668-\u89e3\u7801\u5668\u9884\u8bad\u7ec3\u6a21\u578b\uff08BERT\u3001GPT-2\u3001Transformer-XL\uff09\u4f5c\u4e3a\u51bb\u7ed3\u5d4c\u5165\u5668\uff0c\u4e0e\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668\u914d\u5bf9\u3002\u901a\u8fc7\u63a7\u5236\u9884\u5904\u7406\u6bd4\u8f83\u6c60\u5316\u4e0e\u586b\u5145\u7b56\u7565\uff0c\u4ee5\u53ca\u795e\u7ecf\u7f51\u7edc\u4e0e\u7ebf\u6027\u5206\u7c7b\u5934\u3002\u5728LIAR\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5206\u6790\u5e8f\u5217\u957f\u5ea6\u548c\u805a\u5408\u65b9\u6cd5\u7684\u5f71\u54cd\u3002", "result": "BERT\u5d4c\u5165\u7ed3\u5408\u903b\u8f91\u56de\u5f52\u5728LIAR\u6570\u636e\u96c6\u5206\u5272\u4e0a\u4f18\u4e8e\u795e\u7ecf\u7f51\u7edc\u57fa\u7ebf\u3002\u4e0a\u4e0b\u6587\u81ea\u6ce8\u610f\u529b\u7f16\u7801\u80fd\u6709\u6548\u8fc1\u79fb\uff0c\u5bf9\u622a\u65ad\u5177\u6709\u9c81\u68d2\u6027\uff0c\u7b80\u5355\u6700\u5927\u6216\u5e73\u5747\u6c60\u5316\u5177\u6709\u4f18\u52bf\u3002\u6ce8\u610f\u529b\u57fa\u4e8e\u6807\u8bb0\u7f16\u7801\u5668\u6210\u4e3a\u9a8c\u8bc1\u6027\u4efb\u52a1\u7684\u7a33\u5065\u67b6\u6784\u4e2d\u5fc3\u57fa\u7840\u3002", "conclusion": "\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6807\u8bb0\u7f16\u7801\u5668\u53ef\u4f5c\u4e3a\u5047\u65b0\u95fb\u68c0\u6d4b\u7b49\u9a8c\u8bc1\u6027\u4efb\u52a1\u7684\u7a33\u5065\u3001\u67b6\u6784\u4e2d\u5fc3\u57fa\u7840\uff0c\u80fd\u591f\u6709\u6548\u5206\u79bbTransformer\u8d21\u732e\u4e0e\u5206\u7c7b\u5668\u590d\u6742\u6027\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\u63d0\u4f9b\u6709\u6548\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\u3002"}}
{"id": "2511.22978", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.22978", "abs": "https://arxiv.org/abs/2511.22978", "authors": ["Huaixiao Tou", "Ying Zeng", "Cong Ma", "Muzhi Li", "Minghao Li", "Weijie Yuan", "He Zhang", "Kai Jia"], "title": "ShoppingComp: Are LLMs Really Ready for Your Shopping Cart?", "comment": null, "summary": "We present ShoppingComp, a challenging real-world benchmark for rigorously evaluating LLM-powered shopping agents on three core capabilities: precise product retrieval, expert-level report generation, and safety critical decision making. Unlike prior e-commerce benchmarks, ShoppingComp introduces highly complex tasks under the principle of guaranteeing real products and ensuring easy verifiability, adding a novel evaluation dimension for identifying product safety hazards alongside recommendation accuracy and report quality. The benchmark comprises 120 tasks and 1,026 scenarios, curated by 35 experts to reflect authentic shopping needs. Results reveal stark limitations of current LLMs: even state-of-the-art models achieve low performance (e.g., 11.22% for GPT-5, 3.92% for Gemini-2.5-Flash). These findings highlight a substantial gap between research benchmarks and real-world deployment, where LLMs make critical errors such as failure to identify unsafe product usage or falling for promotional misinformation, leading to harmful recommendations. ShoppingComp fills the gap and thus establishes a new standard for advancing reliable and practical agents in e-commerce.", "AI": {"tldr": "ShoppingComp\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u4e25\u683c\u8bc4\u4f30LLM\u8d2d\u7269\u4ee3\u7406\u5728\u4e09\u4e2a\u6838\u5fc3\u80fd\u529b\u4e0a\u7684\u8868\u73b0\uff1a\u7cbe\u786e\u4ea7\u54c1\u68c0\u7d22\u3001\u4e13\u5bb6\u7ea7\u62a5\u544a\u751f\u6210\u548c\u5b89\u5168\u5173\u952e\u51b3\u7b56\u5236\u5b9a\u3002\u7ed3\u679c\u663e\u793a\u5f53\u524dLLM\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\u7814\u7a76\u57fa\u51c6\u4e0e\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e4b\u95f4\u7684\u5de8\u5927\u5dee\u8ddd\u3002", "motivation": "\u73b0\u6709\u7535\u5b50\u5546\u52a1\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30LLM\u8d2d\u7269\u4ee3\u7406\u7684\u771f\u5b9e\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u4ea7\u54c1\u5b89\u5168\u548c\u5b9e\u9645\u90e8\u7f72\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u66f4\u8d34\u8fd1\u771f\u5b9e\u8d2d\u7269\u9700\u6c42\u3001\u6613\u4e8e\u9a8c\u8bc1\u7684\u57fa\u51c6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u521b\u5efa\u4e86\u5305\u542b120\u4e2a\u4efb\u52a1\u548c1,026\u4e2a\u573a\u666f\u7684ShoppingComp\u57fa\u51c6\uff0c\u753135\u4f4d\u4e13\u5bb6\u7b56\u5212\u4ee5\u53cd\u6620\u771f\u5b9e\u8d2d\u7269\u9700\u6c42\u3002\u57fa\u51c6\u5f3a\u8c03\u771f\u5b9e\u4ea7\u54c1\u548c\u6613\u4e8e\u9a8c\u8bc1\u7684\u539f\u5219\uff0c\u5e76\u5f15\u5165\u4e86\u4ea7\u54c1\u5b89\u5168\u5371\u5bb3\u8bc6\u522b\u8fd9\u4e00\u65b0\u7684\u8bc4\u4f30\u7ef4\u5ea6\u3002", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684LLM\u8868\u73b0\u5f88\u5dee\uff1aGPT-5\u4ec511.22%\uff0cGemini-2.5-Flash\u4ec53.92%\u3002LLM\u5728\u8bc6\u522b\u4e0d\u5b89\u5168\u4ea7\u54c1\u4f7f\u7528\u3001\u907f\u514d\u4fc3\u9500\u8bef\u5bfc\u7b49\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u9519\u8bef\uff0c\u5bfc\u81f4\u6709\u5bb3\u63a8\u8350\u3002", "conclusion": "ShoppingComp\u586b\u8865\u4e86\u7814\u7a76\u57fa\u51c6\u4e0e\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u63a8\u8fdb\u7535\u5b50\u5546\u52a1\u4e2d\u53ef\u9760\u5b9e\u7528\u7684\u4ee3\u7406\u5efa\u7acb\u4e86\u65b0\u6807\u51c6\u3002\u7ed3\u679c\u8868\u660eLLM\u5728\u771f\u5b9e\u8d2d\u7269\u573a\u666f\u4e2d\u4ecd\u6709\u5f88\u5927\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2511.23041", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23041", "abs": "https://arxiv.org/abs/2511.23041", "authors": ["Dong Nguyen", "Laura Rosseel"], "title": "Social Perceptions of English Spelling Variation on Twitter: A Comparative Analysis of Human and LLM Responses", "comment": null, "summary": "Spelling variation (e.g. funnnn vs. fun) can influence the social perception of texts and their writers: we often have various associations with different forms of writing (is the text informal? does the writer seem young?). In this study, we focus on the social perception of spelling variation in online writing in English and study to what extent this perception is aligned between humans and large language models (LLMs). Building on sociolinguistic methodology, we compare LLM and human ratings on three key social attributes of spelling variation (formality, carefulness, age). We find generally strong correlations in the ratings between humans and LLMs. However, notable differences emerge when we analyze the distribution of ratings and when comparing between different types of spelling variation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u82f1\u8bed\u5728\u7ebf\u62fc\u5199\u53d8\u4f53\uff08\u5982funnnn vs. fun\uff09\u7684\u793e\u4f1a\u611f\u77e5\uff08\u6b63\u5f0f\u6027\u3001\u8c28\u614e\u6027\u3001\u5e74\u9f84\uff09\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u4f46\u5728\u8bc4\u5206\u5206\u5e03\u548c\u4e0d\u540c\u7c7b\u578b\u53d8\u4f53\u95f4\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u62fc\u5199\u53d8\u4f53\u5f71\u54cd\u6587\u672c\u548c\u4f5c\u8005\u7684\u793e\u4f1a\u611f\u77e5\uff08\u5982\u975e\u6b63\u5f0f\u6027\u3001\u5e74\u8f7b\u611f\uff09\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u8fd9\u79cd\u793e\u4f1a\u611f\u77e5\u7684\u8bc4\u4f30\u662f\u5426\u4e00\u81f4\u3002", "method": "\u57fa\u4e8e\u793e\u4f1a\u8bed\u8a00\u5b66\u65b9\u6cd5\uff0c\u6bd4\u8f83LLM\u548c\u4eba\u7c7b\u5bf9\u62fc\u5199\u53d8\u4f53\u5728\u4e09\u4e2a\u5173\u952e\u793e\u4f1a\u5c5e\u6027\uff08\u6b63\u5f0f\u6027\u3001\u8c28\u614e\u6027\u3001\u5e74\u9f84\uff09\u4e0a\u7684\u8bc4\u5206\u3002", "result": "\u4eba\u7c7b\u4e0eLLM\u8bc4\u5206\u603b\u4f53\u4e0a\u5b58\u5728\u5f3a\u76f8\u5173\u6027\uff0c\u4f46\u5728\u8bc4\u5206\u5206\u5e03\u5206\u6790\u4ee5\u53ca\u4e0d\u540c\u7c7b\u578b\u62fc\u5199\u53d8\u4f53\u6bd4\u8f83\u65f6\u51fa\u73b0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "LLM\u5728\u6355\u6349\u62fc\u5199\u53d8\u4f53\u7684\u793e\u4f1a\u611f\u77e5\u65b9\u9762\u4e0e\u4eba\u7c7b\u6709\u826f\u597d\u5bf9\u9f50\uff0c\u4f46\u5728\u67d0\u4e9b\u65b9\u9762\u4ecd\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6a21\u578b\u5982\u4f55\u5185\u5316\u793e\u4f1a\u8bed\u8a00\u89c4\u8303\u3002"}}
{"id": "2511.23056", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23056", "abs": "https://arxiv.org/abs/2511.23056", "authors": ["Paulo J. N. Pinto", "Armando J. Pinho", "Diogo Pratas"], "title": "Decoding the Past: Explainable Machine Learning Models for Dating Historical Texts", "comment": null, "summary": "Accurately dating historical texts is essential for organizing and interpreting cultural heritage collections. This article addresses temporal text classification using interpretable, feature-engineered tree-based machine learning models. We integrate five feature categories - compression-based, lexical structure, readability, neologism detection, and distance features - to predict the temporal origin of English texts spanning five centuries. Comparative analysis shows that these feature domains provide complementary temporal signals, with combined models outperforming any individual feature set. On a large-scale corpus, we achieve 76.7% accuracy for century-scale prediction and 26.1% for decade-scale classification, substantially above random baselines (20% and 2.3%). Under relaxed temporal precision, performance increases to 96.0% top-2 accuracy for centuries and 85.8% top-10 accuracy for decades. The final model exhibits strong ranking capabilities with AUCROC up to 94.8% and AUPRC up to 83.3%, and maintains controlled errors with mean absolute deviations of 27 years and 30 years, respectively. For authentication-style tasks, binary models around key thresholds (e.g., 1850-1900) reach 85-98% accuracy. Feature importance analysis identifies distance features and lexical structure as most informative, with compression-based features providing complementary signals. SHAP explainability reveals systematic linguistic evolution patterns, with the 19th century emerging as a pivot point across feature domains. Cross-dataset evaluation on Project Gutenberg highlights domain adaptation challenges, with accuracy dropping by 26.4 percentage points, yet the computational efficiency and interpretability of tree-based models still offer a scalable, explainable alternative to neural architectures.", "AI": {"tldr": "\u4f7f\u7528\u53ef\u89e3\u91ca\u7684\u7279\u5f81\u5de5\u7a0b\u6811\u6a21\u578b\u8fdb\u884c\u5386\u53f2\u6587\u672c\u5e74\u4ee3\u5206\u7c7b\uff0c\u6574\u5408\u538b\u7f29\u3001\u8bcd\u6c47\u7ed3\u6784\u3001\u53ef\u8bfb\u6027\u3001\u65b0\u8bcd\u68c0\u6d4b\u548c\u8ddd\u79bb\u7279\u5f81\uff0c\u5728\u4e16\u7eaa\u548c\u5341\u5e74\u5c3a\u5ea6\u4e0a\u53d6\u5f97\u663e\u8457\u9884\u6d4b\u6027\u80fd", "motivation": "\u51c6\u786e\u786e\u5b9a\u5386\u53f2\u6587\u672c\u7684\u5e74\u4ee3\u5bf9\u4e8e\u6587\u5316\u9057\u4ea7\u6536\u85cf\u7684\u7ec4\u7ec7\u548c\u89e3\u91ca\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u89e3\u91ca\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6765\u66ff\u4ee3\u9ed1\u76d2\u795e\u7ecf\u7f51\u7edc\u6a21\u578b", "method": "\u6574\u5408\u4e94\u79cd\u7279\u5f81\u7c7b\u522b\uff1a\u538b\u7f29\u7279\u5f81\u3001\u8bcd\u6c47\u7ed3\u6784\u7279\u5f81\u3001\u53ef\u8bfb\u6027\u7279\u5f81\u3001\u65b0\u8bcd\u68c0\u6d4b\u7279\u5f81\u548c\u8ddd\u79bb\u7279\u5f81\uff0c\u4f7f\u7528\u57fa\u4e8e\u6811\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u65f6\u95f4\u5206\u7c7b", "result": "\u5728\u4e16\u7eaa\u5c3a\u5ea6\u9884\u6d4b\u8fbe\u523076.7%\u51c6\u786e\u7387\uff0c\u5341\u5e74\u5c3a\u5ea6\u8fbe\u523026.1%\uff1b\u5728\u5bbd\u677e\u65f6\u95f4\u7cbe\u5ea6\u4e0b\uff0c\u4e16\u7eaatop-2\u51c6\u786e\u738796.0%\uff0c\u5341\u5e74top-10\u51c6\u786e\u738785.8%\uff1bAUCROC\u6700\u9ad894.8%\uff0cAUPRC\u6700\u9ad883.3%\uff1b\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u663e\u793a\u8ddd\u79bb\u7279\u5f81\u548c\u8bcd\u6c47\u7ed3\u6784\u6700\u6709\u6548", "conclusion": "\u7279\u5f81\u5de5\u7a0b\u6811\u6a21\u578b\u4e3a\u5386\u53f2\u6587\u672c\u5e74\u4ee3\u5206\u7c7b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e0d\u540c\u7279\u5f81\u57df\u63d0\u4f9b\u4e92\u8865\u7684\u65f6\u95f4\u4fe1\u53f7\uff0c19\u4e16\u7eaa\u662f\u8bed\u8a00\u6f14\u53d8\u7684\u5173\u952e\u8f6c\u6298\u70b9\uff0c\u4f46\u8de8\u6570\u636e\u96c6\u8bc4\u4f30\u663e\u793a\u9886\u57df\u9002\u5e94\u6311\u6218"}}
{"id": "2511.23057", "categories": ["cs.CL", "cs.LG", "econ.GN"], "pdf": "https://arxiv.org/pdf/2511.23057", "abs": "https://arxiv.org/abs/2511.23057", "authors": ["Sidharth Rony", "Jack Patman"], "title": "Standard Occupation Classifier -- A Natural Language Processing Approach", "comment": null, "summary": "Standard Occupational Classifiers (SOC) are systems used to categorize and classify different types of jobs and occupations based on their similarities in terms of job duties, skills, and qualifications. Integrating these facets with Big Data from job advertisement offers the prospect to investigate labour demand that is specific to various occupations. This project investigates the use of recent developments in natural language processing to construct a classifier capable of assigning an occupation code to a given job advertisement. We develop various classifiers for both UK ONS SOC and US O*NET SOC, using different Language Models. We find that an ensemble model, which combines Google BERT and a Neural Network classifier while considering job title, description, and skills, achieved the highest prediction accuracy. Specifically, the ensemble model exhibited a classification accuracy of up to 61% for the lower (or fourth) tier of SOC, and 72% for the third tier of SOC. This model could provide up to date, accurate information on the evolution of the labour market using job advertisements.", "AI": {"tldr": "\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u6784\u5efa\u804c\u4e1a\u5206\u7c7b\u5668\uff0c\u901a\u8fc7\u96c6\u6210Google BERT\u548c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7ed3\u5408\u804c\u4f4d\u6807\u9898\u3001\u63cf\u8ff0\u548c\u6280\u80fd\u4fe1\u606f\uff0c\u5b9e\u73b0\u5bf9\u62db\u8058\u5e7f\u544a\u7684\u804c\u4e1a\u7f16\u7801\u81ea\u52a8\u5206\u7c7b\u3002", "motivation": "\u5c06\u6807\u51c6\u804c\u4e1a\u5206\u7c7b\u7cfb\u7edf\uff08SOC\uff09\u4e0e\u62db\u8058\u5e7f\u544a\u5927\u6570\u636e\u7ed3\u5408\uff0c\u53ef\u4ee5\u7814\u7a76\u7279\u5b9a\u804c\u4e1a\u7684\u52b3\u52a8\u529b\u9700\u6c42\u3002\u5f53\u524d\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u4e3a\u62db\u8058\u5e7f\u544a\u5206\u914d\u804c\u4e1a\u4ee3\u7801\u7684\u5206\u7c7b\u5668\uff0c\u4ee5\u63d0\u4f9b\u53ca\u65f6\u51c6\u786e\u7684\u52b3\u52a8\u529b\u5e02\u573a\u6f14\u53d8\u4fe1\u606f\u3002", "method": "\u5f00\u53d1\u591a\u79cd\u5206\u7c7b\u5668\u7528\u4e8e\u82f1\u56fdONS SOC\u548c\u7f8e\u56fdO*NET SOC\u7cfb\u7edf\uff0c\u4f7f\u7528\u4e0d\u540c\u7684\u8bed\u8a00\u6a21\u578b\u3002\u6700\u7ec8\u6784\u5efa\u96c6\u6210\u6a21\u578b\uff0c\u7ed3\u5408Google BERT\u548c\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u5668\uff0c\u540c\u65f6\u8003\u8651\u804c\u4f4d\u6807\u9898\u3001\u63cf\u8ff0\u548c\u6280\u80fd\u4fe1\u606f\u3002", "result": "\u96c6\u6210\u6a21\u578b\u5728SOC\u7b2c\u56db\u5c42\u7ea7\uff08\u8f83\u4f4e\u5c42\u7ea7\uff09\u7684\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523061%\uff0c\u5728\u7b2c\u4e09\u5c42\u7ea7\u7684\u51c6\u786e\u7387\u8fbe\u523072%\u3002\u8be5\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u57fa\u4e8e\u62db\u8058\u5e7f\u544a\u7684\u53ca\u65f6\u51c6\u786e\u7684\u52b3\u52a8\u529b\u5e02\u573a\u6f14\u53d8\u4fe1\u606f\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u53ef\u4ee5\u6709\u6548\u6784\u5efa\u804c\u4e1a\u5206\u7c7b\u5668\uff0c\u96c6\u6210\u6a21\u578b\u5728\u804c\u4e1a\u7f16\u7801\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u4e3a\u52b3\u52a8\u529b\u5e02\u573a\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u5de5\u5177\u3002"}}
{"id": "2511.23059", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23059", "abs": "https://arxiv.org/abs/2511.23059", "authors": ["Jiatong Han"], "title": "Conveying Imagistic Thinking in TCM Translation: A Prompt Engineering and LLM-Based Evaluation Framework", "comment": "3 figures", "summary": "Traditional Chinese Medicine theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis. Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient and replicable HITL methodological pathway for translation of ancient, concept-dense texts like TCM.", "AI": {"tldr": "\u672c\u7814\u7a76\u91c7\u7528\u4eba\u673a\u534f\u540c\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u5f15\u5bfcDeepSeek V3.1\u8bc6\u522b\u300a\u9ec4\u5e1d\u5185\u7ecf\u300b\u4e2d\u7684\u9690\u55bb\u548c\u8f6c\u55bb\uff0c\u751f\u6210\u8ba4\u77e5\u4f18\u5316\u7684\u7ffb\u8bd1\uff0c\u76f8\u6bd4\u4eba\u5de5\u7ffb\u8bd1\u548c\u57fa\u7ebf\u6a21\u578b\u7ffb\u8bd1\u5728\u4e94\u4e2a\u8ba4\u77e5\u7ef4\u5ea6\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4e2d\u533b\u7406\u8bba\u5efa\u7acb\u5728\u610f\u8c61\u601d\u7ef4\u57fa\u7840\u4e0a\uff0c\u73b0\u6709\u82f1\u8bd1\u591a\u91c7\u7528\u76f4\u8bd1\uff0c\u96be\u4ee5\u8ba9\u76ee\u6807\u8bed\u8bfb\u8005\u91cd\u6784\u6982\u5ff5\u7f51\u7edc\u5e76\u5e94\u7528\u4e8e\u4e34\u5e8a\u5b9e\u8df5\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7ffb\u8bd1\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4eba\u673a\u534f\u540c\u6846\u67b6\uff0c\u9009\u53d6\u300a\u9ec4\u5e1d\u5185\u7ecf\u300b\u56db\u6bb5\u57fa\u7840\u7406\u8bba\u6587\u672c\uff0c\u901a\u8fc7\u63d0\u793a\u5f0f\u8ba4\u77e5\u652f\u67b6\u5f15\u5bfcDeepSeek V3.1\u8bc6\u522b\u9690\u55bb\u548c\u8f6c\u55bb\uff0c\u751f\u6210\u7ffb\u8bd1\u3002\u8bc4\u4f30\u9636\u6bb5\u7528ChatGPT 5 Pro\u548cGemini 2.5 Pro\u6a21\u62df\u4e09\u7c7b\u771f\u5b9e\u8bfb\u8005\uff0c\u5bf9\u4eba\u5de5\u7ffb\u8bd1\u3001\u57fa\u7ebf\u6a21\u578b\u7ffb\u8bd1\u548c\u63d0\u793a\u8c03\u6574\u7ffb\u8bd1\u8fdb\u884c\u4e94\u7ef4\u5ea6\u8bc4\u5206\uff0c\u5e76\u8fdb\u884c\u7ed3\u6784\u5316\u8bbf\u8c08\u548c\u89e3\u91ca\u73b0\u8c61\u5b66\u5206\u6790\u3002", "result": "\u63d0\u793a\u8c03\u6574\u7684LLM\u7ffb\u8bd1\u5728\u6240\u6709\u4e94\u4e2a\u8ba4\u77e5\u7ef4\u5ea6\u8868\u73b0\u6700\u4f73\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u8de8\u6a21\u578b\u548c\u8de8\u89d2\u8272\u4e00\u81f4\u6027\u3002\u8bbf\u8c08\u4e3b\u9898\u63ed\u793a\u4e86\u4eba\u5de5\u4e0e\u673a\u5668\u7ffb\u8bd1\u7684\u5dee\u5f02\u3001\u9690\u55bb\u548c\u8f6c\u55bb\u4f20\u9012\u7684\u6709\u6548\u7b56\u7565\u4ee5\u53ca\u8bfb\u8005\u7684\u8ba4\u77e5\u504f\u597d\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u4e2d\u533b\u7b49\u6982\u5ff5\u5bc6\u96c6\u7684\u53e4\u7c4d\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u4e00\u6761\u8ba4\u77e5\u3001\u9ad8\u6548\u4e14\u53ef\u590d\u5236\u7684\u4eba\u673a\u534f\u540c\u65b9\u6cd5\u8def\u5f84\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4f20\u8fbe\u4e2d\u533b\u7406\u8bba\u7684\u610f\u8c61\u601d\u7ef4\u548c\u6982\u5ff5\u7f51\u7edc\u3002"}}
{"id": "2511.23088", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23088", "abs": "https://arxiv.org/abs/2511.23088", "authors": ["Akhil Rajeev P", "Annarao Kulkarni"], "title": "Accent Placement Models for Rigvedic Sanskrit Text", "comment": "Submitted to AACL-IJCNLP 2025", "summary": "The Rigveda, among the oldest Indian texts in Vedic Sanskrit, employs a distinctive pitch-accent system : ud\u0101tta, anud\u0101tta, svarita whose marks encode melodic and interpretive cues but are often absent from modern e-texts. This work develops a parallel corpus of accented-unaccented \u015blokas and conducts a controlled comparison of three strategies for automatic accent placement in Rigvedic verse: (i) full fine-tuning of ByT5, a byte-level Transformer that operates directly on Unicode combining marks, (ii) a from-scratch BiLSTM-CRF sequence-labeling baseline, and (iii) LoRA-based parameter-efficient fine-tuning atop ByT5.\n  Evaluation uses Word Error Rate (WER) and Character Error Rate (CER) for orthographic fidelity, plus a task-specific Diacritic Error Rate (DER) that isolates accent edits. Full ByT5 fine-tuning attains the lowest error across all metrics; LoRA offers strong efficiency-accuracy trade-offs, and BiLSTM-CRF serves as a transparent baseline. The study underscores practical requirements for accent restoration - Unicode-safe preprocessing, mark-aware tokenization, and evaluation that separates grapheme from accent errors - and positions heritage-language technology as an emerging NLP area connecting computational modeling with philological and pedagogical aims. Results establish reproducible baselines for Rigvedic accent restoration and provide guidance for downstream tasks such as accent-aware OCR, ASR/chant synthesis, and digital scholarship.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u5e26\u91cd\u97f3/\u65e0\u91cd\u97f3\u300a\u68a8\u4ff1\u5420\u9640\u300b\u8bd7\u8282\u7684\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u6bd4\u8f83\u4e86\u4e09\u79cd\u81ea\u52a8\u91cd\u97f3\u6807\u6ce8\u7b56\u7565\uff1a\u5168\u5fae\u8c03ByT5\u3001\u4ece\u5934\u8bad\u7ec3\u7684BiLSTM-CRF\u57fa\u7ebf\u3001\u4ee5\u53ca\u57fa\u4e8eLoRA\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u3002\u5168\u5fae\u8c03ByT5\u5728\u6240\u6709\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u300a\u68a8\u4ff1\u5420\u9640\u300b\u4f5c\u4e3a\u6700\u53e4\u8001\u7684\u68b5\u6587\u6587\u672c\u4e4b\u4e00\uff0c\u4f7f\u7528\u72ec\u7279\u7684\u97f3\u9ad8\u91cd\u97f3\u7cfb\u7edf\uff08ud\u0101tta\u3001anud\u0101tta\u3001svarita\uff09\uff0c\u8fd9\u4e9b\u91cd\u97f3\u6807\u8bb0\u7f16\u7801\u4e86\u65cb\u5f8b\u548c\u89e3\u91ca\u7ebf\u7d22\uff0c\u4f46\u5728\u73b0\u4ee3\u7535\u5b50\u6587\u672c\u4e2d\u7ecf\u5e38\u7f3a\u5931\u3002\u9700\u8981\u5f00\u53d1\u81ea\u52a8\u91cd\u97f3\u6062\u590d\u6280\u672f\u6765\u652f\u6301\u6587\u5316\u9057\u4ea7\u8bed\u8a00\u7684\u7814\u7a76\u3001\u6559\u5b66\u548c\u6570\u5b57\u5316\u5e94\u7528\u3002", "method": "1. \u6784\u5efa\u5e26\u91cd\u97f3/\u65e0\u91cd\u97f3\u8bd7\u8282\u7684\u5e73\u884c\u8bed\u6599\u5e93\uff1b2. \u6bd4\u8f83\u4e09\u79cd\u81ea\u52a8\u91cd\u97f3\u6807\u6ce8\u7b56\u7565\uff1a\u5168\u5fae\u8c03ByT5\uff08\u76f4\u63a5\u5728Unicode\u7ec4\u5408\u6807\u8bb0\u4e0a\u64cd\u4f5c\u7684\u5b57\u8282\u7ea7Transformer\uff09\u3001\u4ece\u5934\u8bad\u7ec3\u7684BiLSTM-CRF\u5e8f\u5217\u6807\u6ce8\u57fa\u7ebf\u3001\u57fa\u4e8eLoRA\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff1b3. \u4f7f\u7528WER\u3001CER\u548c\u4e13\u95e8\u7684\u91cd\u97f3\u9519\u8bef\u7387\uff08DER\uff09\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5168\u5fae\u8c03ByT5\u5728\u6240\u6709\u6307\u6807\uff08WER\u3001CER\u3001DER\uff09\u4e0a\u8fbe\u5230\u6700\u4f4e\u9519\u8bef\u7387\uff1bLoRA\u63d0\u4f9b\u4e86\u826f\u597d\u7684\u6548\u7387-\u51c6\u786e\u6027\u6743\u8861\uff1bBiLSTM-CRF\u4f5c\u4e3a\u900f\u660e\u57fa\u7ebf\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u91cd\u97f3\u6062\u590d\u7684\u5b9e\u9645\u8981\u6c42\uff1aUnicode\u5b89\u5168\u9884\u5904\u7406\u3001\u6807\u8bb0\u611f\u77e5\u5206\u8bcd\u3001\u4ee5\u53ca\u5206\u79bb\u5b57\u5f62\u548c\u91cd\u97f3\u9519\u8bef\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u300a\u68a8\u4ff1\u5420\u9640\u300b\u91cd\u97f3\u6062\u590d\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u57fa\u7ebf\uff0c\u4e3a\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u91cd\u97f3\u611f\u77e5OCR\u3001ASR/\u541f\u5531\u5408\u6210\u3001\u6570\u5b57\u5b66\u672f\u7814\u7a76\uff09\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002\u5c06\u9057\u4ea7\u8bed\u8a00\u6280\u672f\u5b9a\u4f4d\u4e3a\u8fde\u63a5\u8ba1\u7b97\u5efa\u6a21\u4e0e\u6587\u732e\u5b66\u3001\u6559\u5b66\u76ee\u6807\u7684NLP\u65b0\u5174\u9886\u57df\u3002"}}
{"id": "2511.23101", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23101", "abs": "https://arxiv.org/abs/2511.23101", "authors": ["Francesco Di Cursi", "Chiara Boldrini", "Marco Conti", "Andrea Passarella"], "title": "Mind Reading or Misreading? LLMs on the Big Five Personality Test", "comment": "Funding: SoBigDatait (IR0000013), FAIR (PE00000013), ICSC (CN00000013)", "summary": "We evaluate large language models (LLMs) for automatic personality prediction from text under the binary Five Factor Model (BIG5). Five models -- including GPT-4 and lightweight open-source alternatives -- are tested across three heterogeneous datasets (Essays, MyPersonality, Pandora) and two prompting strategies (minimal vs. enriched with linguistic and psychological cues). Enriched prompts reduce invalid outputs and improve class balance, but also introduce a systematic bias toward predicting trait presence. Performance varies substantially: Openness and Agreeableness are relatively easier to detect, while Extraversion and Neuroticism remain challenging. Although open-source models sometimes approach GPT-4 and prior benchmarks, no configuration yields consistently reliable predictions in zero-shot binary settings. Moreover, aggregate metrics such as accuracy and macro-F1 mask significant asymmetries, with per-class recall offering clearer diagnostic value. These findings show that current out-of-the-box LLMs are not yet suitable for APPT, and that careful coordination of prompt design, trait framing, and evaluation metrics is essential for interpretable results.", "AI": {"tldr": "\u8bc4\u4f30LLM\u5728\u4e8c\u5143\u4e94\u56e0\u7d20\u6a21\u578b\u4e0b\u4ece\u6587\u672c\u81ea\u52a8\u9884\u6d4b\u4eba\u683c\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4e0d\u7a33\u5b9a\uff0c\u9700\u8981\u7cbe\u5fc3\u8bbe\u8ba1\u63d0\u793a\u548c\u8bc4\u4f30\u6307\u6807", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u4eba\u683c\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u8868\u73b0\uff0c\u63a2\u7d22\u4e0d\u540c\u6a21\u578b\u3001\u6570\u636e\u96c6\u548c\u63d0\u793a\u7b56\u7565\u5bf9\u9884\u6d4b\u6548\u679c\u7684\u5f71\u54cd", "method": "\u4f7f\u7528\u4e94\u4e2aLLM\uff08\u5305\u62ecGPT-4\u548c\u8f7b\u91cf\u7ea7\u5f00\u6e90\u6a21\u578b\uff09\u5728\u4e09\u4e2a\u5f02\u6784\u6570\u636e\u96c6\uff08Essays\u3001MyPersonality\u3001Pandora\uff09\u4e0a\u6d4b\u8bd5\uff0c\u91c7\u7528\u4e24\u79cd\u63d0\u793a\u7b56\u7565\uff08\u57fa\u7840\u63d0\u793avs\u5305\u542b\u8bed\u8a00\u5b66\u548c\u5fc3\u7406\u5b66\u7ebf\u7d22\u7684\u589e\u5f3a\u63d0\u793a\uff09", "result": "\u589e\u5f3a\u63d0\u793a\u51cf\u5c11\u65e0\u6548\u8f93\u51fa\u5e76\u6539\u5584\u7c7b\u522b\u5e73\u8861\uff0c\u4f46\u5f15\u5165\u7cfb\u7edf\u6027\u504f\u5411\u9884\u6d4b\u7279\u8d28\u5b58\u5728\uff1b\u5f00\u653e\u6027\u548c\u5b9c\u4eba\u6027\u76f8\u5bf9\u5bb9\u6613\u68c0\u6d4b\uff0c\u5916\u5411\u6027\u548c\u795e\u7ecf\u8d28\u8f83\u96be\uff1b\u5f00\u6e90\u6a21\u578b\u6709\u65f6\u63a5\u8fd1GPT-4\u548c\u5148\u524d\u57fa\u51c6\uff0c\u4f46\u6ca1\u6709\u914d\u7f6e\u80fd\u5728\u96f6\u6837\u672c\u4e8c\u5143\u8bbe\u7f6e\u4e2d\u4ea7\u751f\u4e00\u81f4\u53ef\u9760\u7684\u9884\u6d4b", "conclusion": "\u5f53\u524d\u5f00\u7bb1\u5373\u7528\u7684LLM\u5c1a\u4e0d\u9002\u5408\u81ea\u52a8\u4eba\u683c\u9884\u6d4b\u4efb\u52a1\uff0c\u9700\u8981\u7cbe\u5fc3\u534f\u8c03\u63d0\u793a\u8bbe\u8ba1\u3001\u7279\u8d28\u6846\u67b6\u548c\u8bc4\u4f30\u6307\u6807\u624d\u80fd\u83b7\u5f97\u53ef\u89e3\u91ca\u7684\u7ed3\u679c"}}
{"id": "2511.23119", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23119", "abs": "https://arxiv.org/abs/2511.23119", "authors": ["Mengjie Liu", "Jiahui Peng", "Pei Chu", "Jiantao Qiu", "Ren Ma", "He Zhu", "Rui Min", "Lindong Lu", "Wenchang Ning", "Linfeng Hou", "Kaiwen Liu", "Yuan Qu", "Zhenxiang Li", "Chao Xu", "Zhongying Tu", "Wentao Zhang", "Conghui He"], "title": "Dripper: Token-Efficient Main HTML Extraction with a Lightweight LM", "comment": null, "summary": "Accurately and efficiently extracting main content from general web pages is of great significance for obtaining training data for large models. Using well-pre-trained decoder-only generative language models offers excellent document comprehension capabilities, thereby effectively enhancing parsing quality. However, it remains constrained by issues such as context window length, inference cost, and format hallucination. We present Dripper, an efficient HTML main content extraction framework powered by lightweight language models, which addresses these challenges through four key innovations: (1) We design a specialized HTML simplification algorithm that reduces input token count to 22\\% compared to raw HTML while preserving critical structural information; (2) We reformulate main content extraction as a semantic block sequence classification task, significantly reducing inference cost; (3) We introduce a controlled decoding mechanism that strictly constrains the output space through logits processors, effectively eliminating hallucination issues common in small-scale models; (4) We propose WebMainBench, an evaluation dataset containing over 7,800 web pages with meticulously human-annotated main content extraction labels. Experimental results demonstrate that using only a 0.6B parameter model, Dripper achieves state-of-the-art performance across all evaluation benchmarks and outperforms all baseline methods, attaining an ROUGE-N F1 score of 81.58\\%( 83.13\\% with fall-back strategy) on our proposed WebMainBench dataset.", "AI": {"tldr": "Dripper\uff1a\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548HTML\u4e3b\u8981\u5185\u5bb9\u63d0\u53d6\u6846\u67b6\uff0c\u901a\u8fc7HTML\u7b80\u5316\u3001\u5e8f\u5217\u5206\u7c7b\u3001\u53d7\u63a7\u89e3\u7801\u7b49\u521b\u65b0\uff0c\u4ec5\u75280.6B\u53c2\u6570\u5373\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u4ece\u7f51\u9875\u4e2d\u51c6\u786e\u9ad8\u6548\u63d0\u53d6\u4e3b\u8981\u5185\u5bb9\u5bf9\u83b7\u53d6\u5927\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u4e8e\u9884\u8bad\u7ec3\u751f\u6210\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u7a97\u53e3\u957f\u5ea6\u3001\u63a8\u7406\u6210\u672c\u548c\u683c\u5f0f\u5e7b\u89c9\u7b49\u95ee\u9898", "method": "1) \u8bbe\u8ba1\u4e13\u95e8\u7684HTML\u7b80\u5316\u7b97\u6cd5\uff0c\u5c06\u8f93\u5165token\u51cf\u5c11\u5230\u539f\u59cbHTML\u768422%\uff1b2) \u5c06\u4e3b\u8981\u5185\u5bb9\u63d0\u53d6\u91cd\u6784\u4e3a\u8bed\u4e49\u5757\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\uff1b3) \u5f15\u5165\u53d7\u63a7\u89e3\u7801\u673a\u5236\u901a\u8fc7logits\u5904\u7406\u5668\u4e25\u683c\u7ea6\u675f\u8f93\u51fa\u7a7a\u95f4\uff1b4) \u63d0\u51fa\u5305\u542b7800+\u7f51\u9875\u7684WebMainBench\u8bc4\u4f30\u6570\u636e\u96c6", "result": "\u4ec5\u4f7f\u75280.6B\u53c2\u6570\u6a21\u578b\uff0cDripper\u5728\u6240\u6709\u8bc4\u4f30\u57fa\u51c6\u4e0a\u5747\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5728WebMainBench\u6570\u636e\u96c6\u4e0a\u83b7\u5f9781.58%\u7684ROUGE-N F1\u5206\u6570\uff08\u4f7f\u7528\u56de\u9000\u7b56\u7565\u53ef\u8fbe83.13%\uff09", "conclusion": "Dripper\u901a\u8fc7\u521b\u65b0\u7684HTML\u7b80\u5316\u3001\u4efb\u52a1\u91cd\u6784\u548c\u53d7\u63a7\u89e3\u7801\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u4e0a\u4e0b\u6587\u7a97\u53e3\u3001\u63a8\u7406\u6210\u672c\u548c\u5e7b\u89c9\u65b9\u9762\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u51c6\u786e\u7684\u4e3b\u8981\u5185\u5bb9\u63d0\u53d6"}}
{"id": "2511.23136", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23136", "abs": "https://arxiv.org/abs/2511.23136", "authors": ["Yujiao Yang", "Jing Lian", "Linhui Li"], "title": "Multi-chain Graph Refinement and Selection for Reliable Reasoning in Large Language Models", "comment": null, "summary": "The complex reasoning ability of Large Language Models (LLMs) poses a critical bottleneck for their practical applications. Test-time expansion methods such as Tree-of-Thought (ToT) and Graph-of-Thought (GoT) enhance reasoning by introducing intermediate reasoning structures, tree search, or graph-based exploration mechanisms. However, their reasoning strategies suffer from limited diversity, redundant search branches, and inadequate integration and error correction across heterogeneous reasoning paths. To address these limitations, we propose a novel reasoning framework called Multi-chain Graph Refinement & Selection (MGRS), which first generates multiple diverse reasoning trajectories for a given problem, refines candidate responses using a composite self- and cross-verification strategy, then constructs a reasoning relation graph and estimates the success rate of intermediate nodes, and finally computes cumulative success rates to select the most reliable answer and corresponding reasoning trajectory. Experimental results demonstrate that MGRS significantly advances both the reasoning capability and computational efficiency of reasoning enhancement methods. Across six benchmark datasets spanning four distinct tasks, MGRS achieves an average accuracy of 82.9%, outperforming state-of-the-art baselines by a clear margin of 2.1%. Remarkably, on the 24-point game, MGRS attains 100% accuracy for the first time, while delivering a 13.6x speed-up compared to the leading Forest of Thoughts framework.", "AI": {"tldr": "MGRS\u662f\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u63a8\u7406\u8f68\u8ff9\u3001\u590d\u5408\u9a8c\u8bc1\u3001\u6784\u5efa\u63a8\u7406\u5173\u7cfb\u56fe\u5e76\u8ba1\u7b97\u7d2f\u79ef\u6210\u529f\u7387\uff0c\u663e\u8457\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\uff08\u5982ToT\u3001GoT\uff09\u5b58\u5728\u63a8\u7406\u7b56\u7565\u591a\u6837\u6027\u6709\u9650\u3001\u641c\u7d22\u5206\u652f\u5197\u4f59\u3001\u5f02\u6784\u63a8\u7406\u8def\u5f84\u6574\u5408\u4e0e\u7ea0\u9519\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86LLM\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u591a\u94fe\u56fe\u7cbe\u70bc\u4e0e\u9009\u62e9\uff08MGRS\uff09\u6846\u67b6\uff1a1\uff09\u751f\u6210\u591a\u4e2a\u591a\u6837\u5316\u63a8\u7406\u8f68\u8ff9\uff1b2\uff09\u4f7f\u7528\u590d\u5408\u81ea\u9a8c\u8bc1\u548c\u4ea4\u53c9\u9a8c\u8bc1\u7b56\u7565\u7cbe\u70bc\u5019\u9009\u54cd\u5e94\uff1b3\uff09\u6784\u5efa\u63a8\u7406\u5173\u7cfb\u56fe\u5e76\u4f30\u8ba1\u4e2d\u95f4\u8282\u70b9\u6210\u529f\u7387\uff1b4\uff09\u8ba1\u7b97\u7d2f\u79ef\u6210\u529f\u7387\u4ee5\u9009\u62e9\u6700\u53ef\u9760\u7b54\u6848\u548c\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5728\u6db5\u76d6\u56db\u4e2a\u4efb\u52a1\u7684\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMGRS\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523082.9%\uff0c\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u9ad8\u51fa2.1%\u3002\u572824\u70b9\u6e38\u620f\u4e2d\u9996\u6b21\u5b9e\u73b0100%\u51c6\u786e\u7387\uff0c\u540c\u65f6\u6bd4\u9886\u5148\u7684Forest of Thoughts\u6846\u67b6\u5feb13.6\u500d\u3002", "conclusion": "MGRS\u901a\u8fc7\u591a\u6837\u5316\u63a8\u7406\u8f68\u8ff9\u751f\u6210\u3001\u590d\u5408\u9a8c\u8bc1\u673a\u5236\u548c\u56fe\u57fa\u9009\u62e9\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u63a8\u7406\u589e\u5f3a\u65b9\u6cd5\u7684\u63a8\u7406\u80fd\u529b\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2511.23174", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23174", "abs": "https://arxiv.org/abs/2511.23174", "authors": ["Neemesh Yadav", "Francesco Ortu", "Jiarui Liu", "Joeun Yook", "Bernhard Sch\u00f6lkopf", "Rada Mihalcea", "Alberto Cazzaniga", "Zhijing Jin"], "title": "Are LLMs Good Safety Agents or a Propaganda Engine?", "comment": "15 pages, 7 tables, 4 figures", "summary": "Large Language Models (LLMs) are trained to refuse to respond to harmful content. However, systematic analyses of whether this behavior is truly a reflection of its safety policies or an indication of political censorship, that is practiced globally by countries, is lacking. Differentiating between safety influenced refusals or politically motivated censorship is hard and unclear. For this purpose we introduce PSP, a dataset built specifically to probe the refusal behaviors in LLMs from an explicitly political context. PSP is built by formatting existing censored content from two data sources, openly available on the internet: sensitive prompts in China generalized to multiple countries, and tweets that have been censored in various countries. We study: 1) impact of political sensitivity in seven LLMs through data-driven (making PSP implicit) and representation-level approaches (erasing the concept of politics); and, 2) vulnerability of models on PSP through prompt injection attacks (PIAs). Associating censorship with refusals on content with masked implicit intent, we find that most LLMs perform some form of censorship. We conclude with summarizing major attributes that can cause a shift in refusal distributions across models and contexts of different countries.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86PSP\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u63a2\u7a76LLMs\u62d2\u7edd\u56de\u7b54\u884c\u4e3a\u80cc\u540e\u7684\u653f\u6cbb\u5ba1\u67e5\u56e0\u7d20\uff0c\u53d1\u73b0\u5927\u591a\u6570LLM\u90fd\u5b58\u5728\u67d0\u79cd\u5f62\u5f0f\u7684\u5ba1\u67e5\u673a\u5236", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9LLMs\u62d2\u7edd\u56de\u7b54\u884c\u4e3a\u80cc\u540e\u52a8\u673a\u7684\u7cfb\u7edf\u5206\u6790\u2014\u2014\u662f\u771f\u6b63\u7684\u5b89\u5168\u653f\u7b56\u8fd8\u662f\u653f\u6cbb\u5ba1\u67e5\uff1f\u9700\u8981\u533a\u5206\u5b89\u5168\u62d2\u7edd\u4e0e\u653f\u6cbb\u5ba1\u67e5\u7684\u5dee\u5f02", "method": "\u6784\u5efaPSP\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea\u4e2d\u56fd\u7684\u654f\u611f\u63d0\u793a\u548c\u5404\u56fd\u88ab\u5ba1\u67e5\u7684\u63a8\u6587\uff1b\u91c7\u7528\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff08\u4f7fPSP\u9690\u542b\u5316\uff09\u548c\u8868\u793a\u5c42\u9762\u65b9\u6cd5\uff08\u6d88\u9664\u653f\u6cbb\u6982\u5ff5\uff09\uff1b\u4f7f\u7528\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u6d4b\u8bd5\u6a21\u578b\u8106\u5f31\u6027", "result": "\u5927\u591a\u6570LLM\u90fd\u5b58\u5728\u67d0\u79cd\u5f62\u5f0f\u7684\u5ba1\u67e5\u884c\u4e3a\uff1b\u901a\u8fc7\u5c06\u5ba1\u67e5\u4e0e\u5bf9\u9690\u542b\u610f\u56fe\u5185\u5bb9\u7684\u62d2\u7edd\u5173\u8054\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4e0d\u540c\u56fd\u5bb6\u8bed\u5883\u4e0b\u7684\u62d2\u7edd\u5206\u5e03\u5b58\u5728\u5dee\u5f02", "conclusion": "\u603b\u7ed3\u4e86\u5bfc\u81f4\u4e0d\u540c\u6a21\u578b\u548c\u56fd\u5bb6\u8bed\u5883\u4e0b\u62d2\u7edd\u5206\u5e03\u53d8\u5316\u7684\u4e3b\u8981\u5c5e\u6027\uff0c\u63ed\u793a\u4e86LLMs\u4e2d\u5b58\u5728\u7684\u653f\u6cbb\u5ba1\u67e5\u673a\u5236"}}
{"id": "2511.23184", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23184", "abs": "https://arxiv.org/abs/2511.23184", "authors": ["Wenna Lai", "Haoran Xie", "Guandong Xu", "Qing Li", "S. Joe Qin"], "title": "Listwise Preference Optimization with Element-wise Confusions for Aspect Sentiment Quad Prediction", "comment": "11 pages, 7 figures, and 6 tables", "summary": "Aspect sentiment quad prediction (ASQP) is inherently challenging to predict a structured quadruple with four core sentiment elements, including aspect term (a), aspect category (c), opinion term (o), and sentiment polarity (s). Prior methods relying on marker-based prediction struggle with modeling the intricate relationships among elements and experience sharp performance declines when predicting higher-order elements (e.g., c and s) under standard supervised fine-tuning. To address these limitations, we employ reasoning-based generation to output both the quadruple and a natural language rationale under element prefixes within a unified template, encouraging explicit relational reasoning and interpretability. To further enhance element-wise alignment, we introduce a listwise preference optimization framework for improving structural validity and relational coherence. Specifically, we generate element-wise confusable candidates via syntactic and semantic proximity, then train the model with listwise objectives to prefer the gold candidates over closely competing alternatives. Extensive experiments on four benchmark datasets demonstrate that our framework effectively improves quadruple prediction accuracy and explanation consistency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u751f\u6210\u548c\u5217\u8868\u504f\u597d\u4f18\u5316\u7684ASQP\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u589e\u5f3a\u5143\u7d20\u5173\u7cfb\u5efa\u6a21\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u663e\u8457\u63d0\u5347\u56db\u5143\u7ec4\u9884\u6d4b\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6807\u8bb0\u9884\u6d4b\u7684\u65b9\u6cd5\u96be\u4ee5\u5efa\u6a21ASQP\u4efb\u52a1\u4e2d\u56db\u4e2a\u60c5\u611f\u5143\u7d20\uff08a, c, o, s\uff09\u4e4b\u95f4\u7684\u590d\u6742\u5173\u7cfb\uff0c\u5728\u9884\u6d4b\u9ad8\u9636\u5143\u7d20\uff08\u5982c\u548cs\uff09\u65f6\u6027\u80fd\u4e0b\u964d\u660e\u663e\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5efa\u6a21\u65b9\u6cd5", "method": "1) \u91c7\u7528\u57fa\u4e8e\u63a8\u7406\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u7edf\u4e00\u6a21\u677f\u4e2d\u8f93\u51fa\u56db\u5143\u7ec4\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u8fc7\u7a0b\uff1b2) \u5f15\u5165\u5217\u8868\u504f\u597d\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u53e5\u6cd5\u548c\u8bed\u4e49\u90bb\u8fd1\u751f\u6210\u6df7\u6dc6\u5019\u9009\uff0c\u8bad\u7ec3\u6a21\u578b\u504f\u597d\u9ec4\u91d1\u5019\u9009\u800c\u975e\u7ade\u4e89\u66ff\u4ee3\u9879", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u56db\u5143\u7ec4\u9884\u6d4b\u51c6\u786e\u6027\u548c\u89e3\u91ca\u4e00\u81f4\u6027", "conclusion": "\u901a\u8fc7\u63a8\u7406\u751f\u6210\u548c\u5217\u8868\u504f\u597d\u4f18\u5316\u7684\u7ed3\u5408\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5efa\u6a21ASQP\u4efb\u52a1\u4e2d\u7684\u5143\u7d20\u5173\u7cfb\uff0c\u63d0\u5347\u7ed3\u6784\u5316\u9884\u6d4b\u7684\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027"}}
{"id": "2511.23225", "categories": ["cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.23225", "abs": "https://arxiv.org/abs/2511.23225", "authors": ["Guang Liang", "Jie Shao", "Ningyuan Tang", "Xinyao Liu", "Jianxin Wu"], "title": "TWEO: Transformers Without Extreme Outliers Enables FP8 Training And Quantization For Dummies", "comment": null, "summary": "Native FP8 support in modern hardware is essential for training large Transformers, but is severely hindered by extreme activation outliers. Existing solutions either rely on complex mixed-precision engineering or invasive architectural modifications. This paper fundamentally challenges the conventional wisdom that outliers are data-driven. We demonstrate that extreme outliers are a data-independent, mechanically-produced artifact of training, originating from specific structural properties of the weight matrices (i.e., colinearity). Based on this insight, we propose TWEO (Transformers Without Extreme Outliers), a novel, non-invasive loss function. TWEO effectively prevents extreme outliers via a very simple loss term, which reduces outliers from 10000+ to less than 20. TWEO then enables full-model FP8 pre-training with neither engineering tricks nor architectural changes for both LLM and ViT. When standard FP8 training catastrophically collapses, TWEO achieves performance comparable to the BF16 baseline while delivering a 36% increase in training throughput. Also, TWEO enables a new quantization paradigm. Hardware-friendly W8A8 per-tensor static quantization of LLMs, previously considered completely unusable due to outliers, achieves SOTA performance for the first time on TWEO-trained models.", "AI": {"tldr": "TWEO\u662f\u4e00\u79cd\u65b0\u578b\u975e\u4fb5\u5165\u5f0f\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u89e3\u51b3\u6743\u91cd\u77e9\u9635\u5171\u7ebf\u6027\u95ee\u9898\uff0c\u6709\u6548\u6d88\u9664Transformer\u8bad\u7ec3\u4e2d\u7684\u6781\u7aef\u5f02\u5e38\u503c\uff0c\u5b9e\u73b0\u5168\u6a21\u578bFP8\u9884\u8bad\u7ec3\uff0c\u65e0\u9700\u5de5\u7a0b\u6280\u5de7\u6216\u67b6\u6784\u4fee\u6539\u3002", "motivation": "\u73b0\u4ee3\u786c\u4ef6\u5bf9FP8\u7684\u539f\u751f\u652f\u6301\u5bf9\u4e8e\u8bad\u7ec3\u5927\u578bTransformer\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6781\u7aef\u6fc0\u6d3b\u5f02\u5e38\u503c\u4e25\u91cd\u963b\u788d\u4e86FP8\u7684\u4f7f\u7528\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u4f9d\u8d56\u590d\u6742\u7684\u6df7\u5408\u7cbe\u5ea6\u5de5\u7a0b\uff0c\u8981\u4e48\u9700\u8981\u4fb5\u5165\u5f0f\u67b6\u6784\u4fee\u6539\u3002", "method": "\u8bba\u6587\u6311\u6218\u4e86\u5f02\u5e38\u503c\u662f\u6570\u636e\u9a71\u52a8\u7684\u4f20\u7edf\u89c2\u5ff5\uff0c\u8bc1\u660e\u6781\u7aef\u5f02\u5e38\u503c\u662f\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7531\u6743\u91cd\u77e9\u9635\u7684\u7ed3\u6784\u7279\u6027\uff08\u5171\u7ebf\u6027\uff09\u4ea7\u751f\u7684\u673a\u68b0\u6027\u4f2a\u5f71\u3002\u57fa\u4e8e\u6b64\uff0c\u63d0\u51fa\u4e86TWEO\uff08Transformers Without Extreme Outliers\uff09\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u975e\u4fb5\u5165\u5f0f\u635f\u5931\u51fd\u6570\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u635f\u5931\u9879\u6709\u6548\u9632\u6b62\u6781\u7aef\u5f02\u5e38\u503c\u3002", "result": "TWEO\u5c06\u5f02\u5e38\u503c\u4ece10000+\u51cf\u5c11\u5230\u5c0f\u4e8e20\uff0c\u5b9e\u73b0\u4e86\u5168\u6a21\u578bFP8\u9884\u8bad\u7ec3\uff0c\u5728\u6807\u51c6FP8\u8bad\u7ec3\u707e\u96be\u6027\u5d29\u6e83\u7684\u60c5\u51b5\u4e0b\uff0c\u6027\u80fd\u4e0eBF16\u57fa\u7ebf\u76f8\u5f53\uff0c\u540c\u65f6\u8bad\u7ec3\u541e\u5410\u91cf\u63d0\u9ad836%\u3002\u6b64\u5916\uff0cTWEO\u5b9e\u73b0\u4e86\u786c\u4ef6\u53cb\u597d\u7684W8A8\u6bcf\u5f20\u91cf\u9759\u6001\u91cf\u5316\uff0c\u5728TWEO\u8bad\u7ec3\u7684\u6a21\u578b\u4e0a\u9996\u6b21\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "TWEO\u901a\u8fc7\u89e3\u51b3\u5f02\u5e38\u503c\u7684\u6839\u672c\u539f\u56e0\u2014\u2014\u6743\u91cd\u77e9\u9635\u5171\u7ebf\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f7fFP8\u8bad\u7ec3\u6210\u4e3a\u53ef\u80fd\uff0c\u5e76\u5f00\u542f\u4e86\u65b0\u7684\u91cf\u5316\u8303\u5f0f\uff0c\u4e3a\u5927\u578bTransformer\u7684\u9ad8\u6548\u8bad\u7ec3\u63d0\u4f9b\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\u3002"}}
{"id": "2511.23235", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23235", "abs": "https://arxiv.org/abs/2511.23235", "authors": ["Praveen Gatla", "Anushka", "Nikita Kanwar", "Gouri Sahoo", "Rajesh Kumar Mundotiya"], "title": "Tourism Question Answer System in Indian Language using Domain-Adapted Foundation Models", "comment": null, "summary": "This article presents the first comprehensive study on designing a baseline extractive question-answering (QA) system for the Hindi tourism domain, with a specialized focus on the Varanasi-a cultural and spiritual hub renowned for its Bhakti-Bhaav (devotional ethos). Targeting ten tourism-centric subdomains-Ganga Aarti, Cruise, Food Court, Public Toilet, Kund, Museum, General, Ashram, Temple and Travel, the work addresses the absence of language-specific QA resources in Hindi for culturally nuanced applications. In this paper, a dataset comprising 7,715 Hindi QA pairs pertaining to Varanasi tourism was constructed and subsequently augmented with 27,455 pairs generated via Llama zero-shot prompting. We propose a framework leveraging foundation models-BERT and RoBERTa, fine-tuned using Supervised Fine-Tuning (SFT) and Low-Rank Adaptation (LoRA), to optimize parameter efficiency and task performance. Multiple variants of BERT, including pre-trained languages (e.g., Hindi-BERT), are evaluated to assess their suitability for low-resource domain-specific QA. Evaluation metrics - F1, BLEU, and ROUGE-L - highlight trade-offs between answer precision and linguistic fluency. Experiments demonstrate that LoRA-based fine-tuning achieves competitive performance (85.3\\% F1) while reducing trainable parameters by 98\\% compared to SFT, striking a balance between efficiency and accuracy. Comparative analysis across models reveals that RoBERTa with SFT outperforms BERT variants in capturing contextual nuances, particularly for culturally embedded terms (e.g., Aarti, Kund). This work establishes a foundational baseline for Hindi tourism QA systems, emphasizing the role of LORA in low-resource settings and underscoring the need for culturally contextualized NLP frameworks in the tourism domain.", "AI": {"tldr": "\u9996\u4e2a\u9488\u5bf9\u5370\u5730\u8bed\u65c5\u6e38\u9886\u57df\u7684\u62bd\u53d6\u5f0f\u95ee\u7b54\u7cfb\u7edf\u7814\u7a76\uff0c\u4e13\u6ce8\u4e8e\u74e6\u62c9\u7eb3\u897f\u65c5\u6e38\u6587\u5316\uff0c\u6784\u5efa\u6570\u636e\u96c6\u5e76\u8bc4\u4f30BERT/RoBERTa\u6a21\u578b\u5728SFT\u548cLoRA\u5fae\u8c03\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u5370\u5730\u8bed\u65c5\u6e38\u9886\u57df\u7f3a\u4e4f\u8bed\u8a00\u7279\u5b9a\u548c\u6587\u5316\u654f\u611f\u7684\u95ee\u7b54\u8d44\u6e90\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u74e6\u62c9\u7eb3\u897f\u8fd9\u4e00\u5177\u6709\u4e30\u5bcc\u6587\u5316\u5185\u6db5\u7684\u65c5\u6e38\u76ee\u7684\u5730\uff0c\u9700\u8981\u5efa\u7acb\u57fa\u7840\u7cfb\u7edf\u6765\u652f\u6301\u6587\u5316\u8bed\u5883\u5316\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u3002", "method": "\u6784\u5efa\u5305\u542b7,715\u4e2a\u5370\u5730\u8bed\u95ee\u7b54\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7Llama\u96f6\u6837\u672c\u63d0\u793a\u751f\u621027,455\u4e2a\u589e\u5f3a\u5bf9\u3002\u4f7f\u7528BERT\u548cRoBERTa\u7b49\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u4f4e\u79e9\u9002\u5e94(LoRA)\u4e24\u79cd\u5fae\u8c03\u7b56\u7565\uff0c\u8bc4\u4f30\u591a\u79cdBERT\u53d8\u4f53\u5728\u4f4e\u8d44\u6e90\u9886\u57df\u7279\u5b9a\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002", "result": "LoRA\u5fae\u8c03\u5728\u51cf\u5c1198%\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u540c\u65f6\u8fbe\u523085.3%\u7684F1\u5206\u6570\uff0c\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u95f4\u53d6\u5f97\u5e73\u8861\u3002RoBERTa+SFT\u5728\u6355\u6349\u6587\u5316\u5d4c\u5165\u672f\u8bed(\u5982Aarti\u3001Kund)\u7684\u4e0a\u4e0b\u6587\u7ec6\u5fae\u5dee\u522b\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002\u8bc4\u4f30\u6307\u6807(F1\u3001BLEU\u3001ROUGE-L)\u663e\u793a\u4e86\u7b54\u6848\u7cbe\u786e\u5ea6\u548c\u8bed\u8a00\u6d41\u7545\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5370\u5730\u8bed\u65c5\u6e38\u95ee\u7b54\u7cfb\u7edf\u5efa\u7acb\u4e86\u57fa\u7840\u57fa\u51c6\uff0c\u5f3a\u8c03\u4e86LoRA\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u7a81\u663e\u4e86\u5728\u65c5\u6e38\u9886\u57df\u5f00\u53d1\u6587\u5316\u8bed\u5883\u5316NLP\u6846\u67b6\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2511.23271", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23271", "abs": "https://arxiv.org/abs/2511.23271", "authors": ["Jiancheng Dong", "Pengyue Jia", "Jingyu Peng", "Maolin Wang", "Yuhao Wang", "Lixin Su", "Xin Sun", "Shuaiqiang Wang", "Dawei Yin", "Xiangyu Zhao"], "title": "Behavior-Equivalent Token: Single-Token Replacement for Long Prompts in LLMs", "comment": "15 pages, 5 figures", "summary": "Carefully engineered system prompts play a critical role in guiding the behavior of LLM agents, but their considerable length introduces significant drawbacks, including increased inference latency, higher computational cost, and reduced effective context length. This raises the question of whether such lengthy prompts can be replaced by a drastically reduced number of tokens while preserving their behavioral effect on downstream tasks. To enable this, we propose a lightweight three-stage training framework that learns a single prompt-specific Behavior-Equivalent token ([BE]). The framework first trains [BE] to encode the natural-language content of the original system prompt via reconstruction, and then distills the prompt 's downstream behavior into this single token. Importantly, our method requires no access to model internals, no auxiliary compression models, and no labeled responses. Empirical evaluations on three datasets show that a single [BE] token achieves up to a 3000x reduction in prompt length, while retaining about 98% of the downstream performance of the original system prompts. This substantially reduces inference cost and leaves almost the entire context window available for user inputs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u5355\u4e2a[BE]\u4ee4\u724c\u66ff\u4ee3\u5197\u957f\u7cfb\u7edf\u63d0\u793a\uff0c\u5b9e\u73b03000\u500d\u538b\u7f29\u540c\u65f6\u4fdd\u630198%\u4e0b\u6e38\u6027\u80fd", "motivation": "\u5197\u957f\u7684\u7cfb\u7edf\u63d0\u793a\u4f1a\u589e\u52a0\u63a8\u7406\u5ef6\u8fdf\u3001\u8ba1\u7b97\u6210\u672c\u548c\u51cf\u5c11\u6709\u6548\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u9700\u8981\u5bfb\u627e\u65e2\u80fd\u4fdd\u6301\u884c\u4e3a\u6548\u679c\u53c8\u80fd\u5927\u5e45\u538b\u7f29\u63d0\u793a\u957f\u5ea6\u7684\u65b9\u6cd5", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff1a1)\u901a\u8fc7\u91cd\u6784\u8bad\u7ec3[BE]\u7f16\u7801\u539f\u59cb\u7cfb\u7edf\u63d0\u793a\u7684\u81ea\u7136\u8bed\u8a00\u5185\u5bb9\uff1b2)\u5c06\u63d0\u793a\u7684\u4e0b\u6e38\u884c\u4e3a\u84b8\u998f\u5230\u5355\u4e2a\u4ee4\u724c\u4e2d\uff1b3)\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u3001\u8f85\u52a9\u538b\u7f29\u6a21\u578b\u6216\u6807\u6ce8\u54cd\u5e94", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5355\u4e2a[BE]\u4ee4\u724c\u53ef\u5b9e\u73b0\u9ad8\u8fbe3000\u500d\u7684\u63d0\u793a\u957f\u5ea6\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u7cfb\u7edf\u63d0\u793a\u7ea698%\u7684\u4e0b\u6e38\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c\uff0c\u51e0\u4e4e\u5c06\u6574\u4e2a\u4e0a\u4e0b\u6587\u7a97\u53e3\u7559\u7ed9\u7528\u6237\u8f93\u5165\uff0c\u4e3a\u9ad8\u6548LLM\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2511.23281", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23281", "abs": "https://arxiv.org/abs/2511.23281", "authors": ["Aaron Steiner", "Ralph Peeters", "Christian Bizer"], "title": "MCP vs RAG vs NLWeb vs HTML: A Comparison of the Effectiveness and Efficiency of Different Agent Interfaces to the Web (Technical Report)", "comment": null, "summary": "Large language model agents are increasingly used to automate web tasks such as product search, offer comparison, and checkout. Current research explores different interfaces through which these agents interact with websites, including traditional HTML browsing, retrieval-augmented generation (RAG) over pre-crawled content, communication via Web APIs using the Model Context Protocol (MCP), and natural-language querying through the NLWeb interface. However, no prior work has compared these four architectures within a single controlled environment using identical tasks.\n  To address this gap, we introduce a testbed consisting of four simulated e-shops, each offering its products via HTML, MCP, and NLWeb interfaces. For each interface (HTML, RAG, MCP, and NLWeb) we develop specialized agents that perform the same sets of tasks, ranging from simple product searches and price comparisons to complex queries for complementary or substitute products and checkout processes. We evaluate the agents using GPT 4.1, GPT 5, GPT 5 mini, and Claude Sonnet 4 as underlying LLM. Our evaluation shows that the RAG, MCP and NLWeb agents outperform HTML on both effectiveness and efficiency. Averaged over all tasks, F1 rises from 0.67 for HTML to between 0.75 and 0.77 for the other agents. Token usage falls from about 241k for HTML to between 47k and 140k per task. The runtime per task drops from 291 seconds to between 50 and 62 seconds. The best overall configuration is RAG with GPT 5 achieving an F1 score of 0.87 and a completion rate of 0.79. Also taking cost into consideration, RAG with GPT 5 mini offers a good compromise between API usage fees and performance. Our experiments show the choice of the interaction interface has a substantial impact on both the effectiveness and efficiency of LLM-based web agents.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u56db\u79cdLLM\u7f51\u9875\u4ee3\u7406\u67b6\u6784\uff08HTML\u3001RAG\u3001MCP\u3001NLWeb\uff09\u5728\u7535\u5546\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0RAG\u3001MCP\u548cNLWeb\u5728\u6548\u679c\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edfHTML\u6d4f\u89c8\uff0c\u5176\u4e2dRAG+GPT 5\u914d\u7f6e\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u63a2\u7d22\u4e86LLM\u4ee3\u7406\u4e0e\u7f51\u7ad9\u4ea4\u4e92\u7684\u4e0d\u540c\u63a5\u53e3\uff08HTML\u6d4f\u89c8\u3001RAG\u3001MCP\u3001NLWeb\uff09\uff0c\u4f46\u7f3a\u4e4f\u5728\u7edf\u4e00\u63a7\u5236\u73af\u5883\u4e0b\u5bf9\u8fd9\u4e9b\u67b6\u6784\u7684\u7cfb\u7edf\u6027\u6bd4\u8f83\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b\u56db\u4e2a\u6a21\u62df\u7535\u5546\u7f51\u7ad9\u7684\u5b9e\u9a8c\u5e73\u53f0\uff0c\u6bcf\u4e2a\u7f51\u7ad9\u63d0\u4f9bHTML\u3001MCP\u548cNLWeb\u63a5\u53e3\u3002\u4e3a\u6bcf\u79cd\u63a5\u53e3\uff08HTML\u3001RAG\u3001MCP\u3001NLWeb\uff09\u5f00\u53d1\u4e13\u7528\u4ee3\u7406\uff0c\u6267\u884c\u76f8\u540c\u7684\u7535\u5546\u4efb\u52a1\u5e8f\u5217\uff08\u4ece\u7b80\u5355\u4ea7\u54c1\u641c\u7d22\u5230\u590d\u6742\u67e5\u8be2\u548c\u7ed3\u8d26\u6d41\u7a0b\uff09\u3002\u4f7f\u7528GPT 4.1\u3001GPT 5\u3001GPT 5 mini\u548cClaude Sonnet 4\u4f5c\u4e3a\u5e95\u5c42LLM\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "RAG\u3001MCP\u548cNLWeb\u4ee3\u7406\u5728\u6548\u679c\u548c\u6548\u7387\u4e0a\u5747\u663e\u8457\u4f18\u4e8eHTML\u4ee3\u7406\uff1a\u5e73\u5747F1\u5206\u6570\u4eceHTML\u76840.67\u63d0\u5347\u52300.75-0.77\uff1b\u6bcf\u4efb\u52a1token\u4f7f\u7528\u91cf\u4ece\u7ea6241k\u964d\u81f347k-140k\uff1b\u8fd0\u884c\u65f6\u95f4\u4ece291\u79d2\u964d\u81f350-62\u79d2\u3002\u6700\u4f73\u914d\u7f6e\u662fRAG+GPT 5\uff0cF1\u5206\u6570\u8fbe0.87\uff0c\u5b8c\u6210\u73870.79\u3002\u7efc\u5408\u8003\u8651\u6210\u672c\uff0cRAG+GPT 5 mini\u5728API\u8d39\u7528\u548c\u6027\u80fd\u95f4\u63d0\u4f9b\u4e86\u826f\u597d\u5e73\u8861\u3002", "conclusion": "\u7f51\u9875\u4ea4\u4e92\u63a5\u53e3\u7684\u9009\u62e9\u5bf9\u57fa\u4e8eLLM\u7684\u7f51\u9875\u4ee3\u7406\u7684\u6548\u679c\u548c\u6548\u7387\u6709\u663e\u8457\u5f71\u54cd\u3002RAG\u3001MCP\u548cNLWeb\u7b49\u73b0\u4ee3\u63a5\u53e3\u76f8\u6bd4\u4f20\u7edfHTML\u6d4f\u89c8\u5728\u591a\u4e2a\u7ef4\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u9009\u62e9\u3002"}}
{"id": "2511.23319", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23319", "abs": "https://arxiv.org/abs/2511.23319", "authors": ["Xiang Hu", "Zhanchao Zhou", "Ruiqi Liang", "Zehuan Li", "Wei Wu", "Jianguo Li"], "title": "Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models", "comment": null, "summary": "This work explores the challenge of building ``Machines that Can Remember'', framing long-term memory as the problem of efficient ultra-long context modeling. We argue that this requires three key properties: \\textbf{sparsity}, \\textbf{random-access flexibility}, and \\textbf{length generalization}. To address ultra-long-context modeling, we leverage Hierarchical Sparse Attention (HSA), a novel attention mechanism that satisfies all three properties. We integrate HSA into Transformers to build HSA-UltraLong, which is an 8B-parameter MoE model trained on over 8 trillion tokens and is rigorously evaluated on different tasks with in-domain and out-of-domain context lengths to demonstrate its capability in handling ultra-long contexts. Results show that our model performs comparably to full-attention baselines on in-domain lengths while achieving over 90\\% accuracy on most in-context retrieval tasks with contexts up to 16M. This report outlines our experimental insights and open problems, contributing a foundation for future research in ultra-long context modeling.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faHSA-UltraLong\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u5c42\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u8d85\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u95ee\u9898\uff0c\u572816M\u957f\u5ea6\u4e0a\u4e0b\u6587\u4e0a\u5b9e\u73b090%\u4ee5\u4e0a\u7684\u68c0\u7d22\u51c6\u786e\u7387\u3002", "motivation": "\u6784\u5efa\"\u80fd\u8bb0\u4f4f\u7684\u673a\u5668\"\u9700\u8981\u89e3\u51b3\u8d85\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u95ee\u9898\uff0c\u8fd9\u8981\u6c42\u5177\u5907\u7a00\u758f\u6027\u3001\u968f\u673a\u8bbf\u95ee\u7075\u6d3b\u6027\u548c\u957f\u5ea6\u6cdb\u5316\u4e09\u4e2a\u5173\u952e\u7279\u6027\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230Transformer\u4e2d\u6784\u5efaHSA-UltraLong\u6a21\u578b\uff0880\u4ebf\u53c2\u6570\u7684MoE\u6a21\u578b\uff09\uff0c\u5728\u8d85\u8fc78\u4e07\u4ebftoken\u4e0a\u8bad\u7ec3\u3002", "result": "\u6a21\u578b\u5728\u57df\u5185\u957f\u5ea6\u4e0a\u8868\u73b0\u4e0e\u5168\u6ce8\u610f\u529b\u57fa\u7ebf\u76f8\u5f53\uff0c\u572816M\u957f\u5ea6\u7684\u4e0a\u4e0b\u6587\u68c0\u7d22\u4efb\u52a1\u4e2d\u8fbe\u523090%\u4ee5\u4e0a\u51c6\u786e\u7387\uff0c\u5c55\u793a\u4e86\u5904\u7406\u8d85\u957f\u4e0a\u4e0b\u6587\u7684\u80fd\u529b\u3002", "conclusion": "HSA-UltraLong\u4e3a\u89e3\u51b3\u8d85\u957f\u4e0a\u4e0b\u6587\u5efa\u6a21\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u8d21\u732e\u4e86\u5b9e\u9a8c\u89c1\u89e3\u548c\u5f00\u653e\u95ee\u9898\u3002"}}
{"id": "2511.23325", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23325", "abs": "https://arxiv.org/abs/2511.23325", "authors": ["Horacio Thompson", "Marcelo Errecalde"], "title": "Tackling a Challenging Corpus for Early Detection of Gambling Disorder: UNSL at MentalRiskES 2025", "comment": "In Iberian Language Evaluation Forum (IberLEF 2025), Zaragoza, Spain", "summary": "Gambling disorder is a complex behavioral addiction that is challenging to understand and address, with severe physical, psychological, and social consequences. Early Risk Detection (ERD) on the Web has become a key task in the scientific community for identifying early signs of mental health behaviors based on social media activity. This work presents our participation in the MentalRiskES 2025 challenge, specifically in Task 1, aimed at classifying users at high or low risk of developing a gambling-related disorder. We proposed three methods based on a CPI+DMC approach, addressing predictive effectiveness and decision-making speed as independent objectives. The components were implemented using the SS3, BERT with extended vocabulary, and SBERT models, followed by decision policies based on historical user analysis. Although it was a challenging corpus, two of our proposals achieved the top two positions in the official results, performing notably in decision metrics. Further analysis revealed some difficulty in distinguishing between users at high and low risk, reinforcing the need to explore strategies to improve data interpretation and quality, and to promote more transparent and reliable ERD systems for mental disorders.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u4e8eCPI+DMC\u65b9\u6cd5\u7684\u4e09\u79cd\u6a21\u578b\uff0c\u7528\u4e8eMentalRiskES 2025\u6311\u6218\u8d5bTask 1\uff0c\u65e8\u5728\u901a\u8fc7\u793e\u4ea4\u5a92\u4f53\u6d3b\u52a8\u8bc6\u522b\u8d4c\u535a\u969c\u788d\u9ad8\u98ce\u9669\u7528\u6237\uff0c\u5176\u4e2d\u4e24\u4e2a\u65b9\u6848\u5728\u5b98\u65b9\u7ed3\u679c\u4e2d\u4f4d\u5217\u524d\u4e24\u540d\u3002", "motivation": "\u8d4c\u535a\u969c\u788d\u662f\u4e00\u79cd\u590d\u6742\u7684\u884c\u4e3a\u6210\u763e\uff0c\u5177\u6709\u4e25\u91cd\u7684\u751f\u7406\u3001\u5fc3\u7406\u548c\u793e\u4f1a\u540e\u679c\u3002\u57fa\u4e8e\u793e\u4ea4\u5a92\u4f53\u7684\u65e9\u671f\u98ce\u9669\u68c0\u6d4b\u5df2\u6210\u4e3a\u8bc6\u522b\u5fc3\u7406\u5065\u5eb7\u884c\u4e3a\u65e9\u671f\u8ff9\u8c61\u7684\u5173\u952e\u4efb\u52a1\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u91c7\u7528CPI+DMC\u65b9\u6cd5\u6846\u67b6\uff0c\u4f7f\u7528SS3\u3001\u6269\u5c55\u8bcd\u6c47\u7684BERT\u548cSBERT\u4e09\u79cd\u6a21\u578b\u5b9e\u73b0\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u5386\u53f2\u7528\u6237\u5206\u6790\u7684\u51b3\u7b56\u7b56\u7565\uff0c\u540c\u65f6\u8003\u8651\u9884\u6d4b\u6548\u679c\u548c\u51b3\u7b56\u901f\u5ea6\u4e24\u4e2a\u72ec\u7acb\u76ee\u6807\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8bed\u6599\u5e93\u4e2d\uff0c\u63d0\u51fa\u7684\u4e09\u4e2a\u65b9\u6cd5\u4e2d\u6709\u4e24\u4e2a\u5728\u5b98\u65b9\u7ed3\u679c\u4e2d\u83b7\u5f97\u4e86\u524d\u4e24\u540d\u7684\u6210\u7ee9\uff0c\u5728\u51b3\u7b56\u6307\u6807\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u533a\u5206\u9ad8\u98ce\u9669\u548c\u4f4e\u98ce\u9669\u7528\u6237\u4ecd\u5b58\u5728\u56f0\u96be\u3002", "conclusion": "\u9700\u8981\u63a2\u7d22\u6539\u8fdb\u6570\u636e\u89e3\u91ca\u548c\u8d28\u91cf\u7684\u65b9\u6cd5\uff0c\u4fc3\u8fdb\u66f4\u900f\u660e\u53ef\u9760\u7684\u65e9\u671f\u98ce\u9669\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4ee5\u66f4\u597d\u5730\u8bc6\u522b\u8d4c\u535a\u969c\u788d\u98ce\u9669\u3002"}}
{"id": "2511.23335", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.23335", "abs": "https://arxiv.org/abs/2511.23335", "authors": ["Shuqi Liu", "Han Wu", "Guanzhi Deng", "Jianshu Chen", "Xiaoyang Wang", "Linqi Song"], "title": "Towards Improving Interpretability of Language Model Generation through a Structured Knowledge Discovery Approach", "comment": null, "summary": "Knowledge-enhanced text generation aims to enhance the quality of generated text by utilizing internal or external knowledge sources. While language models have demonstrated impressive capabilities in generating coherent and fluent text, the lack of interpretability presents a substantial obstacle. The limited interpretability of generated text significantly impacts its practical usability, particularly in knowledge-enhanced text generation tasks that necessitate reliability and explainability. Existing methods often employ domain-specific knowledge retrievers that are tailored to specific data characteristics, limiting their generalizability to diverse data types and tasks. To overcome this limitation, we directly leverage the two-tier architecture of structured knowledge, consisting of high-level entities and low-level knowledge triples, to design our task-agnostic structured knowledge hunter. Specifically, we employ a local-global interaction scheme for structured knowledge representation learning and a hierarchical transformer-based pointer network as the backbone for selecting relevant knowledge triples and entities. By combining the strong generative ability of language models with the high faithfulness of the knowledge hunter, our model achieves high interpretability, enabling users to comprehend the model output generation process. Furthermore, we empirically demonstrate the effectiveness of our model in both internal knowledge-enhanced table-to-text generation on the RotoWireFG dataset and external knowledge-enhanced dialogue response generation on the KdConv dataset. Our task-agnostic model outperforms state-of-the-art methods and corresponding language models, setting new standards on the benchmark.", "AI": {"tldr": "\u63d0\u51fa\u4efb\u52a1\u65e0\u5173\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u730e\u624b\uff0c\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u751f\u6210\u80fd\u529b\u548c\u77e5\u8bc6\u730e\u624b\u7684\u9ad8\u4fdd\u771f\u5ea6\uff0c\u63d0\u5347\u77e5\u8bc6\u589e\u5f3a\u6587\u672c\u751f\u6210\u7684\u53ef\u89e3\u91ca\u6027", "motivation": "\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u8fde\u8d2f\u6d41\u7545\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u7528\u6027\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u77e5\u8bc6\u589e\u5f3a\u6587\u672c\u751f\u6210\u4efb\u52a1\u4e2d\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u9488\u5bf9\u7279\u5b9a\u6570\u636e\u7279\u5f81\u7684\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u68c0\u7d22\u5668\uff0c\u9650\u5236\u4e86\u5176\u5728\u4e0d\u540c\u6570\u636e\u7c7b\u578b\u548c\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u76f4\u63a5\u5229\u7528\u7ed3\u6784\u5316\u77e5\u8bc6\u7684\u4e24\u5c42\u67b6\u6784\uff08\u9ad8\u5c42\u5b9e\u4f53\u548c\u4f4e\u5c42\u77e5\u8bc6\u4e09\u5143\u7ec4\uff09\uff0c\u8bbe\u8ba1\u4efb\u52a1\u65e0\u5173\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u730e\u624b\u3002\u91c7\u7528\u5c40\u90e8-\u5168\u5c40\u4ea4\u4e92\u65b9\u6848\u8fdb\u884c\u7ed3\u6784\u5316\u77e5\u8bc6\u8868\u793a\u5b66\u4e60\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u5c42\u6b21\u5316Transformer\u7684\u6307\u9488\u7f51\u7edc\u4f5c\u4e3a\u9aa8\u5e72\uff0c\u9009\u62e9\u76f8\u5173\u77e5\u8bc6\u4e09\u5143\u7ec4\u548c\u5b9e\u4f53\u3002", "result": "\u5728RotoWireFG\u6570\u636e\u96c6\u4e0a\u7684\u5185\u90e8\u77e5\u8bc6\u589e\u5f3a\u8868\u683c\u5230\u6587\u672c\u751f\u6210\u4efb\u52a1\u548cKdConv\u6570\u636e\u96c6\u4e0a\u7684\u5916\u90e8\u77e5\u8bc6\u589e\u5f3a\u5bf9\u8bdd\u54cd\u5e94\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002\u4efb\u52a1\u65e0\u5173\u6a21\u578b\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u65b9\u6cd5\u548c\u76f8\u5e94\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5927\u751f\u6210\u80fd\u529b\u548c\u77e5\u8bc6\u730e\u624b\u7684\u9ad8\u4fdd\u771f\u5ea6\uff0c\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u7406\u89e3\u6a21\u578b\u8f93\u51fa\u751f\u6210\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u4e3a\u77e5\u8bc6\u589e\u5f3a\u6587\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u901a\u7528\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.23370", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23370", "abs": "https://arxiv.org/abs/2511.23370", "authors": ["Antoine Caubri\u00e8re", "Elodie Gauthier"], "title": "Scaling HuBERT for African Languages: From Base to Large and XL", "comment": "Journ\u00e9e d'\u00e9tudes AFIA-ATALA 2025 : Technologies linguistiques pour les langues peu dot\u00e9es", "summary": "Despite recent progress in multilingual speech processing, African languages remain under-represented in both research and deployed systems, particularly when it comes to strong, open-weight encoders that transfer well under low-resource supervision. Self-supervised learning has proven especially promising in such settings, yet most publicly released models targeting African speech remain at BASE scale, leaving unanswered whether larger encoders, trained exclusively on Africa-centric audio, offer tangible benefits and how model capacity interacts with data composition. This work addresses that gap by introducing SSA-HuBERT-Large (317M parameters) and SSA-HuBERT-XL (964M parameters), the first large models trained solely on African speech, alongside a BASE size counterpart. We release these models as open weights: see https://huggingface.co/collections/Orange/african-speech-foundation-models. By conducting a carefully controlled experimental study focused exclusively on Sub-Saharan languages, covering automatic speech recognition (ASR) and language identification (LID) tasks, we demonstrate that larger architectures significantly improve performance by effectively leveraging large audio datasets.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86SSA-HuBERT-Large\u548cSSA-HuBERT-XL\uff0c\u8fd9\u662f\u9996\u4e2a\u5b8c\u5168\u57fa\u4e8e\u975e\u6d32\u8bed\u97f3\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u5e76\u5c55\u793a\u4e86\u66f4\u5927\u67b6\u6784\u5728\u4f4e\u8d44\u6e90\u975e\u6d32\u8bed\u8a00ASR\u548cLID\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u4f18\u52bf\u3002", "motivation": "\u975e\u6d32\u8bed\u8a00\u5728\u8bed\u97f3\u5904\u7406\u7814\u7a76\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\uff0c\u73b0\u6709\u516c\u5f00\u6a21\u578b\u591a\u4e3aBASE\u89c4\u6a21\uff0c\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u975e\u6d32\u8bed\u97f3\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u7f16\u7801\u5668\uff0c\u9700\u8981\u7814\u7a76\u6a21\u578b\u5bb9\u91cf\u4e0e\u6570\u636e\u7ec4\u6210\u5982\u4f55\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u5f00\u53d1\u4e86SSA-HuBERT-Large\uff083.17\u4ebf\u53c2\u6570\uff09\u548cSSA-HuBERT-XL\uff089.64\u4ebf\u53c2\u6570\uff09\u4e24\u4e2a\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u5b8c\u5168\u57fa\u4e8e\u975e\u6d32\u4e2d\u5fc3\u97f3\u9891\u8bad\u7ec3\uff0c\u540c\u65f6\u63d0\u4f9bBASE\u89c4\u6a21\u5bf9\u7167\u6a21\u578b\uff0c\u5728\u6492\u54c8\u62c9\u4ee5\u5357\u975e\u6d32\u8bed\u8a00\u4e0a\u8fdb\u884cASR\u548c\u8bed\u8a00\u8bc6\u522b\u7684\u53d7\u63a7\u5b9e\u9a8c\u7814\u7a76\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u66f4\u5927\u7684\u67b6\u6784\u901a\u8fc7\u6709\u6548\u5229\u7528\u5927\u89c4\u6a21\u97f3\u9891\u6570\u636e\u96c6\uff0c\u5728ASR\u548cLID\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u4e13\u95e8\u9488\u5bf9\u975e\u6d32\u8bed\u97f3\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u6a21\u578b\u7684\u4ef7\u503c\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5b8c\u5168\u57fa\u4e8e\u975e\u6d32\u8bed\u97f3\u8bad\u7ec3\u7684\u5927\u89c4\u6a21\u81ea\u76d1\u7763\u8bed\u97f3\u7f16\u7801\u5668\uff0c\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\uff0c\u8bc1\u660e\u4e86\u66f4\u5927\u6a21\u578b\u5bb9\u91cf\u5728\u4f4e\u8d44\u6e90\u975e\u6d32\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5f00\u6e90\u4e86\u6a21\u578b\u6743\u91cd\u3002"}}
{"id": "2511.23375", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.23375", "abs": "https://arxiv.org/abs/2511.23375", "authors": ["Alexander Sergeev", "Evgeny Kotelnikov"], "title": "Optimizing Multimodal Language Models through Attention-based Interpretability", "comment": "Accepted for ICAI-2025 conference", "summary": "Modern large language models become multimodal, analyzing various data formats like text and images. While fine-tuning is effective for adapting these multimodal language models (MLMs) to downstream tasks, full fine-tuning is computationally expensive. Parameter-Efficient Fine-Tuning (PEFT) methods address this by training only a small portion of model weights. However, MLMs are difficult to interpret, making it challenging to identify which components are most effective for training to balance efficiency and performance. We propose an attention-based interpretability method for MLMs by analyzing attention scores relative to image tokens. The core idea is to identify attention heads that focus on image key objects. We utilize this information to select optimal model components for PEFT in multimodal models. Our contributions include a method for identifying attention heads associated with image key objects, its application to PEFT for image captioning, and the creation of a new dataset containing images, key object masks, and their textual descriptions. We conducted experiments on MLMs with 2-3 billion parameters to validate the method's effectiveness. By calculating Head Impact (HI) scores we quantify an attention head's focus on key objects, indicating its significance in image understanding. Our fine-tuning experiments demonstrate that adapting layers with the highest HI scores leads to the most significant shifts in metrics compared to pre-trained, randomly selected, or lowest-HI-score layers. This indicates that fine-tuning a small percentage (around 0.01%) of parameters in these crucial layers can substantially influence image understanding capabilities.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684MLMs\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u56fe\u50cf\u5173\u952e\u5bf9\u8c61\u7684\u6ce8\u610f\u529b\u5206\u6570\u6765\u8bc6\u522b\u91cd\u8981\u6ce8\u610f\u529b\u5934\uff0c\u5e76\u5e94\u7528\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\uff0c\u5728\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u4e2d\u4ec5\u5fae\u8c030.01%\u53c2\u6570\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b(MLMs)\u96be\u4ee5\u89e3\u91ca\uff0c\u96be\u4ee5\u786e\u5b9a\u54ea\u4e9b\u7ec4\u4ef6\u5bf9\u8bad\u7ec3\u6700\u6709\u6548\uff0c\u4ee5\u5e73\u8861\u6548\u7387\u548c\u6027\u80fd\u3002\u867d\u7136\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u65b9\u6cd5\u53ef\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u9700\u8981\u8bc6\u522b\u6a21\u578b\u4e2d\u6700\u6709\u6548\u7684\u7ec4\u4ef6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff1a\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u5206\u6570\u76f8\u5bf9\u4e8e\u56fe\u50cf\u4ee4\u724c\u7684\u5173\u7cfb\uff0c\u8bc6\u522b\u5173\u6ce8\u56fe\u50cf\u5173\u952e\u5bf9\u8c61\u7684\u6ce8\u610f\u529b\u5934\u3002\u8ba1\u7b97\u5934\u90e8\u5f71\u54cd(HI)\u5206\u6570\u91cf\u5316\u6ce8\u610f\u529b\u5934\u5bf9\u5173\u952e\u5bf9\u8c61\u7684\u5173\u6ce8\u7a0b\u5ea6\u3002\u5229\u7528HI\u5206\u6570\u9009\u62e9\u6700\u4f18\u6a21\u578b\u7ec4\u4ef6\u8fdb\u884cPEFT\uff0c\u7279\u522b\u9488\u5bf9\u56fe\u50cf\u63cf\u8ff0\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff1a\u5fae\u8c03\u5177\u6709\u6700\u9ad8HI\u5206\u6570\u7684\u5c42\u76f8\u6bd4\u9884\u8bad\u7ec3\u3001\u968f\u673a\u9009\u62e9\u6216\u6700\u4f4eHI\u5206\u6570\u5c42\uff0c\u80fd\u5e26\u6765\u6700\u663e\u8457\u7684\u6307\u6807\u53d8\u5316\u3002\u4ec5\u5fae\u8c03\u7ea60.01%\u7684\u53c2\u6570\u5c31\u80fd\u663e\u8457\u5f71\u54cd\u56fe\u50cf\u7406\u89e3\u80fd\u529b\u3002\u521b\u5efa\u4e86\u5305\u542b\u56fe\u50cf\u3001\u5173\u952e\u5bf9\u8c61\u63a9\u7801\u548c\u6587\u672c\u63cf\u8ff0\u7684\u65b0\u6570\u636e\u96c6\u3002", "conclusion": "\u63d0\u51fa\u7684\u6ce8\u610f\u529b\u5206\u6790\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522bMLMs\u4e2d\u5bf9\u56fe\u50cf\u7406\u89e3\u5173\u952e\u7684\u7ec4\u4ef6\uff0c\u4e3a\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u4ec5\u9700\u5fae\u8c03\u6781\u5c0f\u6bd4\u4f8b\u7684\u53c2\u6570\u5c31\u80fd\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e73\u8861\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u6548\u679c\u3002"}}
{"id": "2511.23391", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.23391", "abs": "https://arxiv.org/abs/2511.23391", "authors": ["Jian Li", "Shenglin Yin", "Yujia Zhang", "Alan Zhao", "Xi Chen", "Xiaohui Zhou", "Pengfei Xu"], "title": "Ambiguity Awareness Optimization: Towards Semantic Disambiguation for Direct Preference Optimization", "comment": "Accepted at EMNLP 2025 main", "summary": "Direct Preference Optimization (DPO) is a widely used reinforcement learning from human feedback (RLHF) method across various domains. Recent research has increasingly focused on the role of token importance in improving DPO effectiveness. It is observed that identical or semantically similar content (defined as ambiguous content) frequently appears within the preference pairs. We hypothesize that the presence of ambiguous content during DPO training may introduce ambiguity, thereby limiting further improvements in alignment. Through mathematical analysis and proof-of-concept experiments, we reveal that ambiguous content may potentially introduce ambiguities, thereby degrading performance. To address this issue, we introduce Ambiguity Awareness Optimization (AAO), a simple yet effective approach that automatically re-weights ambiguous content to reduce ambiguities by calculating semantic similarity from preference pairs. Through extensive experiments, we demonstrate that AAO consistently and significantly surpasses state-of-the-art approaches in performance, without markedly increasing response length, across multiple model scales and widely adopted benchmark datasets, including AlpacaEval 2, MT-Bench, and Arena-Hard. Specifically, AAO outperforms DPO by up to 8.9 points on AlpacaEval 2 and achieves an improvement of by up to 15.0 points on Arena-Hard.", "AI": {"tldr": "AAO\u901a\u8fc7\u8bc6\u522b\u504f\u597d\u5bf9\u4e2d\u7684\u8bed\u4e49\u76f8\u4f3c\u5185\u5bb9\u5e76\u91cd\u65b0\u52a0\u6743\uff0c\u51cf\u5c11DPO\u8bad\u7ec3\u4e2d\u7684\u6b67\u4e49\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u9f50\u6027\u80fd", "motivation": "\u7814\u7a76\u53d1\u73b0DPO\u8bad\u7ec3\u4e2d\u504f\u597d\u5bf9\u7ecf\u5e38\u5305\u542b\u76f8\u540c\u6216\u8bed\u4e49\u76f8\u4f3c\u7684\u5185\u5bb9\uff08\u6b67\u4e49\u5185\u5bb9\uff09\uff0c\u8fd9\u4e9b\u5185\u5bb9\u53ef\u80fd\u5f15\u5165\u6b67\u4e49\u9650\u5236\u5bf9\u9f50\u6548\u679c\u7684\u8fdb\u4e00\u6b65\u63d0\u5347", "method": "\u63d0\u51faAmbiguity Awareness Optimization (AAO)\uff0c\u901a\u8fc7\u8ba1\u7b97\u504f\u597d\u5bf9\u4e2d\u7684\u8bed\u4e49\u76f8\u4f3c\u5ea6\u81ea\u52a8\u91cd\u65b0\u52a0\u6743\u6b67\u4e49\u5185\u5bb9\u4ee5\u51cf\u5c11\u6b67\u4e49", "result": "AAO\u5728\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5728AlpacaEval 2\u4e0a\u6bd4DPO\u63d0\u53478.9\u5206\uff0c\u5728Arena-Hard\u4e0a\u63d0\u534715.0\u5206", "conclusion": "AAO\u901a\u8fc7\u51cf\u5c11\u8bad\u7ec3\u4e2d\u7684\u6b67\u4e49\u5185\u5bb9\u6709\u6548\u63d0\u5347DPO\u6027\u80fd\uff0c\u662f\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u4f18\u5316\u65b9\u6cd5"}}
{"id": "2511.23397", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.23397", "abs": "https://arxiv.org/abs/2511.23397", "authors": ["Mahdi Rahmani", "AmirHossein Saffari", "Reyhane Rahmani"], "title": "MegaChat: A Synthetic Persian Q&A Dataset for High-Quality Sales Chatbot Evaluation", "comment": "6 pages, 11 figures, 2 tables", "summary": "Small and medium-sized enterprises (SMEs) in Iran increasingly leverage Telegram for sales, where real-time engagement is essential for conversion. However, developing AI-driven chatbots for this purpose requires large, high-quality question-and-answer (Q&A) datasets, which are typically expensive and resource-intensive to produce, especially for low-resource languages like Persian. In this paper, we introduce MegaChat, the first fully synthetic Persian Q&A dataset designed to evaluate intelligent sales chatbots in Telegram-based e-commerce. We propose a novel, automated multi-agent architecture that generates persona-aware Q&A pairs by collecting data from active Telegram shopping channels. The system employs specialized agents for question generation, validation, and refinement, ensuring the production of realistic and diverse conversational data. To evaluate answer generation, we compare three classic retrieval-augmented generation (RAG) models with our advanced agentic system, which features multi-query retrieval, reranking, and persona-aligned response synthesis. Using GPT-5.1 for evaluation across six quality dimensions, our results show that the agentic architecture outperformed traditional RAG models in 4 out of 5 diverse channels, demonstrating its ability to generate scalable, high-quality datasets without relying on expensive human annotation or complex fine-tuning. MegaChat provides SMEs with an efficient, cost-effective solution for building intelligent customer engagement systems in specialized commercial domains, enabling advancements in multilingual conversational AI for low-resource languages. Download: https://github.com/MegaChat-Tech/MegaChat-DataSet", "AI": {"tldr": "MegaChat\uff1a\u9996\u4e2a\u5b8c\u5168\u5408\u6210\u7684\u6ce2\u65af\u8bed\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30Telegram\u7535\u5546\u667a\u80fd\u9500\u552e\u804a\u5929\u673a\u5668\u4eba\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u5bf9\u8bdd\u6570\u636e\uff0c\u65e0\u9700\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u3002", "motivation": "\u4f0a\u6717\u4e2d\u5c0f\u4f01\u4e1a\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528Telegram\u8fdb\u884c\u9500\u552e\uff0c\u5b9e\u65f6\u4e92\u52a8\u5bf9\u8f6c\u5316\u81f3\u5173\u91cd\u8981\u3002\u4f46\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u6ce2\u65af\u8bed\uff09\u5f00\u53d1AI\u9a71\u52a8\u7684\u804a\u5929\u673a\u5668\u4eba\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u8fd9\u901a\u5e38\u6210\u672c\u9ad8\u6602\u4e14\u8d44\u6e90\u5bc6\u96c6\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u81ea\u52a8\u5316\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u4ece\u6d3b\u8dc3\u7684Telegram\u8d2d\u7269\u9891\u9053\u6536\u96c6\u6570\u636e\uff0c\u751f\u6210\u5177\u6709\u89d2\u8272\u610f\u8bc6\u7684\u95ee\u7b54\u5bf9\u3002\u7cfb\u7edf\u5305\u542b\u4e13\u95e8\u7684\u95ee\u9898\u751f\u6210\u3001\u9a8c\u8bc1\u548c\u4f18\u5316\u667a\u80fd\u4f53\uff0c\u786e\u4fdd\u751f\u6210\u771f\u5b9e\u591a\u6837\u7684\u5bf9\u8bdd\u6570\u636e\u3002\u8bc4\u4f30\u65f6\u6bd4\u8f83\u4e86\u4e09\u79cd\u7ecf\u5178\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6a21\u578b\u4e0e\u5148\u8fdb\u7684\u667a\u80fd\u4f53\u7cfb\u7edf\uff08\u5177\u6709\u591a\u67e5\u8be2\u68c0\u7d22\u3001\u91cd\u6392\u5e8f\u548c\u89d2\u8272\u5bf9\u9f50\u54cd\u5e94\u5408\u6210\u529f\u80fd\uff09\u3002", "result": "\u4f7f\u7528GPT-5.1\u5728\u516d\u4e2a\u8d28\u91cf\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u667a\u80fd\u4f53\u67b6\u6784\u57285\u4e2a\u4e0d\u540c\u9891\u9053\u4e2d\u76844\u4e2a\u4e0a\u4f18\u4e8e\u4f20\u7edfRAG\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u4eba\u5de5\u6807\u6ce8\u6216\u590d\u6742\u5fae\u8c03\u5373\u53ef\u751f\u6210\u53ef\u6269\u5c55\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u80fd\u529b\u3002", "conclusion": "MegaChat\u4e3a\u4e2d\u5c0f\u4f01\u4e1a\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u5728\u4e13\u4e1a\u5546\u4e1a\u9886\u57df\u6784\u5efa\u667a\u80fd\u5ba2\u6237\u4e92\u52a8\u7cfb\u7edf\uff0c\u63a8\u52a8\u4e86\u4f4e\u8d44\u6e90\u8bed\u8a00\u591a\u8bed\u8a00\u5bf9\u8bddAI\u7684\u8fdb\u6b65\u3002"}}
