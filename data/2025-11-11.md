<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 44]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [VLM-driven Skill Selection for Robotic Assembly Tasks](https://arxiv.org/abs/2511.05680)
*Jeong-Jung Kim,Doo-Yeol Koh,Chang-Hyun Kim*

Main category: cs.RO

TL;DR: 结合视觉语言模型与模仿学习的机器人装配框架，通过视觉感知、自然语言理解和基础技能实现灵活的装配操作


<details>
  <summary>Details</summary>
Motivation: 解决机器人装配任务中需要结合视觉感知、语言理解和灵活操作能力的问题，实现适应性强的装配系统

Method: 使用配备夹爪的机器人在3D空间中移动执行装配操作，集成视觉语言模型进行视觉感知和自然语言理解，结合模仿学习的基础技能

Result: 在装配场景中取得高成功率，同时通过结构化基础技能分解保持可解释性

Conclusion: 该框架有效结合了视觉语言模型与模仿学习，为机器人装配提供了灵活且可解释的解决方案

Abstract: This paper presents a robotic assembly framework that combines Vision-Language Models (VLMs) with imitation learning for assembly manipulation tasks. Our system employs a gripper-equipped robot that moves in 3D space to perform assembly operations. The framework integrates visual perception, natural language understanding, and learned primitive skills to enable flexible and adaptive robotic manipulation. Experimental results demonstrate the effectiveness of our approach in assembly scenarios, achieving high success rates while maintaining interpretability through the structured primitive skill decomposition.

</details>


### [2] [A Unified Stochastic Mechanism Underlying Collective Behavior in Ants, Physical Systems, and Robotic Swarms](https://arxiv.org/abs/2511.05785)
*Lianhao Yin,Haiping Yu,Pascal Spino,Daniela Rus*

Main category: cs.RO

TL;DR: 该论文提出了一个统一的随机模型，将生物、物理和机器人集群联系起来，揭示了它们共享的统计机制：在不同能量函数约束下的最大化原理。


<details>
  <summary>Details</summary>
Motivation: 生物集群（如蚁群）通过分散和随机的个体行为实现集体目标，而物理系统（如气体、液体、固体）中的随机粒子运动受熵最大化支配，但不实现集体目标。目前缺乏解释这两种系统随机行为的统一框架。

Method: 通过Formica polyctena蚂蚁的实证证据，揭示了生物和物理系统共享的统计机制：在不同能量函数约束下的最大化。进一步展示了受此原理控制的机器人集群可以表现出可扩展的分散合作。

Result: 发现生物和物理系统共享相同的统计机制，机器人集群能够模仿物理相变行为，且只需最少的个体计算。

Conclusion: 建立了一个连接生物、物理和机器人集群的统一随机模型，为设计稳健和智能的群体机器人提供了可扩展的原理。

Abstract: Biological swarms, such as ant colonies, achieve collective goals through decentralized and stochastic individual behaviors. Similarly, physical systems composed of gases, liquids, and solids exhibit random particle motion governed by entropy maximization, yet do not achieve collective objectives. Despite this analogy, no unified framework exists to explain the stochastic behavior in both biological and physical systems. Here, we present empirical evidence from \textit{Formica polyctena} ants that reveals a shared statistical mechanism underlying both systems: maximization under different energy function constraints. We further demonstrate that robotic swarms governed by this principle can exhibit scalable, decentralized cooperation, mimicking physical phase-like behaviors with minimal individual computation. These findings established a unified stochastic model linking biological, physical, and robotic swarms, offering a scalable principle for designing robust and intelligent swarm robotics.

</details>


### [3] [VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models](https://arxiv.org/abs/2511.05791)
*Manav Kulshrestha,S. Talha Bukhari,Damon Conover,Aniket Bera*

Main category: cs.RO

TL;DR: VLAD-Grasp是一种基于视觉语言模型的零样本抓取检测方法，无需训练即可从RGB-D图像中恢复可执行的抓取姿态，在Cornell和Jacquard数据集上达到或超越最先进监督模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器人抓取方法依赖大规模专家标注，且需要重新训练来处理新物体，限制了其泛化能力和实用性。

Method: 使用大视觉语言模型生成目标图像（直杆"刺穿"物体表示对握抓取），预测深度和分割将其提升到3D，通过主成分分析和无对应优化对齐生成和观测点云来恢复抓取姿态。

Result: 在Cornell和Jacquard数据集上达到或超越最先进监督模型的性能，并在Franka Research 3机器人上展示了零样本泛化到新颖真实世界物体的能力。

Conclusion: 视觉语言基础模型可作为机器人操作的强大先验，实现无需训练的零样本抓取检测。

Abstract: Robotic grasping is a fundamental capability for autonomous manipulation; however, most existing methods rely on large-scale expert annotations and necessitate retraining to handle new objects. We present VLAD-Grasp, a Vision-Language model Assisted zero-shot approach for Detecting grasps. From a single RGB-D image, our method (1) prompts a large vision-language model to generate a goal image where a straight rod "impales" the object, representing an antipodal grasp, (2) predicts depth and segmentation to lift this generated image into 3D, and (3) aligns generated and observed object point clouds via principal component analysis and correspondence-free optimization to recover an executable grasp pose. Unlike prior work, our approach is training-free and does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves performance that is competitive with or superior to that of state-of-the-art supervised models on the Cornell and Jacquard datasets. We further demonstrate zero-shot generalization to novel real-world objects on a Franka Research 3 robot, highlighting vision-language foundation models as powerful priors for robotic manipulation.

</details>


### [4] [ViTaMIn-B: A Reliable and Efficient Visuo-Tactile Bimanual Manipulation Interface](https://arxiv.org/abs/2511.05858)
*Chuanyu Li,Chaoyi Liu,Daotan Wang,Shuyu Zhang,Lusong Li,Zecui Zeng,Fangchen Liu,Jing Xu,Rui Chen*

Main category: cs.RO

TL;DR: ViTaMIn-B是一个用于双手操作任务的手持数据采集系统，包含新型柔性视觉触觉传感器DuoTact和基于Meta Quest控制器的6-DoF双手姿态跟踪方法。


<details>
  <summary>Details</summary>
Motivation: 现有手持设备系统缺乏强大的触觉感知和可靠的姿态跟踪能力，难以处理复杂的双手接触丰富任务。

Method: 设计了柔性框架的DuoTact传感器以承受大接触力并捕获高分辨率接触几何；提出将传感器全局变形重建为3D点云作为策略输入；开发了基于Meta Quest控制器的6-DoF双手姿态获取方法。

Result: 用户研究证实了系统的高效性和可用性；在四个双手操作任务上的实验显示其优于现有系统的任务性能。

Conclusion: ViTaMIn-B系统为复杂双手接触丰富任务提供了更强大和高效的数据采集解决方案。

Abstract: Handheld devices have opened up unprecedented opportunities to collect large-scale, high-quality demonstrations efficiently. However, existing systems often lack robust tactile sensing or reliable pose tracking to handle complex interaction scenarios, especially for bimanual and contact-rich tasks. In this work, we propose ViTaMIn-B, a more capable and efficient handheld data collection system for such tasks. We first design DuoTact, a novel compliant visuo-tactile sensor built with a flexible frame to withstand large contact forces during manipulation while capturing high-resolution contact geometry. To enhance the cross-sensor generalizability, we propose reconstructing the sensor's global deformation as a 3D point cloud and using it as the policy input. We further develop a robust, unified 6-DoF bimanual pose acquisition process using Meta Quest controllers, which eliminates the trajectory drift issue in common SLAM-based methods. Comprehensive user studies confirm the efficiency and high usability of ViTaMIn-B among novice and expert operators. Furthermore, experiments on four bimanual manipulation tasks demonstrate its superior task performance relative to existing systems.

</details>


### [5] [PlaCo: a QP-based robot planning and control framework](https://arxiv.org/abs/2511.06141)
*Marc Duclusaud,Grégoire Passault,Vincent Padois,Olivier Ly*

Main category: cs.RO

TL;DR: PlaCo是一个简化机器人系统QP规划和控制的软件框架，提供高层接口抽象底层数学公式，支持Python快速原型和C++实时性能。


<details>
  <summary>Details</summary>
Motivation: 简化QP规划控制问题的表述和求解，让用户能够以模块化和直观的方式指定任务和约束，而无需处理底层数学公式。

Method: 提供高层接口抽象QP问题的底层数学表述，支持Python绑定用于快速原型开发，以及C++实现用于实时性能。

Result: 开发了PlaCo框架，能够简化机器人系统QP规划控制问题的制定和求解过程。

Conclusion: PlaCo框架成功简化了QP规划控制问题的表述和求解，为机器人系统提供了高效易用的开发工具。

Abstract: This article introduces PlaCo, a software framework designed to simplify the formulation and solution of Quadratic Programming (QP)-based planning and control problems for robotic systems. PlaCo provides a high-level interface that abstracts away the low-level mathematical formulation of QP problems, allowing users to specify tasks and constraints in a modular and intuitive manner. The framework supports both Python bindings for rapid prototyping and a C++ implementation for real-time performance.

</details>


### [6] [OpenVLN: Open-world aerial Vision-Language Navigation](https://arxiv.org/abs/2511.06182)
*Peican Lin,Gan Sun,Chenxi Liu,Fazeng Li,Weihong Ren,Yang Cong*

Main category: cs.RO

TL;DR: 提出了一个数据高效的开放世界空中视觉语言导航框架OpenVLN，能够在有限数据约束下执行语言引导飞行，并增强复杂空中环境中的长时程轨迹规划能力。


<details>
  <summary>Details</summary>
Motivation: 解决室外空中环境复杂性带来的数据采集挑战和无人机长时程轨迹规划需求，这些为空中视觉语言导航引入了新的复杂性。

Method: 重新配置强化学习框架来优化VLM用于无人机导航任务，在有限训练数据下使用基于规则的策略高效微调VLM；同时引入长时程规划器进行轨迹合成，通过基于价值的奖励动态生成精确的无人机动作。

Result: 在TravelUAV基准测试中，相比基线方法，成功率提升4.34%，Oracle成功率提升6.19%，路径长度加权成功率提升4.07%。

Conclusion: 该方法在复杂空中环境中为长时程无人机导航提供了有效的部署方案。

Abstract: Vision-language models (VLMs) have been widely-applied in ground-based vision-language navigation (VLN). However, the vast complexity of outdoor aerial environments compounds data acquisition challenges and imposes long-horizon trajectory planning requirements on Unmanned Aerial Vehicles (UAVs), introducing novel complexities for aerial VLN. To address these challenges, we propose a data-efficient Open-world aerial Vision-Language Navigation (i.e., OpenVLN) framework, which could execute language-guided flight with limited data constraints and enhance long-horizon trajectory planning capabilities in complex aerial environments. Specifically, we reconfigure a reinforcement learning framework to optimize the VLM for UAV navigation tasks, which can efficiently fine-tune VLM by using rule-based policies under limited training data. Concurrently, we introduce a long-horizon planner for trajectory synthesis that dynamically generates precise UAV actions via value-based rewards. To the end, we conduct sufficient navigation experiments on the TravelUAV benchmark with dataset scaling across diverse reward settings. Our method demonstrates consistent performance gains of up to 4.34% in Success Rate, 6.19% in Oracle Success Rate, and 4.07% in Success weighted by Path Length over baseline methods, validating its deployment efficacy for long-horizon UAV navigation in complex aerial environments.

</details>


### [7] [ExpReS-VLA: Specializing Vision-Language-Action Models Through Experience Replay and Retrieval](https://arxiv.org/abs/2511.06202)
*Shahram Najam Syed,Yatharth Ahuja,Arthur Jakobsson,Jeff Ichnowski*

Main category: cs.RO

TL;DR: ExpReS-VLA通过经验回放和检索来专门化预训练的视觉-语言-动作模型，在防止灾难性遗忘的同时显著提升在特定任务上的性能，内存使用减少约97%，在仿真和真实机器人实验中均取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作模型在零样本泛化方面表现良好，但在适应新部署环境时效率低下，且在实际应用中，在有限任务集上保持稳定高性能比广泛泛化更重要。

Method: 使用冻结视觉骨干网络存储紧凑特征表示而非原始图像-动作对，通过余弦相似度检索相关经验进行指导，采用优先经验回放强调成功轨迹，并引入阈值混合对比损失从成功和失败尝试中学习。

Result: 在LIBERO仿真基准上，空间推理任务成功率从82.6%提升至93.1%，长视野任务从61%提升至72.3%；在真实机器人实验中，5个操作任务在已见和未见设置下均达到98%成功率，而朴素微调分别为84.7%和32%。

Conclusion: ExpReS-VLA提供了一种实用且高效的VLA模型专门化方法，仅需31秒和12个演示即可完成适应，适合真实机器人部署。

Abstract: Vision-Language-Action models such as OpenVLA show impressive zero-shot generalization across robotic manipulation tasks but often fail to adapt efficiently to new deployment environments. In many real-world applications, consistent high performance on a limited set of tasks is more important than broad generalization. We propose ExpReS-VLA, a method for specializing pre-trained VLA models through experience replay and retrieval while preventing catastrophic forgetting. ExpReS-VLA stores compact feature representations from the frozen vision backbone instead of raw image-action pairs, reducing memory usage by approximately 97 percent. During deployment, relevant past experiences are retrieved using cosine similarity and used to guide adaptation, while prioritized experience replay emphasizes successful trajectories. We also introduce Thresholded Hybrid Contrastive Loss, which enables learning from both successful and failed attempts. On the LIBERO simulation benchmark, ExpReS-VLA improves success rates from 82.6 to 93.1 percent on spatial reasoning tasks and from 61 to 72.3 percent on long-horizon tasks. On physical robot experiments with five manipulation tasks, it reaches 98 percent success on both seen and unseen settings, compared to 84.7 and 32 percent for naive fine-tuning. Adaptation takes 31 seconds using 12 demonstrations on a single RTX 5090 GPU, making the approach practical for real robot deployment.

</details>


### [8] [Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation](https://arxiv.org/abs/2511.06240)
*Tzu-Jung Lin,Jia-Fong Yeh,Hung-Ting Su,Chung-Yi Lin,Yi-Ting Chen,Winston H. Hsu*

Main category: cs.RO

TL;DR: 提出了一种零样本的机器人基座放置框架，通过结合视觉语言模型的语义理解和几何可行性，显著提高了开放词汇移动操作任务的成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常基于接近度导航而不考虑可操作性，导致频繁的操作失败。需要一种能够同时考虑语义理解和几何约束的基座放置方法。

Method: 采用从粗到精的探索策略，构建跨模态表示（可操作性RGB和障碍物地图+），利用VLM的粗粒度语义先验指导搜索，并通过几何约束细化放置位置。

Result: 在五个不同的开放词汇移动操作任务上评估，系统达到85%的成功率，显著优于传统几何规划器和基于VLM的方法。

Conclusion: 证明了可操作性感知和多模态推理在开放词汇移动操作中通用化、指令条件规划方面的潜力。

Abstract: In open-vocabulary mobile manipulation (OVMM), task success often hinges on the selection of an appropriate base placement for the robot. Existing approaches typically navigate to proximity-based regions without considering affordances, resulting in frequent manipulation failures. We propose Affordance-Guided Coarse-to-Fine Exploration, a zero-shot framework for base placement that integrates semantic understanding from vision-language models (VLMs) with geometric feasibility through an iterative optimization process. Our method constructs cross-modal representations, namely Affordance RGB and Obstacle Map+, to align semantics with spatial context. This enables reasoning that extends beyond the egocentric limitations of RGB perception. To ensure interaction is guided by task-relevant affordances, we leverage coarse semantic priors from VLMs to guide the search toward task-relevant regions and refine placements with geometric constraints, thereby reducing the risk of convergence to local optima. Evaluated on five diverse open-vocabulary mobile manipulation tasks, our system achieves an 85% success rate, significantly outperforming classical geometric planners and VLM-based methods. This demonstrates the promise of affordance-aware and multimodal reasoning for generalizable, instruction-conditioned planning in OVMM.

</details>


### [9] [Robust Differentiable Collision Detection for General Objects](https://arxiv.org/abs/2511.06267)
*Jiayi Chen,Wei Zhao,Liangwang Ruan,Baoquan Chen,He Wang*

Main category: cs.RO

TL;DR: 提出了一种鲁棒高效的微分碰撞检测框架，支持凸面和凹面物体，通过距离基随机平滑、自适应采样和等效梯度传输实现稳健的梯度计算。


<details>
  <summary>Details</summary>
Motivation: 传统碰撞检测算法（如GJK+EPA）不可微分，阻碍了梯度流和基于梯度的优化，限制了在接触丰富任务（如抓取和操作）中的应用。现有方法仅支持凸面物体且缺乏鲁棒性。

Method: 采用距离基一阶随机平滑、自适应采样和等效梯度传输技术，构建支持凸面和凹面物体的微分碰撞检测框架。

Result: 在DexGraspNet和Objaverse的复杂网格上实验显示，相比现有基线有显著改进，并成功应用于灵巧抓取合成以优化抓取质量。

Conclusion: 提出的微分碰撞检测框架在复杂几何体上表现出更好的鲁棒性和效率，为接触丰富的机器人任务提供了有效的梯度优化支持。

Abstract: Collision detection is a core component of robotics applications such as simulation, control, and planning. Traditional algorithms like GJK+EPA compute witness points (i.e., the closest or deepest-penetration pairs between two objects) but are inherently non-differentiable, preventing gradient flow and limiting gradient-based optimization in contact-rich tasks such as grasping and manipulation. Recent work introduced efficient first-order randomized smoothing to make witness points differentiable; however, their direction-based formulation is restricted to convex objects and lacks robustness for complex geometries. In this work, we propose a robust and efficient differentiable collision detection framework that supports both convex and concave objects across diverse scales and configurations. Our method introduces distance-based first-order randomized smoothing, adaptive sampling, and equivalent gradient transport for robust and informative gradient computation. Experiments on complex meshes from DexGraspNet and Objaverse show significant improvements over existing baselines. Finally, we demonstrate a direct application of our method for dexterous grasp synthesis to refine the grasp quality. The code is available at https://github.com/JYChen18/DiffCollision.

</details>


### [10] [External Photoreflective Tactile Sensing Based on Surface Deformation Measurement](https://arxiv.org/abs/2511.06311)
*Seiichi Yamamoto,Hiroki Ishizuka,Takumi Kawasetsu,Koh Hosoda,Takayuki Kameoka,Kango Yanagida,Takato Horii,Sei Ikeda,Osamu Oshiro*

Main category: cs.RO

TL;DR: 提出了一种基于软体机器人机械柔顺性的触觉传感方法，使用外部可附加的光反射模块读取硅胶皮肤表面变形来估计接触力，无需嵌入触觉传感器。


<details>
  <summary>Details</summary>
Motivation: 将传感器置于接触界面之外可降低损坏风险、保持柔软性，并简化制造和维护。相比液体填充或导线嵌入的触觉皮肤，该模块化附加架构增强了耐用性、减少了布线复杂性。

Method: 使用光学传感元件和柔顺皮肤，通过读取皮肤应变模式来估计接触力。开发了原型触觉传感器，并在软体机器人抓手上进行集成演示。

Result: 压缩实验验证了该方法，显示出与理论一致的单调力输出关系、低滞后性、高重复性以及小响应压痕速度。模块能可靠检测抓取事件。

Conclusion: 利用表面柔顺性与外部光学模块为软体机器人提供力感知，同时保持结构灵活性和可制造性，为机器人应用和安全人机协作铺平道路。

Abstract: We present a tactile sensing method enabled by the mechanical compliance of soft robots; an externally attachable photoreflective module reads surface deformation of silicone skin to estimate contact force without embedding tactile transducers. Locating the sensor off the contact interface reduces damage risk, preserves softness, and simplifies fabrication and maintenance. We first characterize the optical sensing element and the compliant skin, thendetermine the design of a prototype tactile sensor. Compression experiments validate the approach, exhibiting a monotonic force output relationship consistent with theory, low hysteresis, high repeatability over repeated cycles, and small response indentation speeds. We further demonstrate integration on a soft robotic gripper, where the module reliably detects grasp events. Compared with liquid filled or wireembedded tactile skins, the proposed modular add on architecture enhances durability, reduces wiring complexity, and supports straightforward deployment across diverse robot geometries. Because the sensing principle reads skin strain patterns, it also suggests extensions to other somatosensory cues such as joint angle or actuator state estimation from surface deformation. Overall, leveraging surface compliance with an external optical module provides a practical and robust route to equip soft robots with force perception while preserving structural flexibility and manufacturability, paving the way for robotic applications and safe human robot collaboration.

</details>


### [11] [Towards Adaptive Humanoid Control via Multi-Behavior Distillation and Reinforced Fine-Tuning](https://arxiv.org/abs/2511.06371)
*Yingnan Zhao,Xinmiao Wang,Dewei Wang,Xinzhe Liu,Dan Lu,Qilong Han,Peng Liu,Chenjia Bai*

Main category: cs.RO

TL;DR: 提出Adaptive Humanoid Control (AHC)方法，通过两阶段框架学习自适应人形机器人运动控制器，能够在不同技能和地形间切换并适应。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要为每个运动技能训练独立策略，导致控制器泛化能力有限，在复杂地形和多样化情境下表现脆弱。

Method: 采用两阶段框架：1) 训练多个主要运动策略并进行多行为蒸馏，获得基础多行为控制器；2) 在多样化地形上进行强化微调，收集在线反馈增强地形适应性。

Result: 在Unitree G1机器人上的仿真和真实实验表明，该方法在各种情境和地形下表现出强大的适应性。

Conclusion: AHC方法成功实现了人形机器人在不同技能和地形间的自适应运动控制，解决了现有方法的泛化限制问题。

Abstract: Humanoid robots are promising to learn a diverse set of human-like locomotion behaviors, including standing up, walking, running, and jumping. However, existing methods predominantly require training independent policies for each skill, yielding behavior-specific controllers that exhibit limited generalization and brittle performance when deployed on irregular terrains and in diverse situations. To address this challenge, we propose Adaptive Humanoid Control (AHC) that adopts a two-stage framework to learn an adaptive humanoid locomotion controller across different skills and terrains. Specifically, we first train several primary locomotion policies and perform a multi-behavior distillation process to obtain a basic multi-behavior controller, facilitating adaptive behavior switching based on the environment. Then, we perform reinforced fine-tuning by collecting online feedback in performing adaptive behaviors on more diverse terrains, enhancing terrain adaptability for the controller. We conduct experiments in both simulation and real-world experiments in Unitree G1 robots. The results show that our method exhibits strong adaptability across various situations and terrains. Project website: https://ahc-humanoid.github.io.

</details>


### [12] [ArtReg: Visuo-Tactile based Pose Tracking and Manipulation of Unseen Articulated Objects](https://arxiv.org/abs/2511.06378)
*Prajval Kumar Murali,Mohsen Kaboli*

Main category: cs.RO

TL;DR: 提出了一种名为ArtReg的新方法，用于在机器人交互过程中对未知物体（单个、多个或铰接式）进行视觉-触觉跟踪，无需先验几何或运动学知识。


<details>
  <summary>Details</summary>
Motivation: 机器人在真实环境中经常遇到具有复杂结构和铰接组件的未知物体，如门、抽屉、柜子和工具。在没有先验几何或运动学知识的情况下感知、跟踪和操作这些物体仍然是机器人学的基本挑战。

Method: ArtReg方法在SE(3)李群中集成视觉-触觉点云到无迹卡尔曼滤波器中用于点云配准。通过推或拉等有目的的操作动作检测可能的铰接关节，并开发了闭环控制器进行目标驱动的铰接物体操作。

Result: 在真实机器人实验中广泛评估了各种未知物体，展示了在变化质心、低光照条件和挑战性视觉背景下的鲁棒性。在标准铰接物体数据集上基准测试显示，相比最先进方法在姿态精度方面有改进。

Conclusion: 利用视觉-触觉信息的鲁棒准确姿态跟踪使机器人能够感知和交互未见过的复杂铰接物体（具有旋转或棱柱关节）。

Abstract: Robots operating in real-world environments frequently encounter unknown objects with complex structures and articulated components, such as doors, drawers, cabinets, and tools. The ability to perceive, track, and manipulate these objects without prior knowledge of their geometry or kinematic properties remains a fundamental challenge in robotics. In this work, we present a novel method for visuo-tactile-based tracking of unseen objects (single, multiple, or articulated) during robotic interaction without assuming any prior knowledge regarding object shape or dynamics. Our novel pose tracking approach termed ArtReg (stands for Articulated Registration) integrates visuo-tactile point clouds in an unscented Kalman Filter formulation in the SE(3) Lie Group for point cloud registration. ArtReg is used to detect possible articulated joints in objects using purposeful manipulation maneuvers such as pushing or hold-pulling with a two-robot team. Furthermore, we leverage ArtReg to develop a closed-loop controller for goal-driven manipulation of articulated objects to move the object into the desired pose configuration. We have extensively evaluated our approach on various types of unknown objects through real robot experiments. We also demonstrate the robustness of our method by evaluating objects with varying center of mass, low-light conditions, and with challenging visual backgrounds. Furthermore, we benchmarked our approach on a standard dataset of articulated objects and demonstrated improved performance in terms of pose accuracy compared to state-of-the-art methods. Our experiments indicate that robust and accurate pose tracking leveraging visuo-tactile information enables robots to perceive and interact with unseen complex articulated objects (with revolute or prismatic joints).

</details>


### [13] [From Demonstrations to Safe Deployment: Path-Consistent Safety Filtering for Diffusion Policies](https://arxiv.org/abs/2511.06385)
*Ralf Römer,Julian Balletshofer,Jakob Thumm,Marco Pavone,Angela P. Schoellig,Matthias Althoff*

Main category: cs.RO

TL;DR: 提出了路径一致性安全过滤（PACS）方法，为扩散策略提供形式化安全保证，同时保持任务成功率


<details>
  <summary>Details</summary>
Motivation: 扩散策略在复杂操作任务中表现出色，但无法保证安全行为，需要外部安全机制。然而这些机制会改变动作，导致训练分布不一致和性能下降

Method: 采用路径一致性制动方法，对生成的动作序列进行轨迹计算，使用基于集合的可达性分析进行实时安全验证

Result: PACS在动态环境中提供形式化安全保证，保持任务成功率，相比反应式安全方法（如控制屏障函数）任务成功率提升高达68%

Conclusion: PACS方法成功解决了扩散策略的安全性问题，在保持学习到的任务完成行为的同时提供形式化安全保证

Abstract: Diffusion policies (DPs) achieve state-of-the-art performance on complex manipulation tasks by learning from large-scale demonstration datasets, often spanning multiple embodiments and environments. However, they cannot guarantee safe behavior, so external safety mechanisms are needed. These, however, alter actions in ways unseen during training, causing unpredictable behavior and performance degradation. To address these problems, we propose path-consistent safety filtering (PACS) for DPs. Our approach performs path-consistent braking on a trajectory computed from the sequence of generated actions. In this way, we keep execution consistent with the policy's training distribution, maintaining the learned, task-completing behavior. To enable a real-time deployment and handle uncertainties, we verify safety using set-based reachability analysis. Our experimental evaluation in simulation and on three challenging real-world human-robot interaction tasks shows that PACS (a) provides formal safety guarantees in dynamic environments, (b) preserves task success rates, and (c) outperforms reactive safety approaches, such as control barrier functions, by up to 68% in terms of task success. Videos are available at our project website: https://tum-lsy.github.io/pacs/.

</details>


### [14] [Whole-Body Control With Terrain Estimation of A 6-DoF Wheeled Bipedal Robot](https://arxiv.org/abs/2511.06397)
*Cong Wen,Yunfei Li,Kexin Liu,Yixin Qiu,Xuanhong Liao,Tianyu Wang,Dingchuan Liu,Tao Zhang,Ximin Lyu*

Main category: cs.RO

TL;DR: 开发了完整的动力学模型和全身控制框架，用于6自由度轮式双足机器人，包含地形估计功能，通过仿真和实验验证了在不平坦地形上的鲁棒性和穿越能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多忽略腿部动力学，限制了机器人的运动潜力，且机器人在不平坦地形上面临挑战。

Method: 建立包含闭环动力学和地面接触模型的完整动力学模型，使用LiDAR惯性里程计和改进的主成分分析进行地形估计，采用PD控制和LQR进行姿态和平衡控制，使用分层优化解决全身控制问题。

Result: 验证了地形估计算法的性能，通过仿真和真实实验证明了算法在不平坦地形上的鲁棒性和穿越能力。

Conclusion: 提出的完整动力学模型和全身控制框架有效解决了轮式双足机器人在不平坦地形上的运动控制问题。

Abstract: Wheeled bipedal robots have garnered increasing attention in exploration and inspection. However, most research simplifies calculations by ignoring leg dynamics, thereby restricting the robot's full motion potential. Additionally, robots face challenges when traversing uneven terrain. To address the aforementioned issue, we develop a complete dynamics model and design a whole-body control framework with terrain estimation for a novel 6 degrees of freedom wheeled bipedal robot. This model incorporates the closed-loop dynamics of the robot and a ground contact model based on the estimated ground normal vector. We use a LiDAR inertial odometry framework and improved Principal Component Analysis for terrain estimation. Task controllers, including PD control law and LQR, are employed for pose control and centroidal dynamics-based balance control, respectively. Furthermore, a hierarchical optimization approach is used to solve the whole-body control problem. We validate the performance of the terrain estimation algorithm and demonstrate the algorithm's robustness and ability to traverse uneven terrain through both simulation and real-world experiments.

</details>


### [15] [Real Garment Benchmark (RGBench): A Comprehensive Benchmark for Robotic Garment Manipulation featuring a High-Fidelity Scalable Simulator](https://arxiv.org/abs/2511.06434)
*Wenkang Hu,Xincheng Tang,Yanzhi E,Yitong Li,Zhengjie Shu,Wei Li,Huamin Wang,Ruigang Yang*

Main category: cs.RO

TL;DR: 提出了Real Garment Benchmark (RGBench)，这是一个用于机器人服装操作的综合性基准测试，包含6000多个服装网格模型、高性能模拟器以及评估模拟质量的协议。


<details>
  <summary>Details</summary>
Motivation: 虽然模拟数据在刚性物体机器人操作方面取得了进展，但由于缺乏变形物体模型和逼真的非刚体模拟器，这一成功尚未应用于变形物体。

Method: 开发了包含多样化服装网格模型的新模拟器，并建立了评估模拟质量的综合协议，通过真实服装动力学的精确测量来验证模拟效果。

Result: 实验表明，该模拟器大幅优于现有布料模拟器，模拟误差减少20%，同时速度提高3倍。

Conclusion: RGBench将公开发布，以加速未来机器人服装操作的研究。

Abstract: While there has been significant progress to use simulated data to learn robotic manipulation of rigid objects, applying its success to deformable objects has been hindered by the lack of both deformable object models and realistic non-rigid body simulators. In this paper, we present Real Garment Benchmark (RGBench), a comprehensive benchmark for robotic manipulation of garments. It features a diverse set of over 6000 garment mesh models, a new high-performance simulator, and a comprehensive protocol to evaluate garment simulation quality with carefully measured real garment dynamics. Our experiments demonstrate that our simulator outperforms currently available cloth simulators by a large margin, reducing simulation error by 20% while maintaining a speed of 3 times faster. We will publicly release RGBench to accelerate future research in robotic garment manipulation. Website: https://rgbench.github.io/

</details>


### [16] [Sim-to-Real Transfer in Deep Reinforcement Learning for Bipedal Locomotion](https://arxiv.org/abs/2511.06465)
*Lingfan Bao,Tianhu Peng,Chengxu Zhou*

Main category: cs.RO

TL;DR: 本章探讨双足机器人深度强化学习中的仿真到现实迁移挑战，分析了仿真差距的来源并提出了两种互补的解决方案框架。


<details>
  <summary>Details</summary>
Motivation: 解决双足机器人深度强化学习在仿真到现实迁移中的关键挑战，即"仿真诅咒"问题，确保训练出的策略能在真实环境中可靠运行。

Method: 采用两种互补方法：一是通过模型中心策略提高仿真器物理保真度来缩小差距；二是通过鲁棒性训练和后部署适应使策略对模型不准确性具有内在韧性。

Result: 提出了一个结构化框架，系统分析了仿真差距的来源（机器人动力学、接触建模、状态估计、数值求解器），并整合了两种解决方案哲学。

Conclusion: 通过整合模型改进和策略强化的双重方法，为开发稳健的仿真到现实解决方案提供了清晰的路线图和评估框架。

Abstract: This chapter addresses the critical challenge of simulation-to-reality (sim-to-real) transfer for deep reinforcement learning (DRL) in bipedal locomotion. After contextualizing the problem within various control architectures, we dissect the ``curse of simulation'' by analyzing the primary sources of sim-to-real gap: robot dynamics, contact modeling, state estimation, and numerical solvers. Building on this diagnosis, we structure the solutions around two complementary philosophies. The first is to shrink the gap through model-centric strategies that systematically improve the simulator's physical fidelity. The second is to harden the policy, a complementary approach that uses in-simulation robustness training and post-deployment adaptation to make the policy inherently resilient to model inaccuracies. The chapter concludes by synthesizing these philosophies into a strategic framework, providing a clear roadmap for developing and evaluating robust sim-to-real solutions.

</details>


### [17] [A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving](https://arxiv.org/abs/2511.06496)
*Keke Long,Jiacheng Guo,Tianyun Zhang,Hongkai Yu,Xiaopeng Li*

Main category: cs.RO

TL;DR: 提出一种基于低秩分解的自包含方法，自动对多个VLM生成的候选描述进行幻觉程度排序，无需外部参考或模型访问，在NuScenes数据集上达到87%的选择准确率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中使用的视觉语言模型会产生幻觉（虚假细节），但在缺乏真实参考和模型内部访问的情况下，检测和缓解幻觉具有挑战性。

Method: 构建句子嵌入矩阵，将其分解为低秩共识分量和稀疏残差，利用残差大小对描述进行排序，选择残差最小的作为最无幻觉的描述。

Result: 在NuScenes数据集上达到87%的选择准确率，比未过滤基线提高19%，比多智能体辩论方法提高6-10%；推理时间减少51-67%。

Conclusion: 该方法能有效识别幻觉最少的描述，排序结果与人类判断强相关，且易于并行化，适用于实时自动驾驶应用。

Abstract: Vision Language Models (VLMs) are increasingly used in autonomous driving to help understand traffic scenes, but they sometimes produce hallucinations, which are false details not grounded in the visual input. Detecting and mitigating hallucinations is challenging when ground-truth references are unavailable and model internals are inaccessible. This paper proposes a novel self-contained low-rank approach to automatically rank multiple candidate captions generated by multiple VLMs based on their hallucination levels, using only the captions themselves without requiring external references or model access. By constructing a sentence-embedding matrix and decomposing it into a low-rank consensus component and a sparse residual, we use the residual magnitude to rank captions: selecting the one with the smallest residual as the most hallucination-free. Experiments on the NuScenes dataset demonstrate that our approach achieves 87% selection accuracy in identifying hallucination-free captions, representing a 19% improvement over the unfiltered baseline and a 6-10% improvement over multi-agent debate method. The sorting produced by sparse error magnitudes shows strong correlation with human judgments of hallucinations, validating our scoring mechanism. Additionally, our method, which can be easily parallelized, reduces inference time by 51-67% compared to debate approaches, making it practical for real-time autonomous driving applications.

</details>


### [18] [Adaptive PID Control for Robotic Systems via Hierarchical Meta-Learning and Reinforcement Learning with Physics-Based Data Augmentation](https://arxiv.org/abs/2511.06500)
*JiaHao Wu,ShengWen Yu*

Main category: cs.RO

TL;DR: 提出了一种结合元学习和强化学习的层次控制框架，通过物理数据增强策略提高样本效率，在机器人PID控制中实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 工业机器人中PID控制器参数手动调优耗时且需要专业知识，需要开发自动化的参数调优方法。

Method: 使用元学习进行PID初始化，结合强化学习进行在线适应，引入基于物理的数据增强策略生成虚拟机器人配置。

Result: 在Franka Panda机械臂上平均改进16.6%（MAE 6.26°），高负载关节改进80.4%；发现优化天花板效应：当元学习基线性能均匀时RL无增益。

Conclusion: RL的有效性高度依赖于元学习基线质量和误差分布，为层次控制系统提供了重要设计指导。

Abstract: Proportional-Integral-Derivative (PID) controllers remain the predominant choice in industrial robotics due to their simplicity and reliability. However, manual tuning of PID parameters for diverse robotic platforms is time-consuming and requires extensive domain expertise. This paper presents a novel hierarchical control framework that combines meta-learning for PID initialization and reinforcement learning (RL) for online adaptation. To address the sample efficiency challenge, a \textit{physics-based data augmentation} strategy is introduced that generates virtual robot configurations by systematically perturbing physical parameters, enabling effective meta-learning with limited real robot data. The proposed approach is evaluated on two heterogeneous platforms: a 9-DOF Franka Panda manipulator and a 12-DOF Laikago quadruped robot. Experimental results demonstrate that the proposed method achieves 16.6\% average improvement on Franka Panda (6.26° MAE), with exceptional gains in high-load joints (J2: 80.4\% improvement from 12.36° to 2.42°). Critically, this work discovers the \textit{optimization ceiling effect}: RL achieves dramatic improvements when meta-learning exhibits localized high-error joints, but provides no benefit (0.0\%) when baseline performance is uniformly strong, as observed in Laikago. The method demonstrates robust performance under disturbances (parameter uncertainty: +19.2\%, no disturbance: +16.6\%, average: +10.0\%) with only 10 minutes of training time. Multi-seed analysis across 100 random initializations confirms stable performance (4.81+/-1.64\% average). These results establish that RL effectiveness is highly dependent on meta-learning baseline quality and error distribution, providing important design guidance for hierarchical control systems.

</details>


### [19] [Koopman global linearization of contact dynamics for robot locomotion and manipulation enables elaborate control](https://arxiv.org/abs/2511.06515)
*Cormac O'Neill,Jasmine Terrones,H. Harry Asada*

Main category: cs.RO

TL;DR: 本文提出了一种使用Koopman算子将接触动力学转化为全局线性模型的方法，实现了机器人在接触变化场景下的凸模型预测控制。


<details>
  <summary>Details</summary>
Motivation: 解决机器人与环境动态接触时的控制难题，特别是接触边界动力学切换导致的非凸优化问题。

Method: 应用Koopman算子将分段接触动力学统一到嵌入空间中的全局线性模型，利用粘弹性接触特性实现无近似的控制输入。

Result: 成功实现了腿式机器人的凸模型预测控制和机械臂动态推动的实时控制，机器人能够在多接触变化的时间范围内发现精细控制策略。

Conclusion: 该方法能够有效处理接触动力学问题，且适用范围超越机器人领域。

Abstract: Controlling robots that dynamically engage in contact with their environment is a pressing challenge. Whether a legged robot making-and-breaking contact with a floor, or a manipulator grasping objects, contact is everywhere. Unfortunately, the switching of dynamics at contact boundaries makes control difficult. Predictive controllers face non-convex optimization problems when contact is involved. Here, we overcome this difficulty by applying Koopman operators to subsume the segmented dynamics due to contact changes into a unified, globally-linear model in an embedding space. We show that viscoelastic contact at robot-environment interactions underpins the use of Koopman operators without approximation to control inputs. This methodology enables the convex Model Predictive Control of a legged robot, and the real-time control of a manipulator engaged in dynamic pushing. In this work, we show that our method allows robots to discover elaborate control strategies in real-time over time horizons with multiple contact changes, and the method is applicable to broad fields beyond robotics.

</details>


### [20] [CoFineLLM: Conformal Finetuning of LLMs for Language-Instructed Robot Planning](https://arxiv.org/abs/2511.06575)
*Jun Wang,Yevgeniy Vorobeychik,Yiannis Kantaros*

Main category: cs.RO

TL;DR: CoFineLLM是一个针对LLM规划器的CP感知微调框架，通过显式减少预测集大小来降低用户干预频率，提高自主部署能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM规划器在长视野任务中可靠性不足，虽然使用Conformal Prediction生成预测集确保正确性，但LLM不确定性不敏感导致预测集过大，需要频繁人工干预。

Method: 提出CoFineLLM框架，通过CP感知的微调方法，显式优化LLM以减少预测集大小，同时保持预测准确性。

Result: 在多个语言指令机器人规划任务中，相比不确定性感知和无感知的基线方法，在预测集大小和求助率方面均有持续改进。

Conclusion: 该方法能有效减少用户干预需求，并在硬件实验中展示了在分布外场景下的鲁棒性。

Abstract: Large Language Models (LLMs) have recently emerged as planners for language-instructed agents, generating sequences of actions to accomplish natural language tasks. However, their reliability remains a challenge, especially in long-horizon tasks, since they often produce overconfident yet wrong outputs. Conformal Prediction (CP) has been leveraged to address this issue by wrapping LLM outputs into prediction sets that contain the correct action with a user-defined confidence. When the prediction set is a singleton, the planner executes that action; otherwise, it requests help from a user. This has led to LLM-based planners that can ensure plan correctness with a user-defined probability. However, as LLMs are trained in an uncertainty-agnostic manner, without awareness of prediction sets, they tend to produce unnecessarily large sets, particularly at higher confidence levels, resulting in frequent human interventions limiting autonomous deployment. To address this, we introduce CoFineLLM (Conformal Finetuning for LLMs), the first CP-aware finetuning framework for LLM-based planners that explicitly reduces prediction-set size and, in turn, the need for user interventions. We evaluate our approach on multiple language-instructed robot planning problems and show consistent improvements over uncertainty-aware and uncertainty-agnostic finetuning baselines in terms of prediction-set size, and help rates. Finally, we demonstrate robustness of our method to out-of-distribution scenarios in hardware experiments.

</details>


### [21] [Underactuated Biomimetic Autonomous Underwater Vehicle for Ecosystem Monitoring](https://arxiv.org/abs/2511.06578)
*Kaustubh Singh,Shivam Kumar,Shashikant Pawar,Sandeep Manjanna*

Main category: cs.RO

TL;DR: 提出了一种欠驱动仿生水下机器人，适用于海洋和淡水环境生态系统监测，通过强化学习技术学习最小驱动行为


<details>
  <summary>Details</summary>
Motivation: 开发适用于海洋和淡水生态系统监测的仿生水下机器人，需要设计能够高效游泳的最小驱动机制

Method: 更新了鱼形机器人的机械设计，特别是尾部摆动机制，并在FishGym模拟器中使用强化学习技术学习游泳行为

Result: 提出了初步的机械设计，并在模拟器中展示了游泳行为，为后续强化学习测试奠定了基础

Conclusion: 该欠驱动仿生水下机器人设计为生态系统监测提供了有前景的解决方案，强化学习方法有望优化其游泳性能

Abstract: In this paper, we present an underactuated biomimetic underwater robot that is suitable for ecosystem monitoring in both marine and freshwater environments. We present an updated mechanical design for a fish-like robot and propose minimal actuation behaviors learned using reinforcement learning techniques. We present our preliminary mechanical design of the tail oscillation mechanism and illustrate the swimming behaviors on FishGym simulator, where the reinforcement learning techniques will be tested on

</details>


### [22] [How Do VLAs Effectively Inherit from VLMs?](https://arxiv.org/abs/2511.06619)
*Chuheng Zhang,Rushuai Yang,Xiaoyu Chen,Kaixin Wang,Li Zhao,Yi Chen,Jiang Bian*

Main category: cs.RO

TL;DR: 提出了GrinningFace诊断基准，用于评估视觉-语言-动作模型如何有效继承视觉语言模型的先验知识，通过表情符号桌面操作任务测试知识迁移效果。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型如何有效继承VLM先验知识的关键问题，表情符号在互联网数据中普遍存在但在机器人数据中罕见，提供了知识迁移的清晰测试场景。

Method: 创建表情符号桌面操作任务，在模拟和真实机器人环境中实现，比较参数高效微调、VLM冻结、联合训练、离散动作预测和潜在动作预测等多种技术。

Result: 系统评估表明保持VLM先验对VLA泛化能力至关重要，为开发真正可泛化的具身AI系统提供了指导原则。

Conclusion: 该工作不仅证明了保持VLM先验对VLA泛化的重要性，还为未来具身AI系统研究建立了指导方针。

Abstract: Vision-language-action (VLA) models hold the promise to attain generalizable embodied control. To achieve this, a pervasive paradigm is to leverage the rich vision-semantic priors of large vision-language models (VLMs). However, the fundamental question persists: How do VLAs effectively inherit the prior knowledge from VLMs? To address this critical question, we introduce a diagnostic benchmark, GrinningFace, an emoji tabletop manipulation task where the robot arm is asked to place objects onto printed emojis corresponding to language instructions. This task design is particularly revealing -- knowledge associated with emojis is ubiquitous in Internet-scale datasets used for VLM pre-training, yet emojis themselves are largely absent from standard robotics datasets. Consequently, they provide a clean proxy: successful task completion indicates effective transfer of VLM priors to embodied control. We implement this diagnostic task in both simulated environment and a real robot, and compare various promising techniques for knowledge transfer. Specifically, we investigate the effects of parameter-efficient fine-tuning, VLM freezing, co-training, predicting discretized actions, and predicting latent actions. Through systematic evaluation, our work not only demonstrates the critical importance of preserving VLM priors for the generalization of VLA but also establishes guidelines for future research in developing truly generalizable embodied AI systems.

</details>


### [23] [Rapidly Learning Soft Robot Control via Implicit Time-Stepping](https://arxiv.org/abs/2511.06667)
*Andrew Choi,Dezhong Tong*

Main category: cs.RO

TL;DR: 本文展示了通过隐式时间步进实现软体机器人快速策略学习的可行性，使用DisMech模拟器结合delta自然曲率控制方法，在四个软体机械臂任务中相比Elastica框架实现了显著加速。


<details>
  <summary>Details</summary>
Motivation: 软体机器人模拟框架稀缺且计算成本高，导致策略学习不可行，而刚性体模拟器已广泛使用。本文旨在解决软体机器人模拟和策略学习的效率问题。

Method: 采用DisMech全隐式软体模拟器，结合delta自然曲率控制方法，通过隐式时间步进进行策略学习。

Result: 在500个环境并行步进时，非接触情况加速6倍，接触丰富场景加速40倍；sim-to-sim评估显示加速不牺牲精度。

Conclusion: 隐式时间步进为软体机器人策略学习提供了罕见的高效解决方案，在获得显著加速的同时保持准确性。

Abstract: With the explosive growth of rigid-body simulators, policy learning in simulation has become the de facto standard for most rigid morphologies. In contrast, soft robotic simulation frameworks remain scarce and are seldom adopted by the soft robotics community. This gap stems partly from the lack of easy-to-use, general-purpose frameworks and partly from the high computational cost of accurately simulating continuum mechanics, which often renders policy learning infeasible. In this work, we demonstrate that rapid soft robot policy learning is indeed achievable via implicit time-stepping. Our simulator of choice, DisMech, is a general-purpose, fully implicit soft-body simulator capable of handling both soft dynamics and frictional contact. We further introduce delta natural curvature control, a method analogous to delta joint position control in rigid manipulators, providing an intuitive and effective means of enacting control for soft robot learning. To highlight the benefits of implicit time-stepping and delta curvature control, we conduct extensive comparisons across four diverse soft manipulator tasks against one of the most widely used soft-body frameworks, Elastica. With implicit time-stepping, parallel stepping of 500 environments achieves up to 6x faster speeds for non-contact cases and up to 40x faster for contact-rich scenarios. Finally, a comprehensive sim-to-sim gap evaluation--training policies in one simulator and evaluating them in another--demonstrates that implicit time-stepping provides a rare free lunch: dramatic speedups achieved without sacrificing accuracy.

</details>


### [24] [Programmable Telescopic Soft Pneumatic Actuators for Deployable and Shape Morphing Soft Robots](https://arxiv.org/abs/2511.06673)
*Joel Kemp,Andre Farinha,David Howard,Krishna Manaswi Digumarti,Josh Pinskier*

Main category: cs.RO

TL;DR: 提出了一种参数化软体执行器PTSPA，通过充气轴向膨胀实现可展开结构，适用于受限空间操作，并展示了在可展开软体四足机器人中的应用。


<details>
  <summary>Details</summary>
Motivation: 软体机器人具有自由形态和连续体特性，但目前缺乏有效利用其设计自由度的可扩展方法。参数化设计集提供了一条实现模块化软体机器人的可行路径。

Method: 开发了参数化几何生成器，通过高层输入定制执行器模型，并通过半自动化实验系统探索关键参数的设计空间。

Result: 表征了执行器的伸缩/弯曲、膨胀和刚度特性，揭示了关键设计参数与性能之间的明确关系。

Conclusion: PTSPA为可展开和形状变形结构以及需要大长度变化的场景提供了新的设计范式。

Abstract: Soft Robotics presents a rich canvas for free-form and continuum devices capable of exerting forces in any direction and transforming between arbitrary configurations. However, there is no current way to tractably and directly exploit the design freedom due to the curse of dimensionality. Parameterisable sets of designs offer a pathway towards tractable, modular soft robotics that appropriately harness the behavioural freeform of soft structures to create rich embodied behaviours. In this work, we present a parametrised class of soft actuators, Programmable Telescopic Soft Pneumatic Actuators (PTSPAs). PTSPAs expand axially on inflation for deployable structures and manipulation in challenging confined spaces. We introduce a parametric geometry generator to customise actuator models from high-level inputs, and explore the new design space through semi-automated experimentation and systematic exploration of key parameters. Using it we characterise the actuators' extension/bending, expansion, and stiffness and reveal clear relationships between key design parameters and performance. Finally we demonstrate the application of the actuators in a deployable soft quadruped whose legs deploy to walk, enabling automatic adaptation to confined spaces. PTSPAs present new design paradigm for deployable and shape morphing structures and wherever large length changes are required.

</details>


### [25] [Physically-Grounded Goal Imagination: Physics-Informed Variational Autoencoder for Self-Supervised Reinforcement Learning](https://arxiv.org/abs/2511.06745)
*Lan Thi Ha Nguyen,Kien Ton Manh,Anh Do Duc,Nam Pham Hai*

Main category: cs.RO

TL;DR: 提出PI-RIG方法，通过物理增强的变分自编码器生成物理一致的可行目标，解决自监督强化学习中目标设置问题


<details>
  <summary>Details</summary>
Motivation: 现有方法如RIG使用VAE在潜在空间生成目标，但会产生物理上不可行的目标，影响学习效率

Method: 提出增强物理信息变分自编码器(Enhanced p3-VAE)，将潜在空间显式分离为物理变量和环境因素，通过微分方程约束和守恒定律强制物理一致性

Result: 实验表明物理信息目标生成显著提高了目标质量，在视觉机器人操作任务中实现更有效的探索和技能获取

Conclusion: PI-RIG通过整合物理约束到VAE训练中，能够生成物理一致且可实现的目标，提升自监督强化学习性能

Abstract: Self-supervised goal-conditioned reinforcement learning enables robots to autonomously acquire diverse skills without human supervision. However, a central challenge is the goal setting problem: robots must propose feasible and diverse goals that are achievable in their current environment. Existing methods like RIG (Visual Reinforcement Learning with Imagined Goals) use variational autoencoder (VAE) to generate goals in a learned latent space but have the limitation of producing physically implausible goals that hinder learning efficiency. We propose Physics-Informed RIG (PI-RIG), which integrates physical constraints directly into the VAE training process through a novel Enhanced Physics-Informed Variational Autoencoder (Enhanced p3-VAE), enabling the generation of physically consistent and achievable goals. Our key innovation is the explicit separation of the latent space into physics variables governing object dynamics and environmental factors capturing visual appearance, while enforcing physical consistency through differential equation constraints and conservation laws. This enables the generation of physically consistent and achievable goals that respect fundamental physical principles such as object permanence, collision constraints, and dynamic feasibility. Through extensive experiments, we demonstrate that this physics-informed goal generation significantly improves the quality of proposed goals, leading to more effective exploration and better skill acquisition in visual robotic manipulation tasks including reaching, pushing, and pick-and-place scenarios.

</details>


### [26] [Semi-distributed Cross-modal Air-Ground Relative Localization](https://arxiv.org/abs/2511.06749)
*Weining Lu,Deer Bin,Lian Ma,Ming Ma,Zhihao Ma,Xiangyang Chen,Longfei Wang,Yixiao Feng,Zhouxian Jiang,Yongliang Shi,Bin Liang*

Main category: cs.RO

TL;DR: 提出了一种半分布式跨模态空地相对定位框架，通过解耦相对定位与状态估计，使用深度学习关键点和全局描述符，在通信带宽低于0.3Mbps的情况下实现高效准确的地空机器人相对定位。


<details>
  <summary>Details</summary>
Motivation: 当前多机器人相对定位方法主要采用相同传感器配置的分布式SLAM系统，与所有机器人的状态估计紧密耦合，限制了灵活性和准确性。需要一种更高效、准确且灵活的跨模态相对定位方案。

Method: UGV和UAV独立执行SLAM并提取深度学习关键点和全局描述符。UGV使用LiDAR、相机和IMU进行局部Bundle Adjustment，分为两个阶段：优化从LiDAR-惯性里程计插值的相机位姿，然后估计UGV与UAV之间的相对相机位姿。采用基于深度学习的描述符实现增量式闭环检测。

Result: 实验结果表明该方法在准确性和效率方面表现优异，通信带宽被有效限制在0.3Mbps以下，相比传统多机器人SLAM传输图像或点云的方法更加高效。

Conclusion: 该方法成功实现了高效、准确且通信效率高的跨模态空地相对定位，为空中-地面协同任务提供了实用的解决方案。

Abstract: Efficient, accurate, and flexible relative localization is crucial in air-ground collaborative tasks. However, current approaches for robot relative localization are primarily realized in the form of distributed multi-robot SLAM systems with the same sensor configuration, which are tightly coupled with the state estimation of all robots, limiting both flexibility and accuracy. To this end, we fully leverage the high capacity of Unmanned Ground Vehicle (UGV) to integrate multiple sensors, enabling a semi-distributed cross-modal air-ground relative localization framework. In this work, both the UGV and the Unmanned Aerial Vehicle (UAV) independently perform SLAM while extracting deep learning-based keypoints and global descriptors, which decouples the relative localization from the state estimation of all agents. The UGV employs a local Bundle Adjustment (BA) with LiDAR, camera, and an IMU to rapidly obtain accurate relative pose estimates. The BA process adopts sparse keypoint optimization and is divided into two stages: First, optimizing camera poses interpolated from LiDAR-Inertial Odometry (LIO), followed by estimating the relative camera poses between the UGV and UAV. Additionally, we implement an incremental loop closure detection algorithm using deep learning-based descriptors to maintain and retrieve keyframes efficiently. Experimental results demonstrate that our method achieves outstanding performance in both accuracy and efficiency. Unlike traditional multi-robot SLAM approaches that transmit images or point clouds, our method only transmits keypoint pixels and their descriptors, effectively constraining the communication bandwidth under 0.3 Mbps. Codes and data will be publicly available on https://github.com/Ascbpiac/cross-model-relative-localization.git.

</details>


### [27] [SlotVLA: Towards Modeling of Object-Relation Representations in Robotic Manipulation](https://arxiv.org/abs/2511.06754)
*Taisei Hanyu,Nhat Chung,Huy Le,Toan Nguyen,Yuki Ikebe,Anthony Gunderman,Duy Nguyen Ho Minh,Khoa Vo,Tung Kieu,Kashu Yamazaki,Chase Rainwater,Anh Nguyen,Ngan Le*

Main category: cs.RO

TL;DR: 提出LIBERO+数据集和SlotVLA框架，探索基于对象关系表示的紧凑、可解释机器人操作


<details>
  <summary>Details</summary>
Motivation: 现有机器人多任务模型依赖密集嵌入，混淆对象和背景线索，存在效率和可解释性问题。受人类基于离散对象及其关系推理的启发，研究对象关系中心表示作为结构化、高效、可解释视觉运动控制的途径

Method: 1) 引入LIBERO+细粒度基准数据集，提供对象中心标注（边界框、掩码标签、实例级时序跟踪）；2) 提出SlotVLA框架：使用基于槽注意力的视觉分词器保持时序对象表示，关系中心解码器生成任务相关嵌入，LLM驱动模块将嵌入转换为可执行动作

Result: 在LIBERO+上的实验表明，对象中心槽和对象关系槽表示大幅减少所需视觉标记数量，同时提供有竞争力的泛化能力

Conclusion: LIBERO+和SlotVLA共同为推进对象关系中心机器人操作提供了紧凑、可解释且有效的基础

Abstract: Inspired by how humans reason over discrete objects and their relationships, we explore whether compact object-centric and object-relation representations can form a foundation for multitask robotic manipulation. Most existing robotic multitask models rely on dense embeddings that entangle both object and background cues, raising concerns about both efficiency and interpretability. In contrast, we study object-relation-centric representations as a pathway to more structured, efficient, and explainable visuomotor control. Our contributions are two-fold. First, we introduce LIBERO+, a fine-grained benchmark dataset designed to enable and evaluate object-relation reasoning in robotic manipulation. Unlike prior datasets, LIBERO+ provides object-centric annotations that enrich demonstrations with box- and mask-level labels as well as instance-level temporal tracking, supporting compact and interpretable visuomotor representations. Second, we propose SlotVLA, a slot-attention-based framework that captures both objects and their relations for action decoding. It uses a slot-based visual tokenizer to maintain consistent temporal object representations, a relation-centric decoder to produce task-relevant embeddings, and an LLM-driven module that translates these embeddings into executable actions. Experiments on LIBERO+ demonstrate that object-centric slot and object-relation slot representations drastically reduce the number of required visual tokens, while providing competitive generalization. Together, LIBERO+ and SlotVLA provide a compact, interpretable, and effective foundation for advancing object-relation-centric robotic manipulation.

</details>


### [28] [Human-Level Actuation for Humanoids](https://arxiv.org/abs/2511.06796)
*MD-Nazmus Sunbeam*

Main category: cs.RO

TL;DR: 本文提出了一个量化评估人形机器人"人类水平"驱动能力的综合框架，包括标准化关节坐标系、人类等效包络和人类水平驱动评分三个核心组件。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人领域普遍声称达到"人类水平"驱动能力，但缺乏量化标准和可比性。峰值扭矩或速度规格无法反映在任务相关姿态和速率下的扭矩、功率和耐力综合表现。

Method: 1) 自由度图谱：使用ISB标准统一关节坐标系和运动范围；2) 人类等效包络：评估机器人在相同关节角度和速率下是否同时满足人类扭矩和功率要求；3) 人类水平驱动评分：综合六个物理因素，包括工作空间覆盖、HEE覆盖、扭矩模式带宽、效率和热可持续性。

Result: 提供了详细的测量协议，包括测力计、电功率监测和热测试，可通过可重复实验获得所有HLAS输入。通过多关节人形机器人的实例展示了HLAS计算过程。

Conclusion: 该框架既可作为人形机器人开发的设计规范，也可作为比较驱动系统的基准标准，所有组件都基于已发表的人类生物力学数据。

Abstract: Claims that humanoid robots achieve ``human-level'' actuation are common but rarely quantified. Peak torque or speed specifications tell us little about whether a joint can deliver the right combination of torque, power, and endurance at task-relevant postures and rates. We introduce a comprehensive framework that makes ``human-level'' measurable and comparable across systems. Our approach has three components. First, a kinematic \emph{DoF atlas} standardizes joint coordinate systems and ranges of motion using ISB-based conventions, ensuring that human and robot joints are compared in the same reference frames. Second, \emph{Human-Equivalence Envelopes (HEE)} define per-joint requirements by measuring whether a robot meets human torque \emph{and} power simultaneously at the same joint angle and rate $(q,ω)$, weighted by positive mechanical work in task-specific bands (walking, stairs, lifting, reaching, and hand actions). Third, the \emph{Human-Level Actuation Score (HLAS)} aggregates six physically grounded factors: workspace coverage (ROM and DoF), HEE coverage, torque-mode bandwidth, efficiency, and thermal sustainability. We provide detailed measurement protocols using dynamometry, electrical power monitoring, and thermal testing that yield every HLAS input from reproducible experiments. A worked example demonstrates HLAS computation for a multi-joint humanoid, showing how the score exposes actuator trade-offs (gearing ratio versus bandwidth and efficiency) that peak-torque specifications obscure. The framework serves as both a design specification for humanoid development and a benchmarking standard for comparing actuation systems, with all components grounded in published human biomechanics data.

</details>


### [29] [Vision-Aided Online A* Path Planning for Efficient and Safe Navigation of Service Robots](https://arxiv.org/abs/2511.06801)
*Praveen Kumar,Tushar Sandhan*

Main category: cs.RO

TL;DR: 提出了一种将轻量级语义感知与实时路径规划相结合的框架，使低成本机器人能够在复杂环境中进行上下文感知导航，区分重要物品与普通障碍物。


<details>
  <summary>Details</summary>
Motivation: 传统导航系统依赖昂贵的LiDAR，虽然几何精度高但缺乏语义理解能力，无法区分重要文件与普通垃圾，导致在人类中心环境中部署受限。

Method: 采用轻量级感知模块与在线A*规划器的紧密集成，通过语义分割识别用户定义的视觉约束，并将其投影为非几何障碍物到全局地图中。

Result: 在高保真仿真和真实机器人平台上验证了框架的鲁棒性和实时性能，证明低成本机器人能够安全导航复杂环境并尊重传统规划器无法识别的关键视觉线索。

Conclusion: 该框架成功填补了语义感知与实时路径规划之间的关键空白，使低成本机器人能够在人类中心环境中实现上下文感知导航。

Abstract: The deployment of autonomous service robots in human-centric environments is hindered by a critical gap in perception and planning. Traditional navigation systems rely on expensive LiDARs that, while geometrically precise, are seman- tically unaware, they cannot distinguish a important document on an office floor from a harmless piece of litter, treating both as physically traversable. While advanced semantic segmentation exists, no prior work has successfully integrated this visual intelligence into a real-time path planner that is efficient enough for low-cost, embedded hardware. This paper presents a frame- work to bridge this gap, delivering context-aware navigation on an affordable robotic platform. Our approach centers on a novel, tight integration of a lightweight perception module with an online A* planner. The perception system employs a semantic segmentation model to identify user-defined visual constraints, enabling the robot to navigate based on contextual importance rather than physical size alone. This adaptability allows an operator to define what is critical for a given task, be it sensitive papers in an office or safety lines in a factory, thus resolving the ambiguity of what to avoid. This semantic perception is seamlessly fused with geometric data. The identified visual constraints are projected as non-geometric obstacles onto a global map that is continuously updated from sensor data, enabling robust navigation through both partially known and unknown environments. We validate our framework through extensive experiments in high-fidelity simulations and on a real-world robotic platform. The results demonstrate robust, real-time performance, proving that a cost- effective robot can safely navigate complex environments while respecting critical visual cues invisible to traditional planners.

</details>


### [30] [Vision-Based System Identification of a Quadrotor](https://arxiv.org/abs/2511.06839)
*Selim Ahmet Iz,Mustafa Unel*

Main category: cs.RO

TL;DR: 该论文研究了基于视觉的系统辨识技术在四旋翼无人机建模与控制中的应用，通过灰箱建模和LQR控制器设计验证了机载视觉系统在系统辨识中的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决四旋翼无人机建模中的复杂性和局限性，特别是推力和阻力系数的不确定性问题，探索基于视觉的系统辨识技术提升四旋翼性能的潜力。

Method: 采用灰箱建模方法减轻不确定性，利用机载视觉系统进行系统辨识，并基于辨识模型设计LQR控制器。

Result: 模型间表现一致，验证了基于视觉的系统辨识技术的有效性，证明了该技术在提升四旋翼性能方面的潜力。

Conclusion: 基于视觉的系统辨识技术能够有效增强四旋翼无人机的建模和控制能力，为未来四旋翼性能提升、故障检测和决策过程研究开辟了新途径。

Abstract: This paper explores the application of vision-based system identification techniques in quadrotor modeling and control. Through experiments and analysis, we address the complexities and limitations of quadrotor modeling, particularly in relation to thrust and drag coefficients. Grey-box modeling is employed to mitigate uncertainties, and the effectiveness of an onboard vision system is evaluated. An LQR controller is designed based on a system identification model using data from the onboard vision system. The results demonstrate consistent performance between the models, validating the efficacy of vision based system identification. This study highlights the potential of vision-based techniques in enhancing quadrotor modeling and control, contributing to improved performance and operational capabilities. Our findings provide insights into the usability and consistency of these techniques, paving the way for future research in quadrotor performance enhancement, fault detection, and decision-making processes.

</details>


### [31] [Multi-Agent AI Framework for Road Situation Detection and C-ITS Message Generation](https://arxiv.org/abs/2511.06892)
*Kailin Tong,Selim Solmaz,Kenan Mujkic,Gottfried Allmer,Bo Leng*

Main category: cs.RO

TL;DR: 提出多智能体AI框架，结合多模态大语言模型和视觉感知进行道路状况监控，在自定义数据集上评估，发现Gemini-2.5-Flash在检测精度和语义理解上不如Gemini-2.0-Flash。


<details>
  <summary>Details</summary>
Motivation: 传统道路状况检测方法在预定义场景中表现良好，但在未知情况下失效且缺乏语义解释，这对可靠的交通推荐至关重要。

Method: 多智能体AI框架，处理摄像头数据并协调专用智能体进行状况检测、距离估计、决策制定和C-ITS消息生成。

Result: 状况检测召回率100%，消息模式正确性完美；但存在误检，在车道数量、行驶车道状态和原因代码方面性能下降。Gemini-2.5-Flash在检测精度和语义理解上不如Gemini-2.0-Flash，且延迟更高。

Conclusion: 需要进一步研究针对智能交通应用优化的专用LLMs或MLLMs的微调工作。

Abstract: Conventional road-situation detection methods achieve strong performance in predefined scenarios but fail in unseen cases and lack semantic interpretation, which is crucial for reliable traffic recommendations. This work introduces a multi-agent AI framework that combines multimodal large language models (MLLMs) with vision-based perception for road-situation monitoring. The framework processes camera feeds and coordinates dedicated agents for situation detection, distance estimation, decision-making, and Cooperative Intelligent Transport System (C-ITS) message generation. Evaluation is conducted on a custom dataset of 103 images extracted from 20 videos of the TAD dataset. Both Gemini-2.0-Flash and Gemini-2.5-Flash were evaluated. The results show 100\% recall in situation detection and perfect message schema correctness; however, both models suffer from false-positive detections and have reduced performance in terms of number of lanes, driving lane status and cause code. Surprisingly, Gemini-2.5-Flash, though more capable in general tasks, underperforms Gemini-2.0-Flash in detection accuracy and semantic understanding and incurs higher latency (Table II). These findings motivate further work on fine-tuning specialized LLMs or MLLMs tailored for intelligent transportation applications.

</details>


### [32] [Integration of Visual SLAM into Consumer-Grade Automotive Localization](https://arxiv.org/abs/2511.06919)
*Luis Diener,Jens Kalkkuhl,Markus Enzweiler*

Main category: cs.RO

TL;DR: 提出了一种融合视觉SLAM和车辆横向动力学模型的框架，用于在线校准陀螺仪，提高消费级车辆的定位精度。


<details>
  <summary>Details</summary>
Motivation: 消费级车辆目前依赖轮式里程计和IMU进行自我运动估计，但这些传感器存在系统误差和校准问题。视觉SLAM在机器人领域已成为标准，但在汽车自我运动估计中的应用仍待探索。

Method: 开发了一个融合视觉SLAM与车辆横向动力学模型的框架，能够在实际驾驶条件下在线校准陀螺仪。

Result: 实验结果表明，基于视觉的集成方法显著提高了陀螺仪校准精度，从而增强了整体定位性能。在公共基准测试中优于现有最先进方法。

Conclusion: 该研究展示了通过视觉SLAM集成提高汽车定位精度的有前景路径，为消费级车辆提供了更准确的自我运动估计解决方案。

Abstract: Accurate ego-motion estimation in consumer-grade vehicles currently relies on proprioceptive sensors, i.e. wheel odometry and IMUs, whose performance is limited by systematic errors and calibration. While visual-inertial SLAM has become a standard in robotics, its integration into automotive ego-motion estimation remains largely unexplored. This paper investigates how visual SLAM can be integrated into consumer-grade vehicle localization systems to improve performance. We propose a framework that fuses visual SLAM with a lateral vehicle dynamics model to achieve online gyroscope calibration under realistic driving conditions. Experimental results demonstrate that vision-based integration significantly improves gyroscope calibration accuracy and thus enhances overall localization performance, highlighting a promising path toward higher automotive localization accuracy. We provide results on both proprietary and public datasets, showing improved performance and superior localization accuracy on a public benchmark compared to state-of-the-art methods.

</details>


### [33] [Raspi$^2$USBL: An open-source Raspberry Pi-Based Passive Inverted Ultra-Short Baseline Positioning System for Underwater Robotics](https://arxiv.org/abs/2511.06998)
*Jin Huang,Yingqiang Wang,Ying Chen*

Main category: cs.RO

TL;DR: Raspi^2USBL是一个基于树莓派的开源被动倒置超短基线定位系统，为水下机器人研究提供低成本解决方案，在实验中实现了0.1%的斜距精度和0.1°的方位精度。


<details>
  <summary>Details</summary>
Motivation: 由于GNSS信号无法穿透海面，精确的水下定位一直是水下机器人的基本挑战。需要开发低成本、可访问的水下定位解决方案。

Method: 系统采用模块化硬件架构，包括水听器阵列、多通道前置放大器、OCXO、树莓派5和DAQ板。使用开源C++软件框架进行高精度时钟同步、信号处理和波束成形，估计信号飞行时间和到达方向。

Result: 在消声池、淡水湖和公海试验中验证，斜距精度优于0.1%，方位精度在0.1°以内，在长达1.3公里的操作距离内保持稳定性能。

Conclusion: 低成本、可复制的硬件可以提供研究级的水下定位精度。开源硬件和软件降低了水下机器人实验室的入门门槛，促进了水下声学导航和群体机器人的协作创新。

Abstract: Precise underwater positioning remains a fundamental challenge for underwater robotics since global navigation satellite system (GNSS) signals cannot penetrate the sea surface. This paper presents Raspi$^2$USBL, an open-source, Raspberry Pi-based passive inverted ultra-short baseline (piUSBL) positioning system designed to provide a low-cost and accessible solution for underwater robotic research. The system comprises a passive acoustic receiver and an active beacon. The receiver adopts a modular hardware architecture that integrates a hydrophone array, a multichannel preamplifier, an oven-controlled crystal oscillator (OCXO), a Raspberry Pi 5, and an MCC-series data acquisition (DAQ) board. Apart from the Pi 5, OCXO, and MCC board, the beacon comprises an impedance-matching network, a power amplifier, and a transmitting transducer. An open-source C++ software framework provides high-precision clock synchronization and triggering for one-way travel-time (OWTT) messaging, while performing real-time signal processing, including matched filtering, array beamforming, and adaptive gain control, to estimate the time of flight (TOF) and direction of arrival (DOA) of received signals. The Raspi$^2$USBL system was experimentally validated in an anechoic tank, freshwater lake, and open-sea trials. Results demonstrate a slant-range accuracy better than 0.1%, a bearing accuracy within 0.1$^\circ$, and stable performance over operational distances up to 1.3 km. These findings confirm that low-cost, reproducible hardware can deliver research-grade underwater positioning accuracy. By releasing both the hardware and software as open-source, Raspi$^2$USBL provides a unified reference platform that lowers the entry barrier for underwater robotics laboratories, fosters reproducibility, and promotes collaborative innovation in underwater acoustic navigation and swarm robotics.

</details>


### [34] [HDCNet: A Hybrid Depth Completion Network for Grasping Transparent and Reflective Objects](https://arxiv.org/abs/2511.07081)
*Guanghu Xie,Mingxu Li,Songwei Wu,Yang Liu,Zongwu Xie,Baoshi Cao,Hong Liu*

Main category: cs.RO

TL;DR: HDCNet：一种结合Transformer、CNN和Mamba架构的新型深度补全网络，专门解决透明和反光物体的深度感知问题，在多个数据集上达到SOTA性能，机器人抓取成功率提升高达60%。


<details>
  <summary>Details</summary>
Motivation: 传统深度传感器在透明和反光物体表面往往无法提供可靠的测量数据，这限制了机器人在感知和抓取任务中的表现。

Method: 设计双分支Transformer-CNN编码器提取模态特定特征；浅层引入轻量级多模态融合模块；网络瓶颈处开发Transformer-Mamba混合融合模块，实现高层语义和全局上下文信息的深度整合。

Result: 在多个公开数据集上达到最先进的深度补全性能；机器人抓取实验显示对透明和反光物体的抓取成功率显著提升，最高达60%。

Conclusion: HDCNet通过有效整合多种架构的优势，成功解决了透明和反光物体的深度感知难题，显著提升了机器人抓取性能。

Abstract: Depth perception of transparent and reflective objects has long been a critical challenge in robotic manipulation.Conventional depth sensors often fail to provide reliable measurements on such surfaces, limiting the performance of robots in perception and grasping tasks. To address this issue, we propose a novel depth completion network,HDCNet,which integrates the complementary strengths of Transformer,CNN and Mamba architectures.Specifically,the encoder is designed as a dual-branch Transformer-CNN framework to extract modality-specific features. At the shallow layers of the encoder, we introduce a lightweight multimodal fusion module to effectively integrate low-level features. At the network bottleneck,a Transformer-Mamba hybrid fusion module is developed to achieve deep integration of high-level semantic and global contextual information, significantly enhancing depth completion accuracy and robustness. Extensive evaluations on multiple public datasets demonstrate that HDCNet achieves state-of-the-art(SOTA) performance in depth completion tasks.Furthermore,robotic grasping experiments show that HDCNet substantially improves grasp success rates for transparent and reflective objects,achieving up to a 60% increase.

</details>


### [35] [Dynamics-Decoupled Trajectory Alignment for Sim-to-Real Transfer in Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2511.07155)
*Thomas Steinecker,Alexander Bienemann,Denis Trescher,Thorsten Luettel,Mirko Maehlisch*

Main category: cs.RO

TL;DR: 提出了一种通过时空对齐策略将运动规划与车辆控制解耦的框架，实现RL智能体从仿真到现实的零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 由于车辆动力学复杂性及仿真与现实不匹配，RL在真实车辆上部署面临挑战，难以准确建模现实世界动力学。

Method: 使用运动学自行车模型训练RL智能体输出连续控制动作，将其行为蒸馏为轨迹预测智能体生成有限范围自车轨迹，通过Stanley控制器控制横向动力学，纵向对齐采用自适应更新机制补偿虚拟与现实轨迹偏差。

Result: 在真实车辆上验证了该方法，证明对齐策略能够实现基于RL的运动规划从仿真到现实的稳健零样本迁移。

Conclusion: 成功将高层轨迹生成与低层车辆控制解耦，为RL在真实车辆上的部署提供了可行方案。

Abstract: Reinforcement learning (RL) has shown promise in robotics, but deploying RL on real vehicles remains challenging due to the complexity of vehicle dynamics and the mismatch between simulation and reality. Factors such as tire characteristics, road surface conditions, aerodynamic disturbances, and vehicle load make it infeasible to model real-world dynamics accurately, which hinders direct transfer of RL agents trained in simulation. In this paper, we present a framework that decouples motion planning from vehicle control through a spatial and temporal alignment strategy between a virtual vehicle and the real system. An RL agent is first trained in simulation using a kinematic bicycle model to output continuous control actions. Its behavior is then distilled into a trajectory-predicting agent that generates finite-horizon ego-vehicle trajectories, enabling synchronization between virtual and real vehicles. At deployment, a Stanley controller governs lateral dynamics, while longitudinal alignment is maintained through adaptive update mechanisms that compensate for deviations between virtual and real trajectories. We validate our approach on a real vehicle and demonstrate that the proposed alignment strategy enables robust zero-shot transfer of RL-based motion planning from simulation to reality, successfully decoupling high-level trajectory generation from low-level vehicle control.

</details>


### [36] [Automated Generation of Continuous-Space Roadmaps for Routing Mobile Robot Fleets](https://arxiv.org/abs/2511.07175)
*Marvin Rüdt,Constantin Enke,Kai Furmans*

Main category: cs.RO

TL;DR: 提出了一种自动化路线图生成方法，在连续空间中结合站点间运输需求和最小距离约束，为移动机器人车队生成优化的导航路线图


<details>
  <summary>Details</summary>
Motivation: 现有方法要么基于网格而牺牲几何精度，要么是连续空间方法但忽略实际约束，需要一种能兼顾几何精度和实际应用需求的路线图生成方法

Method: 结合自由空间离散化、运输需求驱动的K最短路径优化和路径平滑，在连续空间中生成路线图并强制执行节点和边的最小距离约束

Result: 在多个内部物流用例中，该方法始终优于现有基线方法（4连接网格、8连接网格和随机采样），实现了更低的结枃复杂性、更高的冗余度和接近最优的路径长度

Conclusion: 该方法能够为移动机器人车队生成高效且鲁棒的路线图，显著提升系统吞吐量

Abstract: Efficient routing of mobile robot fleets is crucial in intralogistics, where delays and deadlocks can substantially reduce system throughput. Roadmap design, specifying feasible transport routes, directly affects fleet coordination and computational performance. Existing approaches are either grid-based, compromising geometric precision, or continuous-space approaches that disregard practical constraints. This paper presents an automated roadmap generation approach that bridges this gap by operating in continuous-space, integrating station-to-station transport demand and enforcing minimum distance constraints for nodes and edges. By combining free space discretization, transport demand-driven $K$-shortest-path optimization, and path smoothing, the approach produces roadmaps tailored to intralogistics applications. Evaluation across multiple intralogistics use cases demonstrates that the proposed approach consistently outperforms established baselines (4-connected grid, 8-connected grid, and random sampling), achieving lower structural complexity, higher redundancy, and near-optimal path lengths, enabling efficient and robust routing of mobile robot fleets.

</details>


### [37] [Robotic versus Human Teleoperation for Remote Ultrasound](https://arxiv.org/abs/2511.07275)
*David Black,Septimiu Salcudean*

Main category: cs.RO

TL;DR: 比较人类远程操作与机器人远程操作在远程超声检查中的性能差异，发现两者在完成时间和位置精度上无显著差异，人类远程操作在力应用方面更一致且更实用


<details>
  <summary>Details</summary>
Motivation: 解决农村地区超声检查专家短缺问题，开发远程超声技术，比较人类远程操作和机器人远程操作的相对优势

Method: 评估人类与机器人远程操作的实际方面（如设置时间和灵活性），并通过实验比较完成时间、位置跟踪和力一致性等性能指标

Result: 人类远程操作在完成时间或位置精度上没有统计学显著差异（平均差异分别为1.8%和0.5%），并且提供了更一致的力应用，同时更具实用性和可及性

Conclusion: 人类远程操作在性能上与机器人远程操作相当，但更实用和易于获取，特别适合小型社区使用

Abstract: Diagnostic medical ultrasound is widely used, safe, and relatively low cost but requires a high degree of expertise to acquire and interpret the images. Personnel with this expertise are often not available outside of larger cities, leading to difficult, costly travel and long wait times for rural populations. To address this issue, tele-ultrasound techniques are being developed, including robotic teleoperation and recently human teleoperation, in which a novice user is remotely guided in a hand-over-hand manner through mixed reality to perform an ultrasound exam. These methods have not been compared, and their relative strengths are unknown. Human teleoperation may be more practical than robotics for small communities due to its lower cost and complexity, but this is only relevant if the performance is comparable. This paper therefore evaluates the differences between human and robotic teleoperation, examining practical aspects such as setup time and flexibility and experimentally comparing performance metrics such as completion time, position tracking, and force consistency. It is found that human teleoperation does not lead to statistically significant differences in completion time or position accuracy, with mean differences of 1.8% and 0.5%, respectively, and provides more consistent force application despite being substantially more practical and accessible.

</details>


### [38] [PlanT 2.0: Exposing Biases and Structural Flaws in Closed-Loop Driving](https://arxiv.org/abs/2511.07292)
*Simon Gerstenecker,Andreas Geiger,Katrin Renz*

Main category: cs.RO

TL;DR: 本文介绍了PlanT 2.0，一个轻量级、以对象为中心的规划变换器，用于CARLA自动驾驶研究。通过系统性的扰动分析揭示了模型失败的根本原因，并提出了向以数据为中心的开发转变的建议。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶研究过于关注基准性能和方法创新，而缺乏对模型失败、偏见和捷径学习的深入分析。这导致改进缺乏对当前失败原因的深刻理解。

Method: 引入PlanT 2.0，一个基于对象级表示的规划变换器，通过可控的输入扰动（如改变对象位置、添加或删除对象）来系统分析模型行为。

Result: 在CARLA Leaderboard 2.0的挑战性场景中，PlanT 2.0在Longest6 v2、Bench2Drive和CARLA验证路线上实现了最先进的性能。分析揭示了由于障碍物多样性不足导致的场景理解缺乏、刚性专家行为导致的易被利用的捷径，以及对固定专家轨迹集的过拟合等问题。

Conclusion: 基于研究发现，作者主张向以数据为中心的开发转变，重点关注更丰富、更鲁棒、偏见更少的数据集。代码和模型已开源。

Abstract: Most recent work in autonomous driving has prioritized benchmark performance and methodological innovation over in-depth analysis of model failures, biases, and shortcut learning. This has led to incremental improvements without a deep understanding of the current failures. While it is straightforward to look at situations where the model fails, it is hard to understand the underlying reason. This motivates us to conduct a systematic study, where inputs to the model are perturbed and the predictions observed. We introduce PlanT 2.0, a lightweight, object-centric planning transformer designed for autonomous driving research in CARLA. The object-level representation enables controlled analysis, as the input can be easily perturbed (e.g., by changing the location or adding or removing certain objects), in contrast to sensor-based models. To tackle the scenarios newly introduced by the challenging CARLA Leaderboard 2.0, we introduce multiple upgrades to PlanT, achieving state-of-the-art performance on Longest6 v2, Bench2Drive, and the CARLA validation routes. Our analysis exposes insightful failures, such as a lack of scene understanding caused by low obstacle diversity, rigid expert behaviors leading to exploitable shortcuts, and overfitting to a fixed set of expert trajectories. Based on these findings, we argue for a shift toward data-centric development, with a focus on richer, more robust, and less biased datasets. We open-source our code and model at https://github.com/autonomousvision/plant2.

</details>


### [39] [Exact Smooth Reformulations for Trajectory Optimization Under Signal Temporal Logic Specifications](https://arxiv.org/abs/2511.07375)
*Shaohang Han,Joris Verhagen,Jana Tumova*

Main category: cs.RO

TL;DR: 提出了一种基于信号时序逻辑（STL）的运动规划方法，通过轨迹优化和精确的max/min算子重构实现精确、平滑且可靠的STL合成。


<details>
  <summary>Details</summary>
Motivation: 信号时序逻辑（STL）是描述时空需求的有用形式化方法，但现有的STL合成方法存在近似误差或不可微问题，需要开发精确且可微的解决方案。

Method: 将STL合成建模为轨迹优化问题，利用STL鲁棒性语义，引入max和min算子的精确重构以避免近似误差，确保问题的可微性。

Result: 该方法在数值模拟中得到验证，展示了其实际性能，证明能够有效处理STL规范的运动规划问题。

Conclusion: 提出的STL合成方法是精确、平滑且可靠的，为具有复杂时空约束的运动规划问题提供了有效的解决方案。

Abstract: We study motion planning under Signal Temporal Logic (STL), a useful formalism for specifying spatial-temporal requirements. We pose STL synthesis as a trajectory optimization problem leveraging the STL robustness semantics. To obtain a differentiable problem without approximation error, we introduce an exact reformulation of the max and min operators. The resulting method is exact, smooth, and sound. We validate it in numerical simulations, demonstrating its practical performance.

</details>


### [40] [Residual Rotation Correction using Tactile Equivariance](https://arxiv.org/abs/2511.07381)
*Yizhe Zhu,Zhang Ye,Boce Hu,Haibo Zhao,Yu Qi,Dian Wang,Robert Platt*

Main category: cs.RO

TL;DR: EquiTac是一个利用SO(2)对称性改进触觉策略学习的框架，通过重建触觉传感器的表面法向量并使用等变网络预测旋转动作，实现高效样本利用和零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 触觉数据收集成本高昂，因此需要提高触觉策略学习的样本效率。现有方法在处理接触丰富的操作任务时样本效率低且泛化能力不足。

Method: 1) 从触觉传感器RGB输入重建表面法向量；2) 使用SO(2)等变网络预测旋转动作残差；3) 在测试时增强基础视觉运动策略，实现实时旋转校正。

Result: 在真实机器人上，EquiTac仅需少量训练样本就能实现零样本泛化到未见的手内方向，而基线方法即使使用更多训练数据也无法达到相同效果。

Conclusion: 这是首个明确编码触觉等变性进行策略学习的方法，提供了一个轻量级、对称感知的模块，显著提高了接触丰富任务的可靠性。

Abstract: Visuotactile policy learning augments vision-only policies with tactile input, facilitating contact-rich manipulation. However, the high cost of tactile data collection makes sample efficiency the key requirement for developing visuotactile policies. We present EquiTac, a framework that exploits the inherent SO(2) symmetry of in-hand object rotation to improve sample efficiency and generalization for visuotactile policy learning. EquiTac first reconstructs surface normals from raw RGB inputs of vision-based tactile sensors, so rotations of the normal vector field correspond to in-hand object rotations. An SO(2)-equivariant network then predicts a residual rotation action that augments a base visuomotor policy at test time, enabling real-time rotation correction without additional reorientation demonstrations. On a real robot, EquiTac accurately achieves robust zero-shot generalization to unseen in-hand orientations with very few training samples, where baselines fail even with more training data. To our knowledge, this is the first tactile learning method to explicitly encode tactile equivariance for policy learning, yielding a lightweight, symmetry-aware module that improves reliability in contact-rich tasks.

</details>


### [41] [Unified Humanoid Fall-Safety Policy from a Few Demonstrations](https://arxiv.org/abs/2511.07407)
*Zhengjie Xu,Ye Li,Kwan-yee Lin,Stella X. Yu*

Main category: cs.RO

TL;DR: 提出了一种统一策略，将防摔、冲击缓解和快速恢复整合到一个策略中，通过融合稀疏人类演示、强化学习和自适应扩散记忆来实现。


<details>
  <summary>Details</summary>
Motivation: 人形机器人摔倒是一个固有风险，现有方法只关注摔倒的孤立方面（避免摔倒、控制下降或站起），缺乏完整的摔倒恢复策略。

Method: 融合稀疏人类演示与强化学习，结合自适应扩散记忆的安全反应，学习自适应全身行为。

Result: 在仿真和Unitree G1机器人上的实验展示了鲁棒的仿真到现实迁移、更低的冲击力和一致的快速恢复能力。

Conclusion: 该方法为实现更安全、更具韧性的人形机器人在真实环境中运行提供了方向。

Abstract: Falling is an inherent risk of humanoid mobility. Maintaining stability is thus a primary safety focus in robot control and learning, yet no existing approach fully averts loss of balance. When instability does occur, prior work addresses only isolated aspects of falling: avoiding falls, choreographing a controlled descent, or standing up afterward. Consequently, humanoid robots lack integrated strategies for impact mitigation and prompt recovery when real falls defy these scripts. We aim to go beyond keeping balance to make the entire fall-and-recovery process safe and autonomous: prevent falls when possible, reduce impact when unavoidable, and stand up when fallen. By fusing sparse human demonstrations with reinforcement learning and an adaptive diffusion-based memory of safe reactions, we learn adaptive whole-body behaviors that unify fall prevention, impact mitigation, and rapid recovery in one policy. Experiments in simulation and on a Unitree G1 demonstrate robust sim-to-real transfer, lower impact forces, and consistently fast recovery across diverse disturbances, pointing towards safer, more resilient humanoids in real environments. Videos are available at https://firm2025.github.io/.

</details>


### [42] [Using Vision Language Models as Closed-Loop Symbolic Planners for Robotic Applications: A Control-Theoretic Perspective](https://arxiv.org/abs/2511.07410)
*Hao Wang,Sathwik Karnik,Bea Lim,Somil Bansal*

Main category: cs.RO

TL;DR: 研究如何从控制理论视角使用视觉语言模型作为闭环符号规划器，分析控制时域和热启动对性能的影响


<details>
  <summary>Details</summary>
Motivation: LLMs和VLMs在具身符号规划中广泛应用，但如何有效用于闭环符号规划仍待探索。由于它们是黑盒模型，会产生不可预测或代价高昂的错误，使得在高层机器人规划中的使用特别具有挑战性

Method: 从控制理论视角研究VLMs作为闭环符号规划器，设计受控实验研究控制时域和热启动对VLM符号规划器性能的影响

Result: 获得了广泛适用于将VLMs用作闭环符号规划器的见解，并提出了有助于提高VLM符号规划器性能的建议

Conclusion: 通过控制理论视角分析VLM符号规划器的关键参数，为改进其闭环规划性能提供了实用指导

Abstract: Large Language Models (LLMs) and Vision Language Models (VLMs) have been widely used for embodied symbolic planning. Yet, how to effectively use these models for closed-loop symbolic planning remains largely unexplored. Because they operate as black boxes, LLMs and VLMs can produce unpredictable or costly errors, making their use in high-level robotic planning especially challenging. In this work, we investigate how to use VLMs as closed-loop symbolic planners for robotic applications from a control-theoretic perspective. Concretely, we study how the control horizon and warm-starting impact the performance of VLM symbolic planners. We design and conduct controlled experiments to gain insights that are broadly applicable to utilizing VLMs as closed-loop symbolic planners, and we discuss recommendations that can help improve the performance of VLM symbolic planners.

</details>


### [43] [Robot Learning from a Physical World Model](https://arxiv.org/abs/2511.07416)
*Jiageng Mao,Sicheng He,Hao-Ning Wu,Yang You,Shuyang Sun,Zhicheng Wang,Yanan Bao,Huizhong Chen,Leonidas Guibas,Vitor Guizilini,Howard Zhou,Yue Wang*

Main category: cs.RO

TL;DR: PhysWorld是一个通过物理世界建模从视频生成中学习机器人操作的框架，将视频生成与物理重建相结合，实现零样本可泛化的机器人操作。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成模型可以从语言命令和图像合成逼真的视觉演示，但直接将像素运动重定向到机器人会忽略物理约束，导致操作不准确。

Method: 结合视频生成与物理世界重建，通过物体中心残差强化学习将生成的视频运动转化为物理上准确的动作。

Result: 在多样化真实世界任务上的实验表明，PhysWorld相比之前方法显著提高了操作准确性。

Conclusion: PhysWorld将隐式视觉指导转化为物理上可执行的机器人轨迹，无需真实机器人数据收集即可实现零样本泛化。

Abstract: We introduce PhysWorld, a framework that enables robot learning from video generation through physical world modeling. Recent video generation models can synthesize photorealistic visual demonstrations from language commands and images, offering a powerful yet underexplored source of training signals for robotics. However, directly retargeting pixel motions from generated videos to robots neglects physics, often resulting in inaccurate manipulations. PhysWorld addresses this limitation by coupling video generation with physical world reconstruction. Given a single image and a task command, our method generates task-conditioned videos and reconstructs the underlying physical world from the videos, and the generated video motions are grounded into physically accurate actions through object-centric residual reinforcement learning with the physical world model. This synergy transforms implicit visual guidance into physically executable robotic trajectories, eliminating the need for real robot data collection and enabling zero-shot generalizable robotic manipulation. Experiments on diverse real-world tasks demonstrate that PhysWorld substantially improves manipulation accuracy compared to previous approaches. Visit \href{https://pointscoder.github.io/PhysWorld_Web/}{the project webpage} for details.

</details>


### [44] [Lightning Grasp: High Performance Procedural Grasp Synthesis with Contact Fields](https://arxiv.org/abs/2511.07418)
*Zhao-Heng Yin,Pieter Abbeel*

Main category: cs.RO

TL;DR: Lightning Grasp是一种高性能的程序化抓取合成算法，相比最先进方法实现数量级加速，支持不规则工具类物体的无监督抓取生成。


<details>
  <summary>Details</summary>
Motivation: 尽管经过多年研究，灵巧手的实时多样化抓取合成仍然是机器人和计算机图形学中未解决的核心挑战。

Method: 通过关键洞察：使用简单高效的数据结构——接触场（Contact Field），将复杂几何计算与搜索过程解耦，从而降低问题复杂度，实现前所未有的程序化搜索速度。

Result: 算法避免了先前方法的许多限制，如需要精心调优的能量函数和敏感初始化，实现了数量级的速度提升。

Conclusion: 该系统已开源，以推动机器人操作的进一步创新。

Abstract: Despite years of research, real-time diverse grasp synthesis for dexterous hands remains an unsolved core challenge in robotics and computer graphics. We present Lightning Grasp, a novel high-performance procedural grasp synthesis algorithm that achieves orders-of-magnitude speedups over state-of-the-art approaches, while enabling unsupervised grasp generation for irregular, tool-like objects. The method avoids many limitations of prior approaches, such as the need for carefully tuned energy functions and sensitive initialization. This breakthrough is driven by a key insight: decoupling complex geometric computation from the search process via a simple, efficient data structure - the Contact Field. This abstraction collapses the problem complexity, enabling a procedural search at unprecedented speeds. We open-source our system to propel further innovation in robotic manipulation.

</details>
