{"id": "2510.08705", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08705", "abs": "https://arxiv.org/abs/2510.08705", "authors": ["Noah Steinkr\u00fcger", "Nisarga Nilavadi", "Wolfram Burgard", "Tanja Katharina Kaiser"], "title": "ConPoSe: LLM-Guided Contact Point Selection for Scalable Cooperative Object Pushing", "comment": null, "summary": "Object transportation in cluttered environments is a fundamental task in\nvarious domains, including domestic service and warehouse logistics. In\ncooperative object transport, multiple robots must coordinate to move objects\nthat are too large for a single robot. One transport strategy is pushing, which\nonly requires simple robots. However, careful selection of robot-object contact\npoints is necessary to push the object along a preplanned path. Although this\nselection can be solved analytically, the solution space grows combinatorially\nwith the number of robots and object size, limiting scalability. Inspired by\nhow humans rely on common-sense reasoning for cooperative transport, we propose\ncombining the reasoning capabilities of Large Language Models with local search\nto select suitable contact points. Our LLM-guided local search method for\ncontact point selection, ConPoSe, successfully selects contact points for a\nvariety of shapes, including cuboids, cylinders, and T-shapes. We demonstrate\nthat ConPoSe scales better with the number of robots and object size than the\nanalytical approach, and also outperforms pure LLM-based selection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u548c\u5c40\u90e8\u641c\u7d22\u7684\u63a5\u89e6\u70b9\u9009\u62e9\u65b9\u6cd5ConPoSe\uff0c\u7528\u4e8e\u591a\u673a\u5668\u4eba\u534f\u4f5c\u63a8\u52a8\u7269\u4f53\u5728\u6742\u4e71\u73af\u5883\u4e2d\u7684\u8fd0\u8f93\uff0c\u76f8\u6bd4\u89e3\u6790\u65b9\u6cd5\u548c\u7eafLLM\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u5728\u6742\u4e71\u73af\u5883\u4e2d\u8fdb\u884c\u7269\u4f53\u8fd0\u8f93\u662f\u670d\u52a1\u673a\u5668\u4eba\u548c\u4ed3\u50a8\u7269\u6d41\u7684\u57fa\u672c\u4efb\u52a1\u3002\u591a\u673a\u5668\u4eba\u534f\u4f5c\u63a8\u52a8\u5927\u578b\u7269\u4f53\u65f6\uff0c\u9700\u8981\u4ed4\u7ec6\u9009\u62e9\u673a\u5668\u4eba\u4e0e\u7269\u4f53\u7684\u63a5\u89e6\u70b9\u4ee5\u786e\u4fdd\u6309\u9884\u5b9a\u8def\u5f84\u79fb\u52a8\u3002\u867d\u7136\u89e3\u6790\u65b9\u6cd5\u53ef\u4ee5\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u968f\u7740\u673a\u5668\u4eba\u6570\u91cf\u548c\u7269\u4f53\u5c3a\u5bf8\u7684\u589e\u52a0\uff0c\u89e3\u7a7a\u95f4\u5448\u7ec4\u5408\u5f0f\u589e\u957f\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u5c40\u90e8\u641c\u7d22\u6765\u9009\u62e9\u5408\u9002\u7684\u63a5\u89e6\u70b9\u3002\u63d0\u51fa\u7684ConPoSe\u65b9\u6cd5\u5229\u7528LLM\u8fdb\u884c\u5e38\u8bc6\u63a8\u7406\u6307\u5bfc\u5c40\u90e8\u641c\u7d22\u8fc7\u7a0b\uff0c\u4e3a\u957f\u65b9\u4f53\u3001\u5706\u67f1\u4f53\u548cT\u5f62\u7b49\u591a\u79cd\u5f62\u72b6\u7684\u7269\u4f53\u9009\u62e9\u63a5\u89e6\u70b9\u3002", "result": "ConPoSe\u6210\u529f\u4e3a\u591a\u79cd\u5f62\u72b6\u7684\u7269\u4f53\u9009\u62e9\u4e86\u5408\u9002\u7684\u63a5\u89e6\u70b9\u3002\u76f8\u6bd4\u89e3\u6790\u65b9\u6cd5\uff0cConPoSe\u5728\u673a\u5668\u4eba\u6570\u91cf\u548c\u7269\u4f53\u5c3a\u5bf8\u589e\u52a0\u65f6\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4e14\u6027\u80fd\u4f18\u4e8e\u7eaf\u57fa\u4e8eLLM\u7684\u9009\u62e9\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u5c40\u90e8\u641c\u7d22\u7684ConPoSe\u65b9\u6cd5\u4e3a\u591a\u673a\u5668\u4eba\u534f\u4f5c\u7269\u4f53\u8fd0\u8f93\u4e2d\u7684\u63a5\u89e6\u70b9\u9009\u62e9\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2510.08753", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08753", "abs": "https://arxiv.org/abs/2510.08753", "authors": ["A. Wang", "C. Jiang", "M. Przystupa", "J. Valentine", "M. Jagersand"], "title": "Point and Go: Intuitive Reference Frame Reallocation in Mode Switching for Assistive Robotics", "comment": "7 Pages, 5 figures", "summary": "Operating high degree of freedom robots can be difficult for users of\nwheelchair mounted robotic manipulators. Mode switching in Cartesian space has\nseveral drawbacks such as unintuitive control reference frames, separate\ntranslation and orientation control, and limited movement capabilities that\nhinder performance. We propose Point and Go mode switching, which reallocates\nthe Cartesian mode switching reference frames into a more intuitive action\nspace comprised of new translation and rotation modes. We use a novel sweeping\nmotion to point the gripper, which defines the new translation axis along the\nrobot base frame's horizontal plane. This creates an intuitive `point and go'\ntranslation mode that allows the user to easily perform complex, human-like\nmovements without switching control modes. The system's rotation mode combines\nposition control with a refined end-effector oriented frame that provides\nprecise and consistent robot actions in various end-effector poses. We verified\nits effectiveness through initial experiments, followed by a three-task user\nstudy that compared our method to Cartesian mode switching and a state of the\nart learning method. Results show that Point and Go mode switching reduced\ncompletion times by 31\\%, pauses by 41\\%, and mode switches by 33\\%, while\nreceiving significantly favorable responses in user surveys.", "AI": {"tldr": "\u63d0\u51faPoint and Go\u6a21\u5f0f\u5207\u6362\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u5206\u914d\u7b1b\u5361\u5c14\u6a21\u5f0f\u5207\u6362\u53c2\u8003\u7cfb\u5230\u66f4\u76f4\u89c2\u7684\u52a8\u4f5c\u7a7a\u95f4\uff0c\u51cf\u5c11\u8f6e\u6905\u673a\u5668\u4eba\u64cd\u4f5c\u96be\u5ea6\uff0c\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7b1b\u5361\u5c14\u7a7a\u95f4\u6a21\u5f0f\u5207\u6362\u5b58\u5728\u63a7\u5236\u53c2\u8003\u7cfb\u4e0d\u76f4\u89c2\u3001\u5e73\u79fb\u548c\u65cb\u8f6c\u63a7\u5236\u5206\u79bb\u3001\u8fd0\u52a8\u80fd\u529b\u6709\u9650\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u8f6e\u6905\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u65b0\u9896\u7684\u626b\u63a0\u8fd0\u52a8\u6307\u5411\u5939\u5177\uff0c\u5728\u673a\u5668\u4eba\u57fa\u5ea7\u6c34\u5e73\u9762\u5b9a\u4e49\u65b0\u7684\u5e73\u79fb\u8f74\uff0c\u521b\u5efa\u76f4\u89c2\u7684'\u6307\u5411\u5e76\u79fb\u52a8'\u5e73\u79fb\u6a21\u5f0f\uff1b\u65cb\u8f6c\u6a21\u5f0f\u7ed3\u5408\u4f4d\u7f6e\u63a7\u5236\u548c\u7cbe\u5316\u7684\u672b\u7aef\u6267\u884c\u5668\u5b9a\u5411\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u663e\u793aPoint and Go\u6a21\u5f0f\u5207\u6362\u4f7f\u5b8c\u6210\u65f6\u95f4\u51cf\u5c1131%\u3001\u6682\u505c\u51cf\u5c1141%\u3001\u6a21\u5f0f\u5207\u6362\u51cf\u5c1133%\uff0c\u7528\u6237\u8c03\u67e5\u83b7\u5f97\u663e\u8457\u79ef\u6781\u53cd\u9988\u3002", "conclusion": "Point and Go\u6a21\u5f0f\u5207\u6362\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u8f6e\u6905\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u76f4\u89c2\u6027\u548c\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u7b1b\u5361\u5c14\u6a21\u5f0f\u5207\u6362\u548c\u5148\u8fdb\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2510.08754", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.08754", "abs": "https://arxiv.org/abs/2510.08754", "authors": ["David Nguyen", "Zulfiqar Zaidi", "Kevin Karol", "Jessica Hodgins", "Zhaoming Xie"], "title": "Whole Body Model Predictive Control for Spin-Aware Quadrupedal Table Tennis", "comment": "Submitted to appear in IEEE ICRA 2026", "summary": "Developing table tennis robots that mirror human speed, accuracy, and ability\nto predict and respond to the full range of ball spins remains a significant\nchallenge for legged robots. To demonstrate these capabilities we present a\nsystem to play dynamic table tennis for quadrupedal robots that integrates high\nspeed perception, trajectory prediction, and agile control. Our system uses\nexternal cameras for high-speed ball localization, physical models with learned\nresiduals to infer spin and predict trajectories, and a novel model predictive\ncontrol (MPC) formulation for agile full-body control. Notably, a continuous\nset of stroke strategies emerge automatically from different ball return\nobjectives using this control paradigm. We demonstrate our system in the real\nworld on a Spot quadruped, evaluate accuracy of each system component, and\nexhibit coordination through the system's ability to aim and return balls with\nvarying spin types. As a further demonstration, the system is able to rally\nwith human players.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u7684\u52a8\u6001\u4e52\u4e53\u7403\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u9ad8\u901f\u611f\u77e5\u3001\u8f68\u8ff9\u9884\u6d4b\u548c\u654f\u6377\u63a7\u5236\uff0c\u80fd\u591f\u4e0e\u4eba\u7c7b\u73a9\u5bb6\u8fdb\u884c\u5bf9\u6253\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u5339\u914d\u4eba\u7c7b\u901f\u5ea6\u3001\u7cbe\u5ea6\u548c\u9884\u6d4b\u5404\u79cd\u7403\u65cb\u8f6c\u80fd\u529b\u7684\u4e52\u4e53\u7403\u673a\u5668\u4eba\u5bf9\u817f\u5f0f\u673a\u5668\u4eba\u6765\u8bf4\u4ecd\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5916\u90e8\u6444\u50cf\u5934\u8fdb\u884c\u9ad8\u901f\u7403\u5b9a\u4f4d\uff0c\u91c7\u7528\u5e26\u5b66\u4e60\u6b8b\u5dee\u7684\u7269\u7406\u6a21\u578b\u63a8\u65ad\u65cb\u8f6c\u548c\u9884\u6d4b\u8f68\u8ff9\uff0c\u4ee5\u53ca\u65b0\u9896\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u516c\u5f0f\u8fdb\u884c\u654f\u6377\u5168\u8eab\u63a7\u5236\u3002", "result": "\u5728Spot\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u6f14\u793a\uff0c\u80fd\u591f\u7784\u51c6\u5e76\u56de\u51fb\u4e0d\u540c\u65cb\u8f6c\u7c7b\u578b\u7684\u7403\uff0c\u5e76\u80fd\u4e0e\u4eba\u7c7b\u73a9\u5bb6\u8fdb\u884c\u5bf9\u6253\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u56db\u8db3\u673a\u5668\u4eba\u5728\u52a8\u6001\u4e52\u4e53\u7403\u4efb\u52a1\u4e2d\u7684\u534f\u8c03\u80fd\u529b\uff0c\u81ea\u52a8\u6d8c\u73b0\u51fa\u8fde\u7eed\u7684\u51fb\u7403\u7b56\u7565\uff0c\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u7684\u654f\u6377\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u8303\u4f8b\u3002"}}
{"id": "2510.08787", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08787", "abs": "https://arxiv.org/abs/2510.08787", "authors": ["Yiming Li", "Nael Darwiche", "Amirreza Razmjoo", "Sichao Liu", "Yilun Du", "Auke Ijspeert", "Sylvain Calinon"], "title": "Geometry-aware Policy Imitation", "comment": "21 pages, 13 figures. In submission", "summary": "We propose a Geometry-aware Policy Imitation (GPI) approach that rethinks\nimitation learning by treating demonstrations as geometric curves rather than\ncollections of state-action samples. From these curves, GPI derives distance\nfields that give rise to two complementary control primitives: a progression\nflow that advances along expert trajectories and an attraction flow that\ncorrects deviations. Their combination defines a controllable, non-parametric\nvector field that directly guides robot behavior. This formulation decouples\nmetric learning from policy synthesis, enabling modular adaptation across\nlow-dimensional robot states and high-dimensional perceptual inputs. GPI\nnaturally supports multimodality by preserving distinct demonstrations as\nseparate models and allows efficient composition of new demonstrations through\nsimple additions to the distance field. We evaluate GPI in simulation and on\nreal robots across diverse tasks. Experiments show that GPI achieves higher\nsuccess rates than diffusion-based policies while running 20 times faster,\nrequiring less memory, and remaining robust to perturbations. These results\nestablish GPI as an efficient, interpretable, and scalable alternative to\ngenerative approaches for robotic imitation learning. Project website:\nhttps://yimingli1998.github.io/projects/GPI/", "AI": {"tldr": "GPI\u5c06\u6f14\u793a\u89c6\u4e3a\u51e0\u4f55\u66f2\u7ebf\u800c\u975e\u72b6\u6001-\u52a8\u4f5c\u6837\u672c\uff0c\u901a\u8fc7\u8ddd\u79bb\u573a\u751f\u6210\u63a8\u8fdb\u6d41\u548c\u5438\u5f15\u6d41\u4e24\u79cd\u63a7\u5236\u539f\u8bed\uff0c\u6784\u5efa\u53ef\u63a7\u7684\u975e\u53c2\u6570\u5316\u5411\u91cf\u573a\u76f4\u63a5\u6307\u5bfc\u673a\u5668\u4eba\u884c\u4e3a\u3002", "motivation": "\u91cd\u65b0\u601d\u8003\u6a21\u4eff\u5b66\u4e60\uff0c\u5c06\u6f14\u793a\u6570\u636e\u89c6\u4e3a\u51e0\u4f55\u66f2\u7ebf\uff0c\u5b9e\u73b0\u5ea6\u91cf\u5b66\u4e60\u4e0e\u7b56\u7565\u5408\u6210\u7684\u89e3\u8026\uff0c\u652f\u6301\u6a21\u5757\u5316\u9002\u5e94\u548c\u591a\u6a21\u6001\u6f14\u793a\u3002", "method": "\u4ece\u6f14\u793a\u66f2\u7ebf\u63a8\u5bfc\u8ddd\u79bb\u573a\uff0c\u751f\u6210\u63a8\u8fdb\u6d41\uff08\u6cbf\u4e13\u5bb6\u8f68\u8ff9\u524d\u8fdb\uff09\u548c\u5438\u5f15\u6d41\uff08\u7ea0\u6b63\u504f\u5dee\uff09\uff0c\u7ec4\u5408\u6210\u53ef\u63a7\u5411\u91cf\u573a\u76f4\u63a5\u6307\u5bfc\u673a\u5668\u4eba\u884c\u4e3a\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cGPI\u6bd4\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u83b7\u5f97\u66f4\u9ad8\u6210\u529f\u7387\uff0c\u8fd0\u884c\u901f\u5ea6\u5feb20\u500d\uff0c\u5185\u5b58\u9700\u6c42\u66f4\u5c11\uff0c\u5bf9\u6270\u52a8\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "GPI\u4e3a\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\u7684\u751f\u6210\u65b9\u6cd5\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2510.08807", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.08807", "abs": "https://arxiv.org/abs/2510.08807", "authors": ["Zhenyu Zhao", "Hongyi Jing", "Xiawei Liu", "Jiageng Mao", "Abha Jha", "Hanwen Yang", "Rong Xue", "Sergey Zakharor", "Vitor Guizilini", "Yue Wang"], "title": "Humanoid Everyday: A Comprehensive Robotic Dataset for Open-World Humanoid Manipulation", "comment": null, "summary": "From loco-motion to dextrous manipulation, humanoid robots have made\nremarkable strides in demonstrating complex full-body capabilities. However,\nthe majority of current robot learning datasets and benchmarks mainly focus on\nstationary robot arms, and the few existing humanoid datasets are either\nconfined to fixed environments or limited in task diversity, often lacking\nhuman-humanoid interaction and lower-body locomotion. Moreover, there are a few\nstandardized evaluation platforms for benchmarking learning-based policies on\nhumanoid data. In this work, we present Humanoid Everyday, a large-scale and\ndiverse humanoid manipulation dataset characterized by extensive task variety\ninvolving dextrous object manipulation, human-humanoid interaction,\nlocomotion-integrated actions, and more. Leveraging a highly efficient\nhuman-supervised teleoperation pipeline, Humanoid Everyday aggregates\nhigh-quality multimodal sensory data, including RGB, depth, LiDAR, and tactile\ninputs, together with natural language annotations, comprising 10.3k\ntrajectories and over 3 million frames of data across 260 tasks across 7 broad\ncategories. In addition, we conduct an analysis of representative policy\nlearning methods on our dataset, providing insights into their strengths and\nlimitations across different task categories. For standardized evaluation, we\nintroduce a cloud-based evaluation platform that allows researchers to\nseamlessly deploy their policies in our controlled setting and receive\nperformance feedback. By releasing Humanoid Everyday along with our policy\nlearning analysis and a standardized cloud-based evaluation platform, we intend\nto advance research in general-purpose humanoid manipulation and lay the\ngroundwork for more capable and embodied robotic agents in real-world\nscenarios. Our dataset, data collection code, and cloud evaluation website are\nmade publicly available on our project website.", "AI": {"tldr": "\u63d0\u51fa\u4e86Humanoid Everyday\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6837\u5316\u7684\u4eba\u5f62\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u96c6\uff0c\u5305\u542b10.3k\u8f68\u8ff9\u548c300\u4e07\u5e27\u6570\u636e\uff0c\u6db5\u76d6260\u4e2a\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u4e91\u7aef\u8bc4\u4f30\u5e73\u53f0\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u5b66\u4e60\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u56fa\u5b9a\u673a\u68b0\u81c2\uff0c\u73b0\u6709\u4eba\u5f62\u673a\u5668\u4eba\u6570\u636e\u96c6\u8981\u4e48\u5c40\u9650\u4e8e\u56fa\u5b9a\u73af\u5883\uff0c\u8981\u4e48\u4efb\u52a1\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u4eba\u673a\u4ea4\u4e92\u548c\u4e0b\u534a\u8eab\u8fd0\u52a8\uff0c\u4e14\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u5e73\u53f0\u3002", "method": "\u4f7f\u7528\u9ad8\u6548\u7684\u4eba\u7c7b\u76d1\u7763\u9065\u64cd\u4f5c\u6d41\u7a0b\uff0c\u6536\u96c6\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u611f\u77e5\u6570\u636e\uff08RGB\u3001\u6df1\u5ea6\u3001LiDAR\u3001\u89e6\u89c9\u8f93\u5165\uff09\u548c\u81ea\u7136\u8bed\u8a00\u6807\u6ce8\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b10.3k\u8f68\u8ff9\u548c300\u4e07\u5e27\u6570\u636e\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u6db5\u76d67\u5927\u7c7b\u522b260\u4e2a\u4efb\u52a1\uff0c\u5e76\u5bf9\u4ee3\u8868\u6027\u7b56\u7565\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u6790\u3002", "conclusion": "\u901a\u8fc7\u53d1\u5e03Humanoid Everyday\u6570\u636e\u96c6\u3001\u7b56\u7565\u5b66\u4e60\u5206\u6790\u548c\u6807\u51c6\u5316\u4e91\u7aef\u8bc4\u4f30\u5e73\u53f0\uff0c\u65e8\u5728\u63a8\u52a8\u901a\u7528\u4eba\u5f62\u673a\u5668\u4eba\u64cd\u4f5c\u7814\u7a76\uff0c\u4e3a\u73b0\u5b9e\u573a\u666f\u4e2d\u66f4\u5f3a\u5927\u7684\u5177\u8eab\u667a\u80fd\u4f53\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2510.08811", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08811", "abs": "https://arxiv.org/abs/2510.08811", "authors": ["Jiurun Song", "Xiao Liang", "Minghui Zheng"], "title": "Adaptive Motion Planning via Contact-Based Intent Inference for Human-Robot Collaboration", "comment": null, "summary": "Human-robot collaboration (HRC) requires robots to adapt their motions to\nhuman intent to ensure safe and efficient cooperation in shared spaces.\nAlthough large language models (LLMs) provide high-level reasoning for\ninferring human intent, their application to reliable motion planning in HRC\nremains challenging. Physical human-robot interaction (pHRI) is intuitive but\noften relies on continuous kinesthetic guidance, which imposes burdens on\noperators. To address these challenges, a contact-informed adaptive\nmotion-planning framework is introduced to infer human intent directly from\nphysical contact and employ the inferred intent for online motion correction in\nHRC. First, an optimization-based force estimation method is proposed to infer\nhuman-intended contact forces and locations from joint torque measurements and\na robot dynamics model, thereby reducing cost and installation complexity while\nenabling whole-body sensitivity. Then, a torque-based contact detection\nmechanism with link-level localization is introduced to reduce the optimization\nsearch space and to enable real-time estimation. Subsequently, a\ncontact-informed adaptive motion planner is developed to infer human intent\nfrom contacts and to replan robot motion online, while maintaining smoothness\nand adapting to human corrections. Finally, experiments on a 7-DOF manipulator\nare conducted to demonstrate the accuracy of the proposed force estimation\nmethod and the effectiveness of the contact-informed adaptive motion planner\nunder perception uncertainty in HRC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a5\u89e6\u611f\u77e5\u7684\u81ea\u9002\u5e94\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u7269\u7406\u63a5\u89e6\u76f4\u63a5\u63a8\u65ad\u4eba\u7c7b\u610f\u56fe\uff0c\u5e76\u7528\u4e8e\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u5728\u7ebf\u8fd0\u52a8\u6821\u6b63\u3002", "motivation": "\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u4e2d\u673a\u5668\u4eba\u5982\u4f55\u5b89\u5168\u9ad8\u6548\u5730\u9002\u5e94\u4eba\u7c7b\u610f\u56fe\u7684\u95ee\u9898\u3002\u4f20\u7edf\u65b9\u6cd5\u4e2d\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u53ef\u9760\u5e94\u7528\u4e8e\u8fd0\u52a8\u89c4\u5212\uff0c\u800c\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u9700\u8981\u6301\u7eed\u7684\u8fd0\u52a8\u5f15\u5bfc\uff0c\u7ed9\u64cd\u4f5c\u8005\u5e26\u6765\u8d1f\u62c5\u3002", "method": "1) \u57fa\u4e8e\u4f18\u5316\u7684\u529b\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ece\u5173\u8282\u626d\u77e9\u6d4b\u91cf\u548c\u673a\u5668\u4eba\u52a8\u529b\u5b66\u6a21\u578b\u63a8\u65ad\u4eba\u7c7b\u610f\u56fe\u7684\u63a5\u89e6\u529b\u548c\u4f4d\u7f6e\uff1b2) \u57fa\u4e8e\u626d\u77e9\u7684\u63a5\u89e6\u68c0\u6d4b\u673a\u5236\uff0c\u5b9e\u73b0\u94fe\u63a5\u7ea7\u5b9a\u4f4d\uff1b3) \u63a5\u89e6\u611f\u77e5\u7684\u81ea\u9002\u5e94\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u5728\u7ebf\u91cd\u65b0\u89c4\u5212\u673a\u5668\u4eba\u8fd0\u52a8\u3002", "result": "\u57287\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u529b\u4f30\u8ba1\u65b9\u6cd5\u5177\u6709\u51c6\u786e\u6027\uff0c\u63a5\u89e6\u611f\u77e5\u7684\u81ea\u9002\u5e94\u8fd0\u52a8\u89c4\u5212\u5668\u5728\u4eba\u673a\u534f\u4f5c\u4e2d\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\u5177\u6709\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7269\u7406\u63a5\u89e6\u76f4\u63a5\u63a8\u65ad\u4eba\u7c7b\u610f\u56fe\uff0c\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u4eba\u673a\u534f\u4f5c\u8fd0\u52a8\u89c4\u5212\uff0c\u51cf\u5c11\u4e86\u64cd\u4f5c\u8d1f\u62c5\u548c\u5b89\u88c5\u590d\u6742\u6027\u3002"}}
{"id": "2510.08812", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.08812", "abs": "https://arxiv.org/abs/2510.08812", "authors": ["Grace Ra Kim", "Hailey Warner", "Duncan Eddy", "Evan Astle", "Zachary Booth", "Edward Balaban", "Mykel J. Kochenderfer"], "title": "Adaptive Science Operations in Deep Space Missions Using Offline Belief State Planning", "comment": "7 pages, 4 tables, 5 figures, accepted in IEEE ISPARO 2026", "summary": "Deep space missions face extreme communication delays and environmental\nuncertainty that prevent real-time ground operations. To support autonomous\nscience operations in communication-constrained environments, we present a\npartially observable Markov decision process (POMDP) framework that adaptively\nsequences spacecraft science instruments. We integrate a Bayesian network into\nthe POMDP observation space to manage the high-dimensional and uncertain\nmeasurements typical of astrobiology missions. This network compactly encodes\ndependencies among measurements and improves the interpretability and\ncomputational tractability of science data. Instrument operation policies are\ncomputed offline, allowing resource-aware plans to be generated and thoroughly\nvalidated prior to launch. We use the Enceladus Orbilander's proposed Life\nDetection Suite (LDS) as a case study, demonstrating how Bayesian network\nstructure and reward shaping influence system performance. We compare our\nmethod against the mission's baseline Concept of Operations (ConOps),\nevaluating both misclassification rates and performance in off-nominal sample\naccumulation scenarios. Our approach reduces sample identification errors by\nnearly 40%", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6df1\u7a7a\u4efb\u52a1\u4e2d\u81ea\u9002\u5e94\u6392\u5e8f\u822a\u5929\u5668\u79d1\u5b66\u4eea\u5668\uff0c\u4ee5\u652f\u6301\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e0b\u7684\u81ea\u4e3b\u79d1\u5b66\u64cd\u4f5c\u3002", "motivation": "\u6df1\u7a7a\u4efb\u52a1\u9762\u4e34\u6781\u7aef\u7684\u901a\u4fe1\u5ef6\u8fdf\u548c\u73af\u5883\u4e0d\u786e\u5b9a\u6027\uff0c\u65e0\u6cd5\u8fdb\u884c\u5b9e\u65f6\u5730\u9762\u64cd\u4f5c\uff0c\u9700\u8981\u81ea\u4e3b\u79d1\u5b66\u64cd\u4f5c\u80fd\u529b\u3002", "method": "\u5c06\u8d1d\u53f6\u65af\u7f51\u7edc\u96c6\u6210\u5230POMDP\u89c2\u6d4b\u7a7a\u95f4\u4e2d\uff0c\u7ba1\u7406\u9ad8\u7ef4\u548c\u4e0d\u786e\u5b9a\u7684\u6d4b\u91cf\u6570\u636e\uff1b\u79bb\u7ebf\u8ba1\u7b97\u4eea\u5668\u64cd\u4f5c\u7b56\u7565\uff0c\u5141\u8bb8\u5728\u53d1\u5c04\u524d\u751f\u6210\u8d44\u6e90\u611f\u77e5\u8ba1\u5212\u5e76\u8fdb\u884c\u5145\u5206\u9a8c\u8bc1\u3002", "result": "\u4f7f\u7528Enceladus Orbilander\u7684Life Detection Suite\u4f5c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u4e0e\u57fa\u7ebf\u6982\u5ff5\u64cd\u4f5c\u76f8\u6bd4\uff0c\u6837\u672c\u8bc6\u522b\u9519\u8bef\u7387\u964d\u4f4e\u4e86\u8fd140%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8d1d\u53f6\u65af\u7f51\u7edc\u7ed3\u6784\u548c\u5956\u52b1\u5851\u9020\u63d0\u9ad8\u4e86\u7cfb\u7edf\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6837\u672c\u8bc6\u522b\u9519\u8bef\uff0c\u9002\u7528\u4e8e\u901a\u4fe1\u53d7\u9650\u7684\u6df1\u7a7a\u79d1\u5b66\u4efb\u52a1\u3002"}}
{"id": "2510.08851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08851", "abs": "https://arxiv.org/abs/2510.08851", "authors": ["Le Mao", "Andrew H. Liu", "Renos Zabounidis", "Zachary Kingston", "Joseph Campbell"], "title": "CDE: Concept-Driven Exploration for Reinforcement Learning", "comment": "Preprint", "summary": "Intelligent exploration remains a critical challenge in reinforcement\nlearning (RL), especially in visual control tasks. Unlike low-dimensional\nstate-based RL, visual RL must extract task-relevant structure from raw pixels,\nmaking exploration inefficient. We propose Concept-Driven Exploration (CDE),\nwhich leverages a pre-trained vision-language model (VLM) to generate\nobject-centric visual concepts from textual task descriptions as weak,\npotentially noisy supervisory signals. Rather than directly conditioning on\nthese noisy signals, CDE trains a policy to reconstruct the concepts via an\nauxiliary objective, using reconstruction accuracy as an intrinsic reward to\nguide exploration toward task-relevant objects. Because the policy internalizes\nthese concepts, VLM queries are only needed during training, reducing\ndependence on external models during deployment. Across five challenging\nsimulated visual manipulation tasks, CDE achieves efficient, targeted\nexploration and remains robust to noisy VLM predictions. Finally, we\ndemonstrate real-world transfer by deploying CDE on a Franka Research 3 arm,\nattaining an 80\\% success rate in a real-world manipulation task.", "AI": {"tldr": "\u63d0\u51faCDE\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u6587\u672c\u4efb\u52a1\u63cf\u8ff0\u751f\u6210\u7269\u4f53\u4e2d\u5fc3\u89c6\u89c9\u6982\u5ff5\uff0c\u901a\u8fc7\u91cd\u5efa\u8fd9\u4e9b\u6982\u5ff5\u4f5c\u4e3a\u5185\u5728\u5956\u52b1\u6765\u6307\u5bfc\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u3002", "motivation": "\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u4ece\u539f\u59cb\u50cf\u7d20\u4e2d\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u7ed3\u6784\uff0c\u5bfc\u81f4\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\uff0c\u667a\u80fd\u63a2\u7d22\u4ecd\u662f\u5173\u952e\u6311\u6218\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3VLM\u4ece\u6587\u672c\u4efb\u52a1\u63cf\u8ff0\u751f\u6210\u7269\u4f53\u4e2d\u5fc3\u89c6\u89c9\u6982\u5ff5\uff0c\u8bad\u7ec3\u7b56\u7565\u901a\u8fc7\u8f85\u52a9\u76ee\u6807\u91cd\u5efa\u8fd9\u4e9b\u6982\u5ff5\uff0c\u5c06\u91cd\u5efa\u7cbe\u5ea6\u4f5c\u4e3a\u5185\u5728\u5956\u52b1\u6765\u6307\u5bfc\u63a2\u7d22\u3002", "result": "\u5728\u4e94\u4e2a\u6a21\u62df\u89c6\u89c9\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u5b9a\u5411\u63a2\u7d22\uff0c\u5bf9\u566a\u58f0VLM\u9884\u6d4b\u4fdd\u6301\u9c81\u68d2\u6027\uff1b\u5728\u771f\u5b9e\u4e16\u754cFranka\u673a\u68b0\u81c2\u4e0a\u8fbe\u523080%\u6210\u529f\u7387\u3002", "conclusion": "CDE\u65b9\u6cd5\u901a\u8fc7\u6982\u5ff5\u91cd\u5efa\u5185\u5728\u5956\u52b1\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u7684\u63a2\u7d22\u95ee\u9898\uff0c\u51cf\u5c11\u4e86\u5bf9\u5916\u90e8\u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002"}}
{"id": "2510.08880", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08880", "abs": "https://arxiv.org/abs/2510.08880", "authors": ["Baoshan Song", "Xiao Xia", "Penggao Yan", "Yihan Zhong", "Weisong Wen", "Li-Ta Hsu"], "title": "Online IMU-odometer Calibration using GNSS Measurements for Autonomous Ground Vehicle Localization", "comment": "Submitted to IEEE Transactions on Intelligent Transportation Systems", "summary": "Accurate calibration of intrinsic (odometer scaling factors) and extrinsic\nparameters (IMU-odometer translation and rotation) is essential for autonomous\nground vehicle localization. Existing GNSS-aided approaches often rely on\npositioning results or raw measurements without ambiguity resolution, and their\nobservability properties remain underexplored. This paper proposes a tightly\ncoupled online calibration method that fuses IMU, odometer, and raw GNSS\nmeasurements (pseudo-range, carrier-phase, and Doppler) within an extendable\nfactor graph optimization (FGO) framework, incorporating outlier mitigation and\nambiguity resolution. Observability analysis reveals that two horizontal\ntranslation and three rotation parameters are observable under general motion,\nwhile vertical translation remains unobservable. Simulation and real-world\nexperiments demonstrate superior calibration and localization performance over\nstate-of-the-art loosely coupled methods. Specifically, the IMU-odometer\npositioning using our calibrated parameters achieves the absolute maximum error\nof 17.75 m while the one of LC method is 61.51 m, achieving up to 71.14 percent\nimprovement. To foster further research, we also release the first open-source\ndataset that combines IMU, 2D odometer, and raw GNSS measurements from both\nrover and base stations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7d27\u8026\u5408\u7684\u5728\u7ebf\u6821\u51c6\u65b9\u6cd5\uff0c\u878d\u5408IMU\u3001\u91cc\u7a0b\u8ba1\u548c\u539f\u59cbGNSS\u6d4b\u91cf\u6570\u636e\uff0c\u7528\u4e8e\u81ea\u4e3b\u5730\u9762\u8f66\u8f86\u7684\u6807\u5b9a\u548c\u5b9a\u4f4d\u3002", "motivation": "\u73b0\u6709GNSS\u8f85\u52a9\u65b9\u6cd5\u4f9d\u8d56\u5b9a\u4f4d\u7ed3\u679c\u6216\u672a\u89e3\u51b3\u6a21\u7cca\u5ea6\u7684\u539f\u59cb\u6d4b\u91cf\uff0c\u4e14\u53ef\u89c2\u6d4b\u6027\u5206\u6790\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u7cbe\u786e\u7684\u6807\u5b9a\u65b9\u6cd5\u3002", "method": "\u5728\u53ef\u6269\u5c55\u56e0\u5b50\u56fe\u4f18\u5316\u6846\u67b6\u4e2d\u878d\u5408IMU\u3001\u91cc\u7a0b\u8ba1\u548c\u539f\u59cbGNSS\u6d4b\u91cf\uff08\u4f2a\u8ddd\u3001\u8f7d\u6ce2\u76f8\u4f4d\u548c\u591a\u666e\u52d2\uff09\uff0c\u5305\u542b\u5f02\u5e38\u503c\u6291\u5236\u548c\u6a21\u7cca\u5ea6\u89e3\u51b3\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u677e\u8026\u5408\u65b9\u6cd5\uff0c\u6807\u5b9a\u548c\u5b9a\u4f4d\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0cIMU-\u91cc\u7a0b\u8ba1\u5b9a\u4f4d\u7edd\u5bf9\u6700\u5927\u8bef\u5dee\u4ece61.51\u7c73\u964d\u81f317.75\u7c73\uff0c\u63d0\u534771.14%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u4e3b\u5730\u9762\u8f66\u8f86\u5b9a\u4f4d\u4e2d\u7684\u6807\u5b9a\u95ee\u9898\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u7ed3\u5408IMU\u30012D\u91cc\u7a0b\u8ba1\u548c\u539f\u59cbGNSS\u6d4b\u91cf\u7684\u5f00\u6e90\u6570\u636e\u96c6\u3002"}}
{"id": "2510.08884", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08884", "abs": "https://arxiv.org/abs/2510.08884", "authors": ["Alexandre Lopes", "Catarina Barata", "Plinio Moreno"], "title": "Model-Based Lookahead Reinforcement Learning for in-hand manipulation", "comment": null, "summary": "In-Hand Manipulation, as many other dexterous tasks, remains a difficult\nchallenge in robotics by combining complex dynamic systems with the capability\nto control and manoeuvre various objects using its actuators. This work\npresents the application of a previously developed hybrid Reinforcement\nLearning (RL) Framework to In-Hand Manipulation task, verifying that it is\ncapable of improving the performance of the task. The model combines concepts\nof both Model-Free and Model-Based Reinforcement Learning, by guiding a trained\npolicy with the help of a dynamic model and value-function through trajectory\nevaluation, as done in Model Predictive Control. This work evaluates the\nperformance of the model by comparing it with the policy that will be guided.\nTo fully explore this, various tests are performed using both fully-actuated\nand under-actuated simulated robotic hands to manipulate different objects for\na given task. The performance of the model will also be tested for\ngeneralization tests, by changing the properties of the objects in which both\nthe policy and dynamic model were trained, such as density and size, and\nadditionally by guiding a trained policy in a certain object to perform the\nsame task in a different one. The results of this work show that, given a\npolicy with high average reward and an accurate dynamic model, the hybrid\nframework improves the performance of in-hand manipulation tasks for most test\ncases, even when the object properties are changed. However, this improvement\ncomes at the expense of increasing the computational cost, due to the\ncomplexity of trajectory evaluation.", "AI": {"tldr": "\u5c06\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5e94\u7528\u4e8e\u624b\u5185\u64cd\u4f5c\u4efb\u52a1\uff0c\u7ed3\u5408\u6a21\u578b\u65e0\u5173\u548c\u6a21\u578b\u76f8\u5173\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f68\u8ff9\u8bc4\u4f30\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u4f1a\u589e\u52a0\u8ba1\u7b97\u6210\u672c", "motivation": "\u624b\u5185\u64cd\u4f5c\u662f\u673a\u5668\u4eba\u9886\u57df\u7684\u6311\u6218\u6027\u4efb\u52a1\uff0c\u9700\u8981\u63a7\u5236\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u5e76\u64cd\u7eb5\u5404\u79cd\u7269\u4f53\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3", "method": "\u4f7f\u7528\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u6a21\u578b\u65e0\u5173\u548c\u6a21\u578b\u76f8\u5173RL\uff0c\u901a\u8fc7\u52a8\u6001\u6a21\u578b\u548c\u4ef7\u503c\u51fd\u6570\u8fdb\u884c\u8f68\u8ff9\u8bc4\u4f30\uff08\u7c7b\u4f3c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff09\uff0c\u5728\u5b8c\u5168\u9a71\u52a8\u548c\u6b20\u9a71\u52a8\u4eff\u771f\u673a\u68b0\u624b\u4e0a\u6d4b\u8bd5", "result": "\u5728\u5927\u591a\u6570\u6d4b\u8bd5\u6848\u4f8b\u4e2d\uff0c\u6df7\u5408\u6846\u67b6\u63d0\u5347\u4e86\u624b\u5185\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5373\u4f7f\u7269\u4f53\u5c5e\u6027\u53d1\u751f\u53d8\u5316\u4e5f\u80fd\u4fdd\u6301\u6539\u8fdb\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u589e\u52a0", "conclusion": "\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u624b\u5185\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u9700\u8981\u6743\u8861\u6027\u80fd\u63d0\u5347\u4e0e\u8ba1\u7b97\u6210\u672c"}}
{"id": "2510.08953", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.08953", "abs": "https://arxiv.org/abs/2510.08953", "authors": ["Cheng Ouyang", "Moeen Ul Islam", "Dong Chen", "Kaixiang Zhang", "Zhaojian Li", "Xiaobo Tan"], "title": "Direct Data-Driven Predictive Control for a Three-dimensional Cable-Driven Soft Robotic Arm", "comment": null, "summary": "Soft robots offer significant advantages in safety and adaptability, yet\nachieving precise and dynamic control remains a major challenge due to their\ninherently complex and nonlinear dynamics. Recently, Data-enabled Predictive\nControl (DeePC) has emerged as a promising model-free approach that bypasses\nexplicit system identification by directly leveraging input-output data. While\nDeePC has shown success in other domains, its application to soft robots\nremains underexplored, particularly for three-dimensional (3D) soft robotic\nsystems. This paper addresses this gap by developing and experimentally\nvalidating an effective DeePC framework on a 3D, cable-driven soft arm.\nSpecifically, we design and fabricate a soft robotic arm with a thick tubing\nbackbone for stability, a dense silicone body with large cavities for strength\nand flexibility, and rigid endcaps for secure termination. Using this platform,\nwe implement DeePC with singular value decomposition (SVD)-based dimension\nreduction for two key control tasks: fixed-point regulation and trajectory\ntracking in 3D space. Comparative experiments with a baseline model-based\ncontroller demonstrate DeePC's superior accuracy, robustness, and adaptability,\nhighlighting its potential as a practical solution for dynamic control of soft\nrobots.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u9884\u6d4b\u63a7\u5236(DeePC)\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u63a7\u5236\u6846\u67b6\uff0c\u5e76\u57283D\u7f06\u9a71\u8f6f\u81c2\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u4f20\u7edf\u6a21\u578b\u63a7\u5236\u5668\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u5177\u6709\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u4f18\u52bf\uff0c\u4f46\u7531\u4e8e\u5176\u590d\u6742\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7279\u6027\uff0c\u5b9e\u73b0\u7cbe\u786e\u52a8\u6001\u63a7\u5236\u4ecd\u9762\u4e34\u6311\u6218\u3002DeePC\u4f5c\u4e3a\u4e00\u79cd\u65e0\u9700\u663e\u5f0f\u7cfb\u7edf\u8fa8\u8bc6\u7684\u6a21\u578b\u65e0\u5173\u65b9\u6cd5\uff0c\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8bbe\u8ba1\u5236\u9020\u4e86\u5177\u6709\u539a\u7ba1\u72b6\u9aa8\u67b6\u3001\u5bc6\u96c6\u7845\u80f6\u4f53\u548c\u521a\u6027\u7aef\u76d6\u76843D\u7f06\u9a71\u8f6f\u81c2\uff0c\u91c7\u7528\u57fa\u4e8e\u5947\u5f02\u503c\u5206\u89e3\u964d\u7ef4\u7684DeePC\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u56fa\u5b9a\u70b9\u8c03\u8282\u548c3D\u7a7a\u95f4\u8f68\u8ff9\u8ddf\u8e2a\u4e24\u79cd\u63a7\u5236\u4efb\u52a1\u3002", "result": "\u4e0e\u57fa\u7ebf\u6a21\u578b\u63a7\u5236\u5668\u76f8\u6bd4\uff0cDeePC\u5728\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u52a8\u6001\u63a7\u5236\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "DeePC\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u52a8\u6001\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u73b0\u4e86\u5728\u590d\u67423D\u8f6f\u4f53\u7cfb\u7edf\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2510.08973", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.08973", "abs": "https://arxiv.org/abs/2510.08973", "authors": ["Bibekananda Patra", "Aditya Mahesh Kolte", "Sandipan Bandyopadhyay"], "title": "A geometrical approach to solve the proximity of a point to an axisymmetric quadric in space", "comment": null, "summary": "This paper presents the classification of a general quadric into an\naxisymmetric quadric (AQ) and the solution to the problem of the proximity of a\ngiven point to an AQ. The problem of proximity in $R^3$ is reduced to the same\nin $R^2$, which is not found in the literature. A new method to solve the\nproblem in $R^2$ is used based on the geometrical properties of the conics,\nsuch as sub-normal, length of the semi-major axis, eccentricity, slope and\nradius. Furthermore, the problem in $R^2$ is categorised into two and three\nmore sub-cases for parabola and ellipse/hyperbola, respectively, depending on\nthe location of the point, which is a novel approach as per the authors'\nknowledge. The proposed method is suitable for implementation in a common\nprogramming language, such as C and proved to be faster than a commercial\nlibrary, namely, Bullet.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4e00\u822c\u4e8c\u6b21\u66f2\u9762\u5206\u7c7b\u4e3a\u8f74\u5bf9\u79f0\u4e8c\u6b21\u66f2\u9762(AQ)\u7684\u65b9\u6cd5\uff0c\u5e76\u89e3\u51b3\u4e86\u70b9\u5230AQ\u7684\u90bb\u8fd1\u5ea6\u95ee\u9898\u3002\u901a\u8fc7\u5c06R^3\u7a7a\u95f4\u7684\u95ee\u9898\u7b80\u5316\u4e3aR^2\u7a7a\u95f4\uff0c\u5229\u7528\u5706\u9525\u66f2\u7ebf\u7684\u51e0\u4f55\u7279\u6027\u5f00\u53d1\u4e86\u65b0\u7b97\u6cd5\uff0c\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5546\u4e1a\u5e93Bullet\u3002", "motivation": "\u89e3\u51b3\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u70b9\u5230\u8f74\u5bf9\u79f0\u4e8c\u6b21\u66f2\u9762\u7684\u90bb\u8fd1\u5ea6\u8ba1\u7b97\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4e0d\u9ad8\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "method": "\u5c06R^3\u7a7a\u95f4\u7684\u90bb\u8fd1\u5ea6\u95ee\u9898\u7b80\u5316\u4e3aR^2\u7a7a\u95f4\uff0c\u57fa\u4e8e\u5706\u9525\u66f2\u7ebf\u7684\u51e0\u4f55\u7279\u6027(\u5982\u6b21\u6cd5\u7ebf\u3001\u534a\u957f\u8f74\u957f\u5ea6\u3001\u504f\u5fc3\u7387\u3001\u659c\u7387\u548c\u534a\u5f84)\u5f00\u53d1\u65b0\u65b9\u6cd5\uff0c\u5e76\u6839\u636e\u70b9\u7684\u4f4d\u7f6e\u5c06\u95ee\u9898\u5206\u4e3a\u629b\u7269\u7ebf\u548c\u692d\u5706/\u53cc\u66f2\u7ebf\u7684\u5b50\u60c5\u51b5\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u9002\u5408\u5728C\u7b49\u7f16\u7a0b\u8bed\u8a00\u4e2d\u5b9e\u73b0\uff0c\u7ecf\u6d4b\u8bd5\u6bd4\u5546\u4e1a\u5e93Bullet\u66f4\u5feb\u3002", "conclusion": "\u901a\u8fc7\u51e0\u4f55\u7279\u6027\u5206\u6790\u548c\u95ee\u9898\u5206\u7c7b\uff0c\u6210\u529f\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u70b9\u5230\u8f74\u5bf9\u79f0\u4e8c\u6b21\u66f2\u9762\u90bb\u8fd1\u5ea6\u8ba1\u7b97\u65b9\u6cd5\uff0c\u5728\u4e09\u7ef4\u5230\u4e8c\u7ef4\u7684\u7b80\u5316\u8fc7\u7a0b\u4e2d\u5b9e\u73b0\u4e86\u6027\u80fd\u4f18\u5316\u3002"}}
{"id": "2510.09013", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2510.09013", "abs": "https://arxiv.org/abs/2510.09013", "authors": ["Daniel A. Williams", "Airlie Chapman", "Daniel R. Little", "Chris Manzie"], "title": "Trust Modeling and Estimation in Human-Autonomy Interactions", "comment": "10 pages. 13 figures", "summary": "Advances in the control of autonomous systems have accompanied an expansion\nin the potential applications for autonomous robotic systems. The success of\napplications involving humans depends on the quality of interaction between the\nautonomous system and the human supervisor, which is particularly affected by\nthe degree of trust that the supervisor places in the autonomous system. Absent\nfrom the literature are models of supervisor trust dynamics that can\naccommodate asymmetric responses to autonomous system performance and the\nintermittent nature of supervisor-autonomous system communication. This paper\nfocuses on formulating an estimated model of supervisor trust that incorporates\nboth of these features by employing a switched linear system structure with\nevent-triggered sampling of the model input and output. Trust response data\ncollected in a user study with 51 participants were then used identify\nparameters for a switched linear model-based observer of supervisor trust.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5207\u6362\u7ebf\u6027\u7cfb\u7edf\u7684\u76d1\u7763\u8005\u4fe1\u4efb\u52a8\u6001\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u8003\u8651\u4e86\u81ea\u4e3b\u7cfb\u7edf\u6027\u80fd\u7684\u4e0d\u5bf9\u79f0\u54cd\u5e94\u548c\u95f4\u6b47\u6027\u901a\u4fe1\u7279\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u732e\u7f3a\u4e4f\u80fd\u591f\u5904\u7406\u81ea\u4e3b\u7cfb\u7edf\u6027\u80fd\u4e0d\u5bf9\u79f0\u54cd\u5e94\u548c\u76d1\u7763\u8005-\u81ea\u4e3b\u7cfb\u7edf\u95f4\u6b47\u6027\u901a\u4fe1\u7684\u4fe1\u4efb\u52a8\u6001\u6a21\u578b\uff0c\u8fd9\u5f71\u54cd\u4e86\u4eba\u673a\u4ea4\u4e92\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u5207\u6362\u7ebf\u6027\u7cfb\u7edf\u7ed3\u6784\uff0c\u7ed3\u5408\u4e8b\u4ef6\u89e6\u53d1\u7684\u6a21\u578b\u8f93\u5165\u8f93\u51fa\u91c7\u6837\uff0c\u57fa\u4e8e51\u540d\u53c2\u4e0e\u8005\u7684\u7528\u6237\u7814\u7a76\u6570\u636e\u8bc6\u522b\u6a21\u578b\u53c2\u6570\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5207\u6362\u7ebf\u6027\u6a21\u578b\u7684\u76d1\u7763\u8005\u4fe1\u4efb\u89c2\u6d4b\u5668\uff0c\u80fd\u591f\u51c6\u786e\u6355\u6349\u4fe1\u4efb\u52a8\u6001\u53d8\u5316\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u6709\u6548\u8868\u5f81\u76d1\u7763\u8005\u5bf9\u81ea\u4e3b\u7cfb\u7edf\u7684\u4fe1\u4efb\u52a8\u6001\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2510.09036", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09036", "abs": "https://arxiv.org/abs/2510.09036", "authors": ["Chuanrui Zhang", "Zhengxian Wu", "Guanxing Lu", "Yansong Tang", "Ziwei Wang"], "title": "iMoWM: Taming Interactive Multi-Modal World Model for Robotic Manipulation", "comment": null, "summary": "Learned world models hold significant potential for robotic manipulation, as\nthey can serve as simulator for real-world interactions. While extensive\nprogress has been made in 2D video-based world models, these approaches often\nlack geometric and spatial reasoning, which is essential for capturing the\nphysical structure of the 3D world. To address this limitation, we introduce\niMoWM, a novel interactive world model designed to generate color images, depth\nmaps, and robot arm masks in an autoregressive manner conditioned on actions.\nTo overcome the high computational cost associated with three-dimensional\ninformation, we propose MMTokenizer, which unifies multi-modal inputs into a\ncompact token representation. This design enables iMoWM to leverage large-scale\npretrained VideoGPT models while maintaining high efficiency and incorporating\nricher physical information. With its multi-modal representation, iMoWM not\nonly improves the visual quality of future predictions but also serves as an\neffective simulator for model-based reinforcement learning (MBRL) and\nfacilitates real-world imitation learning. Extensive experiments demonstrate\nthe superiority of iMoWM across these tasks, showcasing the advantages of\nmulti-modal world modeling for robotic manipulation. Homepage:\nhttps://xingyoujun.github.io/imowm/", "AI": {"tldr": "iMoWM\u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001token\u5316\u5668\u7edf\u4e00\u5904\u7406\u5f69\u8272\u56fe\u50cf\u3001\u6df1\u5ea6\u56fe\u548c\u673a\u5668\u4eba\u81c2\u63a9\u7801\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u6574\u5408\u4e30\u5bcc\u7684\u7269\u7406\u4fe1\u606f\uff0c\u63d0\u5347\u672a\u6765\u9884\u6d4b\u8d28\u91cf\u5e76\u652f\u6301\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u76842D\u89c6\u9891\u4e16\u754c\u6a21\u578b\u7f3a\u4e4f\u51e0\u4f55\u548c\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u63493D\u4e16\u754c\u7684\u7269\u7406\u7ed3\u6784\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faiMoWM\u4ea4\u4e92\u5f0f\u4e16\u754c\u6a21\u578b\u548cMMTokenizer\u591a\u6a21\u6001token\u5316\u5668\uff0c\u5c06\u591a\u6a21\u6001\u8f93\u5165\u7edf\u4e00\u4e3a\u7d27\u51d1\u7684token\u8868\u793a\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684VideoGPT\u6a21\u578b\u8fdb\u884c\u81ea\u56de\u5f52\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660eiMoWM\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u771f\u5b9e\u4e16\u754c\u6a21\u4eff\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "\u591a\u6a21\u6001\u4e16\u754c\u5efa\u6a21\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u5e26\u6765\u4e86\u663e\u8457\u4f18\u52bf\uff0ciMoWM\u901a\u8fc7\u6574\u54083D\u51e0\u4f55\u4fe1\u606f\u6709\u6548\u63d0\u5347\u4e86\u4e16\u754c\u6a21\u578b\u7684\u6027\u80fd\u548c\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2510.09080", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2510.09080", "abs": "https://arxiv.org/abs/2510.09080", "authors": ["Shannon Liu", "Maria Teresa Parreira", "Wendy Ju"], "title": "Training Models to Detect Successive Robot Errors from Human Reactions", "comment": "Accepted to NERC '25", "summary": "As robots become more integrated into society, detecting robot errors is\nessential for effective human-robot interaction (HRI). When a robot fails\nrepeatedly, how can it know when to change its behavior? Humans naturally\nrespond to robot errors through verbal and nonverbal cues that intensify over\nsuccessive failures-from confusion and subtle speech changes to visible\nfrustration and impatience. While prior work shows that human reactions can\nindicate robot failures, few studies examine how these evolving responses\nreveal successive failures. This research uses machine learning to recognize\nstages of robot failure from human reactions. In a study with 26 participants\ninteracting with a robot that made repeated conversational errors, behavioral\nfeatures were extracted from video data to train models for individual users.\nThe best model achieved 93.5% accuracy for detecting errors and 84.1% for\nclassifying successive failures. Modeling the progression of human reactions\nenhances error detection and understanding of repeated interaction breakdowns\nin HRI.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u4ece\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u8fde\u7eed\u9519\u8bef\u7684\u53cd\u5e94\u4e2d\u8bc6\u522b\u9519\u8bef\u9636\u6bb5\uff0c\u6700\u4f73\u6a21\u578b\u5728\u68c0\u6d4b\u9519\u8bef\u548c\u5206\u7c7b\u8fde\u7eed\u5931\u8d25\u65b9\u9762\u5206\u522b\u8fbe\u523093.5%\u548c84.1%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u5728\u793e\u4f1a\u4e2d\u7684\u96c6\u6210\u5ea6\u63d0\u9ad8\uff0c\u68c0\u6d4b\u673a\u5668\u4eba\u9519\u8bef\u5bf9\u6709\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u9519\u8bef\u7684\u53cd\u5e94\u4f1a\u968f\u7740\u8fde\u7eed\u5931\u8d25\u800c\u52a0\u5267\uff0c\u4f46\u5f88\u5c11\u6709\u7814\u7a76\u5173\u6ce8\u8fd9\u4e9b\u6f14\u53d8\u53cd\u5e94\u5982\u4f55\u63ed\u793a\u8fde\u7eed\u5931\u8d25\u3002", "method": "\u572826\u540d\u53c2\u4e0e\u8005\u4e0e\u72af\u91cd\u590d\u5bf9\u8bdd\u9519\u8bef\u7684\u673a\u5668\u4eba\u4e92\u52a8\u7684\u7814\u7a76\u4e2d\uff0c\u4ece\u89c6\u9891\u6570\u636e\u4e2d\u63d0\u53d6\u884c\u4e3a\u7279\u5f81\uff0c\u4e3a\u4e2a\u4f53\u7528\u6237\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728\u68c0\u6d4b\u9519\u8bef\u65b9\u9762\u8fbe\u523093.5%\u7684\u51c6\u786e\u7387\uff0c\u5728\u5206\u7c7b\u8fde\u7eed\u5931\u8d25\u65b9\u9762\u8fbe\u523084.1%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u5bf9\u4eba\u7c7b\u53cd\u5e94\u8fdb\u5c55\u8fdb\u884c\u5efa\u6a21\u53ef\u4ee5\u589e\u5f3a\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\uff0c\u5e76\u66f4\u597d\u5730\u7406\u89e3\u4eba\u673a\u4ea4\u4e92\u4e2d\u91cd\u590d\u7684\u4ea4\u4e92\u4e2d\u65ad\u3002"}}
{"id": "2510.09089", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09089", "abs": "https://arxiv.org/abs/2510.09089", "authors": ["Jikai Wang", "Yunqi Cheng", "Kezhi Wang", "Zonghai Chen"], "title": "Robust Visual Teach-and-Repeat Navigation with Flexible Topo-metric Graph Map Representation", "comment": null, "summary": "Visual Teach-and-Repeat Navigation is a direct solution for mobile robot to\nbe deployed in unknown environments. However, robust trajectory repeat\nnavigation still remains challenged due to environmental changing and dynamic\nobjects. In this paper, we propose a novel visual teach-and-repeat navigation\nsystem, which consists of a flexible map representation, robust map matching\nand a map-less local navigation module. During the teaching process, the\nrecorded keyframes are formulated as a topo-metric graph and each node can be\nfurther extended to save new observations. Such representation also alleviates\nthe requirement of globally consistent mapping. To enhance the place\nrecognition performance during repeating process, instead of using\nframe-to-frame matching, we firstly implement keyframe clustering to aggregate\nsimilar connected keyframes into local map and perform place recognition based\non visual frame-tolocal map matching strategy. To promote the local goal\npersistent tracking performance, a long-term goal management algorithm is\nconstructed, which can avoid the robot getting lost due to environmental\nchanges or obstacle occlusion. To achieve the goal without map, a local\ntrajectory-control candidate optimization algorithm is proposed. Extensively\nexperiments are conducted on our mobile platform. The results demonstrate that\nour system is superior to the baselines in terms of robustness and\neffectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9\u6559\u5bfc-\u91cd\u590d\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u7075\u6d3b\u7684\u5730\u56fe\u8868\u793a\u3001\u9c81\u68d2\u7684\u5730\u56fe\u5339\u914d\u548c\u65e0\u5730\u56fe\u5c40\u90e8\u5bfc\u822a\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u73af\u5883\u53d8\u5316\u548c\u52a8\u6001\u7269\u4f53\u5e26\u6765\u7684\u5bfc\u822a\u6311\u6218\u3002", "motivation": "\u89c6\u89c9\u6559\u5bfc-\u91cd\u590d\u5bfc\u822a\u662f\u79fb\u52a8\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u76f4\u63a5\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u7531\u4e8e\u73af\u5883\u53d8\u5316\u548c\u52a8\u6001\u7269\u4f53\u7684\u5b58\u5728\uff0c\u9c81\u68d2\u7684\u8f68\u8ff9\u91cd\u590d\u5bfc\u822a\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6a21\u5757\uff1a1\uff09\u5c06\u5173\u952e\u5e27\u6784\u5efa\u4e3a\u62d3\u6251\u5ea6\u91cf\u56fe\u7684\u7075\u6d3b\u5730\u56fe\u8868\u793a\uff1b2\uff09\u901a\u8fc7\u5173\u952e\u5e27\u805a\u7c7b\u5b9e\u73b0\u89c6\u89c9\u5e27\u5230\u5c40\u90e8\u5730\u56fe\u5339\u914d\u7684\u9c81\u68d2\u5730\u56fe\u5339\u914d\uff1b3\uff09\u5305\u542b\u957f\u671f\u76ee\u6807\u7ba1\u7406\u548c\u5c40\u90e8\u8f68\u8ff9\u63a7\u5236\u4f18\u5316\u7684\u65e0\u5730\u56fe\u5c40\u90e8\u5bfc\u822a\u6a21\u5757\u3002", "result": "\u5728\u79fb\u52a8\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u7cfb\u7edf\u5728\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u89c6\u89c9\u6559\u5bfc-\u91cd\u590d\u5bfc\u822a\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u73af\u5883\u53d8\u5316\u548c\u52a8\u6001\u969c\u788d\u7269\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u8f68\u8ff9\u91cd\u590d\u5bfc\u822a\u3002"}}
{"id": "2510.09096", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09096", "abs": "https://arxiv.org/abs/2510.09096", "authors": ["Xinhu Li", "Ayush Jain", "Zhaojing Yang", "Yigit Korkmaz", "Erdem B\u0131y\u0131k"], "title": "When a Robot is More Capable than a Human: Learning from Constrained Demonstrators", "comment": null, "summary": "Learning from demonstrations enables experts to teach robots complex tasks\nusing interfaces such as kinesthetic teaching, joystick control, and\nsim-to-real transfer. However, these interfaces often constrain the expert's\nability to demonstrate optimal behavior due to indirect control, setup\nrestrictions, and hardware safety. For example, a joystick can move a robotic\narm only in a 2D plane, even though the robot operates in a higher-dimensional\nspace. As a result, the demonstrations collected by constrained experts lead to\nsuboptimal performance of the learned policies. This raises a key question: Can\na robot learn a better policy than the one demonstrated by a constrained\nexpert? We address this by allowing the agent to go beyond direct imitation of\nexpert actions and explore shorter and more efficient trajectories. We use the\ndemonstrations to infer a state-only reward signal that measures task progress,\nand self-label reward for unknown states using temporal interpolation. Our\napproach outperforms common imitation learning in both sample efficiency and\ntask completion time. On a real WidowX robotic arm, it completes the task in 12\nseconds, 10x faster than behavioral cloning, as shown in real-robot videos on\nhttps://sites.google.com/view/constrainedexpert .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u53d7\u9650\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u66f4\u4f18\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a8\u65ad\u72b6\u6001\u5956\u52b1\u4fe1\u53f7\u548c\u81ea\u6807\u8bb0\u672a\u77e5\u72b6\u6001\u5956\u52b1\uff0c\u5b9e\u73b0\u6bd4\u884c\u4e3a\u514b\u9686\u66f4\u9ad8\u6548\u7684\u4efb\u52a1\u5b8c\u6210\u3002", "motivation": "\u73b0\u6709\u6f14\u793a\u63a5\u53e3\uff08\u5982\u9065\u64cd\u4f5c\u3001\u6a21\u62df\u5230\u771f\u5b9e\u8fc1\u79fb\uff09\u9650\u5236\u4e86\u4e13\u5bb6\u5c55\u793a\u6700\u4f18\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u5bfc\u81f4\u5b66\u4e60\u5230\u7684\u7b56\u7565\u6027\u80fd\u4e0d\u4f73\uff0c\u9700\u8981\u8ba9\u673a\u5668\u4eba\u80fd\u591f\u5b66\u4e60\u6bd4\u53d7\u9650\u4e13\u5bb6\u6f14\u793a\u66f4\u597d\u7684\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u6f14\u793a\u63a8\u65ad\u4ec5\u72b6\u6001\u5956\u52b1\u4fe1\u53f7\u6765\u8861\u91cf\u4efb\u52a1\u8fdb\u5c55\uff0c\u5e76\u901a\u8fc7\u65f6\u95f4\u63d2\u503c\u4e3a\u672a\u77e5\u72b6\u6001\u81ea\u6807\u8bb0\u5956\u52b1\uff0c\u8ba9\u667a\u80fd\u4f53\u63a2\u7d22\u6bd4\u4e13\u5bb6\u6f14\u793a\u66f4\u77ed\u66f4\u9ad8\u6548\u7684\u8f68\u8ff9\u3002", "result": "\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u548c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u4e0a\u5747\u4f18\u4e8e\u5e38\u89c1\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u771f\u5b9eWidowX\u673a\u68b0\u81c2\u4e0a\u5b8c\u6210\u4efb\u52a1\u4ec5\u970012\u79d2\uff0c\u6bd4\u884c\u4e3a\u514b\u9686\u5feb10\u500d\u3002", "conclusion": "\u901a\u8fc7\u8d85\u8d8a\u76f4\u63a5\u6a21\u4eff\u4e13\u5bb6\u52a8\u4f5c\uff0c\u63a2\u7d22\u66f4\u9ad8\u6548\u8f68\u8ff9\uff0c\u53ef\u4ee5\u4ece\u53d7\u9650\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u5230\u6bd4\u6f14\u793a\u672c\u8eab\u66f4\u4f18\u7684\u7b56\u7565\u3002"}}
{"id": "2510.09188", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.09188", "abs": "https://arxiv.org/abs/2510.09188", "authors": ["Zihao Mao", "Yunheng Wang", "Yunting Ji", "Yi Yang", "Wenjie Song"], "title": "Decentralized Multi-Robot Relative Navigation in Unknown, Structurally Constrained Environments under Limited Communication", "comment": null, "summary": "Multi-robot navigation in unknown, structurally constrained, and GPS-denied\nenvironments presents a fundamental trade-off between global strategic\nforesight and local tactical agility, particularly under limited communication.\nCentralized methods achieve global optimality but suffer from high\ncommunication overhead, while distributed methods are efficient but lack the\nbroader awareness to avoid deadlocks and topological traps. To address this, we\npropose a fully decentralized, hierarchical relative navigation framework that\nachieves both strategic foresight and tactical agility without a unified\ncoordinate system. At the strategic layer, robots build and exchange\nlightweight topological maps upon opportunistic encounters. This process\nfosters an emergent global awareness, enabling the planning of efficient,\ntrap-avoiding routes at an abstract level. This high-level plan then inspires\nthe tactical layer, which operates on local metric information. Here, a\nsampling-based escape point strategy resolves dense spatio-temporal conflicts\nby generating dynamically feasible trajectories in real time, concurrently\nsatisfying tight environmental and kinodynamic constraints. Extensive\nsimulations and real-world experiments demonstrate that our system\nsignificantly outperforms in success rate and efficiency, especially in\ncommunication-limited environments with complex topological structures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u5206\u5c42\u76f8\u5bf9\u5bfc\u822a\u6846\u67b6\uff0c\u5728GPS\u7f3a\u5931\u73af\u5883\u4e2d\u5b9e\u73b0\u6218\u7565\u8fdc\u89c1\u548c\u6218\u672f\u654f\u6377\u6027\uff0c\u65e0\u9700\u7edf\u4e00\u5750\u6807\u7cfb", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u901a\u4fe1\u5f00\u9500\u5927\u3001\u5206\u5e03\u5f0f\u65b9\u6cd5\u7f3a\u4e4f\u5168\u5c40\u610f\u8bc6\u5bfc\u81f4\u6b7b\u9501\u548c\u62d3\u6251\u9677\u9631\u7684\u95ee\u9898", "method": "\u5206\u5c42\u6846\u67b6\uff1a\u6218\u7565\u5c42\u901a\u8fc7\u8f7b\u91cf\u62d3\u6251\u5730\u56fe\u4ea4\u6362\u5b9e\u73b0\u5168\u5c40\u610f\u8bc6\uff0c\u6218\u672f\u5c42\u4f7f\u7528\u57fa\u4e8e\u91c7\u6837\u7684\u9003\u9038\u70b9\u7b56\u7565\u89e3\u51b3\u65f6\u7a7a\u51b2\u7a81", "result": "\u5728\u901a\u4fe1\u53d7\u9650\u548c\u590d\u6742\u62d3\u6251\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u6548\u7387", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5e73\u8861\u4e86\u5168\u5c40\u6218\u7565\u8fdc\u89c1\u548c\u5c40\u90e8\u6218\u672f\u654f\u6377\u6027\uff0c\u5728GPS\u7f3a\u5931\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02"}}
{"id": "2510.09204", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09204", "abs": "https://arxiv.org/abs/2510.09204", "authors": ["Simon Idoko", "Arun Kumar Singh"], "title": "Flow-Opt: Scalable Centralized Multi-Robot Trajectory Optimization with Flow Matching and Differentiable Optimization", "comment": null, "summary": "Centralized trajectory optimization in the joint space of multiple robots\nallows access to a larger feasible space that can result in smoother\ntrajectories, especially while planning in tight spaces. Unfortunately, it is\noften computationally intractable beyond a very small swarm size. In this\npaper, we propose Flow-Opt, a learning-based approach towards improving the\ncomputational tractability of centralized multi-robot trajectory optimization.\nSpecifically, we reduce the problem to first learning a generative model to\nsample different candidate trajectories and then using a learned\nSafety-Filter(SF) to ensure fast inference-time constraint satisfaction. We\npropose a flow-matching model with a diffusion transformer (DiT) augmented with\npermutation invariant robot position and map encoders as the generative model.\nWe develop a custom solver for our SF and equip it with a neural network that\npredicts context-specific initialization. The initialization network is trained\nin a self-supervised manner, taking advantage of the differentiability of the\nSF solver. We advance the state-of-the-art in the following respects. First, we\nshow that we can generate trajectories of tens of robots in cluttered\nenvironments in a few tens of milliseconds. This is several times faster than\nexisting centralized optimization approaches. Moreover, our approach also\ngenerates smoother trajectories orders of magnitude faster than competing\nbaselines based on diffusion models. Second, each component of our approach can\nbe batched, allowing us to solve a few tens of problem instances in a fraction\nof a second. We believe this is a first such result; no existing approach\nprovides such capabilities. Finally, our approach can generate a diverse set of\ntrajectories between a given set of start and goal locations, which can capture\ndifferent collision-avoidance behaviors.", "AI": {"tldr": "Flow-Opt\uff1a\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u96c6\u4e2d\u5f0f\u591a\u673a\u5668\u4eba\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u6a21\u578b\u91c7\u6837\u5019\u9009\u8f68\u8ff9\uff0c\u5e76\u4f7f\u7528\u5b89\u5168\u8fc7\u6ee4\u5668\u786e\u4fdd\u7ea6\u675f\u6ee1\u8db3\uff0c\u5b9e\u73b0\u6beb\u79d2\u7ea7\u8f68\u8ff9\u751f\u6210", "motivation": "\u96c6\u4e2d\u5f0f\u591a\u673a\u5668\u4eba\u8f68\u8ff9\u4f18\u5316\u5728\u7d27\u7a7a\u95f4\u89c4\u5212\u4e2d\u80fd\u4ea7\u751f\u66f4\u5e73\u6ed1\u7684\u8f68\u8ff9\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u968f\u673a\u5668\u4eba\u6570\u91cf\u589e\u52a0\u800c\u6025\u5267\u4e0a\u5347\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u5927\u89c4\u6a21\u7fa4\u4f53", "method": "\u4f7f\u7528\u57fa\u4e8e\u6d41\u5339\u914d\u548c\u6269\u6563\u53d8\u6362\u5668\u7684\u751f\u6210\u6a21\u578b\u91c7\u6837\u8f68\u8ff9\uff0c\u914d\u5408\u53ef\u5fae\u5b89\u5168\u8fc7\u6ee4\u5668\u53ca\u4e0a\u4e0b\u6587\u7279\u5b9a\u521d\u59cb\u5316\u7f51\u7edc\uff0c\u5b9e\u73b0\u5feb\u901f\u7ea6\u675f\u6ee1\u8db3", "result": "\u80fd\u5728\u6570\u5341\u6beb\u79d2\u5185\u751f\u6210\u6570\u5341\u4e2a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\uff0c\u6bd4\u73b0\u6709\u96c6\u4e2d\u5f0f\u4f18\u5316\u65b9\u6cd5\u5feb\u6570\u500d\uff0c\u6bd4\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u57fa\u7ebf\u65b9\u6cd5\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u4e86\u6279\u91cf\u89e3\u51b3\u6570\u5341\u4e2a\u95ee\u9898\u5b9e\u4f8b\u7684\u80fd\u529b\uff0c\u5e76\u80fd\u751f\u6210\u8d77\u70b9\u5230\u76ee\u6807\u70b9\u4e4b\u95f4\u7684\u591a\u6837\u5316\u8f68\u8ff9\uff0c\u6355\u6349\u4e0d\u540c\u7684\u907f\u78b0\u884c\u4e3a"}}
{"id": "2510.09209", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09209", "abs": "https://arxiv.org/abs/2510.09209", "authors": ["Yuki Kuroda", "Tomoya Takahashi", "Cristian C Beltran-Hernandez", "Masashi Hamaya", "Kazutoshi Tanaka"], "title": "PLEXUS Hand: Lightweight Four-Motor Prosthetic Hand Enabling Precision-Lateral Dexterous Manipulation", "comment": null, "summary": "Electric prosthetic hands should be lightweight to decrease the burden on the\nuser, shaped like human hands for cosmetic purposes, and have motors inside to\nprotect them from damage and dirt. In addition to the ability to perform daily\nactivities, these features are essential for everyday use of the hand. In-hand\nmanipulation is necessary to perform daily activities such as transitioning\nbetween different postures, particularly through rotational movements, such as\nreorienting cards before slot insertion and operating tools such as\nscrewdrivers. However, currently used electric prosthetic hands only achieve\nstatic grasp postures, and existing manipulation approaches require either many\nmotors, which makes the prosthesis heavy for daily use in the hand, or complex\nmechanisms that demand a large internal space and force external motor\nplacement, complicating attachment and exposing the components to damage.\nAlternatively, we combine a single-axis thumb and optimized thumb positioning\nto achieve basic posture and in-hand manipulation, that is, the reorientation\nbetween precision and lateral grasps, using only four motors in a lightweight\n(311 g) prosthetic hand. Experimental validation using primitive objects of\nvarious widths (5-30 mm) and shapes (cylinders and prisms) resulted in success\nrates of 90-100% for reorientation tasks. The hand performed seal stamping and\nUSB device insertion, as well as rotation to operate a screwdriver.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\uff08311\u514b\uff09\u7535\u52a8\u5047\u80a2\u624b\uff0c\u4ec5\u4f7f\u7528\u56db\u4e2a\u7535\u673a\u5b9e\u73b0\u4e86\u57fa\u672c\u6293\u63e1\u59ff\u52bf\u548c\u624b\u5185\u64cd\u4f5c\uff08\u7cbe\u786e\u6293\u63e1\u548c\u4fa7\u5411\u6293\u63e1\u4e4b\u95f4\u7684\u91cd\u65b0\u5b9a\u5411\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5047\u80a2\u624b\u91cd\u91cf\u8fc7\u5927\u6216\u673a\u6784\u590d\u6742\u7684\u95ee\u9898\u3002", "motivation": "\u7535\u52a8\u5047\u80a2\u624b\u9700\u8981\u8f7b\u91cf\u5316\u4ee5\u51cf\u8f7b\u7528\u6237\u8d1f\u62c5\uff0c\u5916\u5f62\u50cf\u4eba\u624b\u4ee5\u7f8e\u89c2\uff0c\u7535\u673a\u5185\u7f6e\u4ee5\u4fdd\u62a4\u514d\u53d7\u635f\u574f\u548c\u6c61\u57a2\u3002\u9664\u4e86\u6267\u884c\u65e5\u5e38\u6d3b\u52a8\u7684\u80fd\u529b\u5916\uff0c\u624b\u5185\u64cd\u4f5c\u5bf9\u4e8e\u65e5\u5e38\u4f7f\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u4f7f\u7528\u7684\u7535\u52a8\u5047\u80a2\u624b\u4ec5\u80fd\u5b9e\u73b0\u9759\u6001\u6293\u63e1\u59ff\u52bf\uff0c\u73b0\u6709\u64cd\u4f5c\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u8bb8\u591a\u7535\u673a\uff08\u4f7f\u5047\u80a2\u8fc7\u91cd\uff09\uff0c\u8981\u4e48\u9700\u8981\u590d\u6742\u673a\u6784\uff08\u5360\u7528\u5927\u5185\u90e8\u7a7a\u95f4\u5e76\u9700\u8981\u5916\u90e8\u7535\u673a\u653e\u7f6e\uff09\u3002", "method": "\u7ed3\u5408\u5355\u8f74\u62c7\u6307\u548c\u4f18\u5316\u7684\u62c7\u6307\u5b9a\u4f4d\uff0c\u4ec5\u4f7f\u7528\u56db\u4e2a\u7535\u673a\u5728\u8f7b\u91cf\u7ea7\uff08311\u514b\uff09\u5047\u80a2\u624b\u4e2d\u5b9e\u73b0\u57fa\u672c\u59ff\u52bf\u548c\u624b\u5185\u64cd\u4f5c\uff08\u7cbe\u786e\u6293\u63e1\u548c\u4fa7\u5411\u6293\u63e1\u4e4b\u95f4\u7684\u91cd\u65b0\u5b9a\u5411\uff09\u3002", "result": "\u4f7f\u7528\u5404\u79cd\u5bbd\u5ea6\uff085-30\u6beb\u7c73\uff09\u548c\u5f62\u72b6\uff08\u5706\u67f1\u4f53\u548c\u68f1\u67f1\uff09\u7684\u539f\u59cb\u7269\u4f53\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u91cd\u65b0\u5b9a\u5411\u4efb\u52a1\u7684\u6210\u529f\u7387\u4e3a90-100%\u3002\u8be5\u624b\u80fd\u591f\u6267\u884c\u5370\u7ae0\u76d6\u7ae0\u548cUSB\u8bbe\u5907\u63d2\u5165\uff0c\u4ee5\u53ca\u65cb\u8f6c\u64cd\u4f5c\u87ba\u4e1d\u5200\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u5355\u8f74\u62c7\u6307\u548c\u4f18\u5316\u7684\u62c7\u6307\u5b9a\u4f4d\uff0c\u4ec5\u4f7f\u7528\u56db\u4e2a\u7535\u673a\u5728\u8f7b\u91cf\u7ea7\u5047\u80a2\u624b\u4e2d\u5b9e\u73b0\u4e86\u57fa\u672c\u59ff\u52bf\u548c\u624b\u5185\u64cd\u4f5c\uff0c\u6210\u529f\u5b8c\u6210\u4e86\u5404\u79cd\u65e5\u5e38\u4efb\u52a1\u7684\u91cd\u65b0\u5b9a\u5411\u64cd\u4f5c\u3002"}}
{"id": "2510.09221", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09221", "abs": "https://arxiv.org/abs/2510.09221", "authors": ["Jingyuan Sun", "Chaoran Wang", "Mingyu Zhang", "Cui Miao", "Hongyu Ji", "Zihan Qu", "Han Sun", "Bing Wang", "Qingyi Si"], "title": "HANDO: Hierarchical Autonomous Navigation and Dexterous Omni-loco-manipulation", "comment": "4 pages, 2 figures, this paper has been accepted for the workshop\n  Perception and Planning for Mobile Manipulation in Changing Environments\n  (PM2CE) at IROS 2025", "summary": "Seamless loco-manipulation in unstructured environments requires robots to\nleverage autonomous exploration alongside whole-body control for physical\ninteraction. In this work, we introduce HANDO (Hierarchical Autonomous\nNavigation and Dexterous Omni-loco-manipulation), a two-layer framework\ndesigned for legged robots equipped with manipulators to perform human-centered\nmobile manipulation tasks. The first layer utilizes a goal-conditioned\nautonomous exploration policy to guide the robot to semantically specified\ntargets, such as a black office chair in a dynamic environment. The second\nlayer employs a unified whole-body loco-manipulation policy to coordinate the\narm and legs for precise interaction tasks-for example, handing a drink to a\nperson seated on the chair. We have conducted an initial deployment of the\nnavigation module, and will continue to pursue finer-grained deployment of\nwhole-body loco-manipulation.", "AI": {"tldr": "HANDO\u6846\u67b6\uff1a\u4e3a\u914d\u5907\u673a\u68b0\u81c2\u7684\u817f\u5f0f\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u53cc\u5c42\u7cfb\u7edf\uff0c\u5b9e\u73b0\u81ea\u4e3b\u5bfc\u822a\u548c\u5168\u8eab\u534f\u8c03\u64cd\u4f5c", "motivation": "\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u65e0\u7f1d\u7684\u79fb\u52a8\u64cd\u4f5c\uff0c\u9700\u8981\u673a\u5668\u4eba\u7ed3\u5408\u81ea\u4e3b\u63a2\u7d22\u548c\u5168\u8eab\u63a7\u5236\u6765\u8fdb\u884c\u7269\u7406\u4ea4\u4e92", "method": "\u53cc\u5c42\u6846\u67b6\uff1a\u7b2c\u4e00\u5c42\u4f7f\u7528\u76ee\u6807\u6761\u4ef6\u81ea\u4e3b\u63a2\u7d22\u7b56\u7565\u5bfc\u822a\u5230\u8bed\u4e49\u6307\u5b9a\u76ee\u6807\uff1b\u7b2c\u4e8c\u5c42\u4f7f\u7528\u7edf\u4e00\u7684\u5168\u8eab\u79fb\u52a8\u64cd\u4f5c\u7b56\u7565\u534f\u8c03\u624b\u81c2\u548c\u817f\u90e8\u8fdb\u884c\u7cbe\u786e\u4ea4\u4e92", "result": "\u5df2\u5b8c\u6210\u5bfc\u822a\u6a21\u5757\u7684\u521d\u6b65\u90e8\u7f72\uff0c\u5c06\u7ee7\u7eed\u63a8\u8fdb\u5168\u8eab\u79fb\u52a8\u64cd\u4f5c\u7684\u66f4\u7cbe\u7ec6\u90e8\u7f72", "conclusion": "HANDO\u6846\u67b6\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6267\u884c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u7684\u53ef\u884c\u65b9\u6848"}}
{"id": "2510.09229", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09229", "abs": "https://arxiv.org/abs/2510.09229", "authors": ["Yuyang Gao", "Haofei Ma", "Pai Zheng"], "title": "Glovity: Learning Dexterous Contact-Rich Manipulation via Spatial Wrench Feedback Teleoperation System", "comment": null, "summary": "We present Glovity, a novel, low-cost wearable teleoperation system that\nintegrates a spatial wrench (force-torque) feedback device with a haptic glove\nfeaturing fingertip Hall sensor calibration, enabling feedback-rich dexterous\nmanipulation. Glovity addresses key challenges in contact-rich tasks by\nproviding intuitive wrench and tactile feedback, while overcoming embodiment\ngaps through precise retargeting. User studies demonstrate significant\nimprovements: wrench feedback boosts success rates in book-flipping tasks from\n48% to 78% and reduces completion time by 25%, while fingertip calibration\nenhances thin-object grasping success significantly compared to commercial\nglove. Furthermore, incorporating wrench signals into imitation learning (via\nDP-R3M) achieves high success rate in novel contact-rich scenarios, such as\nadaptive page flipping and force-aware handovers. All hardware designs,\nsoftware will be open-sourced. Project website: https://glovity.github.io/", "AI": {"tldr": "Glovity\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u7684\u53ef\u7a7f\u6234\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u96c6\u6210\u4e86\u7a7a\u95f4\u529b\u53cd\u9988\u8bbe\u5907\u548c\u5e26\u6709\u6307\u5c16\u970d\u5c14\u4f20\u611f\u5668\u6821\u51c6\u7684\u89e6\u89c9\u624b\u5957\uff0c\u5b9e\u73b0\u4e86\u53cd\u9988\u4e30\u5bcc\u7684\u7075\u5de7\u64cd\u4f5c\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u76f4\u89c2\u7684\u529b\u53cd\u9988\u548c\u89e6\u89c9\u53cd\u9988\u89e3\u51b3\u63a5\u89e6\u5bc6\u96c6\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u7cbe\u786e\u91cd\u5b9a\u5411\u514b\u670d\u4f53\u73b0\u5dee\u8ddd\u3002", "motivation": "\u89e3\u51b3\u63a5\u89e6\u5bc6\u96c6\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u4f9b\u76f4\u89c2\u7684\u529b\u53cd\u9988\u548c\u89e6\u89c9\u53cd\u9988\uff0c\u540c\u65f6\u901a\u8fc7\u7cbe\u786e\u91cd\u5b9a\u5411\u514b\u670d\u4f53\u73b0\u5dee\u8ddd\u3002", "method": "\u96c6\u6210\u7a7a\u95f4\u529b\u53cd\u9988\u8bbe\u5907\u4e0e\u5e26\u6709\u6307\u5c16\u970d\u5c14\u4f20\u611f\u5668\u6821\u51c6\u7684\u89e6\u89c9\u624b\u5957\uff0c\u7ed3\u5408DP-R3M\u8fdb\u884c\u6a21\u4eff\u5b66\u4e60\u3002", "result": "\u529b\u53cd\u9988\u5c06\u4e66\u672c\u7ffb\u9875\u4efb\u52a1\u7684\u6210\u529f\u7387\u4ece48%\u63d0\u5347\u81f378%\uff0c\u5b8c\u6210\u65f6\u95f4\u51cf\u5c1125%\uff1b\u6307\u5c16\u6821\u51c6\u663e\u8457\u63d0\u9ad8\u4e86\u8584\u7269\u4f53\u6293\u53d6\u6210\u529f\u7387\uff1b\u5728\u65b0\u578b\u63a5\u89e6\u5bc6\u96c6\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "Glovity\u7cfb\u7edf\u5728\u63a5\u89e6\u5bc6\u96c6\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u6240\u6709\u786c\u4ef6\u8bbe\u8ba1\u548c\u8f6f\u4ef6\u5c06\u5f00\u6e90\u53d1\u5e03\u3002"}}
{"id": "2510.09254", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09254", "abs": "https://arxiv.org/abs/2510.09254", "authors": ["Dominik Urbaniak", "Alejandro Agostini", "Pol Ramon", "Jan Rosell", "Ra\u00fal Su\u00e1rez", "Michael Suppa"], "title": "Obstacle Avoidance using Dynamic Movement Primitives and Reinforcement Learning", "comment": "8 pages, 7 figures", "summary": "Learning-based motion planning can quickly generate near-optimal\ntrajectories. However, it often requires either large training datasets or\ncostly collection of human demonstrations. This work proposes an alternative\napproach that quickly generates smooth, near-optimal collision-free 3D\nCartesian trajectories from a single artificial demonstration. The\ndemonstration is encoded as a Dynamic Movement Primitive (DMP) and iteratively\nreshaped using policy-based reinforcement learning to create a diverse\ntrajectory dataset for varying obstacle configurations. This dataset is used to\ntrain a neural network that takes as inputs the task parameters describing the\nobstacle dimensions and location, derived automatically from a point cloud, and\noutputs the DMP parameters that generate the trajectory. The approach is\nvalidated in simulation and real-robot experiments, outperforming a RRT-Connect\nbaseline in terms of computation and execution time, as well as trajectory\nlength, while supporting multi-modal trajectory generation for different\nobstacle geometries and end-effector dimensions. Videos and the implementation\ncode are available at https://github.com/DominikUrbaniak/obst-avoid-dmp-pi2.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ece\u5355\u4e2a\u4eba\u5de5\u6f14\u793a\u5feb\u901f\u751f\u6210\u5e73\u6ed1\u3001\u63a5\u8fd1\u6700\u4f18\u7684\u65e0\u78b0\u649e3D\u7b1b\u5361\u5c14\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u52a8\u6001\u8fd0\u52a8\u57fa\u5143\u7f16\u7801\u6f14\u793a\u5e76\u901a\u8fc7\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u8fed\u4ee3\u91cd\u5851\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u8f68\u8ff9\u53c2\u6570\uff0c\u5728\u8ba1\u7b97\u548c\u6267\u884c\u65f6\u95f4\u4e0a\u4f18\u4e8eRRT-Connect\u57fa\u51c6\u3002", "motivation": "\u57fa\u4e8e\u5b66\u4e60\u7684\u8fd0\u52a8\u89c4\u5212\u901a\u5e38\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6216\u6602\u8d35\u7684\u4eba\u7c7b\u6f14\u793a\u6536\u96c6\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5355\u4e2a\u4eba\u5de5\u6f14\u793a\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u8f68\u8ff9\u3002", "method": "\u5c06\u6f14\u793a\u7f16\u7801\u4e3a\u52a8\u6001\u8fd0\u52a8\u57fa\u5143\uff0c\u4f7f\u7528\u57fa\u4e8e\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u8fed\u4ee3\u91cd\u5851\u751f\u6210\u591a\u6837\u5316\u8f68\u8ff9\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6839\u636e\u969c\u788d\u7269\u53c2\u6570\u8f93\u51faDMP\u53c2\u6570\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\uff0c\u5728\u8ba1\u7b97\u548c\u6267\u884c\u65f6\u95f4\u3001\u8f68\u8ff9\u957f\u5ea6\u65b9\u9762\u4f18\u4e8eRRT-Connect\u57fa\u51c6\uff0c\u652f\u6301\u4e0d\u540c\u969c\u788d\u7269\u51e0\u4f55\u548c\u672b\u7aef\u6267\u884c\u5668\u5c3a\u5bf8\u7684\u591a\u6a21\u6001\u8f68\u8ff9\u751f\u6210\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u5355\u4e2a\u4eba\u5de5\u6f14\u793a\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff0c\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u79cd\u573a\u666f\u7684\u8f68\u8ff9\u751f\u6210\u3002"}}
{"id": "2510.09267", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09267", "abs": "https://arxiv.org/abs/2510.09267", "authors": ["Amina Ferrad", "Johann Huber", "Fran\u00e7ois H\u00e9l\u00e9non", "Julien Gleyze", "Mahdi Khoramshahi", "St\u00e9phane Doncieux"], "title": "Placeit! A Framework for Learning Robot Object Placement Skills", "comment": "8 pages, 8 figures. Draft version", "summary": "Robotics research has made significant strides in learning, yet mastering\nbasic skills like object placement remains a fundamental challenge. A key\nbottleneck is the acquisition of large-scale, high-quality data, which is often\na manual and laborious process. Inspired by Graspit!, a foundational work that\nused simulation to automatically generate dexterous grasp poses, we introduce\nPlaceit!, an evolutionary-computation framework for generating valid placement\npositions for rigid objects. Placeit! is highly versatile, supporting tasks\nfrom placing objects on tables to stacking and inserting them. Our experiments\nshow that by leveraging quality-diversity optimization, Placeit! significantly\noutperforms state-of-the-art methods across all scenarios for generating\ndiverse valid poses. A pick&place pipeline built on our framework achieved a\n90% success rate over 120 real-world deployments. This work positions Placeit!\nas a powerful tool for open-environment pick-and-place tasks and as a valuable\nengine for generating the data needed to train simulation-based foundation\nmodels in robotics.", "AI": {"tldr": "Placeit!\u662f\u4e00\u4e2a\u57fa\u4e8e\u8fdb\u5316\u8ba1\u7b97\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u521a\u6027\u7269\u4f53\u7684\u6709\u6548\u653e\u7f6e\u4f4d\u7f6e\uff0c\u652f\u6301\u684c\u9762\u653e\u7f6e\u3001\u5806\u53e0\u548c\u63d2\u5165\u7b49\u4efb\u52a1\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u8fbe\u523090%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u673a\u5668\u4eba\u5b66\u4e60\u9762\u4e34\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u83b7\u53d6\u7684\u74f6\u9888\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u624b\u52a8\u4e14\u8d39\u529b\u7684\u6570\u636e\u6536\u96c6\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u8fdb\u5316\u8ba1\u7b97\u548c\u8d28\u91cf\u591a\u6837\u6027\u4f18\u5316\u6846\u67b6\uff0c\u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u7684\u6709\u6548\u653e\u7f6e\u59ff\u6001\uff0c\u652f\u6301\u591a\u79cd\u653e\u7f6e\u573a\u666f\u3002", "result": "\u5728\u6240\u6709\u6d4b\u8bd5\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8be5\u6846\u67b6\u7684\u62fe\u653e\u7ba1\u9053\u5728120\u6b21\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u8fbe\u523090%\u7684\u6210\u529f\u7387\u3002", "conclusion": "Placeit!\u662f\u5f00\u653e\u73af\u5883\u62fe\u653e\u4efb\u52a1\u7684\u6709\u529b\u5de5\u5177\uff0c\u4e5f\u662f\u8bad\u7ec3\u57fa\u4e8e\u4eff\u771f\u7684\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u6240\u9700\u6570\u636e\u751f\u6210\u7684\u91cd\u8981\u5f15\u64ce\u3002"}}
{"id": "2510.09396", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.09396", "abs": "https://arxiv.org/abs/2510.09396", "authors": ["Sajad Khatiri", "Francisco Eli Vina Barrientos", "Maximilian Wulf", "Paolo Tonella", "Sebastiano Panichella"], "title": "Bridging Research and Practice in Simulation-based Testing of Industrial Robot Navigation Systems", "comment": "12 pages, accepted for publication at IEEE/ACM International\n  Conference on Automated Software Engineering (ASE) 2025 - Industry Showcase\n  Track", "summary": "Ensuring robust robotic navigation in dynamic environments is a key\nchallenge, as traditional testing methods often struggle to cover the full\nspectrum of operational requirements. This paper presents the industrial\nadoption of Surrealist, a simulation-based test generation framework originally\nfor UAVs, now applied to the ANYmal quadrupedal robot for industrial\ninspection. Our method uses a search-based algorithm to automatically generate\nchallenging obstacle avoidance scenarios, uncovering failures often missed by\nmanual testing. In a pilot phase, generated test suites revealed critical\nweaknesses in one experimental algorithm (40.3% success rate) and served as an\neffective benchmark to prove the superior robustness of another (71.2% success\nrate). The framework was then integrated into the ANYbotics workflow for a\nsix-month industrial evaluation, where it was used to test five proprietary\nalgorithms. A formal survey confirmed its value, showing it enhances the\ndevelopment process, uncovers critical failures, provides objective benchmarks,\nand strengthens the overall verification pipeline.", "AI": {"tldr": "\u5c06Surrealist\u4eff\u771f\u6d4b\u8bd5\u6846\u67b6\u4ece\u65e0\u4eba\u673a\u6269\u5c55\u5230ANYmal\u56db\u8db3\u673a\u5668\u4eba\u5de5\u4e1a\u68c0\u6d4b\u5e94\u7528\uff0c\u901a\u8fc7\u641c\u7d22\u7b97\u6cd5\u81ea\u52a8\u751f\u6210\u969c\u788d\u89c4\u907f\u573a\u666f\uff0c\u5728\u5de5\u4e1a\u8bc4\u4f30\u4e2d\u9a8c\u8bc1\u4e86\u4e94\u4e2a\u4e13\u6709\u7b97\u6cd5\u5e76\u53d1\u73b0\u5173\u952e\u6545\u969c\u3002", "motivation": "\u4f20\u7edf\u6d4b\u8bd5\u65b9\u6cd5\u96be\u4ee5\u8986\u76d6\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u5bfc\u822a\u7684\u5168\u90e8\u64cd\u4f5c\u9700\u6c42\uff0c\u9700\u8981\u81ea\u52a8\u5316\u6d4b\u8bd5\u6846\u67b6\u6765\u53d1\u73b0\u624b\u52a8\u6d4b\u8bd5\u9057\u6f0f\u7684\u6545\u969c\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u641c\u7d22\u7684\u7b97\u6cd5\u81ea\u52a8\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u969c\u788d\u89c4\u907f\u6d4b\u8bd5\u573a\u666f\uff0c\u5c06\u6846\u67b6\u96c6\u6210\u5230ANYbotics\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u8fdb\u884c\u5de5\u4e1a\u8bc4\u4f30\u3002", "result": "\u5728\u8bd5\u70b9\u9636\u6bb5\uff0c\u6d4b\u8bd5\u5957\u4ef6\u53d1\u73b0\u4e00\u4e2a\u5b9e\u9a8c\u7b97\u6cd5\u7684\u5173\u952e\u5f31\u70b9\uff08\u6210\u529f\u738740.3%\uff09\uff0c\u5e76\u8bc1\u660e\u53e6\u4e00\u4e2a\u7b97\u6cd5\u7684\u4f18\u8d8a\u9c81\u68d2\u6027\uff08\u6210\u529f\u738771.2%\uff09\u3002\u516d\u4e2a\u6708\u5de5\u4e1a\u8bc4\u4f30\u4e2d\u6d4b\u8bd5\u4e86\u4e94\u4e2a\u4e13\u6709\u7b97\u6cd5\uff0c\u6b63\u5f0f\u8c03\u67e5\u786e\u8ba4\u4e86\u6846\u67b6\u7684\u4ef7\u503c\u3002", "conclusion": "Surrealist\u6846\u67b6\u80fd\u6709\u6548\u589e\u5f3a\u5f00\u53d1\u6d41\u7a0b\uff0c\u53d1\u73b0\u5173\u952e\u6545\u969c\uff0c\u63d0\u4f9b\u5ba2\u89c2\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u52a0\u5f3a\u6574\u4f53\u9a8c\u8bc1\u6d41\u7a0b\uff0c\u5728\u5de5\u4e1a\u673a\u5668\u4eba\u5bfc\u822a\u6d4b\u8bd5\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2510.09459", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.09459", "abs": "https://arxiv.org/abs/2510.09459", "authors": ["Ralf R\u00f6mer", "Adrian Kobras", "Luca Worbis", "Angela P. Schoellig"], "title": "Failure Prediction at Runtime for Generative Robot Policies", "comment": "Accepted to NeurIPS 2025", "summary": "Imitation learning (IL) with generative models, such as diffusion and flow\nmatching, has enabled robots to perform complex, long-horizon tasks. However,\ndistribution shifts from unseen environments or compounding action errors can\nstill cause unpredictable and unsafe behavior, leading to task failure. Early\nfailure prediction during runtime is therefore essential for deploying robots\nin human-centered and safety-critical environments. We propose FIPER, a general\nframework for Failure Prediction at Runtime for generative IL policies that\ndoes not require failure data. FIPER identifies two key indicators of impending\nfailure: (i) out-of-distribution (OOD) observations detected via random network\ndistillation in the policy's embedding space, and (ii) high uncertainty in\ngenerated actions measured by a novel action-chunk entropy score. Both failure\nprediction scores are calibrated using a small set of successful rollouts via\nconformal prediction. A failure alarm is triggered when both indicators,\naggregated over short time windows, exceed their thresholds. We evaluate FIPER\nacross five simulation and real-world environments involving diverse failure\nmodes. Our results demonstrate that FIPER better distinguishes actual failures\nfrom benign OOD situations and predicts failures more accurately and earlier\nthan existing methods. We thus consider this work an important step towards\nmore interpretable and safer generative robot policies. Code, data and videos\nare available at https://tum-lsy.github.io/fiper_website.", "AI": {"tldr": "FIPER\u662f\u4e00\u4e2a\u65e0\u9700\u5931\u8d25\u6570\u636e\u7684\u8fd0\u884c\u65f6\u6545\u969c\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u6d4b\u5206\u5e03\u5916\u89c2\u6d4b\u548c\u52a8\u4f5c\u4e0d\u786e\u5b9a\u6027\u6765\u9884\u6d4b\u751f\u6210\u5f0f\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u7684\u6545\u969c\u3002", "motivation": "\u751f\u6210\u5f0f\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5206\u5e03\u504f\u79fb\u548c\u52a8\u4f5c\u8bef\u5dee\u7d2f\u79ef\u4f1a\u5bfc\u81f4\u4e0d\u53ef\u9884\u6d4b\u7684\u4e0d\u5b89\u5168\u884c\u4e3a\uff0c\u9700\u8981\u8fd0\u884c\u65f6\u6545\u969c\u9884\u6d4b\u6765\u786e\u4fdd\u673a\u5668\u4eba\u90e8\u7f72\u5b89\u5168\u3002", "method": "\u4f7f\u7528\u968f\u673a\u7f51\u7edc\u84b8\u998f\u68c0\u6d4b\u5206\u5e03\u5916\u89c2\u6d4b\uff0c\u63d0\u51fa\u52a8\u4f5c\u5757\u71b5\u5206\u6570\u91cf\u5316\u52a8\u4f5c\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u4fdd\u5f62\u9884\u6d4b\u6821\u51c6\u9884\u6d4b\u5206\u6570\uff0c\u5f53\u4e24\u4e2a\u6307\u6807\u8d85\u8fc7\u9608\u503c\u65f6\u89e6\u53d1\u6545\u969c\u8b66\u62a5\u3002", "result": "\u5728\u4e94\u4e2a\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\u663e\u793a\uff0cFIPER\u80fd\u66f4\u597d\u5730\u533a\u5206\u5b9e\u9645\u6545\u969c\u4e0e\u826f\u6027\u5206\u5e03\u5916\u60c5\u51b5\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u51c6\u786e\u3001\u66f4\u65e9\u5730\u9884\u6d4b\u6545\u969c\u3002", "conclusion": "FIPER\u662f\u8fc8\u5411\u66f4\u53ef\u89e3\u91ca\u3001\u66f4\u5b89\u5168\u7684\u751f\u6210\u5f0f\u673a\u5668\u4eba\u7b56\u7565\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u673a\u5668\u4eba\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6545\u969c\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.09483", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09483", "abs": "https://arxiv.org/abs/2510.09483", "authors": ["Lars Ohnemus", "Nils Hantke", "Max Wei\u00dfer", "Kai Furmans"], "title": "FOGMACHINE -- Leveraging Discrete-Event Simulation and Scene Graphs for Modeling Hierarchical, Interconnected Environments under Partial Observations from Mobile Agents", "comment": "submitted to the IEEE for possible publication; 8 pages, 3 figures, 1\n  table", "summary": "Dynamic Scene Graphs (DSGs) provide a structured representation of\nhierarchical, interconnected environments, but current approaches struggle to\ncapture stochastic dynamics, partial observability, and multi-agent activity.\nThese aspects are critical for embodied AI, where agents must act under\nuncertainty and delayed perception. We introduce FOGMACHINE , an open-source\nframework that fuses DSGs with discrete-event simulation to model object\ndynamics, agent observations, and interactions at scale. This setup enables the\nstudy of uncertainty propagation, planning under limited perception, and\nemergent multi-agent behavior. Experiments in urban scenarios illustrate\nrealistic temporal and spatial patterns while revealing the challenges of\nbelief estimation under sparse observations. By combining structured\nrepresentations with efficient simulation, FOGMACHINE establishes an effective\ntool for benchmarking, model training, and advancing embodied AI in complex,\nuncertain environments.", "AI": {"tldr": "FOGMACHINE\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u5c06\u52a8\u6001\u573a\u666f\u56fe\u4e0e\u79bb\u6563\u4e8b\u4ef6\u6a21\u62df\u76f8\u7ed3\u5408\uff0c\u7528\u4e8e\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u5efa\u6a21\u5bf9\u8c61\u52a8\u6001\u3001\u667a\u80fd\u4f53\u89c2\u5bdf\u548c\u4ea4\u4e92\uff0c\u652f\u6301\u591a\u667a\u80fd\u4f53\u884c\u4e3a\u7814\u7a76\u548c\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u5206\u6790\u3002", "motivation": "\u5f53\u524d\u52a8\u6001\u573a\u666f\u56fe\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u968f\u673a\u52a8\u6001\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u548c\u591a\u667a\u80fd\u4f53\u6d3b\u52a8\uff0c\u800c\u8fd9\u4e9b\u5bf9\u4e8e\u5177\u8eabAI\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u5ef6\u8fdf\u611f\u77e5\u4e0b\u7684\u51b3\u7b56\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5c06\u52a8\u6001\u573a\u666f\u56fe\u4e0e\u79bb\u6563\u4e8b\u4ef6\u6a21\u62df\u878d\u5408\uff0c\u5efa\u6a21\u5bf9\u8c61\u52a8\u6001\u3001\u667a\u80fd\u4f53\u89c2\u5bdf\u548c\u4ea4\u4e92\uff0c\u652f\u6301\u5927\u89c4\u6a21\u4eff\u771f\u3002", "result": "\u5728\u57ce\u5e02\u573a\u666f\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u771f\u5b9e\u7684\u65f6\u7a7a\u6a21\u5f0f\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5728\u7a00\u758f\u89c2\u5bdf\u4e0b\u4fe1\u5ff5\u4f30\u8ba1\u7684\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u8868\u793a\u548c\u9ad8\u6548\u6a21\u62df\uff0cFOGMACHINE\u4e3a\u590d\u6742\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u57fa\u51c6\u6d4b\u8bd5\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u5177\u8eabAI\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2510.09497", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.09497", "abs": "https://arxiv.org/abs/2510.09497", "authors": ["Noah Barnes", "Ji Woong Kim", "Lingyun Di", "Hannah Qu", "Anuruddha Bhattacharjee", "Miroslaw Janowski", "Dheeraj Gandhi", "Bailey Felix", "Shaopeng Jiang", "Olivia Young", "Mark Fuge", "Ryan D. Sochol", "Jeremy D. Brown", "Axel Krieger"], "title": "Autonomous Soft Robotic Guidewire Navigation via Imitation Learning", "comment": null, "summary": "In endovascular surgery, endovascular interventionists push a thin tube\ncalled a catheter, guided by a thin wire to a treatment site inside the\npatient's blood vessels to treat various conditions such as blood clots,\naneurysms, and malformations. Guidewires with robotic tips can enhance\nmaneuverability, but they present challenges in modeling and control.\nAutomation of soft robotic guidewire navigation has the potential to overcome\nthese challenges, increasing the precision and safety of endovascular\nnavigation. In other surgical domains, end-to-end imitation learning has shown\npromising results. Thus, we develop a transformer-based imitation learning\nframework with goal conditioning, relative action outputs, and automatic\ncontrast dye injections to enable generalizable soft robotic guidewire\nnavigation in an aneurysm targeting task. We train the model on 36 different\nmodular bifurcated geometries, generating 647 total demonstrations under\nsimulated fluoroscopy, and evaluate it on three previously unseen vascular\ngeometries. The model can autonomously drive the tip of the robot to the\naneurysm location with a success rate of 83% on the unseen geometries,\noutperforming several baselines. In addition, we present ablation and baseline\nstudies to evaluate the effectiveness of each design and data collection\nchoice. Project website: https://softrobotnavigation.github.io/", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u8f6f\u4f53\u673a\u5668\u4eba\u5bfc\u4e1d\u5728\u8840\u7ba1\u5185\u5bfc\u822a\uff0c\u5728\u52a8\u8109\u7624\u9776\u5411\u4efb\u52a1\u4e2d\u5b9e\u73b083%\u7684\u6210\u529f\u7387", "motivation": "\u89e3\u51b3\u8f6f\u4f53\u673a\u5668\u4eba\u5bfc\u4e1d\u5728\u8840\u7ba1\u5185\u5bfc\u822a\u4e2d\u7684\u5efa\u6a21\u548c\u63a7\u5236\u6311\u6218\uff0c\u63d0\u9ad8\u8840\u7ba1\u5185\u5bfc\u822a\u7684\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027", "method": "\u5f00\u53d1\u57fa\u4e8eTransformer\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u76ee\u6807\u6761\u4ef6\u3001\u76f8\u5bf9\u52a8\u4f5c\u8f93\u51fa\u548c\u81ea\u52a8\u5bf9\u6bd4\u5242\u6ce8\u5c04\u529f\u80fd\uff0c\u572836\u79cd\u4e0d\u540c\u5206\u53c9\u51e0\u4f55\u7ed3\u6784\u4e0a\u8fdb\u884c\u8bad\u7ec3", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u8840\u7ba1\u51e0\u4f55\u7ed3\u6784\u4e0a\uff0c\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u5c06\u673a\u5668\u4eba\u5c16\u7aef\u5bfc\u822a\u81f3\u52a8\u8109\u7624\u4f4d\u7f6e\uff0c\u6210\u529f\u7387\u8fbe\u523083%\uff0c\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u8f6f\u4f53\u673a\u5668\u4eba\u5bfc\u4e1d\u5bfc\u822a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u8840\u7ba1\u5185\u624b\u672f\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2510.09526", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09526", "abs": "https://arxiv.org/abs/2510.09526", "authors": ["Chenghao Wang", "Kaushik Venkatesh Krishnamurthy", "Shreyansh Pitroda", "Adarsh Salagame", "Ioannis Mandralis", "Eric Sihite", "Alireza Ramezani", "Morteza Gharib"], "title": "Dynamic Quadrupedal Legged and Aerial Locomotion via Structure Repurposing", "comment": null, "summary": "Multi-modal ground-aerial robots have been extensively studied, with a\nsignificant challenge lying in the integration of conflicting requirements\nacross different modes of operation. The Husky robot family, developed at\nNortheastern University, and specifically the Husky v.2 discussed in this\nstudy, addresses this challenge by incorporating posture manipulation and\nthrust vectoring into multi-modal locomotion through structure repurposing.\nThis quadrupedal robot features leg structures that can be repurposed for\ndynamic legged locomotion and flight. In this paper, we present the hardware\ndesign of the robot and report primary results on dynamic quadrupedal legged\nlocomotion and hovering.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Husky v.2\u591a\u6a21\u6001\u5730\u9762-\u7a7a\u4e2d\u673a\u5668\u4eba\u7684\u786c\u4ef6\u8bbe\u8ba1\uff0c\u8be5\u673a\u5668\u4eba\u901a\u8fc7\u7ed3\u6784\u91cd\u5229\u7528\u5b9e\u73b0\u4e86\u52a8\u6001\u56db\u8db3\u884c\u8d70\u548c\u60ac\u505c\u98de\u884c\u529f\u80fd\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u673a\u5668\u4eba\u5728\u4e0d\u540c\u64cd\u4f5c\u6a21\u5f0f\u4e0b\u7684\u51b2\u7a81\u9700\u6c42\u6574\u5408\u95ee\u9898\uff0c\u5b9e\u73b0\u5730\u9762\u79fb\u52a8\u548c\u7a7a\u4e2d\u98de\u884c\u7684\u65e0\u7f1d\u5207\u6362\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u91cd\u5229\u7528\u6280\u672f\uff0c\u5c06\u817f\u90e8\u7ed3\u6784\u91cd\u65b0\u914d\u7f6e\u7528\u4e8e\u52a8\u6001\u56db\u8db3\u884c\u8d70\u548c\u98de\u884c\uff0c\u7ed3\u5408\u59ff\u6001\u64cd\u7eb5\u548c\u63a8\u529b\u77e2\u91cf\u63a7\u5236\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u52a8\u6001\u56db\u8db3\u884c\u8d70\u548c\u60ac\u505c\u98de\u884c\u529f\u80fd\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u673a\u5668\u4eba\u7684\u53ef\u884c\u6027\u3002", "conclusion": "Husky v.2\u673a\u5668\u4eba\u901a\u8fc7\u521b\u65b0\u7684\u7ed3\u6784\u91cd\u5229\u7528\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u673a\u5668\u4eba\u7684\u96c6\u6210\u6311\u6218\uff0c\u4e3a\u5730\u9762-\u7a7a\u4e2d\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.09543", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09543", "abs": "https://arxiv.org/abs/2510.09543", "authors": ["Chenghao Wang", "Arjun Viswanathan", "Eric Sihite", "Alireza Ramezani"], "title": "Guiding Energy-Efficient Locomotion through Impact Mitigation Rewards", "comment": null, "summary": "Animals achieve energy-efficient locomotion by their implicit passive\ndynamics, a marvel that has captivated roboticists for decades.Recently,\nmethods incorporated Adversarial Motion Prior (AMP) and Reinforcement learning\n(RL) shows promising progress to replicate Animals' naturalistic motion.\nHowever, such imitation learning approaches predominantly capture explicit\nkinematic patterns, so-called gaits, while overlooking the implicit passive\ndynamics. This work bridges this gap by incorporating a reward term guided by\nImpact Mitigation Factor (IMF), a physics-informed metric that quantifies a\nrobot's ability to passively mitigate impacts. By integrating IMF with AMP, our\napproach enables RL policies to learn both explicit motion trajectories from\nanimal reference motion and the implicit passive dynamic. We demonstrate energy\nefficiency improvements of up to 32%, as measured by the Cost of Transport\n(CoT), across both AMP and handcrafted reward structure.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u51b2\u51fb\u7f13\u89e3\u56e0\u5b50(IMF)\u548c\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c(AMP)\u7684\u65b9\u6cd5\uff0c\u4f7f\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u80fd\u591f\u540c\u65f6\u5b66\u4e60\u52a8\u7269\u7684\u663e\u6027\u8fd0\u52a8\u8f68\u8ff9\u548c\u9690\u6027\u88ab\u52a8\u52a8\u529b\u5b66\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe32%\u7684\u80fd\u6e90\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u52a8\u7269\u901a\u8fc7\u5176\u56fa\u6709\u7684\u88ab\u52a8\u52a8\u529b\u5b66\u5b9e\u73b0\u9ad8\u80fd\u6548\u8fd0\u52a8\uff0c\u4f46\u73b0\u6709\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u6355\u6349\u663e\u6027\u6b65\u6001\u6a21\u5f0f\uff0c\u5ffd\u7565\u4e86\u9690\u6027\u88ab\u52a8\u52a8\u529b\u5b66\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u7269\u7406\u7684\u51b2\u51fb\u7f13\u89e3\u56e0\u5b50(IMF)\u4f5c\u4e3a\u5956\u52b1\u9879\uff0c\u7ed3\u5408\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c(AMP)\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u673a\u5668\u4eba\u5b66\u4e60\u52a8\u7269\u7684\u663e\u6027\u8fd0\u52a8\u8f68\u8ff9\u548c\u9690\u6027\u88ab\u52a8\u52a8\u529b\u5b66\u3002", "result": "\u5728AMP\u548c\u624b\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u7ed3\u6784\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe32%\u7684\u8fd0\u8f93\u6210\u672c(CoT)\u6539\u5584\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6e90\u6548\u7387\u3002", "conclusion": "\u7ed3\u5408IMF\u548cAMP\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b66\u4e60\u52a8\u7269\u7684\u88ab\u52a8\u52a8\u529b\u5b66\u7279\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u5b9e\u73b0\u66f4\u81ea\u7136\u3001\u9ad8\u6548\u7684\u4eff\u751f\u8fd0\u52a8\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2510.09574", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.09574", "abs": "https://arxiv.org/abs/2510.09574", "authors": ["Daria de tinguy", "Tim Verbelen", "Emilio Gamba", "Bart Dhoedt"], "title": "Zero-shot Structure Learning and Planning for Autonomous Robot Navigation using Active Inference", "comment": "yet to be submitted", "summary": "Autonomous navigation in unfamiliar environments requires robots to\nsimultaneously explore, localise, and plan under uncertainty, without relying\non predefined maps or extensive training. We present a biologically inspired,\nActive Inference-based framework, Active Inference MAPping and Planning\n(AIMAPP). This model unifies mapping, localisation, and decision-making within\na single generative model. Inspired by hippocampal navigation, it uses\ntopological reasoning, place-cell encoding, and episodic memory to guide\nbehaviour. The agent builds and updates a sparse topological map online, learns\nstate transitions dynamically, and plans actions by minimising Expected Free\nEnergy. This allows it to balance goal-directed and exploratory behaviours. We\nimplemented a ROS-compatible navigation system that is sensor and\nrobot-agnostic, capable of integrating with diverse hardware configurations. It\noperates in a fully self-supervised manner, is resilient to drift, and supports\nboth exploration and goal-directed navigation without any pre-training. We\ndemonstrate robust performance in large-scale real and simulated environments\nagainst state-of-the-art planning models, highlighting the system's\nadaptability to ambiguous observations, environmental changes, and sensor\nnoise. The model offers a biologically inspired, modular solution to scalable,\nself-supervised navigation in unstructured settings. AIMAPP is available at\nhttps://github.com/decide-ugent/AIMAPP.", "AI": {"tldr": "AIMAPP\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u7684\u751f\u7269\u542f\u53d1\u5f0f\u81ea\u4e3b\u5bfc\u822a\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u5efa\u56fe\u3001\u5b9a\u4f4d\u548c\u51b3\u7b56\uff0c\u80fd\u591f\u5728\u964c\u751f\u73af\u5883\u4e2d\u8fdb\u884c\u5728\u7ebf\u62d3\u6251\u5efa\u56fe\u3001\u52a8\u6001\u5b66\u4e60\u72b6\u6001\u8f6c\u79fb\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u5316\u671f\u671b\u81ea\u7531\u80fd\u91cf\u6765\u89c4\u5212\u884c\u52a8\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u964c\u751f\u73af\u5883\u4e2d\u540c\u65f6\u8fdb\u884c\u63a2\u7d22\u3001\u5b9a\u4f4d\u548c\u89c4\u5212\u7684\u95ee\u9898\uff0c\u4e0d\u4f9d\u8d56\u9884\u5b9a\u4e49\u5730\u56fe\u6216\u5927\u91cf\u8bad\u7ec3\uff0c\u5b9e\u73b0\u5b8c\u5168\u81ea\u76d1\u7763\u7684\u5bfc\u822a\u3002", "method": "\u4f7f\u7528\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff0c\u7ed3\u5408\u6d77\u9a6c\u4f53\u5bfc\u822a\u7684\u751f\u7269\u542f\u53d1\u673a\u5236\uff0c\u5305\u62ec\u62d3\u6251\u63a8\u7406\u3001\u4f4d\u7f6e\u7ec6\u80de\u7f16\u7801\u548c\u60c5\u666f\u8bb0\u5fc6\uff0c\u6784\u5efa\u7a00\u758f\u62d3\u6251\u5730\u56fe\u5e76\u52a8\u6001\u5b66\u4e60\u72b6\u6001\u8f6c\u79fb\u3002", "result": "\u5728\u5927\u578b\u771f\u5b9e\u548c\u6a21\u62df\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u80fd\uff0c\u80fd\u591f\u9002\u5e94\u6a21\u7cca\u89c2\u6d4b\u3001\u73af\u5883\u53d8\u5316\u548c\u4f20\u611f\u5668\u566a\u58f0\uff0c\u652f\u6301\u63a2\u7d22\u548c\u76ee\u6807\u5bfc\u5411\u5bfc\u822a\u3002", "conclusion": "AIMAPP\u63d0\u4f9b\u4e86\u4e00\u4e2a\u751f\u7269\u542f\u53d1\u7684\u6a21\u5757\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u81ea\u76d1\u7763\u5bfc\u822a\u3002"}}
