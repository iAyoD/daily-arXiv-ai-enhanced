{"id": "2511.17540", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.17540", "abs": "https://arxiv.org/abs/2511.17540", "authors": ["Ryudai Iwakami", "Bo Peng", "Hiroyuki Hanyu", "Tasuku Ishigooka", "Takuya Azumi"], "title": "AUTOSAR AP and ROS 2 Collaboration Framework", "comment": "9 pages. This version includes minor \\lstlisting configuration adjustments for successful compilation. The page count is now nine pages due to the addition of author information. There are no other significant changes to the content or layout. Originally published at Euromicro Conference DSD 2024", "summary": "The field of autonomous vehicle research is advancing rapidly, necessitating platforms that meet real-time performance, safety, and security requirements for practical deployment. AUTOSAR Adaptive Platform (AUTOSAR AP) is widely adopted in development to meet these criteria; however, licensing constraints and tool implementation challenges limit its use in research. Conversely, Robot Operating System 2 (ROS 2) is predominantly used in research within the autonomous driving domain, leading to a disparity between research and development platforms that hinders swift commercialization. This paper proposes a collaboration framework that enables AUTOSAR AP and ROS 2 to communicate with each other using a Data Distribution Service for Real-Time Systems (DDS). In contrast, AUTOSAR AP uses Scalable service-Oriented Middleware over IP (SOME/IP) for communication. The proposed framework bridges these protocol differences, ensuring seamless interaction between the two platforms. We validate the functionality and performance of our bridge converter through empirical analysis, demonstrating its efficiency in conversion time and ease of integration with ROS 2 tools. Furthermore, the availability of the proposed collaboration framework is improved by automatically generating a configuration file for the proposed bridge converter.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u8fde\u63a5AUTOSAR AP\u548cROS 2\u7684\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7DDS\u534f\u8bae\u6865\u63a5\u901a\u4fe1\u5dee\u5f02\uff0c\u5b9e\u73b0\u4e24\u4e2a\u5e73\u53f0\u7684\u65e0\u7f1d\u4ea4\u4e92\u3002", "motivation": "AUTOSAR AP\u53d7\u8bb8\u53ef\u9650\u5236\u96be\u4ee5\u7528\u4e8e\u7814\u7a76\uff0c\u800cROS 2\u4e3b\u8981\u5728\u7814\u7a76\u4e2d\u4f7f\u7528\uff0c\u5bfc\u81f4\u7814\u53d1\u5e73\u53f0\u5dee\u5f02\u963b\u788d\u5546\u4e1a\u5316\u8fdb\u7a0b\u3002", "method": "\u4f7f\u7528DDS\u534f\u8bae\u6784\u5efa\u6865\u63a5\u8f6c\u6362\u5668\uff0c\u89e3\u51b3AUTOSAR AP\u7684SOME/IP\u4e0eROS 2\u4e4b\u95f4\u7684\u901a\u4fe1\u534f\u8bae\u5dee\u5f02\uff0c\u5e76\u81ea\u52a8\u751f\u6210\u914d\u7f6e\u6587\u4ef6\u3002", "result": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u9a8c\u8bc1\u4e86\u6865\u63a5\u8f6c\u6362\u5668\u7684\u529f\u80fd\u548c\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8f6c\u6362\u65f6\u95f4\u548c\u4e0eROS 2\u5de5\u5177\u96c6\u6210\u65b9\u9762\u7684\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u534f\u4f5c\u6846\u67b6\u6709\u6548\u8fde\u63a5\u4e86AUTOSAR AP\u548cROS 2\uff0c\u4fc3\u8fdb\u4e86\u7814\u53d1\u5e73\u53f0\u95f4\u7684\u65e0\u7f1d\u534f\u4f5c\uff0c\u52a0\u901f\u4e86\u5546\u4e1a\u5316\u8fdb\u7a0b\u3002"}}
{"id": "2511.17578", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17578", "abs": "https://arxiv.org/abs/2511.17578", "authors": ["Neelotpal Dutta", "Tianyu Zhang", "Tao Liu", "Yongxue Chen", "Charlie C. L. Wang"], "title": "Implicit Neural Field-Based Process Planning for Multi-Axis Manufacturing: Direct Control over Collision Avoidance and Toolpath Geometry", "comment": null, "summary": "Existing curved-layer-based process planning methods for multi-axis manufacturing address collisions only indirectly and generate toolpaths in a post-processing step, leaving toolpath geometry uncontrolled during optimization. We present an implicit neural field-based framework for multi-axis process planning that overcomes these limitations by embedding both layer generation and toolpath design within a single differentiable pipeline. Using sinusoidally activated neural networks to represent layers and toolpaths as implicit fields, our method enables direct evaluation of field values and derivatives at any spatial point, thereby allowing explicit collision avoidance and joint optimization of manufacturing layers and toolpaths. We further investigate how network hyperparameters and objective definitions influence singularity behavior and topology transitions, offering built-in mechanisms for regularization and stability control. The proposed approach is demonstrated on examples in both additive and subtractive manufacturing, validating its generality and effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u795e\u7ecf\u573a\u7684\u591a\u8f74\u5236\u9020\u5de5\u827a\u89c4\u5212\u6846\u67b6\uff0c\u5c06\u5c42\u751f\u6210\u548c\u5200\u5177\u8def\u5f84\u8bbe\u8ba1\u96c6\u6210\u5728\u5355\u4e2a\u53ef\u5fae\u5206\u7ba1\u9053\u4e2d\uff0c\u5b9e\u73b0\u76f4\u63a5\u78b0\u649e\u907f\u514d\u548c\u8054\u5408\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f2f\u66f2\u5c42\u7684\u591a\u8f74\u5236\u9020\u5de5\u827a\u89c4\u5212\u65b9\u6cd5\u53ea\u80fd\u95f4\u63a5\u5904\u7406\u78b0\u649e\u95ee\u9898\uff0c\u5e76\u5728\u540e\u5904\u7406\u6b65\u9aa4\u4e2d\u751f\u6210\u5200\u5177\u8def\u5f84\uff0c\u5bfc\u81f4\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u65e0\u6cd5\u63a7\u5236\u5200\u5177\u8def\u5f84\u51e0\u4f55\u5f62\u72b6\u3002", "method": "\u4f7f\u7528\u6b63\u5f26\u6fc0\u6d3b\u795e\u7ecf\u7f51\u7edc\u5c06\u5c42\u548c\u5200\u5177\u8def\u5f84\u8868\u793a\u4e3a\u9690\u5f0f\u573a\uff0c\u6784\u5efa\u53ef\u5fae\u5206\u7ba1\u9053\uff0c\u53ef\u76f4\u63a5\u8bc4\u4f30\u4efb\u610f\u7a7a\u95f4\u70b9\u7684\u573a\u503c\u548c\u5bfc\u6570\uff0c\u5b9e\u73b0\u663e\u5f0f\u78b0\u649e\u907f\u514d\u548c\u8054\u5408\u4f18\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u589e\u6750\u5236\u9020\u548c\u51cf\u6750\u5236\u9020\u793a\u4f8b\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u5176\u901a\u7528\u6027\u548c\u6709\u6548\u6027\uff0c\u80fd\u591f\u63a7\u5236\u5947\u70b9\u884c\u4e3a\u548c\u62d3\u6251\u8f6c\u6362\u3002", "conclusion": "\u63d0\u51fa\u7684\u9690\u5f0f\u795e\u7ecf\u573a\u6846\u67b6\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u591a\u8f74\u5236\u9020\u63d0\u4f9b\u4e86\u66f4\u76f4\u63a5\u548c\u53ef\u63a7\u7684\u5de5\u827a\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.17603", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.17603", "abs": "https://arxiv.org/abs/2511.17603", "authors": ["Chelsea-Xi Chen", "Zhe Zhang", "Aven-Le Zhou"], "title": "Translating Cultural Choreography from Humanoid Forms to Robotic Arm", "comment": null, "summary": "Robotic arm choreography often reproduces trajectories while missing cultural semantics. This study examines whether symbolic posture transfer with joint space compatible notation can preserve semantic fidelity on a six-degree-of-freedom arm and remain portable across morphologies. We implement ROPERA, a three-stage pipeline for encoding culturally codified postures, composing symbolic sequences, and decoding to servo commands. A scene from Kunqu opera, \\textit{The Peony Pavilion}, serves as the material for evaluation. The procedure includes corpus-based posture selection, symbolic scoring, direct joint angle execution, and a visual layer with light painting and costume-informed colors. Results indicate reproducible execution with intended timing and cultural legibility reported by experts and audiences. The study points to non-anthropocentric cultural preservation and portable authoring workflows. Future work will design dance-informed transition profiles, extend the notation to locomotion with haptic, musical, and spatial cues, and test portability across platforms.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86ROPERA\u7cfb\u7edf\uff0c\u901a\u8fc7\u7b26\u53f7\u59ff\u6001\u8f6c\u79fb\u548c\u5173\u8282\u7a7a\u95f4\u517c\u5bb9\u7b26\u53f7\u6765\u4fdd\u6301\u6587\u5316\u8bed\u4e49\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u516d\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u4e0a\u5b9e\u73b0\u6606\u66f2\u300a\u7261\u4e39\u4ead\u300b\u7684\u821e\u8e48\u52a8\u4f5c\u91cd\u73b0\u3002", "motivation": "\u673a\u5668\u4eba\u624b\u81c2\u7f16\u821e\u901a\u5e38\u53ea\u91cd\u73b0\u8f68\u8ff9\u800c\u7f3a\u4e4f\u6587\u5316\u8bed\u4e49\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4fdd\u6301\u6587\u5316\u8bed\u4e49\u4fdd\u771f\u5ea6\u4e14\u8de8\u5f62\u6001\u53ef\u79fb\u690d\u7684\u65b9\u6cd5\u3002", "method": "\u5b9e\u73b0ROPERA\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a\u7f16\u7801\u6587\u5316\u7f16\u7801\u59ff\u6001\u3001\u7ec4\u5408\u7b26\u53f7\u5e8f\u5217\u3001\u89e3\u7801\u4e3a\u4f3a\u670d\u547d\u4ee4\u3002\u4f7f\u7528\u6606\u66f2\u300a\u7261\u4e39\u4ead\u300b\u573a\u666f\u4f5c\u4e3a\u8bc4\u4f30\u6750\u6599\uff0c\u5305\u62ec\u57fa\u4e8e\u8bed\u6599\u5e93\u7684\u59ff\u6001\u9009\u62e9\u3001\u7b26\u53f7\u8bb0\u8c31\u3001\u76f4\u63a5\u5173\u8282\u89d2\u5ea6\u6267\u884c\u4ee5\u53ca\u5e26\u6709\u5149\u7ed8\u548c\u670d\u88c5\u8272\u5f69\u7684\u53ef\u89c6\u5316\u5c42\u3002", "result": "\u7ed3\u679c\u663e\u793a\u53ef\u91cd\u73b0\u7684\u6267\u884c\u6548\u679c\uff0c\u5177\u6709\u9884\u671f\u7684\u65f6\u95f4\u5b89\u6392\uff0c\u4e13\u5bb6\u548c\u89c2\u4f17\u62a5\u544a\u4e86\u6587\u5316\u53ef\u8bfb\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6307\u5411\u975e\u4eba\u7c7b\u4e2d\u5fc3\u7684\u6587\u5316\u4fdd\u62a4\u548c\u53ef\u79fb\u690d\u7684\u521b\u4f5c\u5de5\u4f5c\u6d41\u7a0b\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u8bbe\u8ba1\u821e\u8e48\u542f\u53d1\u7684\u8fc7\u6e21\u8f6e\u5ed3\uff0c\u5c06\u7b26\u53f7\u6269\u5c55\u5230\u5177\u6709\u89e6\u89c9\u3001\u97f3\u4e50\u548c\u7a7a\u95f4\u7ebf\u7d22\u7684\u79fb\u52a8\uff0c\u5e76\u6d4b\u8bd5\u8de8\u5e73\u53f0\u7684\u53ef\u79fb\u690d\u6027\u3002"}}
{"id": "2511.17608", "categories": ["cs.RO", "physics.optics"], "pdf": "https://arxiv.org/pdf/2511.17608", "abs": "https://arxiv.org/abs/2511.17608", "authors": ["Yunlong Guo", "John Canning", "Zenon Chaczko", "Gang-Ding Peng"], "title": "Robot joint characterisation and control using a magneto-optical rotary encoder", "comment": null, "summary": "A robust and compact magneto-optical rotary encoder for the characterisation of robotic rotary joints is demonstrated. The system employs magnetic field-induced optical attenuation in a double-pass configuration using rotating nonuniform magnets around an optical circulator operating in reflection. The encoder tracks continuous 360\u00b0 rotation with rotation sweep rates from \u03bd = 135 \u00b0/s to \u03bd = 370 \u00b0/s, and an angular resolution of \u0394\u03b8 = 0.3\u00b0. This offers a low-cost and reliable alternative to conventional robot rotation encoders while maintaining competitive performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u4eba\u65cb\u8f6c\u5173\u8282\u8868\u5f81\u7684\u7d27\u51d1\u578b\u78c1\u5149\u65cb\u8f6c\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u78c1\u573a\u8bf1\u5bfc\u5149\u5b66\u8870\u51cf\u5b9e\u73b0360\u00b0\u8fde\u7eed\u65cb\u8f6c\u8ddf\u8e2a\uff0c\u5177\u6709\u4f4e\u6210\u672c\u548c\u9ad8\u53ef\u9760\u6027\u3002", "motivation": "\u4e3a\u673a\u5668\u4eba\u65cb\u8f6c\u5173\u8282\u63d0\u4f9b\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u53ef\u9760\u7684\u66ff\u4ee3\u4f20\u7edf\u65cb\u8f6c\u7f16\u7801\u5668\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u80fd\u3002", "method": "\u91c7\u7528\u53cc\u901a\u914d\u7f6e\uff0c\u5229\u7528\u65cb\u8f6c\u975e\u5747\u5300\u78c1\u4f53\u5728\u5149\u5b66\u73af\u884c\u5668\u53cd\u5c04\u6a21\u5f0f\u4e0b\u4ea7\u751f\u78c1\u573a\u8bf1\u5bfc\u7684\u5149\u5b66\u8870\u51cf\u6548\u5e94\u3002", "result": "\u7f16\u7801\u5668\u80fd\u591f\u8ddf\u8e2a360\u00b0\u8fde\u7eed\u65cb\u8f6c\uff0c\u65cb\u8f6c\u626b\u63cf\u901f\u7387\u4ece135\u00b0/s\u5230370\u00b0/s\uff0c\u89d2\u5206\u8fa8\u7387\u4e3a0.3\u00b0\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u4f20\u7edf\u673a\u5668\u4eba\u65cb\u8f6c\u7f16\u7801\u5668\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f4e\u6210\u672c\u4e14\u53ef\u9760\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u6027\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2511.17559", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.17559", "abs": "https://arxiv.org/abs/2511.17559", "authors": ["Gyubok Lee", "Woosog Chay", "Edward Choi"], "title": "SCARE: A Benchmark for SQL Correction and Question Answerability Classification for Reliable EHR Question Answering", "comment": "ML4H 2025 Proceedings", "summary": "Recent advances in Large Language Models (LLMs) have enabled the development of text-to-SQL models that allow clinicians to query structured data stored in Electronic Health Records (EHRs) using natural language. However, deploying these models for EHR question answering (QA) systems in safety-critical clinical environments remains challenging: incorrect SQL queries-whether caused by model errors or problematic user inputs-can undermine clinical decision-making and jeopardize patient care. While prior work has mainly focused on improving SQL generation accuracy or filtering questions before execution, there is a lack of a unified benchmark for evaluating independent post-hoc verification mechanisms (i.e., a component that inspects and validates the generated SQL before execution), which is crucial for safe deployment. To fill this gap, we introduce SCARE, a benchmark for evaluating methods that function as a post-hoc safety layer in EHR QA systems. SCARE evaluates the joint task of (1) classifying question answerability (i.e., determining whether a question is answerable, ambiguous, or unanswerable) and (2) verifying or correcting candidate SQL queries. The benchmark comprises 4,200 triples of questions, candidate SQL queries, and expected model outputs, grounded in the MIMIC-III, MIMIC-IV, and eICU databases. It covers a diverse set of questions and corresponding candidate SQL queries generated by seven different text-to-SQL models, ensuring a realistic and challenging evaluation. Using SCARE, we benchmark a range of approaches-from two-stage methods to agentic frameworks. Our experiments reveal a critical trade-off between question classification and SQL error correction, highlighting key challenges and outlining directions for future research.", "AI": {"tldr": "SCARE\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u95ee\u7b54\u7cfb\u7edf\u4e2d\u540e\u7f6e\u5b89\u5168\u5c42\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u6ce8\u4e8e\u95ee\u9898\u53ef\u56de\u7b54\u6027\u5206\u7c7b\u548cSQL\u67e5\u8be2\u9a8c\u8bc1/\u4fee\u6b63\u7684\u8054\u5408\u4efb\u52a1\u3002", "motivation": "\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u90e8\u7f72\u6587\u672c\u5230SQL\u6a21\u578b\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u9519\u8bef\u7684SQL\u67e5\u8be2\u53ef\u80fd\u5371\u53ca\u60a3\u8005\u62a4\u7406\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9\u540e\u7f6e\u9a8c\u8bc1\u673a\u5236\u7684\u7edf\u4e00\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u5305\u542b4,200\u4e2a\u95ee\u9898-SQL\u67e5\u8be2-\u671f\u671b\u8f93\u51fa\u7684\u4e09\u5143\u7ec4\u6570\u636e\u96c6\uff0c\u57fa\u4e8eMIMIC-III\u3001MIMIC-IV\u548ceICU\u6570\u636e\u5e93\uff0c\u6db5\u76d67\u79cd\u4e0d\u540c\u6587\u672c\u5230SQL\u6a21\u578b\u751f\u6210\u7684\u591a\u6837\u5316\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u4e86\u95ee\u9898\u5206\u7c7b\u548cSQL\u9519\u8bef\u4fee\u6b63\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "conclusion": "SCARE\u57fa\u51c6\u586b\u8865\u4e86\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u95ee\u7b54\u7cfb\u7edf\u5b89\u5168\u90e8\u7f72\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u540e\u7f6e\u9a8c\u8bc1\u673a\u5236\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2511.17720", "categories": ["cs.RO", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2511.17720", "abs": "https://arxiv.org/abs/2511.17720", "authors": ["Sean Cowan", "Pietro Fanti", "Leon B. S. Williams", "Chit Hong Yam", "Kaneyasu Asakuma", "Yuichiro Nada", "Dario Izzo"], "title": "Vision-Guided Optic Flow Navigation for Small Lunar Missions", "comment": null, "summary": "Private lunar missions are faced with the challenge of robust autonomous navigation while operating under stringent constraints on mass, power, and computational resources. This work proposes a motion-field inversion framework that uses optical flow and rangefinder-based depth estimation as a lightweight CPU-based solution for egomotion estimation during lunar descent. We extend classical optical flow formulations by integrating them with depth modeling strategies tailored to the geometry for lunar/planetary approach, descent, and landing, specifically, planar and spherical terrain approximations parameterized by a laser rangefinder. Motion field inversion is performed through a least-squares framework, using sparse optical flow features extracted via the pyramidal Lucas-Kanade algorithm. We verify our approach using synthetically generated lunar images over the challenging terrain of the lunar south pole, using CPU budgets compatible with small lunar landers. The results demonstrate accurate velocity estimation from approach to landing, with sub-10% error for complex terrain and on the order of 1% for more typical terrain, as well as performances suitable for real-time applications. This framework shows promise for enabling robust, lightweight on-board navigation for small lunar missions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5149\u6d41\u548c\u6d4b\u8ddd\u4eea\u6df1\u5ea6\u4f30\u8ba1\u7684\u8fd0\u52a8\u573a\u53cd\u6f14\u6846\u67b6\uff0c\u7528\u4e8e\u6708\u7403\u7740\u9646\u8fc7\u7a0b\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\uff0c\u80fd\u591f\u5728\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5b9e\u73b0\u5b9e\u65f6\u3001\u51c6\u786e\u7684\u59ff\u6001\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u79c1\u4eba\u6708\u7403\u4efb\u52a1\u5728\u8d28\u91cf\u3001\u529f\u8017\u548c\u8ba1\u7b97\u8d44\u6e90\u4e25\u683c\u9650\u5236\u4e0b\u5b9e\u73b0\u9c81\u68d2\u81ea\u4e3b\u5bfc\u822a\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5c0f\u578b\u6708\u7403\u7740\u9646\u5668\u7684\u9700\u6c42\u3002", "method": "\u6269\u5c55\u7ecf\u5178\u5149\u6d41\u516c\u5f0f\uff0c\u7ed3\u5408\u9488\u5bf9\u6708\u7403/\u884c\u661f\u63a5\u8fd1\u3001\u4e0b\u964d\u548c\u7740\u9646\u51e0\u4f55\u5f62\u72b6\u5b9a\u5236\u7684\u6df1\u5ea6\u5efa\u6a21\u7b56\u7565\uff0c\u4f7f\u7528\u6fc0\u5149\u6d4b\u8ddd\u4eea\u53c2\u6570\u5316\u7684\u5e73\u9762\u548c\u7403\u5f62\u5730\u5f62\u8fd1\u4f3c\uff0c\u901a\u8fc7\u6700\u5c0f\u4e8c\u4e58\u6846\u67b6\u8fdb\u884c\u8fd0\u52a8\u573a\u53cd\u6f14\u3002", "result": "\u5728\u6708\u7403\u5357\u6781\u590d\u6742\u5730\u5f62\u4e0a\u7684\u6d4b\u8bd5\u663e\u793a\uff0c\u4ece\u63a5\u8fd1\u5230\u7740\u9646\u9636\u6bb5\u90fd\u80fd\u51c6\u786e\u4f30\u8ba1\u901f\u5ea6\uff0c\u590d\u6742\u5730\u5f62\u8bef\u5dee\u4f4e\u4e8e10%\uff0c\u5178\u578b\u5730\u5f62\u8bef\u5dee\u7ea6\u4e3a1%\uff0c\u6027\u80fd\u9002\u5408\u5b9e\u65f6\u5e94\u7528\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u671b\u4e3a\u5c0f\u578b\u6708\u7403\u4efb\u52a1\u63d0\u4f9b\u9c81\u68d2\u3001\u8f7b\u91cf\u7ea7\u7684\u673a\u8f7d\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.17560", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17560", "abs": "https://arxiv.org/abs/2511.17560", "authors": ["Yuechi Zhou", "Yi Su", "Jianxin Zhang", "Juntao Li", "Qingrong Xia", "Zhefeng Wang", "Xinyu Duan", "Baoxing Huai"], "title": "$A^3$: Attention-Aware Accurate KV Cache Fusion for Fast Large Language Model Serving", "comment": null, "summary": "Large language models (LLMs) have demonstrated strong capabilities in processing long contexts, enabling them to tackle tasks involving long textual inputs such as multi-turn conversations, legal documents, or retrieved documents in Retrieval-Augmented Generation (RAG) systems. However, despite their ability to handle long sequences, the resulting decoding latency and memory overhead remain substantial, posing challenges for real-world deployment. Recent advances in KV Cache reuse have shown potential to mitigate these costs, but still suffer from notable performance degradation. To address this issue, we conduct an in-depth investigation of recomputation-based reuse methods and observe that the recomputed tokens often fail to align with the context segments most relevant to the question. This misalignment hinders proper updates to the critical contextual representations. Therefore, we propose the $\\textbf{A}$ttention-$\\textbf{A}$ware $\\textbf{A}$ccurate KV Cache Fusion algorithm ($A^3$), which precomputes and selectively fuses the KV Cache of text chunks based on their relevance to the question, achieving accurate integration with minimal computational overhead. Extensive experiments on various benchmarks and LLMs demonstrate that $A^3$ achieves the best task performance compared to four baselines while reducing the time-to-first-token (TTFT) by 2$\\times$.", "AI": {"tldr": "\u63d0\u51faA\u00b3\u7b97\u6cd5\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u548c\u9009\u62e9\u6027\u878d\u5408\u6587\u672c\u5757\u7684KV\u7f13\u5b58\uff0c\u5728\u51cf\u5c11\u89e3\u7801\u5ef6\u8fdf\u7684\u540c\u65f6\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u5904\u7406\u957f\u4e0a\u4e0b\u6587\uff0c\u4f46\u89e3\u7801\u5ef6\u8fdf\u548c\u5185\u5b58\u5f00\u9500\u4ecd\u7136\u5f88\u5927\uff0c\u73b0\u6709KV\u7f13\u5b58\u91cd\u7528\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u6ce8\u610f\u529b\u611f\u77e5\u7684\u51c6\u786eKV\u7f13\u5b58\u878d\u5408\u7b97\u6cd5(A\u00b3)\uff0c\u57fa\u4e8e\u95ee\u9898\u76f8\u5173\u6027\u9884\u8ba1\u7b97\u548c\u9009\u62e9\u6027\u878d\u5408\u6587\u672c\u5757\u7684KV\u7f13\u5b58\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548cLLM\u4e0a\uff0cA\u00b3\u76f8\u6bd4\u56db\u4e2a\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u6700\u4f73\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u9996token\u65f6\u95f4\u51cf\u5c112\u500d\u3002", "conclusion": "A\u00b3\u7b97\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u7684\u5ef6\u8fdf\u95ee\u9898\uff0c\u5b9e\u73b0\u51c6\u786eKV\u7f13\u5b58\u878d\u5408\u5e76\u663e\u8457\u63d0\u5347\u6548\u7387\u3002"}}
{"id": "2511.17765", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.17765", "abs": "https://arxiv.org/abs/2511.17765", "authors": ["Darren Chiu", "Zhehui Huang", "Ruohai Ge", "Gaurav S. Sukhatme"], "title": "LEARN: Learning End-to-End Aerial Resource-Constrained Multi-Robot Navigation", "comment": "20 pages, 15 figures", "summary": "Nano-UAV teams offer great agility yet face severe navigation challenges due to constrained onboard sensing, communication, and computation. Existing approaches rely on high-resolution vision or compute-intensive planners, rendering them infeasible for these platforms. We introduce LEARN, a lightweight, two-stage safety-guided reinforcement learning (RL) framework for multi-UAV navigation in cluttered spaces. Our system combines low-resolution Time-of-Flight (ToF) sensors and a simple motion planner with a compact, attention-based RL policy. In simulation, LEARN outperforms two state-of-the-art planners by $10\\%$ while using substantially fewer resources. We demonstrate LEARN's viability on six Crazyflie quadrotors, achieving fully onboard flight in diverse indoor and outdoor environments at speeds up to $2.0 m/s$ and traversing $0.2 m$ gaps.", "AI": {"tldr": "LEARN\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4e24\u9636\u6bb5\u5b89\u5168\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u65e0\u4eba\u673a\u5728\u590d\u6742\u7a7a\u95f4\u4e2d\u7684\u5bfc\u822a\uff0c\u7ed3\u5408\u4f4e\u5206\u8fa8\u7387ToF\u4f20\u611f\u5668\u548c\u7d27\u51d1\u7684\u6ce8\u610f\u529bRL\u7b56\u7565\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7eb3\u7c73\u65e0\u4eba\u673a\u4e0a\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u7eb3\u7c73\u65e0\u4eba\u673a\u56e2\u961f\u5177\u6709\u9ad8\u654f\u6377\u6027\uff0c\u4f46\u53d7\u9650\u4e8e\u673a\u8f7d\u4f20\u611f\u3001\u901a\u4fe1\u548c\u8ba1\u7b97\u80fd\u529b\uff0c\u73b0\u6709\u57fa\u4e8e\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u6216\u8ba1\u7b97\u5bc6\u96c6\u578b\u89c4\u5212\u5668\u7684\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u8fd9\u4e9b\u5e73\u53f0\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5b89\u5168\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4f4e\u5206\u8fa8\u7387\u98de\u884c\u65f6\u95f4\u4f20\u611f\u5668\u3001\u7b80\u5355\u8fd0\u52a8\u89c4\u5212\u5668\u548c\u7d27\u51d1\u7684\u6ce8\u610f\u529b\u673a\u5236RL\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u4e2d\u6027\u80fd\u4f18\u4e8e\u4e24\u79cd\u6700\u5148\u8fdb\u89c4\u5212\u566810%\uff0c\u8d44\u6e90\u6d88\u8017\u663e\u8457\u51cf\u5c11\uff1b\u57286\u67b6Crazyflie\u56db\u65cb\u7ffc\u4e0a\u5b9e\u73b0\u5168\u673a\u8f7d\u98de\u884c\uff0c\u5728\u5ba4\u5185\u5916\u73af\u5883\u4e2d\u901f\u5ea6\u8fbe2.0m/s\uff0c\u53ef\u7a7f\u8d8a0.2m\u95f4\u9699\u3002", "conclusion": "LEARN\u6846\u67b6\u8bc1\u660e\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7eb3\u7c73\u65e0\u4eba\u673a\u5e73\u53f0\u4e0a\u5b9e\u73b0\u9ad8\u6548\u591a\u673a\u5bfc\u822a\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5c0f\u578b\u65e0\u4eba\u673a\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.17561", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17561", "abs": "https://arxiv.org/abs/2511.17561", "authors": ["Huimin Ren", "Yan Liang", "Baiqiao Su", "Chaobo Sun", "Hengtong Lu", "Kaike Zhang", "Chen Wei"], "title": "LexInstructEval: Lexical Instruction Following Evaluation for Large Language Models", "comment": null, "summary": "The ability of Large Language Models (LLMs) to precisely follow complex and fine-grained lexical instructions is a cornerstone of their utility and controllability. However, evaluating this capability remains a significant challenge. Current methods either rely on subjective and costly human evaluation or on automated LLM-as-a-judge systems, which suffer from inherent biases and unreliability. Existing programmatic benchmarks, while objective, often lack the expressiveness to test intricate, compositional constraints at a granular level. To address these limitations, we introduce LexInstructEval, a new benchmark and evaluation framework for fine-grained lexical instruction following. Our framework is built upon a formal, rule-based grammar that deconstructs complex instructions into a canonical <Procedure, Relation, Value> triplet. This grammar enables the systematic generation of a diverse dataset through a multi-stage, human-in-the-loop pipeline and facilitates objective verification via a transparent, programmatic engine. We release our dataset and open-source evaluation tools to facilitate further research into the controllability and reliability of LLMs.", "AI": {"tldr": "LexInstructEval\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u8bcd\u6c47\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u57fa\u4e8e\u89c4\u5219\u7684\u8bed\u6cd5\u5c06\u590d\u6742\u6307\u4ee4\u89e3\u6784\u4e3a<\u8fc7\u7a0b\u3001\u5173\u7cfb\u3001\u503c>\u4e09\u5143\u7ec4\uff0c\u5b9e\u73b0\u5ba2\u89c2\u9a8c\u8bc1\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30LLM\u9075\u5faa\u590d\u6742\u7ec6\u7c92\u5ea6\u8bcd\u6c47\u6307\u4ee4\u80fd\u529b\u7684\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4eba\u5de5\u8bc4\u4f30\u4e3b\u89c2\u4e14\u6602\u8d35\uff0c\u81ea\u52a8LLM-as-a-judge\u7cfb\u7edf\u5b58\u5728\u504f\u89c1\u548c\u4e0d\u53ef\u9760\u6027\uff0c\u73b0\u6709\u7a0b\u5e8f\u5316\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u8868\u8fbe\u590d\u6742\u7ec4\u5408\u7ea6\u675f\u7684\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u6b63\u5f0f\u7684\u57fa\u4e8e\u89c4\u5219\u8bed\u6cd5\uff0c\u5c06\u590d\u6742\u6307\u4ee4\u89e3\u6784\u4e3a<\u8fc7\u7a0b\u3001\u5173\u7cfb\u3001\u503c>\u4e09\u5143\u7ec4\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u4eba\u5de5\u53c2\u4e0e\u6d41\u7a0b\u7cfb\u7edf\u751f\u6210\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u900f\u660e\u7684\u7a0b\u5e8f\u5316\u5f15\u64ce\u8fdb\u884c\u5ba2\u89c2\u9a8c\u8bc1\u3002", "result": "\u5f00\u53d1\u4e86LexInstructEval\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u5305\u542b\u6570\u636e\u96c6\u548c\u5f00\u6e90\u8bc4\u4f30\u5de5\u5177\uff0c\u652f\u6301\u5bf9LLM\u53ef\u63a7\u6027\u548c\u53ef\u9760\u6027\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002", "conclusion": "LexInstructEval\u4e3a\u89e3\u51b3LLM\u7ec6\u7c92\u5ea6\u8bcd\u6c47\u6307\u4ee4\u9075\u5faa\u8bc4\u4f30\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u3001\u5ba2\u89c2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4fc3\u8fdb\u4e86LLM\u53ef\u63a7\u6027\u548c\u53ef\u9760\u6027\u7814\u7a76\u7684\u53d1\u5c55\u3002"}}
{"id": "2511.17774", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17774", "abs": "https://arxiv.org/abs/2511.17774", "authors": ["Salma Mozaffari", "Daniel Ruan", "William van den Bogert", "Nima Fazeli", "Sigrid Adriaenssens", "Arash Adel"], "title": "Learning Diffusion Policies for Robotic Manipulation of Timber Joinery under Fabrication Uncertainty", "comment": null, "summary": "Construction uncertainties such as fabrication inaccuracies and material imperfections pose a significant challenge to contact-rich robotic manipulation by hindering precise and robust assembly. In this paper, we explore the performance and robustness of diffusion policy learning as a promising solution for contact-sensitive robotic assembly at construction scale, using timber mortise and tenon joints as a case study. A two-phase study is conducted: first, to evaluate policy performance and applicability; second, to assess robustness in handling fabrication uncertainties simulated as randomized perturbations to the mortise position. The best-performing policy achieved a total average success rate of 75% with perturbations up to 10 mm, including 100% success in unperturbed cases. The results demonstrate the potential of sensory-motor diffusion policies to generalize to a wide range of complex, contact-rich assembly tasks across construction and manufacturing, advancing robotic construction under uncertainty and contributing to safer, more efficient building practices.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u6269\u6563\u7b56\u7565\u5b66\u4e60\u5728\u5efa\u7b51\u5c3a\u5ea6\u63a5\u89e6\u654f\u611f\u673a\u5668\u4eba\u88c5\u914d\u4e2d\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u4ee5\u6728\u5de5\u69ab\u536f\u63a5\u5934\u4e3a\u6848\u4f8b\uff0c\u5728\u5b58\u5728\u5236\u9020\u4e0d\u786e\u5b9a\u6027\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e8675%\u7684\u5e73\u5747\u6210\u529f\u7387\u3002", "motivation": "\u5efa\u7b51\u4e2d\u7684\u5236\u9020\u8bef\u5dee\u548c\u6750\u6599\u7f3a\u9677\u7b49\u4e0d\u786e\u5b9a\u6027\u7ed9\u63a5\u89e6\u5bc6\u96c6\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\uff0c\u963b\u788d\u4e86\u7cbe\u786e\u548c\u7a33\u5065\u7684\u88c5\u914d\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7814\u7a76\uff1a\u9996\u5148\u8bc4\u4f30\u7b56\u7565\u6027\u80fd\u548c\u9002\u7528\u6027\uff0c\u5176\u6b21\u8bc4\u4f30\u5904\u7406\u5236\u9020\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\uff0c\u901a\u8fc7\u968f\u673a\u6270\u52a8\u69ab\u773c\u4f4d\u7f6e\u6765\u6a21\u62df\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u6700\u4f73\u7b56\u7565\u5728\u9ad8\u8fbe10\u6beb\u7c73\u7684\u6270\u52a8\u4e0b\u5b9e\u73b0\u4e8675%\u7684\u603b\u5e73\u5747\u6210\u529f\u7387\uff0c\u5176\u4e2d\u65e0\u6270\u52a8\u60c5\u51b5\u4e0b\u8fbe\u5230100%\u6210\u529f\u7387\u3002", "conclusion": "\u7ed3\u679c\u8868\u660e\u611f\u89c9\u8fd0\u52a8\u6269\u6563\u7b56\u7565\u6709\u6f5c\u529b\u63a8\u5e7f\u5230\u5efa\u7b51\u548c\u5236\u9020\u4e1a\u4e2d\u5404\u79cd\u590d\u6742\u3001\u63a5\u89e6\u5bc6\u96c6\u7684\u88c5\u914d\u4efb\u52a1\uff0c\u63a8\u52a8\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u673a\u5668\u4eba\u5efa\u7b51\u53d1\u5c55\uff0c\u4fc3\u8fdb\u66f4\u5b89\u5168\u3001\u66f4\u9ad8\u6548\u7684\u5efa\u7b51\u5b9e\u8df5\u3002"}}
{"id": "2511.17562", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17562", "abs": "https://arxiv.org/abs/2511.17562", "authors": ["Wei Tian", "YuhaoZhou"], "title": "ChineseErrorCorrector3-4B: State-of-the-Art Chinese Spelling and Grammar Corrector", "comment": null, "summary": "This paper introduces ChineseErrorCorrector3-4B, a unified model for Chinese spelling and grammatical error correction based on Qwen3-4B. The model demonstrates outstanding performance in general text correction tasks and achieves state-of-the-art results in both spelling correction (CSC) and grammatical correction (CGC). On several authoritative benchmark datasets -- including SIGHAN-2015, EC-LAW, MCSC, and NaCGEC -- the model's F1 and F0.5 scores significantly surpass existing publicly available models, ranking first in both spelling and grammatical error correction tasks.", "AI": {"tldr": "\u57fa\u4e8eQwen3-4B\u5f00\u53d1\u7684\u4e2d\u6587\u62fc\u5199\u548c\u8bed\u6cd5\u7ea0\u9519\u7edf\u4e00\u6a21\u578bChineseErrorCorrector3-4B\uff0c\u5728\u591a\u4e2a\u6743\u5a01\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u4e2d\u6587\u62fc\u5199\u548c\u8bed\u6cd5\u7ea0\u9519\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u4e2d\u6587\u6587\u672c\u7ea0\u9519\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u57fa\u4e8eQwen3-4B\u6784\u5efa\u7edf\u4e00\u7684\u4e2d\u6587\u62fc\u5199\u548c\u8bed\u6cd5\u7ea0\u9519\u6a21\u578bChineseErrorCorrector3-4B\u3002", "result": "\u5728SIGHAN-2015\u3001EC-LAW\u3001MCSC\u548cNaCGEC\u7b49\u6743\u5a01\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u6a21\u578b\u7684F1\u548cF0.5\u5206\u6570\u663e\u8457\u8d85\u8fc7\u73b0\u6709\u516c\u5f00\u6a21\u578b\uff0c\u5728\u62fc\u5199\u548c\u8bed\u6cd5\u7ea0\u9519\u4efb\u52a1\u4e2d\u5747\u6392\u540d\u7b2c\u4e00\u3002", "conclusion": "ChineseErrorCorrector3-4B\u5728\u4e2d\u6587\u62fc\u5199\u548c\u8bed\u6cd5\u7ea0\u9519\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u662f\u76ee\u524d\u6027\u80fd\u6700\u4f18\u7684\u516c\u5f00\u6a21\u578b\u3002"}}
{"id": "2511.17777", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17777", "abs": "https://arxiv.org/abs/2511.17777", "authors": ["Ravi Prakash", "Vincent Y. Wang", "Arpit Mishra", "Devi Yuliarti", "Pei Zhong", "Ryan P. McNabb", "Patrick J. Codd", "Leila J. Bridgeman"], "title": "See, Plan, Cut: MPC-Based Autonomous Volumetric Robotic Laser Surgery with OCT Guidance", "comment": "9 pages, 8 figures", "summary": "Robotic laser systems offer the potential for sub-millimeter, non-contact, high-precision tissue resection, yet existing platforms lack volumetric planning and intraoperative feedback. We present RATS (Robot-Assisted Tissue Surgery), an intelligent opto-mechanical, optical coherence tomography (OCT)-guided robotic platform designed for autonomous volumetric soft tissue resection in surgical applications. RATS integrates macro-scale RGB-D imaging, micro-scale OCT, and a fiber-coupled surgical laser, calibrated through a novel multistage alignment pipeline that achieves OCT-to-laser calibration accuracy of 0.161+-0.031mm on tissue phantoms and ex vivo porcine tissue. A super-Gaussian laser-tissue interaction (LTI) model characterizes ablation crater morphology with an average RMSE of 0.231+-0.121mm, outperforming Gaussian baselines. A sampling-based model predictive control (MPC) framework operates directly on OCT voxel data to generate constraint-aware resection trajectories with closed-loop feedback, achieving 0.842mm RMSE and improving intersection-over-union agreement by 64.8% compared to feedforward execution. With OCT, RATS detects subsurface structures and modifies the planner's objective to preserve them, demonstrating clinical feasibility.", "AI": {"tldr": "RATS\u662f\u4e00\u4e2a\u667a\u80fd\u5149\u5b66\u673a\u68b0\u5e73\u53f0\uff0c\u96c6\u6210OCT\u5f15\u5bfc\u548c\u624b\u672f\u6fc0\u5149\uff0c\u7528\u4e8e\u81ea\u4e3b\u8f6f\u7ec4\u7ec7\u4f53\u79ef\u5207\u9664\uff0c\u901a\u8fc7\u591a\u7ea7\u6821\u51c6\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u624b\u672f\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u6fc0\u5149\u7cfb\u7edf\u7f3a\u4e4f\u4f53\u79ef\u89c4\u5212\u548c\u672f\u4e2d\u53cd\u9988\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8fdb\u884c\u81ea\u4e3b\u4f53\u79ef\u8f6f\u7ec4\u7ec7\u5207\u9664\u7684\u667a\u80fd\u624b\u672f\u5e73\u53f0\u3002", "method": "\u96c6\u6210RGB-D\u6210\u50cf\u3001OCT\u548c\u5149\u7ea4\u8026\u5408\u624b\u672f\u6fc0\u5149\uff0c\u91c7\u7528\u591a\u7ea7\u6821\u51c6\u7ba1\u9053\uff0c\u4f7f\u7528\u8d85\u9ad8\u65af\u6fc0\u5149-\u7ec4\u7ec7\u76f8\u4e92\u4f5c\u7528\u6a21\u578b\uff0c\u5e76\u57fa\u4e8e\u91c7\u6837\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u76f4\u63a5\u5728OCT\u4f53\u7d20\u6570\u636e\u4e0a\u64cd\u4f5c\u3002", "result": "OCT\u5230\u6fc0\u5149\u6821\u51c6\u7cbe\u5ea6\u8fbe0.161\u00b10.031mm\uff0c\u6fc0\u5149-\u7ec4\u7ec7\u76f8\u4e92\u4f5c\u7528\u6a21\u578b\u5e73\u5747RMSE\u4e3a0.231\u00b10.121mm\uff0c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5b9e\u73b00.842mm RMSE\uff0c\u76f8\u6bd4\u524d\u9988\u6267\u884c\u63d0\u9ad8IoU\u4e00\u81f4\u602764.8%\u3002", "conclusion": "RATS\u80fd\u591f\u68c0\u6d4b\u4e9a\u8868\u9762\u7ed3\u6784\u5e76\u4fee\u6539\u89c4\u5212\u76ee\u6807\u4ee5\u4fdd\u62a4\u8fd9\u4e9b\u7ed3\u6784\uff0c\u5c55\u793a\u4e86\u4e34\u5e8a\u53ef\u884c\u6027\u3002"}}
{"id": "2511.17565", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17565", "abs": "https://arxiv.org/abs/2511.17565", "authors": ["Sarthak Chakraborty", "Suman Nath", "Xuchao Zhang", "Chetan Bansal", "Indranil Gupta"], "title": "Generative Caching for Structurally Similar Prompts and Responses", "comment": null, "summary": "Large Language Models (LLMs) are increasingly being used to plan, reason, and execute tasks across diverse scenarios. In use cases like repeatable workflows and agentic settings, prompts are often reused with minor variations while having a similar structure for recurring tasks. This opens up opportunities for caching. However, exact prompt matching fails on such structurally similar prompts, while semantic caching may produce incorrect responses by ignoring critical differences. To address this, we introduce \\ourmethod{}, a generative cache that produces variation-aware responses for structurally similar prompts. \\ourmethod{} identifies reusable response patterns across similar prompt structures and synthesizes customized outputs for new requests. We show that \\ourmethod{} achieves 83\\% cache hit rate, while having minimal incorrect hits on datasets without prompt repetition. In agentic workflows, it improves cache hit rate by $\\sim$20\\% and reduces end-to-end execution latency by $\\sim$34\\% compared to standard prompt matching.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5f0f\u7f13\u5b58\u65b9\u6cd5\uff0c\u80fd\u591f\u4e3a\u7ed3\u6784\u76f8\u4f3c\u7684\u63d0\u793a\u751f\u6210\u53d8\u4f53\u611f\u77e5\u7684\u54cd\u5e94\uff0c\u63d0\u9ad8\u7f13\u5b58\u547d\u4e2d\u7387\u548c\u6267\u884c\u6548\u7387", "motivation": "\u5728\u91cd\u590d\u6027\u5de5\u4f5c\u6d41\u548c\u4ee3\u7406\u573a\u666f\u4e2d\uff0c\u63d0\u793a\u901a\u5e38\u5177\u6709\u76f8\u4f3c\u7ed3\u6784\u4f46\u5b58\u5728\u5fae\u5c0f\u53d8\u5316\uff0c\u7cbe\u786e\u5339\u914d\u4f1a\u5931\u8d25\uff0c\u800c\u8bed\u4e49\u7f13\u5b58\u53ef\u80fd\u5ffd\u7565\u5173\u952e\u5dee\u5f02\u5bfc\u81f4\u9519\u8bef\u54cd\u5e94", "method": "\u8bc6\u522b\u76f8\u4f3c\u63d0\u793a\u7ed3\u6784\u4e2d\u7684\u53ef\u91cd\u7528\u54cd\u5e94\u6a21\u5f0f\uff0c\u4e3a\u65b0\u8bf7\u6c42\u5408\u6210\u5b9a\u5236\u5316\u8f93\u51fa", "result": "\u5728\u65e0\u63d0\u793a\u91cd\u590d\u7684\u6570\u636e\u96c6\u4e0a\u8fbe\u523083%\u7684\u7f13\u5b58\u547d\u4e2d\u7387\uff0c\u9519\u8bef\u547d\u4e2d\u7387\u6781\u4f4e\uff1b\u5728\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6\u63d0\u793a\u5339\u914d\u63d0\u9ad8\u7ea620%\u7684\u7f13\u5b58\u547d\u4e2d\u7387\uff0c\u51cf\u5c11\u7ea634%\u7684\u7aef\u5230\u7aef\u6267\u884c\u5ef6\u8fdf", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7ed3\u6784\u76f8\u4f3c\u63d0\u793a\u7684\u7f13\u5b58\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u91cd\u590d\u6027\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0"}}
{"id": "2511.17781", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17781", "abs": "https://arxiv.org/abs/2511.17781", "authors": ["Kristy Sakano", "Jianyu An", "Dinesh Manocha", "Huan Xu"], "title": "SAFE-SMART: Safety Analysis and Formal Evaluation using STL Metrics for Autonomous RoboTs", "comment": null, "summary": "We present a novel, regulator-driven approach for post hoc safety evaluation of learning-based, black-box autonomous mobile robots, ensuring ongoing compliance with evolving, human-defined safety rules. In our iterative workflow, human safety requirements are translated by regulators into Signal Temporal Logic (STL) specifications. Rollout traces from the black-box model are externally verified for compliance, yielding quantitative safety metrics, Total Robustness Value (TRV) and Largest Robustness Value (LRV), which measure average and worst-case specification adherence. These metrics inform targeted retraining and iterative improvement by model designers. We apply our method across two different applications: a virtual driving scenario and an autonomous mobile robot navigating a complex environment, and observe statistically significant improvements across both scenarios. In the virtual driving scenario, we see a 177% increase in traces adhering to the simulation speed limit, a 1138% increase in traces minimizing off-road driving, and a 16% increase in traces successfully reaching the goal within the time limit. In the autonomous navigation scenario, there is a 300% increase in traces avoiding sharp turns, a 200% increase in traces reaching the goal within the time limit, and a 49% increase in traces minimizing time spent near obstacles. Finally, we validate our approach on a TurtleBot3 robot in the real world, and demonstrate improved obstacle navigation with safety buffers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76d1\u7ba1\u7684\u540e\u9a8c\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u57fa\u4e8e\u5b66\u4e60\u7684\u9ed1\u76d2\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u901a\u8fc7Signal Temporal Logic\u89c4\u8303\u5c06\u4eba\u7c7b\u5b89\u5168\u9700\u6c42\u8f6c\u5316\u4e3a\u53ef\u91cf\u5316\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u8fed\u4ee3\u6539\u8fdb\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5b66\u4e60\u578b\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u8fd9\u4e9b\u9ed1\u76d2\u7cfb\u7edf\u80fd\u591f\u6301\u7eed\u7b26\u5408\u4e0d\u65ad\u6f14\u8fdb\u7684\u4eba\u7c7b\u5b89\u5168\u89c4\u5219\uff0c\u800c\u4f20\u7edf\u7684\u9a8c\u8bc1\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u8fd9\u79cd\u52a8\u6001\u53d8\u5316\u7684\u5b89\u5168\u9700\u6c42\u3002", "method": "\u91c7\u7528\u76d1\u7ba1\u9a71\u52a8\u7684\u5de5\u4f5c\u6d41\u7a0b\uff1a\u5c06\u4eba\u7c7b\u5b89\u5168\u9700\u6c42\u8f6c\u5316\u4e3aSTL\u89c4\u8303\uff0c\u5bf9\u9ed1\u76d2\u6a21\u578b\u7684\u8f68\u8ff9\u8fdb\u884c\u5916\u90e8\u9a8c\u8bc1\uff0c\u8ba1\u7b97TRV\u548cLRV\u4e24\u4e2a\u91cf\u5316\u5b89\u5168\u6307\u6807\uff0c\u57fa\u4e8e\u8fd9\u4e9b\u6307\u6807\u8fdb\u884c\u9488\u5bf9\u6027\u91cd\u8bad\u7ec3\u548c\u8fed\u4ee3\u6539\u8fdb\u3002", "result": "\u5728\u4e24\u4e2a\u5e94\u7528\u573a\u666f\u4e2d\u5747\u89c2\u5bdf\u5230\u663e\u8457\u6539\u8fdb\uff1a\u865a\u62df\u9a7e\u9a76\u573a\u666f\u4e2d\uff0c\u9075\u5b88\u9650\u901f\u7684\u8f68\u8ff9\u589e\u52a0177%\uff0c\u51cf\u5c11\u8d8a\u91ce\u9a7e\u9a76\u7684\u8f68\u8ff9\u589e\u52a01138%\uff0c\u6309\u65f6\u5230\u8fbe\u76ee\u6807\u7684\u8f68\u8ff9\u589e\u52a016%\uff1b\u81ea\u4e3b\u5bfc\u822a\u573a\u666f\u4e2d\uff0c\u907f\u514d\u6025\u8f6c\u5f2f\u7684\u8f68\u8ff9\u589e\u52a0300%\uff0c\u6309\u65f6\u5230\u8fbe\u76ee\u6807\u7684\u8f68\u8ff9\u589e\u52a0200%\uff0c\u51cf\u5c11\u9760\u8fd1\u969c\u788d\u7269\u65f6\u95f4\u7684\u8f68\u8ff9\u589e\u52a049%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u5b66\u4e60\u578b\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u5b89\u5168\u6027\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684TurtleBot3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u6539\u8fdb\u7684\u969c\u788d\u7269\u5bfc\u822a\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u8be5\u76d1\u7ba1\u9a71\u52a8\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2511.17572", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2511.17572", "abs": "https://arxiv.org/abs/2511.17572", "authors": ["Patrick Gerard", "Aiden Chang", "Svitlana Volkova"], "title": "Community-Aligned Behavior Under Uncertainty: Evidence of Epistemic Stance Transfer in LLMs", "comment": "37 pages, EurIPS 2025", "summary": "When large language models (LLMs) are aligned to a specific online community, do they exhibit generalizable behavioral patterns that mirror that community's attitudes and responses to new uncertainty, or are they simply recalling patterns from training data? We introduce a framework to test epistemic stance transfer: targeted deletion of event knowledge, validated with multiple probes, followed by evaluation of whether models still reproduce the community's organic response patterns under ignorance. Using Russian--Ukrainian military discourse and U.S. partisan Twitter data, we find that even after aggressive fact removal, aligned LLMs maintain stable, community-specific behavioral patterns for handling uncertainty. These results provide evidence that alignment encodes structured, generalizable behaviors beyond surface mimicry. Our framework offers a systematic way to detect behavioral biases that persist under ignorance, advancing efforts toward safer and more transparent LLM deployments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6d4b\u8bd5\u8ba4\u8bc6\u7acb\u573a\u8f6c\u79fb\u7684\u6846\u67b6\uff0c\u53d1\u73b0\u5728\u5220\u9664\u4e8b\u4ef6\u77e5\u8bc6\u540e\uff0c\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4ecd\u80fd\u4fdd\u6301\u793e\u533a\u7279\u5b9a\u7684\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u884c\u4e3a\u6a21\u5f0f\uff0c\u8868\u660e\u5bf9\u9f50\u7f16\u7801\u4e86\u8d85\u8d8a\u8868\u9762\u6a21\u4eff\u7684\u7ed3\u6784\u5316\u3001\u53ef\u6cdb\u5316\u884c\u4e3a\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u793e\u533a\u5bf9\u9f50\u540e\uff0c\u662f\u8868\u73b0\u51fa\u53ef\u6cdb\u5316\u7684\u884c\u4e3a\u6a21\u5f0f\u53cd\u6620\u793e\u533a\u6001\u5ea6\uff0c\u8fd8\u662f\u4ec5\u4ec5\u56de\u5fc6\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6a21\u5f0f\u3002", "method": "\u5f15\u5165\u8ba4\u8bc6\u7acb\u573a\u8f6c\u79fb\u6d4b\u8bd5\u6846\u67b6\uff1a\u901a\u8fc7\u76ee\u6807\u6027\u5220\u9664\u4e8b\u4ef6\u77e5\u8bc6\uff0c\u5e76\u4f7f\u7528\u591a\u4e2a\u63a2\u9488\u9a8c\u8bc1\uff0c\u7136\u540e\u8bc4\u4f30\u6a21\u578b\u5728\u65e0\u77e5\u72b6\u6001\u4e0b\u662f\u5426\u4ecd\u80fd\u91cd\u73b0\u793e\u533a\u7684\u6709\u673a\u54cd\u5e94\u6a21\u5f0f\u3002\u4f7f\u7528\u4fc4\u7f57\u65af-\u4e4c\u514b\u5170\u519b\u4e8b\u8ba8\u8bba\u548c\u7f8e\u56fd\u515a\u6d3e\u63a8\u7279\u6570\u636e\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5373\u4f7f\u5728\u6fc0\u8fdb\u7684\u4e8b\u5b9e\u5220\u9664\u540e\uff0c\u5bf9\u9f50\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4ecd\u4fdd\u6301\u7a33\u5b9a\u7684\u3001\u793e\u533a\u7279\u5b9a\u7684\u4e0d\u786e\u5b9a\u6027\u5904\u7406\u884c\u4e3a\u6a21\u5f0f\u3002", "conclusion": "\u5bf9\u9f50\u7f16\u7801\u4e86\u7ed3\u6784\u5316\u3001\u53ef\u6cdb\u5316\u7684\u884c\u4e3a\uff0c\u8d85\u8d8a\u4e86\u8868\u9762\u6a21\u4eff\u3002\u8be5\u6846\u67b6\u4e3a\u68c0\u6d4b\u5728\u65e0\u77e5\u72b6\u6001\u4e0b\u6301\u7eed\u5b58\u5728\u7684\u884c\u4e3a\u504f\u5dee\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0\u66f4\u5b89\u5168\u900f\u660e\u7684LLM\u90e8\u7f72\u3002"}}
{"id": "2511.17798", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17798", "abs": "https://arxiv.org/abs/2511.17798", "authors": ["Francesco D'Orazio", "Sepehr Samavi", "Xintong Du", "Siqi Zhou", "Giuseppe Oriolo", "Angela P. Schoellig"], "title": "SM$^2$ITH: Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control", "comment": null, "summary": "Mobile manipulators are designed to perform complex sequences of navigation and manipulation tasks in human-centered environments. While recent optimization-based methods such as Hierarchical Task Model Predictive Control (HTMPC) enable efficient multitask execution with strict task priorities, they have so far been applied mainly to static or structured scenarios. Extending these approaches to dynamic human-centered environments requires predictive models that capture how humans react to the actions of the robot. This work introduces Safe Mobile Manipulation with Interactive Human Prediction via Task-Hierarchical Bilevel Model Predictive Control (SM$^2$ITH), a unified framework that combines HTMPC with interactive human motion prediction through bilevel optimization that jointly accounts for robot and human dynamics. The framework is validated on two different mobile manipulators, the Stretch 3 and the Ridgeback-UR10, across three experimental settings: (i) delivery tasks with different navigation and manipulation priorities, (ii) sequential pick-and-place tasks with different human motion prediction models, and (iii) interactions involving adversarial human behavior. Our results highlight how interactive prediction enables safe and efficient coordination, outperforming baselines that rely on weighted objectives or open-loop human models.", "AI": {"tldr": "SM\u00b2ITH\u6846\u67b6\u7ed3\u5408\u5206\u5c42\u4efb\u52a1\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u4e0e\u4ea4\u4e92\u5f0f\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u5b9e\u73b0\u79fb\u52a8\u673a\u5668\u4eba\u5728\u52a8\u6001\u4eba\u673a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u5e94\u7528\u4e8e\u9759\u6001\u6216\u7ed3\u6784\u5316\u573a\u666f\uff0c\u9700\u8981\u6269\u5c55\u81f3\u52a8\u6001\u4eba\u673a\u73af\u5883\uff0c\u8003\u8651\u4eba\u7c7b\u5bf9\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u53cd\u5e94\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u5206\u5c42\u53cc\u5c42\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u5206\u5c42\u4efb\u52a1MPC\u4e0e\u4ea4\u4e92\u5f0f\u4eba\u4f53\u8fd0\u52a8\u9884\u6d4b\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u540c\u65f6\u8003\u8651\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u52a8\u529b\u5b66\u3002", "result": "\u5728\u4e24\u4e2a\u79fb\u52a8\u673a\u68b0\u81c2\u4e0a\u9a8c\u8bc1\uff0c\u5728\u9012\u9001\u4efb\u52a1\u3001\u987a\u5e8f\u62fe\u653e\u4efb\u52a1\u548c\u5bf9\u6297\u6027\u4eba\u7c7b\u884c\u4e3a\u4ea4\u4e92\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u57fa\u4e8e\u52a0\u6743\u76ee\u6807\u6216\u5f00\u73af\u4eba\u7c7b\u6a21\u578b\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u4ea4\u4e92\u5f0f\u9884\u6d4b\u80fd\u591f\u5b9e\u73b0\u5b89\u5168\u9ad8\u6548\u7684\u534f\u8c03\uff0c\u5728\u52a8\u6001\u4eba\u673a\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u79fb\u52a8\u673a\u68b0\u81c2\u7684\u64cd\u4f5c\u6027\u80fd\u3002"}}
{"id": "2511.17575", "categories": ["cs.CL", "stat.ME", "stat.ML", "stat.OT"], "pdf": "https://arxiv.org/pdf/2511.17575", "abs": "https://arxiv.org/abs/2511.17575", "authors": ["Vladimir Berman"], "title": "Random Text, Zipf's Law, Critical Length,and Implications for Large Language Models", "comment": null, "summary": "We study a deliberately simple, fully non-linguistic model of text: a sequence of independent draws from a finite alphabet of letters plus a single space symbol. A word is defined as a maximal block of non-space symbols. Within this symbol-level framework, which assumes no morphology, syntax, or semantics, we derive several structural results. First, word lengths follow a geometric distribution governed solely by the probability of the space symbol. Second, the expected number of words of a given length, and the expected number of distinct words of that length, admit closed-form expressions based on a coupon-collector argument. This yields a critical word length k* at which word types transition from appearing many times on average to appearing at most once. Third, combining the exponential growth of the number of possible strings of length k with the exponential decay of the probability of each string, we obtain a Zipf-type rank-frequency law p(r) proportional to r^{-alpha}, with an exponent determined explicitly by the alphabet size and the space probability.\n  Our contribution is twofold. Mathematically, we give a unified derivation linking word lengths, vocabulary growth, critical length, and rank-frequency structure in a single explicit model. Conceptually, we argue that this provides a structurally grounded null model for both natural-language word statistics and token statistics in large language models. The results show that Zipf-like patterns can arise purely from combinatorics and segmentation, without optimization principles or linguistic organization, and help clarify which phenomena require deeper explanation beyond random-text structure.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u5168\u975e\u8bed\u8a00\u5b66\u7684\u6587\u672c\u6a21\u578b\uff0c\u901a\u8fc7\u72ec\u7acb\u62bd\u53d6\u6709\u9650\u5b57\u6bcd\u8868\u52a0\u7a7a\u683c\u7b26\u53f7\u6765\u7814\u7a76\u6587\u672c\u7ed3\u6784\u3002\u8be5\u6a21\u578b\u5728\u6ca1\u6709\u4efb\u4f55\u5f62\u6001\u3001\u53e5\u6cd5\u6216\u8bed\u4e49\u5047\u8bbe\u7684\u60c5\u51b5\u4e0b\uff0c\u63a8\u5bfc\u51fa\u51e0\u4f55\u5206\u5e03\u7684\u5b57\u957f\u3001\u8bcd\u6c47\u589e\u957f\u89c4\u5f8b\u548cZipf\u578b\u79e9\u9891\u5206\u5e03\u3002", "motivation": "\u4e3a\u81ea\u7136\u8bed\u8a00\u5355\u8bcd\u7edf\u8ba1\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684token\u7edf\u8ba1\u63d0\u4f9b\u4e00\u4e2a\u7ed3\u6784\u4e0a\u57fa\u4e8e\u57fa\u7840\u7684\u96f6\u6a21\u578b\uff0c\u6f84\u6e05\u54ea\u4e9b\u73b0\u8c61\u9700\u8981\u8d85\u51fa\u968f\u673a\u6587\u672c\u7ed3\u6784\u7684\u6df1\u5c42\u89e3\u91ca\u3002", "method": "\u4f7f\u7528\u6709\u9650\u5b57\u6bcd\u8868\u52a0\u7a7a\u683c\u7b26\u53f7\u7684\u72ec\u7acb\u62bd\u53d6\u6a21\u578b\uff0c\u5b9a\u4e49\u5355\u8bcd\u4e3a\u6700\u5927\u975e\u7a7a\u683c\u7b26\u53f7\u5757\uff0c\u901a\u8fc7\u7ec4\u5408\u6570\u5b66\u548c\u4f18\u60e0\u5238\u6536\u96c6\u8005\u8bba\u8bc1\u63a8\u5bfc\u7ed3\u6784\u7ed3\u679c\u3002", "result": "\u63a8\u5bfc\u51fa\u5b57\u957f\u670d\u4ece\u51e0\u4f55\u5206\u5e03\uff0c\u8bcd\u6c47\u589e\u957f\u548c\u5173\u952e\u5b57\u957f\u6709\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5f97\u5230Zipf\u578b\u79e9\u9891\u5f8bp(r) \u221d r^{-\u03b1}\uff0c\u6307\u6570\u7531\u5b57\u6bcd\u8868\u5927\u5c0f\u548c\u7a7a\u683c\u6982\u7387\u786e\u5b9a\u3002", "conclusion": "Zipf-like\u6a21\u5f0f\u53ef\u4ee5\u7eaf\u7cb9\u4ece\u7ec4\u5408\u6570\u5b66\u548c\u5206\u5272\u4e2d\u4ea7\u751f\uff0c\u65e0\u9700\u4f18\u5316\u539f\u5219\u6216\u8bed\u8a00\u7ec4\u7ec7\uff0c\u8fd9\u4e3a\u7406\u89e3\u8bed\u8a00\u7edf\u8ba1\u73b0\u8c61\u63d0\u4f9b\u4e86\u7ed3\u6784\u57fa\u7840\u7684\u7a7a\u6a21\u578b\u3002"}}
{"id": "2511.17889", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17889", "abs": "https://arxiv.org/abs/2511.17889", "authors": ["Ting Huang", "Dongjian Li", "Rui Yang", "Zeyu Zhang", "Zida Yang", "Hao Tang"], "title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots", "comment": null, "summary": "Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1.", "AI": {"tldr": "MobileVLA-R1\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u591a\u7c92\u5ea6\u601d\u7ef4\u94fe\u6570\u636e\u96c6\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u56db\u8db3\u673a\u5668\u4eba\u7684\u663e\u5f0f\u63a8\u7406\u548c\u8fde\u7eed\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5c06\u9ad8\u7ea7\u8bed\u4e49\u63a8\u7406\u4e0e\u4f4e\u7ea7\u9a71\u52a8\u8fde\u63a5\u8d77\u6765\uff0c\u5bfc\u81f4\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5b58\u5728\u4e0d\u7a33\u5b9a\u7684\u63a5\u5730\u548c\u5f31\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u6784\u5efaMobileVLA-CoT\u5927\u89c4\u6a21\u601d\u7ef4\u94fe\u6570\u636e\u96c6\uff0c\u91c7\u7528\u76d1\u7763CoT\u5bf9\u9f50\u4e0eGRPO\u5f3a\u5316\u5b66\u4e60\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u3002", "result": "\u5728VLN\u548cVLA\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u7ea65%\uff0c\u5728\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u80fd\u3002", "conclusion": "MobileVLA-R1\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u548c\u8fde\u7eed\u63a7\u5236\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56db\u8db3\u673a\u5668\u4eba\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u63a5\u5730\u95ee\u9898\u3002"}}
{"id": "2511.17746", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17746", "abs": "https://arxiv.org/abs/2511.17746", "authors": ["Sharaj Kunjar", "Alyssa Hasegawa Smith", "Tyler R Mckenzie", "Rushali Mohbe", "Samuel V Scarpino", "Brooke Foucault Welles"], "title": "Computational frame analysis revisited: On LLMs for studying news coverage", "comment": null, "summary": "Computational approaches have previously shown various promises and pitfalls when it comes to the reliable identification of media frames. Generative LLMs like GPT and Claude are increasingly being used as content analytical tools, but how effective are they for frame analysis? We address this question by systematically evaluating them against their computational predecessors: bag-of-words models and encoder-only transformers; and traditional manual coding procedures. Our analysis rests on a novel gold standard dataset that we inductively and iteratively developed through the study, investigating six months of news coverage of the US Mpox epidemic of 2022. While we discover some potential applications for generative LLMs, we demonstrate that they were consistently outperformed by manual coders, and in some instances, by smaller language models. Some form of human validation was always necessary to determine appropriate model choice. Additionally, by examining how the suitability of various approaches depended on the nature of different tasks that were part of our frame analytical workflow, we provide insights as to how researchers may leverage the complementarity of these approaches to use them in tandem. We conclude by endorsing a methodologically pluralistic approach and put forth a roadmap for computational frame analysis for researchers going forward.", "AI": {"tldr": "\u8bc4\u4f30\u751f\u6210\u5f0fLLM\u5728\u5a92\u4f53\u6846\u67b6\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u88ab\u4eba\u5de5\u7f16\u7801\u5458\u548c\u8f83\u5c0f\u8bed\u8a00\u6a21\u578b\u8d85\u8d8a\uff0c\u9700\u8981\u4eba\u5de5\u9a8c\u8bc1\u6765\u786e\u5b9a\u5408\u9002\u7684\u6a21\u578b\u9009\u62e9\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u5f0fLLM\u4f5c\u4e3a\u5185\u5bb9\u5206\u6790\u5de5\u5177\u5728\u5a92\u4f53\u6846\u67b6\u8bc6\u522b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e0e\u4f20\u7edf\u8ba1\u7b97\u65b9\u6cd5\u53ca\u4eba\u5de5\u7f16\u7801\u8fdb\u884c\u6bd4\u8f83\u3002", "method": "\u4f7f\u7528\u65b0\u9896\u7684\u91d1\u6807\u51c6\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u8bc4\u4f30GPT\u3001Claude\u7b49\u751f\u6210\u5f0fLLM\u4e0e\u8bcd\u888b\u6a21\u578b\u3001\u7f16\u7801\u5668\u8f6c\u6362\u5668\u53ca\u4eba\u5de5\u7f16\u7801\u5728Mpox\u75ab\u60c5\u65b0\u95fb\u6846\u67b6\u5206\u6790\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u751f\u6210\u5f0fLLM\u5728\u67d0\u4e9b\u5e94\u7528\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u603b\u4f53\u4e0a\u88ab\u4eba\u5de5\u7f16\u7801\u5458\u8d85\u8d8a\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u88ab\u8f83\u5c0f\u8bed\u8a00\u6a21\u578b\u8d85\u8d8a\u3002\u9700\u8981\u4eba\u5de5\u9a8c\u8bc1\u6765\u786e\u5b9a\u5408\u9002\u7684\u6a21\u578b\u9009\u62e9\u3002", "conclusion": "\u652f\u6301\u65b9\u6cd5\u591a\u5143\u4e3b\u4e49\uff0c\u63d0\u51fa\u8ba1\u7b97\u6846\u67b6\u5206\u6790\u8def\u7ebf\u56fe\uff0c\u5efa\u8bae\u7814\u7a76\u4eba\u5458\u5229\u7528\u8fd9\u4e9b\u65b9\u6cd5\u7684\u4e92\u8865\u6027\u8fdb\u884c\u534f\u540c\u4f7f\u7528\u3002"}}
{"id": "2511.17898", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17898", "abs": "https://arxiv.org/abs/2511.17898", "authors": ["Weixi Song", "Zhetao Chen", "Tao Xu", "Xianchao Zeng", "Xinyu Zhou", "Lixin Yang", "Donglin Wang", "Cewu Lu", "Yong-Lu Li"], "title": "L1 Sample Flow for Efficient Visuomotor Learning", "comment": null, "summary": "Denoising-based models, such as diffusion and flow matching, have been a critical component of robotic manipulation for their strong distribution-fitting and scaling capacity. Concurrently, several works have demonstrated that simple learning objectives, such as L1 regression, can achieve performance comparable to denoising-based methods on certain tasks, while offering faster convergence and inference. In this paper, we focus on how to combine the advantages of these two paradigms: retaining the ability of denoising models to capture multi-modal distributions and avoid mode collapse while achieving the efficiency of the L1 regression objective. To achieve this vision, we reformulate the original v-prediction flow matching and transform it into sample-prediction with the L1 training objective. We empirically show that the multi-modality can be expressed via a single ODE step. Thus, we propose \\textbf{L1 Flow}, a two-step sampling schedule that generates a suboptimal action sequence via a single integration step and then reconstructs the precise action sequence through a single prediction. The proposed method largely retains the advantages of flow matching while reducing the iterative neural function evaluations to merely two and mitigating the potential performance degradation associated with direct sample regression. We evaluate our method with varying baselines and benchmarks, including 8 tasks in MimicGen, 5 tasks in RoboMimic \\& PushT Bench, and one task in the real-world scenario. The results show the advantages of the proposed method with regard to training efficiency, inference speed, and overall performance. \\href{https://song-wx.github.io/l1flow.github.io/}{Project Website.}", "AI": {"tldr": "\u63d0\u51faL1 Flow\u65b9\u6cd5\uff0c\u5c06v\u9884\u6d4b\u6d41\u5339\u914d\u8f6c\u6362\u4e3a\u6837\u672c\u9884\u6d4b\uff0c\u901a\u8fc7\u4e24\u6b65\u91c7\u6837\u5b9e\u73b0\u591a\u6a21\u6001\u52a8\u4f5c\u751f\u6210\uff0c\u5728\u4fdd\u6301\u6d41\u5339\u914d\u4f18\u52bf\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u7ed3\u5408\u53bb\u566a\u6a21\u578b\u7684\u591a\u6a21\u6001\u5206\u5e03\u62df\u5408\u80fd\u529b\u548cL1\u56de\u5f52\u7684\u9ad8\u6548\u6027\uff0c\u907f\u514d\u6a21\u5f0f\u574d\u584c\u540c\u65f6\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "method": "\u5c06v\u9884\u6d4b\u6d41\u5339\u914d\u8f6c\u6362\u4e3a\u6837\u672c\u9884\u6d4b\uff0c\u91c7\u7528\u4e24\u6b65\u91c7\u6837\uff1a\u5355\u6b65\u79ef\u5206\u751f\u6210\u6b21\u4f18\u52a8\u4f5c\u5e8f\u5217\uff0c\u5355\u6b65\u9884\u6d4b\u91cd\u6784\u7cbe\u786e\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u5728MimicGen\u3001RoboMimic\u3001PushT\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8bad\u7ec3\u6548\u7387\u3001\u63a8\u7406\u901f\u5ea6\u548c\u6574\u4f53\u6027\u80fd\u7684\u4f18\u52bf\u3002", "conclusion": "L1 Flow\u6210\u529f\u7ed3\u5408\u4e86\u6d41\u5339\u914d\u548cL1\u56de\u5f52\u7684\u4f18\u70b9\uff0c\u4ec5\u9700\u4e24\u6b21\u795e\u7ecf\u7f51\u7edc\u8bc4\u4f30\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u7684\u591a\u6a21\u6001\u52a8\u4f5c\u751f\u6210\u3002"}}
{"id": "2511.17808", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17808", "abs": "https://arxiv.org/abs/2511.17808", "authors": ["Thales Sales Almeida", "Rodrigo Nogueira", "H\u00e9lio Pedrini"], "title": "PoETa v2: Toward More Robust Evaluation of Large Language Models in Portuguese", "comment": null, "summary": "Large Language Models (LLMs) exhibit significant variations in performance across linguistic and cultural contexts, underscoring the need for systematic evaluation in diverse languages. In this work, we present the most extensive evaluation of LLMs for the Portuguese language to date. Leveraging our newly introduced PoETa v2 benchmark -- a comprehensive suite of over 40 tasks in Portuguese -- we assess more than 20 models covering a broad spectrum of training scales and computational resources. Our study reveals how computational investment and language-specific adaptation impact performance in Portuguese, while also analyzing performance gaps in comparison to equivalent tasks in English. Through this benchmark and analysis, PoETa v2 lays the groundwork for future research on Portuguese language modeling and evaluation. The benchmark is available at https://github.com/PoETaV2/PoETaV2.", "AI": {"tldr": "PoETa v2\u662f\u8fc4\u4eca\u4e3a\u6b62\u5bf9\u8461\u8404\u7259\u8bedLLMs\u6700\u5e7f\u6cdb\u7684\u8bc4\u4f30\uff0c\u5305\u542b40\u591a\u4e2a\u4efb\u52a1\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u8bc4\u4f30\u4e8620\u591a\u4e2a\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u8ba1\u7b97\u6295\u8d44\u548c\u8bed\u8a00\u7279\u5b9a\u9002\u5e94\u5bf9\u8461\u8404\u7259\u8bed\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "LLMs\u5728\u4e0d\u540c\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u591a\u79cd\u8bed\u8a00\u3002\u8461\u8404\u7259\u8bed\u4f5c\u4e3a\u91cd\u8981\u8bed\u8a00\uff0c\u7f3a\u4e4f\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u5f15\u5165PoETa v2\u57fa\u51c6\uff0c\u5305\u542b40\u591a\u4e2a\u8461\u8404\u7259\u8bed\u4efb\u52a1\uff0c\u8bc4\u4f3020\u591a\u4e2a\u4e0d\u540c\u8bad\u7ec3\u89c4\u6a21\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u6a21\u578b\uff0c\u5e76\u4e0e\u82f1\u8bed\u7b49\u6548\u4efb\u52a1\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u8ba1\u7b97\u6295\u8d44\u548c\u8bed\u8a00\u7279\u5b9a\u9002\u5e94\u5bf9\u8461\u8404\u7259\u8bed\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4e86\u4e0e\u82f1\u8bed\u4efb\u52a1\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "PoETa v2\u4e3a\u8461\u8404\u7259\u8bed\u8bed\u8a00\u5efa\u6a21\u548c\u8bc4\u4f30\u7684\u672a\u6765\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u516c\u5f00\u53ef\u7528\u7684\u57fa\u51c6\u3002"}}
{"id": "2511.17925", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.17925", "abs": "https://arxiv.org/abs/2511.17925", "authors": ["Jeonghwan Kim", "Wontaek Kim", "Yidan Lu", "Jin Cheng", "Fatemeh Zargarbashi", "Zicheng Zeng", "Zekun Qi", "Zhiyang Dou", "Nitish Sontakke", "Donghoon Baek", "Sehoon Ha", "Tianyu Li"], "title": "Switch-JustDance: Benchmarking Whole Body Motion Tracking Policies Using a Commercial Console Game", "comment": null, "summary": "Recent advances in whole-body robot control have enabled humanoid and legged robots to perform increasingly agile and coordinated motions. However, standardized benchmarks for evaluating these capabilities in real-world settings, and in direct comparison to humans, remain scarce. Existing evaluations often rely on pre-collected human motion datasets or simulation-based experiments, which limit reproducibility, overlook hardware factors, and hinder fair human-robot comparisons. We present Switch-JustDance, a low-cost and reproducible benchmarking pipeline that leverages motion-sensing console games, Just Dance on the Nintendo Switch, to evaluate robot whole-body control. Using Just Dance on the Nintendo Switch as a representative platform, Switch-JustDance converts in-game choreography into robot-executable motions through streaming, motion reconstruction, and motion retargeting modules and enables users to evaluate controller performance through the game's built-in scoring system. We first validate the evaluation properties of Just Dance, analyzing its reliability, validity, sensitivity, and potential sources of bias. Our results show that the platform provides consistent and interpretable performance measures, making it a suitable tool for benchmarking embodied AI. Building on this foundation, we benchmark three state-of-the-art humanoid whole-body controllers on hardware and provide insights into their relative strengths and limitations.", "AI": {"tldr": "Switch-JustDance\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u53ef\u590d\u73b0\u7684\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u4efb\u5929\u5802Switch\u7684Just Dance\u6e38\u620f\u6765\u8bc4\u4f30\u673a\u5668\u4eba\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u6e38\u620f\u5185\u7f6e\u8bc4\u5206\u7cfb\u7edf\u8fdb\u884c\u91cf\u5316\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u9884\u6536\u96c6\u7684\u4eba\u7c7b\u8fd0\u52a8\u6570\u636e\u6216\u4eff\u771f\u5b9e\u9a8c\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\uff0c\u96be\u4ee5\u5b9e\u73b0\u771f\u5b9e\u73af\u5883\u4e0b\u7684\u53ef\u590d\u73b0\u6027\u548c\u4eba\u673a\u516c\u5e73\u6bd4\u8f83\u3002", "method": "\u901a\u8fc7Just Dance\u6e38\u620f\u5e73\u53f0\uff0c\u6784\u5efa\u5305\u542b\u6d41\u5a92\u4f53\u4f20\u8f93\u3001\u8fd0\u52a8\u91cd\u5efa\u548c\u8fd0\u52a8\u91cd\u5b9a\u5411\u6a21\u5757\u7684\u6d41\u6c34\u7ebf\uff0c\u5c06\u6e38\u620f\u7f16\u821e\u8f6c\u6362\u4e3a\u673a\u5668\u4eba\u53ef\u6267\u884c\u52a8\u4f5c\uff0c\u5229\u7528\u6e38\u620f\u5185\u7f6e\u8bc4\u5206\u7cfb\u7edf\u8bc4\u4f30\u63a7\u5236\u5668\u6027\u80fd\u3002", "result": "\u9a8c\u8bc1\u4e86Just Dance\u5e73\u53f0\u7684\u53ef\u9760\u6027\u3001\u6709\u6548\u6027\u3001\u654f\u611f\u6027\u548c\u6f5c\u5728\u504f\u5dee\uff0c\u8bc1\u660e\u5176\u80fd\u63d0\u4f9b\u4e00\u81f4\u4e14\u53ef\u89e3\u91ca\u7684\u6027\u80fd\u5ea6\u91cf\uff0c\u9002\u5408\u4f5c\u4e3a\u5177\u8eabAI\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\u3002", "conclusion": "Switch-JustDance\u4e3a\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u6210\u529f\u8bc4\u4f30\u4e86\u4e09\u79cd\u5148\u8fdb\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u5668\u7684\u76f8\u5bf9\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2511.17813", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2511.17813", "abs": "https://arxiv.org/abs/2511.17813", "authors": ["Scott Merrill", "Shashank Srivastava"], "title": "Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation", "comment": "8 pages (29 pages including appendix), 18 figures. Code and datasets are available at https://github.com/smerrillunc/action-aware-llms. Submitted to ACL 2026", "summary": "Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this \"action-aware\" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u6d41\u7a0b\uff0c\u5c06Zoom\u516c\u5f00\u5f55\u97f3\u8f6c\u6362\u4e3a\u5e26\u6709\u8bf4\u8bdd\u4eba\u8eab\u4efd\u3001\u4eba\u7269\u753b\u50cf\u548c\u8bed\u7528\u884c\u4e3a\u6807\u7b7e\u7684\u8f6c\u5f55\u6587\u672c\uff0c\u5e76\u53d1\u5e03\u4e86\u4e09\u4e2a\u5730\u65b9\u653f\u5e9c\u5ba1\u8bae\u6570\u636e\u96c6\u3002\u4f7f\u7528\u8fd9\u4e9b\"\u884c\u4e3a\u611f\u77e5\"\u6570\u636e\u5fae\u8c03LLM\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6a21\u62df\u5ba1\u8bae\u7684\u771f\u5b9e\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u591a\u65b9\u5ba1\u8bae\u65f6\u7f3a\u4e4f\u8bf4\u8bdd\u4eba\u8eab\u4efd\u6570\u636e\u7684\u95ee\u9898\uff0c\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\u4ea7\u751f\u7684\u533f\u540d\u8bf4\u8bdd\u4eba\u6807\u7b7e\u65e0\u6cd5\u6355\u6349\u4e00\u81f4\u7684\u4eba\u7c7b\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u590d\u73b0\u7684\u6d41\u7a0b\uff0c\u5c06\u516c\u5f00Zoom\u5f55\u97f3\u8f6c\u6362\u4e3a\u5e26\u6709\u8bf4\u8bdd\u4eba\u8eab\u4efd\u3001\u4eba\u7269\u753b\u50cf\u548c\u8bed\u7528\u884c\u4e3a\u6807\u7b7e\uff08\u5982[propose_motion]\uff09\u7684\u8f6c\u5f55\u6587\u672c\uff0c\u5e76\u57fa\u4e8e\u8fd9\u4e9b\"\u884c\u4e3a\u611f\u77e5\"\u6570\u636e\u5fae\u8c03LLM\u6765\u5efa\u6a21\u7279\u5b9a\u53c2\u4e0e\u8005\u3002", "result": "\u4f7f\u7528\u8be5\u65b9\u6cd5\u5fae\u8c03\u7684LLM\u5728\u56f0\u60d1\u5ea6\u4e0a\u964d\u4f4e\u4e8667%\uff0c\u5728\u8bf4\u8bdd\u4eba\u5fe0\u5b9e\u5ea6\u548c\u771f\u5b9e\u6027\u65b9\u9762\u7684\u5206\u7c7b\u5668\u6027\u80fd\u6307\u6807\u51e0\u4e4e\u7ffb\u500d\u3002\u56fe\u7075\u5f0f\u4eba\u5de5\u8bc4\u4f30\u663e\u793a\u6a21\u62df\u5ba1\u8bae\u4e0e\u771f\u5b9e\u5ba1\u8bae\u96be\u4ee5\u533a\u5206\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u73b0\u5b9e\u516c\u6c11\u6a21\u62df\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u5ea6\u771f\u5b9e\u7684\u5ba1\u8bae\u6a21\u62df\u3002"}}
{"id": "2511.17961", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17961", "abs": "https://arxiv.org/abs/2511.17961", "authors": ["Hao Wang", "Xiaobao Wei", "Ying Li", "Qingpo Wuwu", "Dongli Wu", "Jiajun Cao", "Ming Lu", "Wenzhao Zheng", "Shanghang Zhang"], "title": "RoboArmGS: High-Quality Robotic Arm Splatting via B\u00e9zier Curve Refinement", "comment": null, "summary": "Building high-quality digital assets of robotic arms is crucial yet challenging for the Real2Sim2Real pipeline. Current approaches naively bind static 3D Gaussians according to URDF links, forcing them to follow an URDF-rigged motion passively. However, real-world arm motion is noisy, and the idealized URDF-rigged motion cannot accurately model it, leading to severe rendering artifacts in 3D Gaussians. To address these challenges, we propose RoboArmGS, a novel hybrid representation that refines the URDF-rigged motion with learnable B\u00e9zier curves, enabling more accurate real-world motion modeling. To be more specific, we present a learnable B\u00e9zier Curve motion refiner that corrects per-joint residuals to address mismatches between real-world motion and URDF-rigged motion. RoboArmGS enables the learning of more accurate real-world motion while achieving a coherent binding of 3D Gaussians across arm parts. To support future research, we contribute a carefully collected dataset named RoboArm4D, which comprises several widely used robotic arms for evaluating the quality of building high-quality digital assets. We evaluate our approach on RoboArm4D, and RoboArmGS achieves state-of-the-art performance in real-world motion modeling and rendering quality. The code and dataset will be released.", "AI": {"tldr": "RoboArmGS\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u8d1d\u585e\u5c14\u66f2\u7ebf\u7ec6\u5316URDF\u9aa8\u67b6\u8fd0\u52a8\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u624b\u81c2\u771f\u5b9e\u8fd0\u52a8\u4e0e\u7406\u60f3\u5316URDF\u8fd0\u52a8\u4e0d\u5339\u914d\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e863D\u9ad8\u65af\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5c06\u9759\u60013D\u9ad8\u65af\u7b80\u5355\u5730\u7ed1\u5b9a\u5230URDF\u94fe\u63a5\u4e0a\uff0c\u8feb\u4f7f\u5b83\u4eec\u88ab\u52a8\u8ddf\u968fURDF\u9aa8\u67b6\u8fd0\u52a8\u3002\u7136\u800c\u771f\u5b9e\u4e16\u754c\u7684\u624b\u81c2\u8fd0\u52a8\u5b58\u5728\u566a\u58f0\uff0c\u7406\u60f3\u5316\u7684URDF\u9aa8\u67b6\u8fd0\u52a8\u65e0\u6cd5\u51c6\u786e\u5efa\u6a21\uff0c\u5bfc\u81f43D\u9ad8\u65af\u6e32\u67d3\u51fa\u73b0\u4e25\u91cd\u4f2a\u5f71\u3002", "method": "\u63d0\u51faRoboArmGS\u6df7\u5408\u8868\u793a\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u8d1d\u585e\u5c14\u66f2\u7ebf\u8fd0\u52a8\u7ec6\u5316\u5668\u6765\u6821\u6b63\u6bcf\u4e2a\u5173\u8282\u7684\u6b8b\u5dee\uff0c\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u8fd0\u52a8\u4e0eURDF\u9aa8\u67b6\u8fd0\u52a8\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5b9e\u73b03D\u9ad8\u65af\u5728\u624b\u81c2\u5404\u90e8\u4ef6\u95f4\u7684\u8fde\u8d2f\u7ed1\u5b9a\u3002", "result": "\u5728RoboArm4D\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cRoboArmGS\u5728\u771f\u5b9e\u4e16\u754c\u8fd0\u52a8\u5efa\u6a21\u548c\u6e32\u67d3\u8d28\u91cf\u65b9\u9762\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "RoboArmGS\u80fd\u591f\u5b66\u4e60\u66f4\u51c6\u786e\u7684\u771f\u5b9e\u4e16\u754c\u8fd0\u52a8\uff0c\u540c\u65f6\u5b9e\u73b03D\u9ad8\u65af\u5728\u624b\u81c2\u90e8\u4ef6\u95f4\u7684\u8fde\u8d2f\u7ed1\u5b9a\uff0c\u4e3a\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u5b57\u8d44\u4ea7\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.17854", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.17854", "abs": "https://arxiv.org/abs/2511.17854", "authors": ["Allen Roush", "Devin Gonier", "John Hines", "Judah Goldfeder", "Philippe Martin Wyder", "Sanjay Basu", "Ravid Shwartz Ziv"], "title": "A superpersuasive autonomous policy debating system", "comment": "Accepted to CLIP workshop at AAAI 2026", "summary": "The capacity for highly complex, evidence-based, and strategically adaptive persuasion remains a formidable great challenge for artificial intelligence. Previous work, like IBM Project Debater, focused on generating persuasive speeches in simplified and shortened debate formats intended for relatively lay audiences. We introduce DeepDebater, a novel autonomous system capable of participating in and winning a full, unmodified, two-team competitive policy debate. Our system employs a hierarchical architecture of specialized multi-agent workflows, where teams of LLM-powered agents collaborate and critique one another to perform discrete argumentative tasks. Each workflow utilizes iterative retrieval, synthesis, and self-correction using a massive corpus of policy debate evidence (OpenDebateEvidence) and produces complete speech transcripts, cross-examinations, and rebuttals. We introduce a live, interactive end-to-end presentation pipeline that renders debates with AI speech and animation: transcripts are surface-realized and synthesized to audio with OpenAI TTS, and then displayed as talking-head portrait videos with EchoMimic V1. Beyond fully autonomous matches (AI vs AI), DeepDebater supports hybrid human-AI operation: human debaters can intervene at any stage, and humans can optionally serve as opponents against AI in any speech, allowing AI-human and AI-AI rounds. In preliminary evaluations against human-authored cases, DeepDebater produces qualitatively superior argumentative components and consistently wins simulated rounds as adjudicated by an independent autonomous judge. Expert human debate coaches also prefer the arguments, evidence, and cases constructed by DeepDebater. We open source all code, generated speech transcripts, audio and talking head video here: https://github.com/Hellisotherpeople/DeepDebater/tree/main", "AI": {"tldr": "DeepDebater\u662f\u4e00\u4e2a\u80fd\u591f\u53c2\u4e0e\u5e76\u8d62\u5f97\u5b8c\u6574\u653f\u7b56\u8fa9\u8bba\u7684\u81ea\u4e3bAI\u7cfb\u7edf\uff0c\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u8fa9\u8bba\u8bc1\u636e\u5e93\uff0c\u751f\u6210\u5b8c\u6574\u7684\u8fa9\u8bba\u6f14\u8bb2\u3001\u8d28\u8be2\u548c\u53cd\u9a73\u3002", "motivation": "\u89e3\u51b3AI\u5728\u590d\u6742\u3001\u57fa\u4e8e\u8bc1\u636e\u4e14\u5177\u6709\u6218\u7565\u9002\u5e94\u6027\u7684\u8bf4\u670d\u80fd\u529b\u65b9\u9762\u7684\u91cd\u5927\u6311\u6218\uff0c\u8d85\u8d8a\u4e4b\u524d\u7b80\u5316\u7684\u8fa9\u8bba\u7cfb\u7edf\u5982IBM Project Debater\u3002", "method": "\u91c7\u7528\u5206\u5c42\u67b6\u6784\u7684\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0cLLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u56e2\u961f\u534f\u4f5c\u6267\u884c\u7279\u5b9a\u8fa9\u8bba\u4efb\u52a1\uff0c\u4f7f\u7528\u8fed\u4ee3\u68c0\u7d22\u3001\u5408\u6210\u548c\u81ea\u6821\u6b63\u6280\u672f\uff0c\u57fa\u4e8e\u5927\u89c4\u6a21\u653f\u7b56\u8fa9\u8bba\u8bc1\u636e\u5e93OpenDebateEvidence\u3002", "result": "\u5728\u521d\u6b65\u8bc4\u4f30\u4e2d\uff0cDeepDebater\u4ea7\u751f\u8d28\u91cf\u66f4\u9ad8\u7684\u8bba\u8bc1\u7ec4\u4ef6\uff0c\u5728\u6a21\u62df\u8fa9\u8bba\u4e2d\u6301\u7eed\u83b7\u80dc\uff0c\u72ec\u7acb\u81ea\u4e3b\u88c1\u5224\u548c\u4eba\u7c7b\u8fa9\u8bba\u6559\u7ec3\u90fd\u66f4\u504f\u597d\u5176\u6784\u5efa\u7684\u8bba\u70b9\u3001\u8bc1\u636e\u548c\u6848\u4f8b\u3002", "conclusion": "DeepDebater\u5c55\u793a\u4e86AI\u5728\u590d\u6742\u653f\u7b56\u8fa9\u8bba\u4e2d\u7684\u80fd\u529b\uff0c\u652f\u6301\u5168\u81ea\u4e3b\u548c\u6df7\u5408\u4eba\u673a\u64cd\u4f5c\u6a21\u5f0f\uff0c\u4e3aAI\u8bf4\u670d\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2511.17992", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.17992", "abs": "https://arxiv.org/abs/2511.17992", "authors": ["Chungeng Tian", "Fenghua He", "Ning Hao"], "title": "Unobservable Subspace Evolution and Alignment for Consistent Visual-Inertial Navigation", "comment": "20 pages, 16 figures", "summary": "The inconsistency issue in the Visual-Inertial Navigation System (VINS) is a long-standing and fundamental challenge. While existing studies primarily attribute the inconsistency to observability mismatch, these analyses are often based on simplified theoretical formulations that consider only prediction and SLAM correction. Such formulations fail to cover the non-standard estimation steps, such as MSCKF correction and delayed initialization, which are critical for practical VINS estimators. Furthermore, the lack of a comprehensive understanding of how inconsistency dynamically emerges across estimation steps has hindered the development of precise and efficient solutions. As a result, current approaches often face a trade-off between estimator accuracy, consistency, and implementation complexity. To address these limitations, this paper proposes a novel analysis framework termed Unobservable Subspace Evolution (USE), which systematically characterizes how the unobservable subspace evolves throughout the entire estimation pipeline by explicitly tracking changes in its evaluation points. This perspective sheds new light on how individual estimation steps contribute to inconsistency. Our analysis reveals that observability misalignment induced by certain steps is the antecedent of observability mismatch. Guided by this insight, we propose a simple yet effective solution paradigm, Unobservable Subspace Alignment (USA), which eliminates inconsistency by selectively intervening only in those estimation steps that induce misalignment. We design two USA methods: transformation-based and re-evaluation-based, both offering accurate and computationally lightweight solutions. Extensive simulations and real-world experiments validate the effectiveness of the proposed methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Unobservable Subspace Evolution (USE)\u5206\u6790\u6846\u67b6\u548cUnobservable Subspace Alignment (USA)\u89e3\u51b3\u65b9\u6848\uff0c\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u4e86\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5c06VINS\u4e0d\u4e00\u81f4\u6027\u5f52\u56e0\u4e8e\u53ef\u89c2\u6d4b\u6027\u5931\u914d\uff0c\u4f46\u5206\u6790\u57fa\u4e8e\u7b80\u5316\u7684\u7406\u8bba\u6846\u67b6\uff0c\u672a\u80fd\u6db5\u76d6MSCKF\u6821\u6b63\u548c\u5ef6\u8fdf\u521d\u59cb\u5316\u7b49\u975e\u6807\u51c6\u4f30\u8ba1\u6b65\u9aa4\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u4e0d\u4e00\u81f4\u6027\u5982\u4f55\u52a8\u6001\u4ea7\u751f\u7684\u5168\u9762\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86USE\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u8ddf\u8e2a\u4e0d\u53ef\u89c2\u6d4b\u5b50\u7a7a\u95f4\u8bc4\u4f30\u70b9\u7684\u53d8\u5316\u6765\u7cfb\u7edf\u63cf\u8ff0\u5176\u5728\u5b8c\u6574\u4f30\u8ba1\u6d41\u7a0b\u4e2d\u7684\u6f14\u5316\uff1b\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86USA\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u57fa\u4e8e\u53d8\u6362\u548c\u57fa\u4e8e\u91cd\u8bc4\u4f30\u7684\u4e24\u79cd\u65b9\u6cd5\u3002", "result": "\u5206\u6790\u53d1\u73b0\u67d0\u4e9b\u6b65\u9aa4\u5f15\u8d77\u7684\u53ef\u89c2\u6d4b\u6027\u9519\u4f4d\u662f\u53ef\u89c2\u6d4b\u6027\u5931\u914d\u7684\u524d\u56e0\uff1b\u63d0\u51fa\u7684USA\u65b9\u6cd5\u901a\u8fc7\u9009\u62e9\u6027\u5e72\u9884\u5f15\u53d1\u9519\u4f4d\u7684\u4f30\u8ba1\u6b65\u9aa4\u6765\u6d88\u9664\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "USE\u6846\u67b6\u4e3a\u7406\u89e3VINS\u4e0d\u4e00\u81f4\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0cUSA\u65b9\u6cd5\u63d0\u4f9b\u4e86\u51c6\u786e\u4e14\u8ba1\u7b97\u8f7b\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2511.17908", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.17908", "abs": "https://arxiv.org/abs/2511.17908", "authors": ["Debashish Chakraborty", "Eugene Yang", "Daniel Khashabi", "Dawn Lawrie", "Kevin Duh"], "title": "Principled Context Engineering for RAG: Statistical Guarantees via Conformal Prediction", "comment": "Preprint", "summary": "Retrieval-Augmented Generation (RAG) enhances factual grounding in large language models (LLMs) by incorporating retrieved evidence, but LLM accuracy declines when long or noisy contexts exceed the model's effective attention span. Existing pre-generation filters rely on heuristics or uncalibrated LLM confidence scores, offering no statistical control over retained evidence. We evaluate and demonstrate context engineering through conformal prediction, a coverage-controlled filtering framework that removes irrelevant content while preserving recall of supporting evidence. Using both embedding- and LLM-based scoring functions, we test this approach on the NeuCLIR and RAGTIME collections. Conformal filtering consistently meets its target coverage, ensuring that a specified fraction of relevant snippets are retained, and reduces retained context by 2-3x relative to unfiltered retrieval. On NeuCLIR, downstream factual accuracy measured by ARGUE F1 improves under strict filtering and remains stable at moderate coverage, indicating that most discarded material is redundant or irrelevant. These results demonstrate that conformal prediction enables reliable, coverage-controlled context reduction in RAG, offering a model-agnostic and principled approach to context engineering.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fdd\u5f62\u9884\u6d4b\u7684\u8986\u76d6\u63a7\u5236\u8fc7\u6ee4\u6846\u67b6\uff0c\u7528\u4e8e\u5728RAG\u7cfb\u7edf\u4e2d\u53bb\u9664\u65e0\u5173\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u8bc1\u76f8\u5173\u8bc1\u636e\u7684\u53ec\u56de\u7387\u3002\u8be5\u65b9\u6cd5\u80fd\u51cf\u5c112-3\u500d\u7684\u4e0a\u4e0b\u6587\u5185\u5bb9\uff0c\u540c\u65f6\u4fdd\u6301\u4e0b\u6e38\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u5728\u5904\u7406\u957f\u6587\u672c\u6216\u566a\u58f0\u4e0a\u4e0b\u6587\u65f6\uff0cLLM\u7684\u51c6\u786e\u6027\u4f1a\u4e0b\u964d\uff0c\u4e14\u73b0\u6709\u9884\u751f\u6210\u8fc7\u6ee4\u5668\u7f3a\u4e4f\u5bf9\u4fdd\u7559\u8bc1\u636e\u7684\u7edf\u8ba1\u63a7\u5236\u3002", "method": "\u4f7f\u7528\u4fdd\u5f62\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165\u548cLLM\u8bc4\u5206\u51fd\u6570\u5bf9\u68c0\u7d22\u5185\u5bb9\u8fdb\u884c\u8986\u76d6\u63a7\u5236\u8fc7\u6ee4\uff0c\u5728NeuCLIR\u548cRAGTIME\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4fdd\u5f62\u8fc7\u6ee4\u59cb\u7ec8\u8fbe\u5230\u76ee\u6807\u8986\u76d6\u5ea6\uff0c\u51cf\u5c112-3\u500d\u4fdd\u7559\u4e0a\u4e0b\u6587\uff0c\u5728\u4e25\u683c\u8fc7\u6ee4\u4e0bARGUE F1\u6307\u6807\u5f97\u5230\u6539\u5584\uff0c\u5728\u4e2d\u7b49\u8986\u76d6\u5ea6\u4e0b\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "\u4fdd\u5f62\u9884\u6d4b\u4e3aRAG\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u8986\u76d6\u63a7\u5236\u7684\u4e0a\u4e0b\u6587\u7f29\u51cf\u65b9\u6cd5\uff0c\u662f\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u4e14\u539f\u5219\u6027\u7684\u4e0a\u4e0b\u6587\u5de5\u7a0b\u65b9\u6cd5\u3002"}}
{"id": "2511.18085", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18085", "abs": "https://arxiv.org/abs/2511.18085", "authors": ["Yuxuan Wu", "Guangming Wang", "Zhiheng Yang", "Maoqing Yao", "Brian Sheil", "Hesheng Wang"], "title": "Continually Evolving Skill Knowledge in Vision Language Action Model", "comment": null, "summary": "Developing general robot intelligence in open environments requires continual skill learning. Recent Vision-Language-Action (VLA) models leverage massive pretraining data to support diverse manipulation tasks, but they still depend heavily on task-specific fine-tuning, revealing a lack of continual learning capability. Existing continual learning methods are also resource-intensive to scale to VLA models. We propose Stellar VLA, a knowledge-driven continual learning framework with two variants: T-Stellar, modeling task-centric knowledge space, and TS-Stellar, capturing hierarchical task-skill structure. Stellar VLA enables self-supervised knowledge evolution through joint learning of task latent representation and the knowledge space, reducing annotation needs. Knowledge-guided expert routing provide task specialization without extra network parameters, lowering training overhead.Experiments on the LIBERO benchmark and real-world tasks show over 50 percentage average improvement in final success rates relative to baselines. TS-Stellar further excels in complex action inference, and in-depth analyses verify effective knowledge retention and discovery. Our code will be released soon.", "AI": {"tldr": "Stellar VLA\u662f\u4e00\u4e2a\u77e5\u8bc6\u9a71\u52a8\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u6f5c\u5728\u8868\u793a\u548c\u77e5\u8bc6\u7a7a\u95f4\u7684\u8054\u5408\u5b66\u4e60\u5b9e\u73b0\u81ea\u6211\u76d1\u7763\u7684\u77e5\u8bc6\u6f14\u5316\uff0c\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u63d0\u5347\u8d85\u8fc750%\u7684\u6700\u7ec8\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u867d\u7136\u80fd\u652f\u6301\u591a\u6837\u5316\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u4f46\u4e25\u91cd\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u7684\u5fae\u8c03\uff0c\u7f3a\u4e4f\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002\u73b0\u6709\u7684\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u4e5f\u96be\u4ee5\u6269\u5c55\u5230VLA\u6a21\u578b\u3002", "method": "\u63d0\u51faStellar VLA\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u53d8\u4f53\uff1aT-Stellar\u5efa\u6a21\u4efb\u52a1\u4e2d\u5fc3\u77e5\u8bc6\u7a7a\u95f4\uff0cTS-Stellar\u6355\u83b7\u5206\u5c42\u4efb\u52a1-\u6280\u80fd\u7ed3\u6784\u3002\u901a\u8fc7\u77e5\u8bc6\u5f15\u5bfc\u7684\u4e13\u5bb6\u8def\u7531\u5b9e\u73b0\u4efb\u52a1\u4e13\u4e1a\u5316\uff0c\u65e0\u9700\u989d\u5916\u7f51\u7edc\u53c2\u6570\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u63d0\u5347\u8d85\u8fc750%\u7684\u6700\u7ec8\u6210\u529f\u7387\u3002TS-Stellar\u5728\u590d\u6742\u52a8\u4f5c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "Stellar VLA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u7684\u6301\u7eed\u5b66\u4e60\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u77e5\u8bc6\u4fdd\u7559\u548c\u53d1\u73b0\uff0c\u964d\u4f4e\u4e86\u8bad\u7ec3\u5f00\u9500\u548c\u6807\u6ce8\u9700\u6c42\u3002"}}
{"id": "2511.17910", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17910", "abs": "https://arxiv.org/abs/2511.17910", "authors": ["Yuliang Zhan", "Xinyu Tang", "Han Wan", "Jian Li", "Ji-Rong Wen", "Hao Sun"], "title": "L2V-CoT: Cross-Modal Transfer of Chain-of-Thought Reasoning via Latent Intervention", "comment": "AAAI 2026 oral", "summary": "Recently, Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs), but Vision-Language Models (VLMs) still struggle with multi-step reasoning tasks due to limited multimodal reasoning data. To bridge this gap, researchers have explored methods to transfer CoT reasoning from LLMs to VLMs. However, existing approaches either need high training costs or require architectural alignment. In this paper, we use Linear Artificial Tomography (LAT) to empirically show that LLMs and VLMs share similar low-frequency latent representations of CoT reasoning despite architectural differences. Based on this insight, we propose L2V-CoT, a novel training-free latent intervention approach that transfers CoT reasoning from LLMs to VLMs. L2V-CoT extracts and resamples low-frequency CoT representations from LLMs in the frequency domain, enabling dimension matching and latent injection into VLMs during inference to enhance reasoning capabilities. Extensive experiments demonstrate that our approach consistently outperforms training-free baselines and even surpasses supervised methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faL2V-CoT\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u57df\u7684\u4f4e\u9891\u6f5c\u5728\u8868\u793a\u5e72\u9884\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5c06LLMs\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u5230VLMs\u4e0a\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9ad8\u8bad\u7ec3\u6210\u672c\u6216\u67b6\u6784\u5bf9\u9f50\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u4eba\u5de5\u65ad\u5c42\u626b\u63cf\u53d1\u73b0LLMs\u548cVLMs\u5728\u601d\u7ef4\u94fe\u63a8\u7406\u4e0a\u5171\u4eab\u76f8\u4f3c\u7684\u4f4e\u9891\u6f5c\u5728\u8868\u793a\uff0c\u63d0\u51faL2V-CoT\u65b9\u6cd5\u5728\u9891\u7387\u57df\u63d0\u53d6\u548c\u91cd\u91c7\u6837LLMs\u7684\u4f4e\u9891CoT\u8868\u793a\uff0c\u901a\u8fc7\u7ef4\u5ea6\u5339\u914d\u548c\u6f5c\u5728\u6ce8\u5165\u589e\u5f3aVLMs\u7684\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u751a\u81f3\u8d85\u8fc7\u6709\u76d1\u7763\u65b9\u6cd5\u3002", "conclusion": "LLMs\u548cVLMs\u5728\u601d\u7ef4\u94fe\u63a8\u7406\u4e0a\u5b58\u5728\u5171\u4eab\u7684\u4f4e\u9891\u6f5c\u5728\u8868\u793a\uff0c\u901a\u8fc7\u9891\u7387\u57df\u5e72\u9884\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u3002"}}
{"id": "2511.18086", "categories": ["cs.RO", "cs.NI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18086", "abs": "https://arxiv.org/abs/2511.18086", "authors": ["Miguel Louren\u00e7o", "Ant\u00f3nio Grilo"], "title": "Anti-Jamming based on Null-Steering Antennas and Intelligent UAV Swarm Behavior", "comment": "10 pages", "summary": "Unmanned Aerial Vehicle (UAV) swarms represent a key advancement in autonomous systems, enabling coordinated missions through inter-UAV communication. However, their reliance on wireless links makes them vulnerable to jamming, which can disrupt coordination and mission success. This work investigates whether a UAV swarm can effectively overcome jamming while maintaining communication and mission efficiency.\n  To address this, a unified optimization framework combining Genetic Algorithms (GA), Supervised Learning (SL), and Reinforcement Learning (RL) is proposed. The mission model, structured into epochs and timeslots, allows dynamic path planning, antenna orientation, and swarm formation while progressively enforcing collision rules. Null-steering antennas enhance resilience by directing antenna nulls toward interference sources.\n  Results show that the GA achieved stable, collision-free trajectories but with high computational cost. SL models replicated GA-based configurations but struggled to generalize under dynamic or constrained settings. RL, trained via Proximal Policy Optimization (PPO), demonstrated adaptability and real-time decision-making with consistent communication and lower computational demand. Additionally, the Adaptive Movement Model generalized UAV motion to arbitrary directions through a rotation-based mechanism, validating the scalability of the proposed system.\n  Overall, UAV swarms equipped with null-steering antennas and guided by intelligent optimization algorithms effectively mitigate jamming while maintaining communication stability, formation cohesion, and collision safety. The proposed framework establishes a unified, flexible, and reproducible basis for future research on resilient swarm communication systems.", "AI": {"tldr": "UAV\u96c6\u7fa4\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u3001\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u4e00\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u96f6\u9677\u5929\u7ebf\u6280\u672f\uff0c\u6709\u6548\u62b5\u5fa1\u5e72\u6270\u5e76\u4fdd\u6301\u901a\u4fe1\u7a33\u5b9a\u6027\u548c\u4efb\u52a1\u6548\u7387\u3002", "motivation": "\u65e0\u4eba\u673a\u96c6\u7fa4\u4f9d\u8d56\u65e0\u7ebf\u901a\u4fe1\uff0c\u5bb9\u6613\u53d7\u5230\u5e72\u6270\u653b\u51fb\uff0c\u8fd9\u4f1a\u7834\u574f\u534f\u8c03\u548c\u4efb\u52a1\u6210\u529f\u7387\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5982\u4f55\u6709\u6548\u514b\u670d\u5e72\u6270\u3002", "method": "\u63d0\u51fa\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u3001\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u7edf\u4e00\u4f18\u5316\u6846\u67b6\uff0c\u91c7\u7528\u5206\u65f6\u6bb5\u4efb\u52a1\u6a21\u578b\u8fdb\u884c\u52a8\u6001\u8def\u5f84\u89c4\u5212\u3001\u5929\u7ebf\u5b9a\u5411\u548c\u96c6\u7fa4\u7f16\u961f\uff0c\u4f7f\u7528\u96f6\u9677\u5929\u7ebf\u6280\u672f\u5c06\u5929\u7ebf\u96f6\u9677\u6307\u5411\u5e72\u6270\u6e90\u3002", "result": "\u9057\u4f20\u7b97\u6cd5\u83b7\u5f97\u7a33\u5b9a\u65e0\u78b0\u649e\u8f68\u8ff9\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff1b\u76d1\u7763\u5b66\u4e60\u80fd\u590d\u5236\u9057\u4f20\u7b97\u6cd5\u914d\u7f6e\u4f46\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff1b\u5f3a\u5316\u5b66\u4e60\u5177\u6709\u9002\u5e94\u6027\u548c\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\uff0c\u901a\u4fe1\u7a33\u5b9a\u4e14\u8ba1\u7b97\u9700\u6c42\u4f4e\u3002", "conclusion": "\u914d\u5907\u96f6\u9677\u5929\u7ebf\u548c\u667a\u80fd\u4f18\u5316\u7b97\u6cd5\u7684\u65e0\u4eba\u673a\u96c6\u7fa4\u80fd\u6709\u6548\u7f13\u89e3\u5e72\u6270\uff0c\u4fdd\u6301\u901a\u4fe1\u7a33\u5b9a\u3001\u7f16\u961f\u51dd\u805a\u548c\u78b0\u649e\u5b89\u5168\uff0c\u4e3a\u5f39\u6027\u96c6\u7fa4\u901a\u4fe1\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u7075\u6d3b\u7684\u57fa\u7840\u3002"}}
{"id": "2511.17923", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17923", "abs": "https://arxiv.org/abs/2511.17923", "authors": ["Wenda Li", "Tongya Zheng", "Shunyu Liu", "Yu Wang", "Kaixuan Chen", "Hanyang Yuan", "Bingde Hu", "Zujie Ren", "Mingli Song", "Gang Chen"], "title": "Towards Efficient LLM-aware Heterogeneous Graph Learning", "comment": null, "summary": "Heterogeneous graphs are widely present in real-world complex networks, where the diversity of node and relation types leads to complex and rich semantics. Efforts for modeling complex relation semantics in heterogeneous graphs are restricted by the limitations of predefined semantic dependencies and the scarcity of supervised signals. The advanced pre-training and fine-tuning paradigm leverages graph structure to provide rich self-supervised signals, but introduces semantic gaps between tasks. Large Language Models (LLMs) offer significant potential to address the semantic issues of relations and tasks in heterogeneous graphs through their strong reasoning capabilities in textual modality, but their incorporation into heterogeneous graphs is largely limited by computational complexity. Therefore, in this paper, we propose an Efficient LLM-Aware (ELLA) framework for heterogeneous graphs, addressing the above issues. To capture complex relation semantics, we propose an LLM-aware Relation Tokenizer that leverages LLM to encode multi-hop, multi-type relations. To reduce computational complexity, we further employ a Hop-level Relation Graph Transformer, which help reduces the complexity of LLM-aware relation reasoning from exponential to linear. To bridge semantic gaps between pre-training and fine-tuning tasks, we introduce the fine-grained task-aware textual Chain-of-Thought (CoT) prompts. Extensive experiments on four heterogeneous graphs show that our proposed ELLA outperforms state-of-the-art methods in the performance and efficiency. In particular, ELLA scales up to 13b-parameter LLMs and achieves up to a 4x speedup compared with existing LLM-based methods. Our code is publicly available at https://github.com/l-wd/ELLA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684LLM\u611f\u77e5\u6846\u67b6ELLA\uff0c\u7528\u4e8e\u89e3\u51b3\u5f02\u8d28\u56fe\u4e2d\u590d\u6742\u5173\u7cfb\u8bed\u4e49\u5efa\u6a21\u95ee\u9898\uff0c\u901a\u8fc7LLM\u611f\u77e5\u7684\u5173\u7cfb\u5206\u8bcd\u5668\u3001\u8df3\u7ea7\u5173\u7cfb\u56fe\u53d8\u6362\u5668\u548c\u4efb\u52a1\u611f\u77e5\u7684\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f02\u8d28\u56fe\u4e2d\u8282\u70b9\u548c\u5173\u7cfb\u7c7b\u578b\u7684\u591a\u6837\u6027\u5bfc\u81f4\u590d\u6742\u4e30\u5bcc\u7684\u8bed\u4e49\uff0c\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u9884\u5b9a\u4e49\u7684\u8bed\u4e49\u4f9d\u8d56\u548c\u7f3a\u4e4f\u76d1\u7763\u4fe1\u53f7\u3002LLM\u5177\u6709\u5f3a\u5927\u7684\u6587\u672c\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u8ba1\u7b97\u590d\u6742\u5ea6\u9650\u5236\u4e86\u5176\u5728\u5f02\u8d28\u56fe\u4e2d\u7684\u5e94\u7528\u3002", "method": "1. LLM\u611f\u77e5\u5173\u7cfb\u5206\u8bcd\u5668\uff1a\u5229\u7528LLM\u7f16\u7801\u591a\u8df3\u591a\u7c7b\u578b\u5173\u7cfb\uff1b2. \u8df3\u7ea7\u5173\u7cfb\u56fe\u53d8\u6362\u5668\uff1a\u5c06LLM\u611f\u77e5\u5173\u7cfb\u63a8\u7406\u590d\u6742\u5ea6\u4ece\u6307\u6570\u7ea7\u964d\u81f3\u7ebf\u6027\u7ea7\uff1b3. \u7ec6\u7c92\u5ea6\u4efb\u52a1\u611f\u77e5\u601d\u7ef4\u94fe\u63d0\u793a\uff1a\u5f25\u5408\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4efb\u52a1\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u3002", "result": "\u5728\u56db\u4e2a\u5f02\u8d28\u56fe\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cELLA\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u53ef\u6269\u5c55\u5230130\u4ebf\u53c2\u6570LLM\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u5b9e\u73b0\u9ad8\u8fbe4\u500d\u52a0\u901f\u3002", "conclusion": "ELLA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u8d28\u56fe\u4e2d\u590d\u6742\u5173\u7cfb\u8bed\u4e49\u5efa\u6a21\u7684\u6311\u6218\uff0c\u901a\u8fc7\u521b\u65b0\u7684LLM\u96c6\u6210\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2511.18088", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18088", "abs": "https://arxiv.org/abs/2511.18088", "authors": ["Ibrahim Alsarraj", "Yuhao Wang", "Abdalla Swikir", "Cesare Stefanini", "Dezhen Song", "Zhanchi Wang", "Ke Wu"], "title": "A Unified Multi-Dynamics Framework for Perception-Oriented Modeling in Tendon-Driven Continuum Robots", "comment": null, "summary": "Tendon-driven continuum robots offer intrinsically safe and contact-rich interactions owing to their kinematic redundancy and structural compliance. However, their perception often depends on external sensors, which increase hardware complexity and limit scalability. This work introduces a unified multi-dynamics modeling framework for tendon-driven continuum robotic systems, exemplified by a spiral-inspired robot named Spirob. The framework integrates motor electrical dynamics, motor-winch dynamics, and continuum robot dynamics into a coherent system model. Within this framework, motor signals such as current and angular displacement are modeled to expose the electromechanical signatures of external interactions, enabling perception grounded in intrinsic dynamics. The model captures and validates key physical behaviors of the real system, including actuation hysteresis and self-contact at motion limits. Building on this foundation, the framework is applied to environmental interaction: first for passive contact detection, verified experimentally against simulation data; then for active contact sensing, where control and perception strategies from simulation are successfully applied to the real robot; and finally for object size estimation, where a policy learned in simulation is directly deployed on hardware. The results demonstrate that the proposed framework provides a physically grounded way to interpret interaction signatures from intrinsic motor signals in tendon-driven continuum robots.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u52a8\u529b\u5b66\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u808c\u8171\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u7535\u673a\u7535\u6c14\u52a8\u529b\u5b66\u3001\u7535\u673a\u5377\u7b52\u52a8\u529b\u5b66\u548c\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u52a8\u529b\u5b66\uff0c\u5b9e\u73b0\u57fa\u4e8e\u5185\u5728\u7535\u673a\u4fe1\u53f7\u7684\u5916\u90e8\u4ea4\u4e92\u611f\u77e5\u3002", "motivation": "\u808c\u8171\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u5177\u6709\u8fd0\u52a8\u5197\u4f59\u548c\u7ed3\u6784\u67d4\u987a\u6027\uff0c\u4f46\u611f\u77e5\u901a\u5e38\u4f9d\u8d56\u5916\u90e8\u4f20\u611f\u5668\uff0c\u589e\u52a0\u4e86\u786c\u4ef6\u590d\u6742\u6027\u548c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u5f00\u53d1\u7edf\u4e00\u7684\u591a\u52a8\u529b\u5b66\u5efa\u6a21\u6846\u67b6\uff0c\u6574\u5408\u7535\u673a\u7535\u6c14\u52a8\u529b\u5b66\u3001\u7535\u673a\u5377\u7b52\u52a8\u529b\u5b66\u548c\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u52a8\u529b\u5b66\uff0c\u901a\u8fc7\u7535\u673a\u7535\u6d41\u548c\u89d2\u4f4d\u79fb\u7b49\u4fe1\u53f7\u63ed\u793a\u5916\u90e8\u4ea4\u4e92\u7684\u673a\u7535\u7279\u5f81\u3002", "result": "\u6a21\u578b\u6210\u529f\u6355\u6349\u5e76\u9a8c\u8bc1\u4e86\u5b9e\u9645\u7cfb\u7edf\u7684\u5173\u952e\u7269\u7406\u884c\u4e3a\uff0c\u5305\u62ec\u9a71\u52a8\u8fdf\u6ede\u548c\u8fd0\u52a8\u6781\u9650\u7684\u81ea\u63a5\u89e6\u3002\u5728\u73af\u5883\u4ea4\u4e92\u5e94\u7528\u4e2d\uff0c\u5b9e\u73b0\u4e86\u88ab\u52a8\u63a5\u89e6\u68c0\u6d4b\u3001\u4e3b\u52a8\u63a5\u89e6\u611f\u77e5\u548c\u7269\u4f53\u5c3a\u5bf8\u4f30\u8ba1\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u808c\u8171\u9a71\u52a8\u8fde\u7eed\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7684\u65b9\u5f0f\u6765\u89e3\u91ca\u6765\u81ea\u5185\u5728\u7535\u673a\u4fe1\u53f7\u7684\u4ea4\u4e92\u7279\u5f81\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u5916\u90e8\u4f20\u611f\u5668\u7684\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2511.17938", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.17938", "abs": "https://arxiv.org/abs/2511.17938", "authors": ["Jianghao Wu", "Yasmeen George", "Jin Ye", "Yicheng Wu", "Daniel F. Schmidt", "Jianfei Cai"], "title": "SPINE: Token-Selective Test-Time Reinforcement Learning with Entropy-Band Regularization", "comment": null, "summary": "Large language models (LLMs) and multimodal LLMs (MLLMs) excel at chain-of-thought reasoning but face distribution shift at test-time and a lack of verifiable supervision. Recent test-time reinforcement learning (TTRL) methods derive label-free pseudo-rewards from self-consistency voting over sampled trajectories, yet they often collapse: the majority-vote reward prevails, responses shorten, and Pass@1 declines. We trace this to uniform sequence updates in which most tokens are low-entropy followers, while a small high-entropy subset determines the reasoning branches. Thus we propose SPINE, a token-selective test-time reinforcement learning framework that (i) updates only forking tokens, the high-entropy branch points identified from forward-pass statistics, and (ii) applies an entropy-band regularizer at those tokens to sustain exploration when entropy is too low and to suppress noisy supervision when it is too high. SPINE plugs into GRPO-style objectives, optionally with a KL anchor, and requires no labels or reward models. Across ten benchmarks spanning multimodal VQA, general and expert QA, mathematical reasoning, and medical QA, SPINE consistently improves Pass@1 over TTRL while avoiding response-length collapse and yielding more stable training dynamics on both LLM and MLLM backbones. These results indicate that aligning updates with chain-of-thought branch points is a simple and label-free mechanism for stable and effective test-time adaptation in reasoning models. Code is available at https://github.com/JianghaoWu/SPINE.", "AI": {"tldr": "SPINE\u662f\u4e00\u4e2a\u57fa\u4e8etoken\u9009\u62e9\u6027\u7684\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53ea\u66f4\u65b0\u9ad8\u71b5\u5206\u652f\u70b9token\u6765\u907f\u514d\u4f20\u7edf\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e2d\u7684\u54cd\u5e94\u957f\u5ea6\u5d29\u6e83\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u4e0a\u7a33\u5b9a\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u5206\u5e03\u504f\u79fb\u3001\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u76d1\u7763\u4ee5\u53ca\u591a\u6570\u6295\u7968\u5956\u52b1\u5bfc\u81f4\u54cd\u5e94\u7f29\u77ed\u548c\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7a33\u5b9a\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u673a\u5236\u3002", "method": "SPINE\u6846\u67b6\uff1a(1) \u53ea\u66f4\u65b0\u5206\u53c9token\uff08\u901a\u8fc7\u524d\u5411\u4f20\u9012\u7edf\u8ba1\u8bc6\u522b\u7684\u9ad8\u71b5\u5206\u652f\u70b9\uff09\uff1b(2) \u5728\u5206\u53c9token\u5904\u5e94\u7528\u71b5\u5e26\u6b63\u5219\u5316\u5668\uff0c\u5728\u71b5\u8fc7\u4f4e\u65f6\u7ef4\u6301\u63a2\u7d22\uff0c\u5728\u71b5\u8fc7\u9ad8\u65f6\u6291\u5236\u566a\u58f0\u76d1\u7763\u3002", "result": "\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPINE\u76f8\u6bd4\u4f20\u7edf\u6d4b\u8bd5\u65f6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6301\u7eed\u63d0\u5347Pass@1\u6027\u80fd\uff0c\u907f\u514d\u54cd\u5e94\u957f\u5ea6\u5d29\u6e83\uff0c\u5e76\u5728LLM\u548cMLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u4ea7\u751f\u66f4\u7a33\u5b9a\u7684\u8bad\u7ec3\u52a8\u6001\u3002", "conclusion": "\u5c06\u66f4\u65b0\u4e0e\u601d\u7ef4\u94fe\u5206\u652f\u70b9\u5bf9\u9f50\u662f\u4e00\u79cd\u7b80\u5355\u4e14\u65e0\u9700\u6807\u7b7e\u7684\u673a\u5236\uff0c\u80fd\u591f\u5728\u63a8\u7406\u6a21\u578b\u4e2d\u5b9e\u73b0\u7a33\u5b9a\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u3002"}}
{"id": "2511.18112", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18112", "abs": "https://arxiv.org/abs/2511.18112", "authors": ["Min Lin", "Xiwen Liang", "Bingqian Lin", "Liu Jingzhi", "Zijian Jiao", "Kehan Li", "Yuhan Ma", "Yuecheng Liu", "Shen Zhao", "Yuzheng Zhuang", "Xiaodan Liang"], "title": "EchoVLA: Robotic Vision-Language-Action Model with Synergistic Declarative Memory for Mobile Manipulation", "comment": null, "summary": "Recent progress in Vision-Language-Action (VLA) models has enabled embodied agents to interpret multimodal instructions and perform complex tasks. However, existing VLAs are mostly confined to short-horizon, table-top manipulation, lacking the memory and reasoning capability required for long-horizon mobile manipulation, where agents must coordinate navigation and manipulation under changing spatial contexts. In this work, we present EchoVLA, a memory-aware VLA model for long-horizon mobile manipulation. EchoVLA incorporates a synergistic declarative memory inspired by the human brain, consisting of a scene memory that maintains a collection of spatial-semantic maps and an episodic memory that stores task-level experiences with multimodal contextual features. During both training and inference, the two memories are individually stored, updated, and retrieved based on current observations, task history, and instructions, and their retrieved representations are fused via coarse- and fine-grained attention to guide mobile-arm diffusion policies. To support large-scale training and evaluation, we further introduce MoMani, an automated benchmark that generates expert-level long-horizon trajectories through multimodal large language model (MLLM)-guided planning and feedback-driven refinement, supplemented with real-robot demonstrations. Experiments in simulated and real-world settings show that EchoVLA improves long-horizon performance, reaching 0.52 SR on manipulation/navigation and 0.31 on mobile manipulation, exceeding $\u03c0_{0.5}$ by +0.08 and +0.11.", "AI": {"tldr": "EchoVLA\u662f\u4e00\u4e2a\u5177\u6709\u8bb0\u5fc6\u80fd\u529b\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u4e13\u4e3a\u957f\u65f6\u7a0b\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u8bbe\u8ba1\uff0c\u901a\u8fc7\u573a\u666f\u8bb0\u5fc6\u548c\u60c5\u666f\u8bb0\u5fc6\u7684\u534f\u540c\u5de5\u4f5c\u6765\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e\u77ed\u65f6\u7a0b\u684c\u9762\u64cd\u4f5c\uff0c\u7f3a\u4e4f\u5904\u7406\u957f\u65f6\u7a0b\u79fb\u52a8\u64cd\u4f5c\u6240\u9700\u7684\u8bb0\u5fc6\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5176\u4e2d\u667a\u80fd\u4f53\u9700\u8981\u5728\u53d8\u5316\u7684\u7a7a\u95f4\u73af\u5883\u4e2d\u534f\u8c03\u5bfc\u822a\u548c\u64cd\u4f5c\u3002", "method": "EchoVLA\u5f15\u5165\u4e86\u53d7\u4eba\u7c7b\u5927\u8111\u542f\u53d1\u7684\u58f0\u660e\u6027\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5305\u62ec\u7ef4\u62a4\u7a7a\u95f4\u8bed\u4e49\u5730\u56fe\u7684\u573a\u666f\u8bb0\u5fc6\u548c\u5b58\u50a8\u4efb\u52a1\u7ea7\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u7279\u5f81\u7684\u60c5\u666f\u8bb0\u5fc6\u3002\u901a\u8fc7\u7c97\u7c92\u5ea6\u548c\u7ec6\u7c92\u5ea6\u6ce8\u610f\u529b\u878d\u5408\u8bb0\u5fc6\u8868\u793a\u6765\u6307\u5bfc\u79fb\u52a8\u81c2\u6269\u6563\u7b56\u7565\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cEchoVLA\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u64cd\u4f5c/\u5bfc\u822a\u4efb\u52a1\u8fbe\u52300.52\u6210\u529f\u7387\uff0c\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\u8fbe\u52300.31\u6210\u529f\u7387\uff0c\u5206\u522b\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u5347\u4e860.08\u548c0.11\u3002", "conclusion": "EchoVLA\u901a\u8fc7\u8bb0\u5fc6\u589e\u5f3a\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u79fb\u52a8\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u6267\u884c\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.17946", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.17946", "abs": "https://arxiv.org/abs/2511.17946", "authors": ["Shuo Zhang", "Fabrizio Gotti", "Fengran Mo", "Jian-Yun Nie"], "title": "Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models", "comment": null, "summary": "Hallucination in large language models (LLMs) is a fundamental challenge, particularly in open-domain question answering. Prior work attempts to detect hallucination with model-internal signals such as token-level entropy or generation consistency, while the connection between pretraining data exposure and hallucination is underexplored. Existing studies show that LLMs underperform on long-tail knowledge, i.e., the accuracy of the generated answer drops for the ground-truth entities that are rare in pretraining. However, examining whether data coverage itself can serve as a detection signal is overlooked. We propose a complementary question: Does lexical training-data coverage of the question and/or generated answer provide additional signal for hallucination detection? To investigate this, we construct scalable suffix arrays over RedPajama's 1.3-trillion-token pretraining corpus to retrieve $n$-gram statistics for both prompts and model generations. We evaluate their effectiveness for hallucination detection across three QA benchmarks. Our observations show that while occurrence-based features are weak predictors when used alone, they yield modest gains when combined with log-probabilities, particularly on datasets with higher intrinsic model uncertainty. These findings suggest that lexical coverage features provide a complementary signal for hallucination detection. All code and suffix-array infrastructure are provided at https://github.com/WWWonderer/ostd.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u9884\u8bad\u7ec3\u6570\u636e\u8986\u76d6\u5ea6\u4f5c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u68c0\u6d4b\u8865\u5145\u4fe1\u53f7\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u5728\u9ad8\u4e0d\u786e\u5b9a\u6027\u6570\u636e\u96c6\u4e0a\uff0c\u8bcd\u6c47\u8986\u76d6\u7279\u5f81\u4e0e\u5bf9\u6570\u6982\u7387\u7ed3\u5408\u80fd\u5e26\u6765\u9002\u5ea6\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f7f\u7528\u6a21\u578b\u5185\u90e8\u4fe1\u53f7\u68c0\u6d4b\u5e7b\u89c9\uff0c\u4f46\u9884\u8bad\u7ec3\u6570\u636e\u66b4\u9732\u4e0e\u5e7b\u89c9\u4e4b\u95f4\u7684\u5173\u7cfb\u7814\u7a76\u4e0d\u8db3\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u95ee\u9898\u6216\u751f\u6210\u7b54\u6848\u7684\u8bcd\u6c47\u8bad\u7ec3\u6570\u636e\u8986\u76d6\u5ea6\u662f\u5426\u80fd\u63d0\u4f9b\u989d\u5916\u7684\u5e7b\u89c9\u68c0\u6d4b\u4fe1\u53f7\u3002", "method": "\u5728RedPajama\u76841.3\u4e07\u4ebftoken\u9884\u8bad\u7ec3\u8bed\u6599\u4e0a\u6784\u5efa\u53ef\u6269\u5c55\u540e\u7f00\u6570\u7ec4\uff0c\u68c0\u7d22\u63d0\u793a\u548c\u6a21\u578b\u751f\u6210\u7684n-gram\u7edf\u8ba1\u4fe1\u606f\uff0c\u5e76\u5728\u4e09\u4e2aQA\u57fa\u51c6\u4e0a\u8bc4\u4f30\u5176\u5e7b\u89c9\u68c0\u6d4b\u6548\u679c\u3002", "result": "\u57fa\u4e8e\u51fa\u73b0\u9891\u7387\u7684\u7279\u5f81\u5355\u72ec\u4f7f\u7528\u65f6\u9884\u6d4b\u80fd\u529b\u8f83\u5f31\uff0c\u4f46\u4e0e\u5bf9\u6570\u6982\u7387\u7ed3\u5408\u65f6\u80fd\u5e26\u6765\u9002\u5ea6\u589e\u76ca\uff0c\u7279\u522b\u662f\u5728\u5185\u5728\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u8f83\u9ad8\u7684\u6570\u636e\u96c6\u4e0a\u3002", "conclusion": "\u8bcd\u6c47\u8986\u76d6\u7279\u5f81\u4e3a\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u8865\u5145\u4fe1\u53f7\uff0c\u8868\u660e\u9884\u8bad\u7ec3\u6570\u636e\u8986\u76d6\u5ea6\u53ef\u4ee5\u4f5c\u4e3a\u6709\u7528\u7684\u68c0\u6d4b\u6307\u6807\u3002"}}
{"id": "2511.18140", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18140", "abs": "https://arxiv.org/abs/2511.18140", "authors": ["Yilong Wang", "Cheng Qian", "Ruomeng Fan", "Edward Johns"], "title": "Observer Actor: Active Vision Imitation Learning with Sparse View Gaussian Splatting", "comment": "Videos are available on our project webpage at https://obact.github.io", "summary": "We propose Observer Actor (ObAct), a novel framework for active vision imitation learning in which the observer moves to optimal visual observations for the actor. We study ObAct on a dual-arm robotic system equipped with wrist-mounted cameras. At test time, ObAct dynamically assigns observer and actor roles: the observer arm constructs a 3D Gaussian Splatting (3DGS) representation from three images, virtually explores this to find an optimal camera pose, then moves to this pose; the actor arm then executes a policy using the observer's observations. This formulation enhances the clarity and visibility of both the object and the gripper in the policy's observations. As a result, we enable the training of ambidextrous policies on observations that remain closer to the occlusion-free training distribution, leading to more robust policies. We study this formulation with two existing imitation learning methods -- trajectory transfer and behavior cloning -- and experiments show that ObAct significantly outperforms static-camera setups: trajectory transfer improves by 145% without occlusion and 233% with occlusion, while behavior cloning improves by 75% and 143%, respectively. Videos are available at https://obact.github.io.", "AI": {"tldr": "ObAct\u662f\u4e00\u4e2a\u4e3b\u52a8\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u914d\u89c2\u5bdf\u8005\u548c\u6267\u884c\u8005\u89d2\u8272\u6765\u4f18\u5316\u89c6\u89c9\u89c2\u5bdf\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7b56\u7565\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u9759\u6001\u76f8\u673a\u8bbe\u7f6e\u4e2d\u7269\u4f53\u548c\u5939\u722a\u906e\u6321\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u53cc\u81c2\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u89c2\u5bdf\u8005\u81c2\u6784\u5efa3D\u9ad8\u65af\u6cfc\u6e85\u8868\u793a\u5e76\u865a\u62df\u63a2\u7d22\u6700\u4f73\u76f8\u673a\u4f4d\u59ff\uff0c\u6267\u884c\u8005\u81c2\u57fa\u4e8e\u4f18\u5316\u540e\u7684\u89c2\u5bdf\u6267\u884c\u7b56\u7565\u3002", "result": "\u76f8\u6bd4\u9759\u6001\u76f8\u673a\u8bbe\u7f6e\uff0c\u8f68\u8ff9\u4f20\u9012\u6027\u80fd\u63d0\u5347145%\uff08\u65e0\u906e\u6321\uff09\u548c233%\uff08\u6709\u906e\u6321\uff09\uff0c\u884c\u4e3a\u514b\u9686\u63d0\u534775%\u548c143%\u3002", "conclusion": "ObAct\u6846\u67b6\u901a\u8fc7\u4e3b\u52a8\u89c6\u89c9\u89c2\u5bdf\u663e\u8457\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u7b56\u7565\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2511.17955", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.17955", "abs": "https://arxiv.org/abs/2511.17955", "authors": ["Dat Thanh Nguyen", "Nguyen Hung Lam", "Anh Hoang-Thi Nguyen", "Trong-Hop Do"], "title": "MTikGuard System: A Transformer-Based Multimodal System for Child-Safe Content Moderation on TikTok", "comment": "Accepted at PACLIC39", "summary": "With the rapid rise of short-form videos, TikTok has become one of the most influential platforms among children and teenagers, but also a source of harmful content that can affect their perception and behavior. Such content, often subtle or deceptive, challenges traditional moderation methods due to the massive volume and real-time nature of uploads. This paper presents MTikGuard, a real-time multimodal harmful content detection system for TikTok, with three key contributions: (1) an extended TikHarm dataset expanded to 4,723 labeled videos by adding diverse real-world samples, (2) a multimodal classification framework integrating visual, audio, and textual features to achieve state-of-the-art performance with 89.37% accuracy and 89.45% F1-score, and (3) a scalable streaming architecture built on Apache Kafka and Apache Spark for real-time deployment. The results demonstrate the effectiveness of combining dataset expansion, advanced multimodal fusion, and robust deployment for practical large-scale social media content moderation. The dataset is available at https://github.com/ntdat-8324/MTikGuard-System.git.", "AI": {"tldr": "MTikGuard\u662f\u4e00\u4e2a\u9488\u5bf9TikTok\u7684\u5b9e\u65f6\u591a\u6a21\u6001\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u6269\u5c55\u6570\u636e\u96c6\u3001\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u548c\u53ef\u6269\u5c55\u6d41\u5f0f\u67b6\u6784\uff0c\u5b9e\u73b0\u4e8689.37%\u7684\u51c6\u786e\u7387\u548c89.45%\u7684F1\u5206\u6570\u3002", "motivation": "TikTok\u4f5c\u4e3a\u513f\u7ae5\u548c\u9752\u5c11\u5e74\u4e2d\u6781\u5177\u5f71\u54cd\u529b\u7684\u5e73\u53f0\uff0c\u5b58\u5728\u5927\u91cf\u6709\u5bb3\u5185\u5bb9\uff0c\u8fd9\u4e9b\u5185\u5bb9\u5f80\u5f80\u9690\u853d\u6216\u5177\u6709\u6b3a\u9a97\u6027\uff0c\u4f20\u7edf\u5ba1\u6838\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u6d77\u91cf\u5b9e\u65f6\u4e0a\u4f20\u7684\u6311\u6218\u3002", "method": "\u6269\u5c55TikHarm\u6570\u636e\u96c6\u81f34,723\u4e2a\u6807\u6ce8\u89c6\u9891\uff1b\u6784\u5efa\u96c6\u6210\u89c6\u89c9\u3001\u97f3\u9891\u548c\u6587\u672c\u7279\u5f81\u7684\u591a\u6a21\u6001\u5206\u7c7b\u6846\u67b6\uff1b\u57fa\u4e8eApache Kafka\u548cApache Spark\u6784\u5efa\u53ef\u6269\u5c55\u7684\u6d41\u5f0f\u67b6\u6784\u3002", "result": "\u7cfb\u7edf\u5728\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\u4e0a\u8fbe\u523089.37%\u7684\u51c6\u786e\u7387\u548c89.45%\u7684F1\u5206\u6570\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u878d\u5408\u548c\u5b9e\u65f6\u90e8\u7f72\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u5408\u6570\u636e\u96c6\u6269\u5c55\u3001\u5148\u8fdb\u7684\u591a\u6a21\u6001\u878d\u5408\u548c\u7a33\u5065\u90e8\u7f72\uff0c\u80fd\u591f\u4e3a\u5927\u89c4\u6a21\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18153", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18153", "abs": "https://arxiv.org/abs/2511.18153", "authors": ["Shreyas Kumar", "Barat S", "Debojit Das", "Yug Desai", "Siddhi Jain", "Rajesh Kumar", "Harish J. Palanthandalam-Madapusi"], "title": "A Coordinated Dual-Arm Framework for Delicate Snap-Fit Assemblies", "comment": "10 pages, 9 figures", "summary": "Delicate snap-fit assemblies, such as inserting a lens into an eye-wear frame or during electronics assembly, demand timely engagement detection and rapid force attenuation to prevent overshoot-induced component damage or assembly failure. We address these challenges with two key contributions. First, we introduce SnapNet, a lightweight neural network that detects snap-fit engagement from joint-velocity transients in real-time, showing that reliable detection can be achieved using proprioceptive signals without external sensors. Second, we present a dynamical-systems-based dual-arm coordination framework that integrates SnapNet driven detection with an event-triggered impedance modulation, enabling accurate alignment and compliant insertion during delicate snap-fit assemblies. Experiments across diverse geometries on a heterogeneous bimanual platform demonstrate high detection accuracy (over 96% recall) and up to a 30% reduction in peak impact forces compared to standard impedance control.", "AI": {"tldr": "\u63d0\u51faSnapNet\u795e\u7ecf\u7f51\u7edc\u5b9e\u65f6\u68c0\u6d4b\u5361\u6263\u88c5\u914d\u7684\u556e\u5408\uff0c\u7ed3\u5408\u57fa\u4e8e\u52a8\u6001\u7cfb\u7edf\u7684\u53cc\u81c2\u534f\u8c03\u6846\u67b6\uff0c\u5b9e\u73b0\u7cbe\u5bc6\u5361\u6263\u88c5\u914d\u4e2d\u7684\u51c6\u786e\u5b9a\u4f4d\u548c\u67d4\u987a\u63d2\u5165\uff0c\u663e\u8457\u964d\u4f4e\u51b2\u51fb\u529b\u3002", "motivation": "\u7cbe\u5bc6\u5361\u6263\u88c5\u914d\uff08\u5982\u955c\u7247\u63d2\u5165\u773c\u955c\u6846\u6216\u7535\u5b50\u7ec4\u88c5\uff09\u9700\u8981\u53ca\u65f6\u68c0\u6d4b\u556e\u5408\u5e76\u5feb\u901f\u8870\u51cf\u529b\uff0c\u4ee5\u9632\u6b62\u8fc7\u51b2\u5bfc\u81f4\u7684\u7ec4\u4ef6\u635f\u574f\u6216\u88c5\u914d\u5931\u8d25\u3002", "method": "1. \u5f15\u5165\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edcSnapNet\uff0c\u4ece\u5173\u8282\u901f\u5ea6\u77ac\u53d8\u4e2d\u5b9e\u65f6\u68c0\u6d4b\u5361\u6263\u556e\u5408\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u52a8\u6001\u7cfb\u7edf\u7684\u53cc\u81c2\u534f\u8c03\u6846\u67b6\uff0c\u96c6\u6210SnapNet\u68c0\u6d4b\u4e0e\u4e8b\u4ef6\u89e6\u53d1\u7684\u963b\u6297\u8c03\u5236\u3002", "result": "\u5728\u5f02\u6784\u53cc\u624b\u5e73\u53f0\u4e0a\u5bf9\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u8fdb\u884c\u5b9e\u9a8c\uff0c\u663e\u793a\u68c0\u6d4b\u51c6\u786e\u7387\u9ad8\uff08\u53ec\u56de\u7387\u8d85\u8fc796%\uff09\uff0c\u4e0e\u6807\u51c6\u963b\u6297\u63a7\u5236\u76f8\u6bd4\u5cf0\u503c\u51b2\u51fb\u529b\u964d\u4f4e\u8fbe30%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ec5\u4f7f\u7528\u672c\u4f53\u611f\u53d7\u4fe1\u53f7\u5373\u53ef\u5b9e\u73b0\u53ef\u9760\u7684\u5361\u6263\u556e\u5408\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u4e8b\u4ef6\u89e6\u53d1\u7684\u963b\u6297\u8c03\u5236\u5b9e\u73b0\u7cbe\u5bc6\u88c5\u914d\u4e2d\u7684\u51c6\u786e\u5b9a\u4f4d\u548c\u67d4\u987a\u63d2\u5165\u3002"}}
{"id": "2511.18054", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18054", "abs": "https://arxiv.org/abs/2511.18054", "authors": ["Gowtham", "Sai Rupesh", "Sanjay Kumar", "Saravanan", "Venkata Chaithanya"], "title": "Blu-WERP (Web Extraction and Refinement Pipeline): A Scalable Pipeline for Preprocessing Large Language Model Datasets", "comment": null, "summary": "High-quality training data is fundamental to large language model (LLM) performance, yet existing preprocessing pipelines often struggle to effectively remove noise and unstructured content from web-scale corpora. This paper presents Blu-WERP, a novel data preprocessing pipeline designed to optimize the quality of Common Crawl WARC files for LLM training. We demonstrate that Blu-WERP significantly outperforms established baselines including DCLM across multiple model scales and evaluation benchmarks. Our pipeline processes CC WARC dumps, implementing advanced filtering and quality assessment mechanisms. We conducted comprehensive evaluations using models with 150M, 400M, 530M, 750M, and 1B parameters, testing against nine standard benchmarks categorized as World Knowledge & Reasoning, Language Understanding, and Commonsense Reasoning. Results show Blu-WERP consistently achieved superior performance across all model scales. At the 1B parameter scale, Relatively Blu-WERP demonstrates a 4.0% and 9.5% aggregate improvement over DCLM and Fineweb respectively, while achieving quality-per-token efficiency gain. Categorical analysis reveals 2.4% improvement in World Knowledge & Reasoning, 6.2% improvement in Language Understanding, and 4.2% improvement in Commonsense Reasoning. These results establish Blu-WERP as a state-of-the-art preprocessing pipeline that substantially improves LLM training data quality and downstream model performance with reduced computational cost. Our findings contribute to the growing body of research on data-centric AI, demonstrating that preprocessing pipeline design significantly impacts LLM capabilities. The Blu-WERP pipeline represents a practical advancement in data quality optimization, offering researchers and practitioners an effective solution for improving LLM training efficiency and model performance.", "AI": {"tldr": "Blu-WERP\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6570\u636e\u9884\u5904\u7406\u7ba1\u9053\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\u548c\u8bc4\u4f30\u57fa\u51c6\u4e0a\u90fd\u80fd\u63d0\u5347LLM\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u548c\u4e0b\u6e38\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u9884\u5904\u7406\u7ba1\u9053\u96be\u4ee5\u6709\u6548\u53bb\u9664\u7f51\u7edc\u89c4\u6a21\u8bed\u6599\u5e93\u4e2d\u7684\u566a\u58f0\u548c\u975e\u7ed3\u6784\u5316\u5185\u5bb9\uff0c\u5f71\u54cd\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "Blu-WERP\u5904\u7406CC WARC\u8f6c\u50a8\u6587\u4ef6\uff0c\u5b9e\u73b0\u9ad8\u7ea7\u8fc7\u6ee4\u548c\u8d28\u91cf\u8bc4\u4f30\u673a\u5236\uff0c\u4f18\u5316Common Crawl WARC\u6587\u4ef6\u8d28\u91cf\u3002", "result": "\u5728150M\u52301B\u53c2\u6570\u7684\u591a\u4e2a\u6a21\u578b\u89c4\u6a21\u4e0a\uff0cBlu-WERP\u57289\u4e2a\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e00\u81f4\u4f18\u4e8e\u57fa\u7ebf\u3002\u57281B\u53c2\u6570\u89c4\u6a21\u4e0b\uff0c\u76f8\u6bd4DCLM\u548cFineweb\u5206\u522b\u5b9e\u73b04.0%\u548c9.5%\u7684\u603b\u4f53\u6539\u8fdb\uff0c\u5e76\u5728\u4e16\u754c\u77e5\u8bc6\u4e0e\u63a8\u7406\u3001\u8bed\u8a00\u7406\u89e3\u3001\u5e38\u8bc6\u63a8\u7406\u4e09\u4e2a\u7c7b\u522b\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "Blu-WERP\u4ee3\u8868\u4e86\u6570\u636e\u8d28\u91cf\u4f18\u5316\u7684\u5b9e\u9645\u8fdb\u5c55\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u63d0\u9ad8LLM\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18170", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18170", "abs": "https://arxiv.org/abs/2511.18170", "authors": ["Kaier Liang", "Licheng Luo", "Yixuan Wang", "Mingyu Cai", "Cristian Ioan Vasile"], "title": "Time-aware Motion Planning in Dynamic Environments with Conformal Prediction", "comment": null, "summary": "Safe navigation in dynamic environments remains challenging due to uncertain obstacle behaviors and the lack of formal prediction guarantees. We propose two motion planning frameworks that leverage conformal prediction (CP): a global planner that integrates Safe Interval Path Planning (SIPP) for uncertainty-aware trajectory generation, and a local planner that performs online reactive planning. The global planner offers distribution-free safety guarantees for long-horizon navigation, while the local planner mitigates inaccuracies in obstacle trajectory predictions through adaptive CP, enabling robust and responsive motion in dynamic environments. To further enhance trajectory feasibility, we introduce an adaptive quantile mechanism in the CP-based uncertainty quantification. Instead of using a fixed confidence level, the quantile is automatically tuned to the optimal value that preserves trajectory feasibility, allowing the planner to adaptively tighten safety margins in regions with higher uncertainty. We validate the proposed framework through numerical experiments conducted in dynamic and cluttered environments. The project page is available at https://time-aware-planning.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u4e2a\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff1a\u5168\u5c40\u89c4\u5212\u5668\u96c6\u6210SIPP\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8f68\u8ff9\u751f\u6210\uff0c\u5c40\u90e8\u89c4\u5212\u5668\u6267\u884c\u5728\u7ebf\u53cd\u5e94\u5f0f\u89c4\u5212\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5171\u5f62\u9884\u6d4b\u589e\u5f3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\u9762\u4e34\u969c\u788d\u7269\u884c\u4e3a\u4e0d\u786e\u5b9a\u6027\u548c\u7f3a\u4e4f\u5f62\u5f0f\u5316\u9884\u6d4b\u4fdd\u8bc1\u7684\u6311\u6218\u3002", "method": "\u5168\u5c40\u89c4\u5212\u5668\u63d0\u4f9b\u65e0\u5206\u5e03\u5b89\u5168\u4fdd\u8bc1\uff0c\u5c40\u90e8\u89c4\u5212\u5668\u901a\u8fc7\u81ea\u9002\u5e94\u5171\u5f62\u9884\u6d4b\u7f13\u89e3\u969c\u788d\u7269\u8f68\u8ff9\u9884\u6d4b\u4e0d\u51c6\u786e\u6027\uff0c\u5e76\u5f15\u5165\u81ea\u9002\u5e94\u5206\u4f4d\u6570\u673a\u5236\u4f18\u5316\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "result": "\u5728\u52a8\u6001\u548c\u62e5\u6324\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u80fd\u591f\u9002\u5e94\u6027\u5730\u6536\u7d27\u5b89\u5168\u8fb9\u754c\uff0c\u5728\u4e0d\u786e\u5b9a\u6027\u8f83\u9ad8\u533a\u57df\u4fdd\u6301\u8f68\u8ff9\u53ef\u884c\u6027\uff0c\u5b9e\u73b0\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u8fd0\u52a8\u89c4\u5212\u3002"}}
{"id": "2511.18146", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18146", "abs": "https://arxiv.org/abs/2511.18146", "authors": ["Yomal De Mel", "Nisansa de Silva"], "title": "GeeSanBhava: Sentiment Tagged Sinhala Music Video Comment Data Set", "comment": null, "summary": "This study introduce GeeSanBhava, a high-quality data set of Sinhala song comments extracted from YouTube manually tagged using Russells Valence-Arousal model by three independent human annotators. The human annotators achieve a substantial inter-annotator agreement (Fleiss kappa = 84.96%). The analysis revealed distinct emotional profiles for different songs, highlighting the importance of comment based emotion mapping. The study also addressed the challenges of comparing comment-based and song-based emotions, mitigating biases inherent in user-generated content. A number of Machine learning and deep learning models were pre-trained on a related large data set of Sinhala News comments in order to report the zero-shot result of our Sinhala YouTube comment data set. An optimized Multi-Layer Perceptron model, after extensive hyperparameter tuning, achieved a ROC-AUC score of 0.887. The model is a three-layer MLP with a configuration of 256, 128, and 64 neurons. This research contributes a valuable annotated dataset and provides insights for future work in Sinhala Natural Language Processing and music emotion recognition.", "AI": {"tldr": "\u672c\u7814\u7a76\u521b\u5efa\u4e86GeeSanBhava\u6570\u636e\u96c6\uff0c\u5305\u542b\u4eceYouTube\u63d0\u53d6\u7684\u50e7\u4f3d\u7f57\u8bed\u6b4c\u66f2\u8bc4\u8bba\uff0c\u4f7f\u7528Russell\u7684\u6548\u4ef7-\u5524\u9192\u6a21\u578b\u624b\u52a8\u6807\u6ce8\uff0c\u5e76\u5f00\u53d1\u4e86\u4f18\u5316\u7684\u591a\u5c42\u611f\u77e5\u5668\u6a21\u578b\u8fdb\u884c\u60c5\u611f\u5206\u7c7b\uff0cROC-AUC\u5f97\u5206\u4e3a0.887\u3002", "motivation": "\u89e3\u51b3\u50e7\u4f3d\u7f57\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u60c5\u611f\u6807\u6ce8\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u57fa\u4e8e\u8bc4\u8bba\u7684\u97f3\u4e50\u60c5\u611f\u8bc6\u522b\uff0c\u5e76\u5e94\u5bf9\u7528\u6237\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u504f\u89c1\u6311\u6218\u3002", "method": "\u4eceYouTube\u63d0\u53d6\u50e7\u4f3d\u7f57\u8bed\u6b4c\u66f2\u8bc4\u8bba\uff0c\u7531\u4e09\u540d\u72ec\u7acb\u6807\u6ce8\u8005\u4f7f\u7528Russell\u6548\u4ef7-\u5524\u9192\u6a21\u578b\u624b\u52a8\u6807\u6ce8\uff1b\u4f7f\u7528\u5728\u50e7\u4f3d\u7f57\u8bed\u65b0\u95fb\u8bc4\u8bba\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u7684\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u6d4b\u8bd5\uff1b\u5f00\u53d1\u4f18\u5316\u7684\u4e09\u5c42MLP\u6a21\u578b\uff08256-128-64\u795e\u7ecf\u5143\u914d\u7f6e\uff09\u3002", "result": "\u6807\u6ce8\u8005\u95f4\u4e00\u81f4\u6027\u9ad8\uff08Fleiss kappa = 84.96%\uff09\uff1b\u4e0d\u540c\u6b4c\u66f2\u5448\u73b0\u660e\u663e\u4e0d\u540c\u7684\u60c5\u611f\u7279\u5f81\uff1b\u4f18\u5316\u7684MLP\u6a21\u578b\u5728\u60c5\u611f\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u52300.887\u7684ROC-AUC\u5f97\u5206\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u50e7\u4f3d\u7f57\u8bedNLP\u548c\u97f3\u4e50\u60c5\u611f\u8bc6\u522b\u9886\u57df\u8d21\u732e\u4e86\u6709\u4ef7\u503c\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2511.18183", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18183", "abs": "https://arxiv.org/abs/2511.18183", "authors": ["Yixuan Jia", "Qingyuan Li", "Jonathan P. How"], "title": "Off-Road Navigation via Implicit Neural Representation of Terrain Traversability", "comment": "9 pages", "summary": "Autonomous off-road navigation requires robots to estimate terrain traversability from onboard sensors and plan accordingly. Conventional approaches typically rely on sampling-based planners such as MPPI to generate short-term control actions that aim to minimize traversal time and risk measures derived from the traversability estimates. These planners can react quickly but optimize only over a short look-ahead window, limiting their ability to reason about the full path geometry, which is important for navigating in challenging off-road environments. Moreover, they lack the ability to adjust speed based on the terrain bumpiness, which is important for smooth navigation on challenging terrains. In this paper, we introduce TRAIL (Traversability with an Implicit Learned Representation), an off-road navigation framework that leverages an implicit neural representation to continuously parameterize terrain properties. This representation yields spatial gradients that enable integration with a novel gradient-based trajectory optimization method that adapts the path geometry and speed profile based on terrain traversability.", "AI": {"tldr": "\u63d0\u51faTRAIL\u6846\u67b6\uff0c\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u8fde\u7eed\u53c2\u6570\u5316\u5730\u5f62\u5c5e\u6027\uff0c\u7ed3\u5408\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\u540c\u65f6\u8c03\u6574\u8def\u5f84\u51e0\u4f55\u548c\u901f\u5ea6\u5256\u9762\u4ee5\u9002\u5e94\u5730\u5f62\u53ef\u901a\u884c\u6027", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u91c7\u6837\u7684\u89c4\u5212\u5668\u53ea\u80fd\u4f18\u5316\u77ed\u671f\u63a7\u5236\u52a8\u4f5c\uff0c\u65e0\u6cd5\u8003\u8651\u5b8c\u6574\u8def\u5f84\u51e0\u4f55\uff0c\u4e14\u7f3a\u4e4f\u6839\u636e\u5730\u5f62\u98a0\u7c38\u5ea6\u8c03\u6574\u901f\u5ea6\u7684\u80fd\u529b", "method": "\u4f7f\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\u8fde\u7eed\u53c2\u6570\u5316\u5730\u5f62\u5c5e\u6027\uff0c\u7ed3\u5408\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\u540c\u65f6\u4f18\u5316\u8def\u5f84\u51e0\u4f55\u548c\u901f\u5ea6\u5256\u9762", "result": "\u5f00\u53d1\u4e86TRAIL\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u8003\u8651\u5730\u5f62\u53ef\u901a\u884c\u6027\u7684\u4f18\u5316\u8def\u5f84\u548c\u901f\u5ea6\u5256\u9762", "conclusion": "TRAIL\u6846\u67b6\u901a\u8fc7\u9690\u5f0f\u8868\u793a\u548c\u68af\u5ea6\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u8def\u5f84\u51e0\u4f55\u548c\u901f\u5ea6\u8c03\u6574\u65b9\u9762\u7684\u5c40\u9650\u6027"}}
{"id": "2511.18162", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18162", "abs": "https://arxiv.org/abs/2511.18162", "authors": ["Sheridan Feucht", "Byron Wallace", "David Bau"], "title": "Vector Arithmetic in Concept and Token Subspaces", "comment": "9 pages, 6 figures. NeurIPS 2025 Mechanistic Interpretability Workshop", "summary": "In order to predict the next token, LLMs must represent semantic and surface-level information about the current word. Previous work identified two types of attention heads that disentangle this information: (i) Concept induction heads, which copy word meanings, and (ii) Token induction heads, which copy literal token representations (Feucht et al., 2025). We show that these heads can be used to identify subspaces of model activations that exhibit coherent semantic structure in Llama-2-7b. Specifically, when we transform hidden states using the attention weights of concept heads, we are able to more accurately perform parallelogram arithmetic (Mikolov et al., 2013) on the resulting hidden states, e.g., showing that \"Athens\" - \"Greece\" + \"China\" = \"Beijing\". This transformation allows for much higher nearest-neighbor accuracy (80%) than direct use of raw hidden states (47%). Analogously, we show that token heads allow for transformations that reveal surface-level word information in hidden states, allowing for operations like \"coding\" - \"code\" + \"dance\" = \"dancing\".", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u6982\u5ff5\u5f52\u7eb3\u5934\u548c\u8bcd\u5143\u5f52\u7eb3\u5934\u6765\u8bc6\u522bLLaMA-2-7b\u6a21\u578b\u6fc0\u6d3b\u4e2d\u7684\u8bed\u4e49\u548c\u8868\u9762\u7ea7\u4fe1\u606f\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u6743\u91cd\u53d8\u6362\u663e\u8457\u63d0\u5347\u5e73\u884c\u56db\u8fb9\u5f62\u7b97\u672f\u7b49\u4efb\u52a1\u7684\u51c6\u786e\u6027\u3002", "motivation": "LLMs\u9700\u8981\u540c\u65f6\u8868\u793a\u8bed\u4e49\u548c\u8868\u9762\u7ea7\u4fe1\u606f\u6765\u9884\u6d4b\u4e0b\u4e00\u4e2a\u8bcd\u5143\uff0c\u5df2\u6709\u7814\u7a76\u53d1\u73b0\u6982\u5ff5\u5f52\u7eb3\u5934\u548c\u8bcd\u5143\u5f52\u7eb3\u5934\u80fd\u591f\u5206\u79bb\u8fd9\u4e24\u79cd\u4fe1\u606f\uff0c\u4f46\u5982\u4f55\u5229\u7528\u8fd9\u4e9b\u5934\u6765\u8bc6\u522b\u6a21\u578b\u6fc0\u6d3b\u4e2d\u7684\u7ed3\u6784\u5316\u5b50\u7a7a\u95f4\u5c1a\u5f85\u63a2\u7d22\u3002", "method": "\u4f7f\u7528\u6982\u5ff5\u5f52\u7eb3\u5934\u7684\u6ce8\u610f\u529b\u6743\u91cd\u53d8\u6362\u9690\u85cf\u72b6\u6001\uff0c\u521b\u5efa\u5177\u6709\u8fde\u8d2f\u8bed\u4e49\u7ed3\u6784\u7684\u5b50\u7a7a\u95f4\uff1b\u540c\u65f6\u4f7f\u7528\u8bcd\u5143\u5f52\u7eb3\u5934\u53d8\u6362\u6765\u63ed\u793a\u8868\u9762\u7ea7\u8bcd\u4fe1\u606f\u3002", "result": "\u6982\u5ff5\u5934\u53d8\u6362\u540e\u7684\u9690\u85cf\u72b6\u6001\u5728\u5e73\u884c\u56db\u8fb9\u5f62\u7b97\u672f\u4efb\u52a1\u4e2d\u8fbe\u523080%\u7684\u6700\u8fd1\u90bb\u51c6\u786e\u7387\uff0c\u8fdc\u9ad8\u4e8e\u539f\u59cb\u9690\u85cf\u72b6\u6001\u768447%\uff1b\u8bcd\u5143\u5934\u53d8\u6362\u80fd\u591f\u6210\u529f\u8fdb\u884c\u8bcd\u5f62\u53d8\u5316\u64cd\u4f5c\u3002", "conclusion": "\u7279\u5b9a\u7c7b\u578b\u7684\u6ce8\u610f\u529b\u5934\u53ef\u4ee5\u8bc6\u522b\u6a21\u578b\u6fc0\u6d3b\u4e2d\u7684\u8bed\u4e49\u548c\u8868\u9762\u7ea7\u4fe1\u606f\u5b50\u7a7a\u95f4\uff0c\u8fd9\u4e9b\u5b50\u7a7a\u95f4\u652f\u6301\u66f4\u51c6\u786e\u7684\u8bed\u4e49\u548c\u8bcd\u5f62\u64cd\u4f5c\uff0c\u4e3a\u7406\u89e3LLMs\u7684\u5185\u90e8\u8868\u793a\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2511.18203", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18203", "abs": "https://arxiv.org/abs/2511.18203", "authors": ["Ziyi Yang", "Benned Hedegaard", "Ahmed Jaafar", "Yichen Wei", "Skye Thompson", "Shreyas S. Raman", "Haotian Fu", "Stefanie Tellex", "George Konidaris", "David Paulius", "Naman Shah"], "title": "SkillWrapper: Generative Predicate Invention for Skill Abstraction", "comment": null, "summary": "Generalizing from individual skill executions to solving long-horizon tasks remains a core challenge in building autonomous agents. A promising direction is learning high-level, symbolic abstractions of the low-level skills of the agents, enabling reasoning and planning independent of the low-level state space. Among possible high-level representations, object-centric skill abstraction with symbolic predicates has been proven to be efficient because of its compatibility with domain-independent planners. Recent advances in foundation models have made it possible to generate symbolic predicates that operate on raw sensory inputs, a process we call generative predicate invention, to facilitate downstream abstraction learning. However, it remains unclear which formal properties the learned representations must satisfy, and how they can be learned to guarantee these properties. In this paper, we address both questions by presenting a formal theory of generative predicate invention for skill abstraction, resulting in symbolic operators that can be used for provably sound and complete planning. Within this framework, we propose SkillWrapper, a method that leverages foundation models to actively collect robot data and learn human-interpretable, plannable representations of black-box skills, using only RGB image observations. Our extensive empirical evaluation in simulation and on real robots shows that SkillWrapper learns abstract representations that enable solving unseen, long-horizon tasks in the real world with black-box skills.", "AI": {"tldr": "\u63d0\u51fa\u4e86SkillWrapper\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u4e3b\u52a8\u6536\u96c6\u673a\u5668\u4eba\u6570\u636e\uff0c\u5b66\u4e60\u53ef\u89e3\u91ca\u3001\u53ef\u89c4\u5212\u7684\u9ed1\u76d2\u6280\u80fd\u62bd\u8c61\u8868\u793a\uff0c\u5b9e\u73b0\u771f\u5b9e\u4e16\u754c\u4e2d\u672a\u89c1\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6c42\u89e3\u3002", "motivation": "\u89e3\u51b3\u4ece\u4e2a\u4f53\u6280\u80fd\u6267\u884c\u6cdb\u5316\u5230\u957f\u65f6\u7a0b\u4efb\u52a1\u6c42\u89e3\u7684\u6838\u5fc3\u6311\u6218\uff0c\u9700\u8981\u5b66\u4e60\u4e0e\u4f4e\u5c42\u72b6\u6001\u7a7a\u95f4\u65e0\u5173\u7684\u9ad8\u5c42\u7b26\u53f7\u62bd\u8c61\u8868\u793a\u6765\u8fdb\u884c\u63a8\u7406\u548c\u89c4\u5212\u3002", "method": "\u63d0\u51faSkillWrapper\u65b9\u6cd5\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u4e3b\u52a8\u6536\u96c6\u673a\u5668\u4eba\u6570\u636e\uff0c\u4ec5\u4f7f\u7528RGB\u56fe\u50cf\u89c2\u5bdf\u5b66\u4e60\u4eba\u7c7b\u53ef\u89e3\u91ca\u3001\u53ef\u89c4\u5212\u7684\u9ed1\u76d2\u6280\u80fd\u62bd\u8c61\u8868\u793a\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSkillWrapper\u5b66\u4e60\u7684\u62bd\u8c61\u8868\u793a\u80fd\u591f\u4f7f\u7528\u9ed1\u76d2\u6280\u80fd\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u4e2d\u672a\u89c1\u7684\u957f\u65f6\u7a0b\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u4e86\u751f\u6210\u8c13\u8bcd\u53d1\u660e\u7684\u5f62\u5f0f\u7406\u8bba\u6846\u67b6\uff0c\u786e\u4fdd\u5b66\u4e60\u5230\u7684\u7b26\u53f7\u64cd\u4f5c\u7b26\u53ef\u7528\u4e8e\u53ef\u8bc1\u660e\u6b63\u786e\u548c\u5b8c\u5907\u7684\u89c4\u5212\uff0cSkillWrapper\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\u3002"}}
{"id": "2511.18177", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18177", "abs": "https://arxiv.org/abs/2511.18177", "authors": ["Elias Lumer", "Matt Melich", "Olivia Zino", "Elena Kim", "Sara Dieter", "Pradeep Honaganahalli Basavaraju", "Vamse Kumar Subbiah", "James A. Burke", "Roberto Hernandez"], "title": "Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models", "comment": "8 pages, 2 figures", "summary": "Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86\u57fa\u4e8e\u5411\u91cf\u7684\u667a\u80fdRAG\u4e0e\u57fa\u4e8e\u5c42\u6b21\u8282\u70b9\u7cfb\u7edf\u7684\u91d1\u878d\u6587\u6863\u68c0\u7d22\u65b9\u6cd5\uff0c\u53d1\u73b0\u5411\u91cfRAG\u5728\u68c0\u7d22\u51c6\u786e\u7387\u548c\u7b54\u6848\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u6392\u548c\u5c0f\u5230\u5927\u5757\u68c0\u7d22\u6280\u672f\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5de5\u4f5c\u7f3a\u4e4f\u5bf9\u91d1\u878d\u6587\u6863\u4e2d\u57fa\u4e8e\u5411\u91cf\u548c\u975e\u5411\u91cfRAG\u67b6\u6784\u7684\u7cfb\u7edf\u6bd4\u8f83\uff0c\u4ee5\u53ca\u9ad8\u7ea7RAG\u6280\u672f\u5bf9\u68c0\u7d22\u51c6\u786e\u6027\u3001\u7b54\u6848\u8d28\u91cf\u3001\u5ef6\u8fdf\u548c\u6210\u672c\u5f71\u54cd\u7684\u5b9e\u8bc1\u7814\u7a76\u3002", "method": "\u8bc4\u4f30\u57fa\u4e8e\u5411\u91cf\u7684\u667a\u80fdRAG\uff08\u4f7f\u7528\u6df7\u5408\u641c\u7d22\u548c\u5143\u6570\u636e\u8fc7\u6ee4\uff09\u4e0e\u57fa\u4e8e\u5c42\u6b21\u8282\u70b9\u7684\u7cfb\u7edf\uff08\u65e0\u9700\u5d4c\u5165\u904d\u5386\u6587\u6863\u7ed3\u6784\uff09\uff0c\u5e76\u6d4b\u8bd5\u4e24\u79cd\u589e\u5f3a\u6280\u672f\uff1a\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u6392\u548c\u5c0f\u5230\u5927\u5757\u68c0\u7d22\u3002", "result": "\u57fa\u4e8e\u5411\u91cf\u7684\u667a\u80fdRAG\u5728150\u4e2a\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u5c42\u6b21\u8282\u70b9\u7cfb\u7edf\u83b7\u5f9768%\u7684\u80dc\u7387\uff1b\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u6392\u5728\u6700\u4f18\u53c2\u6570\u4e0bMRR@5\u63d0\u534759%\uff1b\u5c0f\u5230\u5927\u5757\u68c0\u7d22\u76f8\u6bd4\u57fa\u7ebf\u5757\u5212\u5206\u83b7\u5f9765%\u80dc\u7387\uff0c\u4ec5\u589e\u52a00.2\u79d2\u5ef6\u8fdf\u3002", "conclusion": "\u5728\u91d1\u878d\u95ee\u7b54\u7cfb\u7edf\u4e2d\u5e94\u7528\u9ad8\u7ea7RAG\u6280\u672f\u80fd\u663e\u8457\u63d0\u5347\u68c0\u7d22\u51c6\u786e\u6027\u548c\u7b54\u6848\u8d28\u91cf\uff0c\u4f46\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u9700\u8981\u8003\u8651\u6210\u672c\u4e0e\u6027\u80fd\u7684\u6743\u8861\u3002"}}
{"id": "2511.18215", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18215", "abs": "https://arxiv.org/abs/2511.18215", "authors": ["Shangyuan Yuan", "Preston Fairchild", "Yu Mei", "Xinyu Zhou", "Xiaobo Tan"], "title": "AFT: Appearance-Based Feature Tracking for Markerless and Training-Free Shape Reconstruction of Soft Robots", "comment": null, "summary": "Accurate shape reconstruction is essential for precise control and reliable operation of soft robots. Compared to sensor-based approaches, vision-based methods offer advantages in cost, simplicity, and ease of deployment. However, existing vision-based methods often rely on complex camera setups, specific backgrounds, or large-scale training datasets, limiting their practicality in real-world scenarios. In this work, we propose a vision-based, markerless, and training-free framework for soft robot shape reconstruction that directly leverages the robot's natural surface appearance. These surface features act as implicit visual markers, enabling a hierarchical matching strategy that decouples local partition alignment from global kinematic optimization. Requiring only an initial 3D reconstruction and kinematic alignment, our method achieves real-time shape tracking across diverse environments while maintaining robustness to occlusions and variations in camera viewpoints. Experimental validation on a continuum soft robot demonstrates an average tip error of 2.6% during real-time operation, as well as stable performance in practical closed-loop control tasks. These results highlight the potential of the proposed approach for reliable, low-cost deployment in dynamic real-world settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u3001\u65e0\u6807\u8bb0\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u8f6f\u673a\u5668\u4eba\u5f62\u72b6\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u7528\u673a\u5668\u4eba\u81ea\u7136\u8868\u9762\u7279\u5f81\u4f5c\u4e3a\u9690\u5f0f\u89c6\u89c9\u6807\u8bb0\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5f62\u72b6\u8ddf\u8e2a\u548c\u95ed\u73af\u63a7\u5236\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u76f8\u673a\u8bbe\u7f6e\u3001\u7279\u5b9a\u80cc\u666f\u6216\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5b9e\u7528\u3001\u4f4e\u6210\u672c\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5b9e\u73b0\u8f6f\u673a\u5668\u4eba\u5f62\u72b6\u91cd\u5efa\u3002", "method": "\u4f7f\u7528\u673a\u5668\u4eba\u81ea\u7136\u8868\u9762\u7279\u5f81\u4f5c\u4e3a\u9690\u5f0f\u89c6\u89c9\u6807\u8bb0\uff0c\u91c7\u7528\u5206\u5c42\u5339\u914d\u7b56\u7565\uff0c\u5c06\u5c40\u90e8\u5206\u533a\u5bf9\u9f50\u4e0e\u5168\u5c40\u8fd0\u52a8\u5b66\u4f18\u5316\u89e3\u8026\uff0c\u4ec5\u9700\u521d\u59cb3D\u91cd\u5efa\u548c\u8fd0\u52a8\u5b66\u5bf9\u9f50\u3002", "result": "\u5728\u8fde\u7eed\u8f6f\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u5b9e\u65f6\u64cd\u4f5c\u65f6\u5e73\u5747\u5c16\u7aef\u8bef\u5dee\u4e3a2.6%\uff0c\u5728\u906e\u6321\u548c\u76f8\u673a\u89c6\u89d2\u53d8\u5316\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5728\u95ed\u73af\u63a7\u5236\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u5728\u52a8\u6001\u771f\u5b9e\u73af\u5883\u4e2d\u53ef\u9760\u3001\u4f4e\u6210\u672c\u90e8\u7f72\u7684\u6f5c\u529b\uff0c\u4e3a\u8f6f\u673a\u5668\u4eba\u5f62\u72b6\u91cd\u5efa\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89c6\u89c9\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18194", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18194", "abs": "https://arxiv.org/abs/2511.18194", "authors": ["Faheem Nizar", "Elias Lumer", "Anmol Gulati", "Pradeep Honaganahalli Basavaraju", "Vamse Kumar Subbiah"], "title": "Agent-as-a-Graph: Knowledge Graph-Based Tool and Agent Retrieval for LLM Multi-Agent Systems", "comment": null, "summary": "Recent advances in Large Language Model Multi-Agent Systems enable scalable orchestration and retrieval of specialized, parallelized subagents, each equipped with hundreds or thousands of Model Context Protocol (MCP) servers and tools. However, existing agent, MCP, and retrieval methods typically match queries against a single agent description, obscuring fine-grained tool capabilities of each agent, resulting in suboptimal agent selection. We introduce Agent-as-a-Graph retrieval, a knowledge graph retrieval augmented generation approach that represents both tools and their parent agents as nodes and edges in a knowledge graph. During retrieval, i) relevant agents and tool nodes are first retrieved through vector search, ii) we apply a type-specific weighted reciprocal rank fusion (wRRF) for reranking tools and agents, and iii) parent agents are traversed in the knowledge graph for the final set of agents. We evaluate Agent-as-a-Graph on the LiveMCPBenchmark, achieving 14.9% and 14.6% improvements in Recall@5 and nDCG@5 over prior state-of-the-art retrievers, and 2.4% improvements in wRRF optimizations.", "AI": {"tldr": "\u63d0\u51fa\u4e86Agent-as-a-Graph\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u8868\u793a\u4ee3\u7406\u548c\u5de5\u5177\uff0c\u5728LiveMCPBenchmark\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd", "motivation": "\u73b0\u6709\u4ee3\u7406\u3001MCP\u548c\u68c0\u7d22\u65b9\u6cd5\u901a\u5e38\u53ea\u5339\u914d\u5355\u4e2a\u4ee3\u7406\u63cf\u8ff0\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u6bcf\u4e2a\u4ee3\u7406\u7684\u7ec6\u7c92\u5ea6\u5de5\u5177\u80fd\u529b\uff0c\u5bfc\u81f4\u4ee3\u7406\u9009\u62e9\u4e0d\u7406\u60f3", "method": "\u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\uff0c\u5c06\u5de5\u5177\u53ca\u5176\u7236\u4ee3\u7406\u8868\u793a\u4e3a\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u8282\u70b9\u548c\u8fb9\uff0c\u901a\u8fc7\u5411\u91cf\u641c\u7d22\u3001\u52a0\u6743\u4e92\u9006\u6392\u540d\u878d\u5408\u91cd\u6392\u5e8f\u548c\u77e5\u8bc6\u56fe\u8c31\u904d\u5386\u8fdb\u884c\u68c0\u7d22", "result": "\u5728LiveMCPBenchmark\u4e0a\uff0cRecall@5\u548cnDCG@5\u5206\u522b\u6bd4\u73b0\u6709\u6700\u4f18\u68c0\u7d22\u5668\u63d0\u534714.9%\u548c14.6%\uff0cwRRF\u4f18\u5316\u63d0\u53472.4%", "conclusion": "Agent-as-a-Graph\u65b9\u6cd5\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u8868\u793a\u548c\u68c0\u7d22\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u591a\u4ee3\u7406\u7cfb\u7edf\u4e2d\u4ee3\u7406\u9009\u62e9\u7684\u51c6\u786e\u6027\u548c\u6548\u7387"}}
{"id": "2511.18236", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18236", "abs": "https://arxiv.org/abs/2511.18236", "authors": ["Nuno Soares", "Ant\u00f3nio Grilo"], "title": "APULSE: A Scalable Hybrid Algorithm for the RCSPP on Large-Scale Dense Graphs", "comment": "9 pages", "summary": "The resource-constrained shortest path problem (RCSPP) is a fundamental NP-hard optimization challenge with broad applications, from network routing to autonomous navigation. This problem involves finding a path that minimizes a primary cost subject to a budget on a secondary resource. While various RCSPP solvers exist, they often face critical scalability limitations when applied to the large, dense graphs characteristic of complex, real-world scenarios, making them impractical for time-critical planning. This challenge is particularly acute in domains like mission planning for unmanned ground vehicles (UGVs), which demand solutions on large-scale terrain graphs. This paper introduces APULSE, a hybrid label-setting algorithm designed to efficiently solve the RCSPP on such challenging graphs. APULSE integrates a best-first search guided by an A* heuristic with aggressive, Pulse-style pruning mechanisms and a time-bucketing strategy for effective state-space reduction. A computational study, using a large-scale UGV planning scenario, benchmarks APULSE against state-of-the-art algorithms. The results demonstrate that APULSE consistently finds near-optimal solutions while being orders of magnitude faster and more robust, particularly on large problem instances where competing methods fail. This superior scalability establishes APULSE as an effective solution for RCSPP in complex, large-scale environments, enabling capabilities such as interactive decision support and dynamic replanning.", "AI": {"tldr": "APULSE\u662f\u4e00\u79cd\u6df7\u5408\u6807\u7b7e\u8bbe\u7f6e\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u6700\u77ed\u8def\u5f84\u95ee\u9898(RCSPP)\uff0c\u5728\u5927\u89c4\u6a21\u5bc6\u96c6\u56fe\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "motivation": "\u73b0\u6709RCSPP\u6c42\u89e3\u5668\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u7684\u5927\u89c4\u6a21\u5bc6\u96c6\u56fe\u4e0a\u5b58\u5728\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u65e0\u6cd5\u6ee1\u8db3\u65f6\u95f4\u5173\u952e\u89c4\u5212\u9700\u6c42\uff0c\u7279\u522b\u662f\u5728\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u4efb\u52a1\u89c4\u5212\u7b49\u9886\u57df\u3002", "method": "APULSE\u7ed3\u5408\u4e86A*\u542f\u53d1\u5f0f\u5f15\u5bfc\u7684\u6700\u4f73\u4f18\u5148\u641c\u7d22\u3001\u8109\u51b2\u5f0f\u526a\u679d\u673a\u5236\u548c\u65f6\u95f4\u5206\u6876\u7b56\u7565\uff0c\u5b9e\u73b0\u6709\u6548\u7684\u72b6\u6001\u7a7a\u95f4\u7f29\u51cf\u3002", "result": "\u5728\u5927\u89c4\u6a21UGV\u89c4\u5212\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAPULSE\u59cb\u7ec8\u80fd\u627e\u5230\u63a5\u8fd1\u6700\u4f18\u89e3\uff0c\u901f\u5ea6\u6bd4\u7ade\u4e89\u65b9\u6cd5\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u5728\u5927\u578b\u95ee\u9898\u5b9e\u4f8b\u4e0a\u66f4\u5177\u9c81\u68d2\u6027\u3002", "conclusion": "APULSE\u5728\u590d\u6742\u5927\u89c4\u6a21\u73af\u5883\u4e2d\u4e3aRCSPP\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u51b3\u7b56\u652f\u6301\u548c\u52a8\u6001\u91cd\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2511.18259", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.18259", "abs": "https://arxiv.org/abs/2511.18259", "authors": ["Xiaochen Zheng", "Alvaro Serra", "Ilya Schneider Chernov", "Maddalena Marchesi", "Eunice Musvasva", "Tatyana Y. Doktorova"], "title": "From Archives to Decisions: Multi-Agent Pharmaceutical Co-Scientist for Traceable Drug Discovery and Reverse Translation", "comment": "22 pages, 4 figures, 3 tables", "summary": "Pharmaceutical research and development has accumulated vast, heterogeneous archives of data. Much of this knowledge stems from discontinued programs, and reusing these archives is invaluable for reverse translation. However, in practice, such reuse is often infeasible. In this work, we introduce DiscoVerse, a multi-agent co-scientist designed to support pharmaceutical research and development. The system implements semantic retrieval, cross-document linking, and auditable synthesis on a large historical corpus from Roche. To validate our approach at real-world scale, we selected a subset of 180 molecules from the Roche research repositories, covering over 0.87 billion BPE tokens and more than four decades of research. Given that automated evaluation metrics are poorly aligned with scientific utility, we evaluate the performance of DiscoVerse using blinded expert evaluation of source-linked outputs. To our knowledge, this is the first agentic framework systematically assessed on real pharmaceutical data for reverse translation, enabled by authorized access to confidential, end-to-end drug-development archives. Our contributions include role-specialized agent designs aligned with scientist workflows; human-in-the-loop support for reverse translation; expert evaluation; and a large-scale demonstration showing promising answer accuracy and decision-making insights. In brief, across seven benchmark queries covering 180 molecules, DiscoVerse achieved near-perfect recall ($\\geq 0.99$) with moderate precision ($0.71-0.91$), while qualitative assessments of discontinuation rationale and organ-specific toxicity showed faithful, source-linked synthesis across preclinical and clinical evidence.", "AI": {"tldr": "DiscoVerse\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u534f\u540c\u79d1\u5b66\u5bb6\u7cfb\u7edf\uff0c\u7528\u4e8e\u652f\u6301\u836f\u7269\u7814\u53d1\uff0c\u901a\u8fc7\u8bed\u4e49\u68c0\u7d22\u3001\u8de8\u6587\u6863\u94fe\u63a5\u548c\u53ef\u5ba1\u8ba1\u7684\u5408\u6210\u6280\u672f\uff0c\u5728\u7f57\u6c0f\u5386\u53f2\u6570\u636e\u4e0a\u5b9e\u73b0\u9006\u5411\u8f6c\u5316\u7814\u7a76\u3002", "motivation": "\u836f\u7269\u7814\u53d1\u79ef\u7d2f\u4e86\u6d77\u91cf\u5f02\u6784\u6570\u636e\uff0c\u5176\u4e2d\u8bb8\u591a\u6765\u81ea\u5df2\u7ec8\u6b62\u7684\u7814\u7a76\u9879\u76ee\u3002\u91cd\u65b0\u5229\u7528\u8fd9\u4e9b\u6863\u6848\u5bf9\u9006\u5411\u8f6c\u5316\u7814\u7a76\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u5f80\u5f80\u4e0d\u53ef\u884c\u3002", "method": "\u5f00\u53d1\u4e86DiscoVerse\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u5b9e\u73b0\u8bed\u4e49\u68c0\u7d22\u3001\u8de8\u6587\u6863\u94fe\u63a5\u548c\u53ef\u5ba1\u8ba1\u7684\u5408\u6210\uff0c\u5728\u7f57\u6c0f0.87\u4ebfBPE\u4ee4\u724c\u3001\u8de8\u8d8a40\u591a\u5e74\u7684\u7814\u7a76\u6570\u636e\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u6db5\u76d6180\u4e2a\u5206\u5b50\u76847\u4e2a\u57fa\u51c6\u67e5\u8be2\u4e2d\uff0cDiscoVerse\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u53ec\u56de\u7387(\u22650.99)\u548c\u4e2d\u7b49\u7cbe\u786e\u5ea6(0.71-0.91)\uff0c\u5728\u7ec8\u6b62\u7406\u7531\u548c\u5668\u5b98\u7279\u5f02\u6027\u6bd2\u6027\u8bc4\u4f30\u4e2d\u663e\u793a\u51fa\u5fe0\u5b9e\u3001\u6e90\u94fe\u63a5\u7684\u5408\u6210\u80fd\u529b\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5728\u771f\u5b9e\u836f\u7269\u6570\u636e\u4e0a\u7cfb\u7edf\u8bc4\u4f30\u7684\u9006\u5411\u8f6c\u5316\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u6709\u524d\u666f\u7684\u7b54\u6848\u51c6\u786e\u6027\u548c\u51b3\u7b56\u6d1e\u5bdf\u529b\uff0c\u4e3a\u836f\u7269\u7814\u53d1\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2511.18243", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18243", "abs": "https://arxiv.org/abs/2511.18243", "authors": ["Eashan Vytla", "Bhavanishankar Kalavakolanu", "Andrew Perrault", "Matthew McCrink"], "title": "Dreaming Falcon: Physics-Informed Model-Based Reinforcement Learning for Quadcopters", "comment": null, "summary": "Current control algorithms for aerial robots struggle with robustness in dynamic environments and adverse conditions. Model-based reinforcement learning (RL) has shown strong potential in handling these challenges while remaining sample-efficient. Additionally, Dreamer has demonstrated that online model-based RL can be achieved using a recurrent world model trained on replay buffer data. However, applying Dreamer to aerial systems has been quite challenging due to its sample inefficiency and poor generalization of dynamics models. Our work explores a physics-informed approach to world model learning and improves policy performance. The world model treats the quadcopter as a free-body system and predicts the net forces and moments acting on it, which are then passed through a 6-DOF Runge-Kutta integrator (RK4) to predict future state rollouts. In this paper, we compare this physics-informed method to a standard RNN-based world model. Although both models perform well on the training data, we observed that they fail to generalize to new trajectories, leading to rapid divergence in state rollouts, preventing policy convergence.", "AI": {"tldr": "\u8bba\u6587\u63a2\u7d22\u4e86\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u65e0\u4eba\u673a\u63a7\u5236\uff0c\u5e76\u4e0e\u6807\u51c6RNN\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\uff0c\u53d1\u73b0\u4e24\u79cd\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\u4f46\u65e0\u6cd5\u6cdb\u5316\u5230\u65b0\u8f68\u8ff9\u3002", "motivation": "\u5f53\u524d\u65e0\u4eba\u673a\u63a7\u5236\u7b97\u6cd5\u5728\u52a8\u6001\u73af\u5883\u548c\u6076\u52a3\u6761\u4ef6\u4e0b\u7f3a\u4e4f\u9c81\u68d2\u6027\uff0c\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u6837\u672c\u6548\u7387\u9ad8\u4f46\u96be\u4ee5\u5e94\u7528\u4e8e\u65e0\u4eba\u673a\u7cfb\u7edf\uff0cDreamer\u65b9\u6cd5\u5728\u65e0\u4eba\u673a\u4e0a\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u548c\u52a8\u529b\u5b66\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7269\u7406\u4fe1\u606f\u65b9\u6cd5\u6784\u5efa\u4e16\u754c\u6a21\u578b\uff0c\u5c06\u56db\u65cb\u7ffc\u89c6\u4e3a\u81ea\u7531\u4f53\u7cfb\u7edf\u9884\u6d4b\u51c0\u529b\u548c\u529b\u77e9\uff0c\u901a\u8fc76\u81ea\u7531\u5ea6\u9f99\u683c-\u5e93\u5854\u79ef\u5206\u5668\u9884\u6d4b\u672a\u6765\u72b6\u6001\u5c55\u5f00\uff0c\u5e76\u4e0e\u6807\u51c6RNN\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u4e24\u79cd\u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u90fd\u8868\u73b0\u826f\u597d\uff0c\u4f46\u90fd\u65e0\u6cd5\u6cdb\u5316\u5230\u65b0\u8f68\u8ff9\uff0c\u5bfc\u81f4\u72b6\u6001\u5c55\u5f00\u5feb\u901f\u53d1\u6563\uff0c\u963b\u788d\u4e86\u7b56\u7565\u6536\u655b\u3002", "conclusion": "\u7269\u7406\u4fe1\u606f\u65b9\u6cd5\u867d\u7136\u6539\u8fdb\u4e86\u4e16\u754c\u6a21\u578b\u5b66\u4e60\uff0c\u4f46\u4e0e\u6807\u51c6RNN\u6a21\u578b\u4e00\u6837\u9762\u4e34\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u5e94\u7528\u4e8e\u65e0\u4eba\u673a\u63a7\u5236\u7b56\u7565\u5b66\u4e60\u3002"}}
{"id": "2511.18301", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18301", "abs": "https://arxiv.org/abs/2511.18301", "authors": ["Harsh Rathva", "Pruthwik Mishra", "Shrikant Malviya"], "title": "\"AGI\" team at SHROOM-CAP: Data-Centric Approach to Multilingual Hallucination Detection using XLM-RoBERTa", "comment": "Accepted to the 1st Workshop on Confabulation, Hallucinations & Overgeneration in Multilingual and Practical Settings (CHOMPS) at AACL-IJCNLP 2025", "summary": "The detection of hallucinations in multilingual scientific text generated by Large Language Models (LLMs) presents significant challenges for reliable AI systems. This paper describes our submission to the SHROOM-CAP 2025 shared task on scientific hallucination detection across 9 languages. Unlike most approaches that focus primarily on model architecture, we adopted a data-centric strategy that addressed the critical issue of training data scarcity and imbalance. We unify and balance five existing datasets to create a comprehensive training corpus of 124,821 samples (50% correct, 50% hallucinated), representing a 172x increase over the original SHROOM training data. Our approach fine-tuned XLM-RoBERTa-Large with 560 million parameters on this enhanced dataset, achieves competitive performance across all languages, including \\textbf{2nd place in Gujarati} (zero-shot language) with Factuality F1 of 0.5107, and rankings between 4th-6th place across the remaining 8 languages. Our results demonstrate that systematic data curation can significantly outperform architectural innovations alone, particularly for low-resource languages in zero-shot settings.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6570\u636e\u4e2d\u5fc3\u7684\u7b56\u7565\uff0c\u7edf\u4e00\u5e76\u5e73\u8861\u4e94\u4e2a\u73b0\u6709\u6570\u636e\u96c6\u521b\u5efa\u4e86124,821\u4e2a\u6837\u672c\u7684\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u5728SHROOM-CAP 2025\u591a\u8bed\u8a00\u79d1\u5b66\u6587\u672c\u5e7b\u89c9\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u96f6\u6837\u672c\u8bed\u8a00\u53e4\u5409\u62c9\u7279\u8bed\u4e2d\u83b7\u5f97\u7b2c\u4e8c\u540d\u3002", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u79d1\u5b66\u6587\u672c\u4e2dLLM\u751f\u6210\u5e7b\u89c9\u68c0\u6d4b\u7684\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u3002", "method": "\u91c7\u7528\u6570\u636e\u4e2d\u5fc3\u7b56\u7565\uff0c\u7edf\u4e00\u5e73\u8861\u4e94\u4e2a\u73b0\u6709\u6570\u636e\u96c6\u521b\u5efa\u5927\u89c4\u6a21\u8bad\u7ec3\u8bed\u6599\u5e93\uff0c\u7136\u540e\u4f7f\u75285.6\u4ebf\u53c2\u6570\u7684XLM-RoBERTa-Large\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u57289\u79cd\u8bed\u8a00\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u53e4\u5409\u62c9\u7279\u8bed\uff08\u96f6\u6837\u672c\u8bed\u8a00\uff09\u83b7\u5f97\u7b2c\u4e8c\u540d\uff08Factuality F1\u4e3a0.5107\uff09\uff0c\u5176\u4f598\u79cd\u8bed\u8a00\u6392\u540d4-6\u4f4d\u3002", "conclusion": "\u7cfb\u7edf\u5316\u7684\u6570\u636e\u7ba1\u7406\u7b56\u7565\u80fd\u591f\u663e\u8457\u8d85\u8d8a\u4ec5\u4f9d\u9760\u67b6\u6784\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u3002"}}
{"id": "2511.18270", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18270", "abs": "https://arxiv.org/abs/2511.18270", "authors": ["Zhongkai Chen", "Yihao Sun", "Chao Yan", "Han Zhou", "Xiaojia Xiang", "Jie Jiang"], "title": "Skypilot: Fine-Tuning LLM with Physical Grounding for AAV Coverage Search", "comment": null, "summary": "Autonomous aerial vehicles (AAVs) have played a pivotal role in coverage operations and search missions. Recent advances in large language models (LLMs) offer promising opportunities to augment AAV intelligence. These advances help address complex challenges like area coverage optimization, dynamic path planning, and adaptive decision-making. However, the absence of physical grounding in LLMs leads to hallucination and reproducibility problems in spatial reasoning and decision-making. To tackle these issues, we present Skypilot, an LLM-enhanced two-stage framework that grounds language models in physical reality by integrating monte carlo tree search (MCTS). In the first stage, we introduce a diversified action space that encompasses generate, regenerate, fine-tune, and evaluate operations, coupled with physics-informed reward functions to ensure trajectory feasibility. In the second stage, we fine-tune Qwen3-4B on 23,000 MCTS-generated samples, achieving substantial inference acceleration while maintaining solution quality. Extensive numerical simulations and real-world flight experiments validate the efficiency and superiority of our proposed approach. Detailed information and experimental results are accessible at https://sky-pilot.top.", "AI": {"tldr": "\u63d0\u51fa\u4e86Skypilot\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u5c06\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u7269\u7406\u63a5\u5730\uff0c\u89e3\u51b3\u81ea\u4e3b\u98de\u884c\u5668\u5728\u8986\u76d6\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u548c\u53ef\u91cd\u590d\u6027\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u4e3b\u98de\u884c\u5668\u7684\u8986\u76d6\u64cd\u4f5c\u548c\u641c\u7d22\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u7269\u7406\u63a5\u5730\u5bfc\u81f4\u7a7a\u95f4\u63a8\u7406\u548c\u51b3\u7b56\u4e2d\u51fa\u73b0\u5e7b\u89c9\u548c\u53ef\u91cd\u590d\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u5f15\u5165\u591a\u6837\u5316\u52a8\u4f5c\u7a7a\u95f4\uff08\u751f\u6210\u3001\u91cd\u65b0\u751f\u6210\u3001\u5fae\u8c03\u3001\u8bc4\u4f30\uff09\u548c\u7269\u7406\u611f\u77e5\u5956\u52b1\u51fd\u6570\uff1b\u7b2c\u4e8c\u9636\u6bb5\u572823,000\u4e2aMCTS\u751f\u6210\u6837\u672c\u4e0a\u5fae\u8c03Qwen3-4B\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u548c\u771f\u5b9e\u98de\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6548\u7387\u548c\u4f18\u8d8a\u6027\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u52a0\u901f\u540c\u65f6\u4fdd\u6301\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "conclusion": "Skypilot\u6846\u67b6\u6210\u529f\u5c06\u8bed\u8a00\u6a21\u578b\u7269\u7406\u63a5\u5730\uff0c\u89e3\u51b3\u4e86\u81ea\u4e3b\u98de\u884c\u5668\u51b3\u7b56\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u8986\u76d6\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18306", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18306", "abs": "https://arxiv.org/abs/2511.18306", "authors": ["Mohammad Aqib", "Mohd Hamza", "Ying Hei Chui", "Qipei Mei"], "title": "Table Comprehension in Building Codes using Vision Language Models and Domain-Specific Fine-Tuning", "comment": null, "summary": "Building codes contain critical information for ensuring safety, regulatory compliance, and informed decision-making in construction and engineering. Automated question answering systems over such codes enable quick and accurate access to specific regulatory clauses, improving efficiency and reducing errors. Retrieval-Augmented Generation (RAG) systems are essential for this task as they combine the precision of information retrieval with the generative capabilities of language models. However, tabular data are challenging to extract as they often involve complex layouts, merged cells, multi-row headers, and embedded semantic relationships that are not easily captured by traditional natural language processing techniques and Vision Language Models (VLMs). This paper explores and compares two methods for extracting information from tabular data in building codes using several pre-trained VLMs. First, a direct input method is used, where the image of the page is input directly into the VLMs, which are then tasked with answering questions based on the image. Second, an indirect input method is introduced, which involves converting an image of a page containing tables into the LaTeX code and then answering inquires based on the LaTeX-based input. The experiments find that the direct input method generally resulted in higher accuracy than the indirect input method. To further improve the performance, we fine-tuned each VLM using Low Rank Adaptation (LoRA) on a domain-specific tabular dataset. The fine-tuned models exhibited substantial improvements, with Qwen2.5-VL-3B-Instruct achieving relative accuracy gains exceeding 100%. Our results highlight the potential of parameter-efficient fine-tuning methods to adapt powerful VLMs for understanding complex structured data in specialized fields, such as building code interpretation and regulatory compliance.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e24\u79cd\u4ece\u5efa\u7b51\u89c4\u8303\u8868\u683c\u6570\u636e\u4e2d\u63d0\u53d6\u4fe1\u606f\u7684\u65b9\u6cd5\uff1a\u76f4\u63a5\u8f93\u5165\u6cd5\u548c\u95f4\u63a5\u8f93\u5165\u6cd5\uff08\u901a\u8fc7LaTeX\u8f6c\u6362\uff09\uff0c\u53d1\u73b0\u76f4\u63a5\u8f93\u5165\u6cd5\u51c6\u786e\u7387\u66f4\u9ad8\u3002\u901a\u8fc7LoRA\u5fae\u8c03VLM\u6a21\u578b\uff0cQwen2.5-VL-3B-Instruct\u5b9e\u73b0\u4e86\u8d85\u8fc7100%\u7684\u76f8\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u5efa\u7b51\u89c4\u8303\u5305\u542b\u786e\u4fdd\u5b89\u5168\u548c\u5408\u89c4\u6027\u7684\u5173\u952e\u4fe1\u606f\uff0c\u4f46\u8868\u683c\u6570\u636e\u63d0\u53d6\u56f0\u96be\uff0c\u56e0\u4e3a\u8868\u683c\u5177\u6709\u590d\u6742\u5e03\u5c40\u3001\u5408\u5e76\u5355\u5143\u683c\u3001\u591a\u884c\u8868\u5934\u548c\u5d4c\u5165\u5f0f\u8bed\u4e49\u5173\u7cfb\uff0c\u4f20\u7edfNLP\u6280\u672f\u548cVLM\u96be\u4ee5\u6709\u6548\u5904\u7406\u3002", "method": "\u6bd4\u8f83\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u76f4\u63a5\u8f93\u5165\u6cd5\uff1a\u5c06\u9875\u9762\u56fe\u50cf\u76f4\u63a5\u8f93\u5165VLM\u56de\u7b54\u95ee\u9898\uff1b2\uff09\u95f4\u63a5\u8f93\u5165\u6cd5\uff1a\u5c06\u8868\u683c\u56fe\u50cf\u8f6c\u6362\u4e3aLaTeX\u4ee3\u7801\uff0c\u7136\u540e\u57fa\u4e8eLaTeX\u8f93\u5165\u56de\u7b54\u95ee\u9898\u3002\u4f7f\u7528LoRA\u5bf9VLM\u8fdb\u884c\u9886\u57df\u7279\u5b9a\u7684\u8868\u683c\u6570\u636e\u5fae\u8c03\u3002", "result": "\u76f4\u63a5\u8f93\u5165\u6cd5\u901a\u5e38\u6bd4\u95f4\u63a5\u8f93\u5165\u6cd5\u51c6\u786e\u7387\u66f4\u9ad8\u3002\u7ecf\u8fc7LoRA\u5fae\u8c03\u7684\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0cQwen2.5-VL-3B-Instruct\u5b9e\u73b0\u4e86\u8d85\u8fc7100%\u7684\u76f8\u5bf9\u51c6\u786e\u7387\u589e\u76ca\u3002", "conclusion": "\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\u80fd\u591f\u4f7f\u5f3a\u5927\u7684VLM\u9002\u5e94\u4e13\u4e1a\u9886\u57df\uff08\u5982\u5efa\u7b51\u89c4\u8303\u89e3\u91ca\u548c\u6cd5\u89c4\u5408\u89c4\uff09\u4e2d\u590d\u6742\u7ed3\u6784\u5316\u6570\u636e\u7684\u7406\u89e3\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2511.18293", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18293", "abs": "https://arxiv.org/abs/2511.18293", "authors": ["Shuai Zhang", "Jingsong Mu", "Cancan Zhao", "Leiqi Tian", "Zhijun Xing", "Bo Ouyang", "Xiang Li"], "title": "AIA-UltraNeRF:Acoustic-Impedance-Aware Neural Radiance Field with Hash Encodings for Robotic Ultrasound Reconstruction and Localization", "comment": null, "summary": "Neural radiance field (NeRF) is a promising approach for reconstruction and new view synthesis. However, previous NeRF-based reconstruction methods overlook the critical role of acoustic impedance in ultrasound imaging. Localization methods face challenges related to local minima due to the selection of initial poses. In this study, we design a robotic ultrasound system (RUSS) with an acoustic-impedance-aware ultrasound NeRF (AIA-UltraNeRF) to decouple the scanning and diagnostic processes. Specifically, AIA-UltraNeRF models a continuous function of hash-encoded spatial coordinates for the 3D ultrasound map, allowing for the storage of acoustic impedance without dense sampling. This approach accelerates both reconstruction and inference speeds. We then propose a dual-supervised network that leverages teacher and student models to hash-encode the rendered ultrasound images from the reconstructed map. AIA-UltraNeRF retrieves the most similar hash values without the need to render images again, providing an offline initial image position for localization. Moreover, we develop a RUSS with a spherical remote center of motion mechanism to hold the probe, implementing operator-independent scanning modes that separate image acquisition from diagnostic workflows. Experimental results on a phantom and human subjects demonstrate the effectiveness of acoustic impedance in implicitly characterizing the color of ultrasound images. AIAUltraNeRF achieves both reconstruction and localization with inference speeds that are 9.9 faster than those of vanilla NeRF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u58f0\u963b\u6297\u611f\u77e5\u7684\u8d85\u58f0NeRF\u7cfb\u7edf(AIA-UltraNeRF)\uff0c\u7ed3\u5408\u673a\u5668\u4eba\u8d85\u58f0\u7cfb\u7edf\uff0c\u901a\u8fc7\u54c8\u5e0c\u7f16\u7801\u7a7a\u95f4\u5750\u6807\u6765\u5efa\u6a213D\u8d85\u58f0\u56fe\uff0c\u5b9e\u73b0\u5feb\u901f\u91cd\u5efa\u548c\u5b9a\u4f4d\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u539f\u59cbNeRF\u5feb9.9\u500d\u3002", "motivation": "\u73b0\u6709NeRF\u65b9\u6cd5\u5ffd\u7565\u4e86\u58f0\u963b\u6297\u5728\u8d85\u58f0\u6210\u50cf\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5b9a\u4f4d\u65b9\u6cd5\u56e0\u521d\u59cb\u4f4d\u59ff\u9009\u62e9\u800c\u9762\u4e34\u5c40\u90e8\u6781\u5c0f\u503c\u95ee\u9898\u3002\u9700\u8981\u5c06\u626b\u63cf\u548c\u8bca\u65ad\u8fc7\u7a0b\u89e3\u8026\u3002", "method": "\u8bbe\u8ba1\u673a\u5668\u4eba\u8d85\u58f0\u7cfb\u7edf\uff0c\u91c7\u7528AIA-UltraNeRF\u5bf9\u54c8\u5e0c\u7f16\u7801\u7684\u7a7a\u95f4\u5750\u6807\u5efa\u6a21\u8fde\u7eed\u51fd\u6570\u5b58\u50a8\u58f0\u963b\u6297\uff1b\u63d0\u51fa\u53cc\u76d1\u7763\u7f51\u7edc\u5229\u7528\u5e08\u751f\u6a21\u578b\u54c8\u5e0c\u7f16\u7801\u6e32\u67d3\u56fe\u50cf\uff1b\u5f00\u53d1\u7403\u5f62\u8fdc\u7a0b\u8fd0\u52a8\u4e2d\u5fc3\u673a\u6784\u5b9e\u73b0\u64cd\u4f5c\u8005\u65e0\u5173\u626b\u63cf\u3002", "result": "\u5728\u4f53\u6a21\u548c\u4eba\u4f53\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u58f0\u963b\u6297\u5728\u9690\u5f0f\u8868\u5f81\u8d85\u58f0\u56fe\u50cf\u989c\u8272\u65b9\u9762\u7684\u6709\u6548\u6027\uff0cAIA-UltraNeRF\u5b9e\u73b0\u4e86\u91cd\u5efa\u548c\u5b9a\u4f4d\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u539f\u59cbNeRF\u5feb9.9\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5c06\u58f0\u963b\u6297\u6574\u5408\u5230NeRF\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8d85\u58f0\u56fe\u50cf\u91cd\u5efa\u548c\u5b9a\u4f4d\uff0c\u4e3a\u673a\u5668\u4eba\u8d85\u58f0\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18313", "categories": ["cs.CL", "cs.DB", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18313", "abs": "https://arxiv.org/abs/2511.18313", "authors": ["Joseph Oladokun"], "title": "Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search", "comment": "10 pages", "summary": "Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8def\u5f84\u7ea6\u675f\u68c0\u7d22(PCR)\u65b9\u6cd5\uff0c\u7ed3\u5408\u56fe\u7ed3\u6784\u7ea6\u675f\u548c\u8bed\u4e49\u641c\u7d22\uff0c\u786e\u4fdd\u68c0\u7d22\u4fe1\u606f\u5728\u77e5\u8bc6\u56fe\u8c31\u4e2d\u4fdd\u6301\u903b\u8f91\u5173\u7cfb\uff0c\u63d0\u9ad8LLM\u4ee3\u7406\u63a8\u7406\u7684\u53ef\u9760\u6027\u548c\u8fde\u8d2f\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u4ece\u77e5\u8bc6\u5e93\u68c0\u7d22\u4e0a\u4e0b\u6587\u65f6\uff0c\u7531\u4e8e\u7f3a\u4e4f\u4e0e\u5f53\u524d\u63a8\u7406\u72b6\u6001\u7684\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u5bfc\u81f4\u63a8\u7406\u94fe\u4e0d\u8fde\u8d2f\u3002", "method": "PCR\u65b9\u6cd5\u5c06\u641c\u7d22\u7a7a\u95f4\u9650\u5236\u5728\u4ece\u951a\u8282\u70b9\u53ef\u8fbe\u7684\u8282\u70b9\uff0c\u9632\u6b62\u68c0\u7d22\u7ed3\u6784\u65ad\u5f00\u7684\u4fe1\u606f\u3002\u7ed3\u5408\u7ed3\u6784\u56fe\u7ea6\u675f\u548c\u8bed\u4e49\u641c\u7d22\u3002", "result": "\u5728PathRAG-6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPCR\u5b9e\u73b0100%\u7ed3\u6784\u4e00\u81f4\u6027\uff08\u57fa\u7ebf\u4e3a24-32%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u76f8\u5173\u6027\u5f97\u5206\u3002\u5728\u6280\u672f\u9886\u57df\uff0cPCR\u5728\u6392\u540d10\u65f6\u83b7\u5f97100%\u76f8\u5173\u6027\u548c\u5b8c\u5168\u7ed3\u6784\u4e00\u81f4\u6027\u3002PCR\u5c06\u68c0\u7d22\u4e0a\u4e0b\u6587\u7684\u5e73\u5747\u56fe\u8ddd\u79bb\u51cf\u5c1178%\u3002", "conclusion": "\u8def\u5f84\u7ea6\u675f\u68c0\u7d22\u662f\u63d0\u9ad8LLM\u4ee3\u7406\u63a8\u7406\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u8fde\u8d2f\u6027\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2511.18299", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18299", "abs": "https://arxiv.org/abs/2511.18299", "authors": ["Steven Oh", "Tai Inui", "Magdeline Kuan", "Jia-Yeu Lin"], "title": "MicCheck: Repurposing Off-the-Shelf Pin Microphones for Easy and Low-Cost Contact Sensing", "comment": null, "summary": "Robotic manipulation tasks are contact-rich, yet most imitation learning (IL) approaches rely primarily on vision, which struggles to capture stiffness, roughness, slip, and other fine interaction cues. Tactile signals can address this gap, but existing sensors often require expensive, delicate, or integration-heavy hardware. In this work, we introduce MicCheck, a plug-and-play acoustic sensing approach that repurposes an off-the-shelf Bluetooth pin microphone as a low-cost contact sensor. The microphone clips into a 3D-printed gripper insert and streams audio via a standard USB receiver, requiring no custom electronics or drivers. Despite its simplicity, the microphone provides signals informative enough for both perception and control. In material classification, it achieves 92.9% accuracy on a 10-class benchmark across four interaction types (tap, knock, slow press, drag). For manipulation, integrating pin microphone into an IL pipeline with open source hardware improves the success rate on picking and pouring task from 0.40 to 0.80 and enables reliable execution of contact-rich skills such as unplugging and sound-based sorting. Compared with high-resolution tactile sensors, pin microphones trade spatial detail for cost and ease of integration, offering a practical pathway for deploying acoustic contact sensing in low-cost robot setups.", "AI": {"tldr": "MicCheck\u662f\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u5373\u63d2\u5373\u7528\u7684\u58f0\u5b66\u4f20\u611f\u65b9\u6cd5\uff0c\u5229\u7528\u73b0\u6210\u7684\u84dd\u7259\u9488\u5f0f\u9ea6\u514b\u98ce\u4f5c\u4e3a\u63a5\u89e6\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u97f3\u9891\u4fe1\u53f7\u5b9e\u73b0\u6750\u6599\u5206\u7c7b\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u9700\u8981\u4e30\u5bcc\u7684\u63a5\u89e6\u4fe1\u606f\uff0c\u4f46\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\uff0c\u96be\u4ee5\u6355\u6349\u521a\u5ea6\u3001\u7c97\u7cd9\u5ea6\u3001\u6ed1\u52a8\u7b49\u7cbe\u7ec6\u4ea4\u4e92\u7ebf\u7d22\u3002\u89e6\u89c9\u4fe1\u53f7\u53ef\u4ee5\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4f46\u73b0\u6709\u4f20\u611f\u5668\u901a\u5e38\u6602\u8d35\u3001\u8106\u5f31\u6216\u96c6\u6210\u590d\u6742\u3002", "method": "\u5c06\u73b0\u6210\u7684\u84dd\u7259\u9488\u5f0f\u9ea6\u514b\u98ce\u91cd\u65b0\u7528\u4f5c\u4f4e\u6210\u672c\u63a5\u89e6\u4f20\u611f\u5668\uff0c\u901a\u8fc73D\u6253\u5370\u7684\u5939\u6301\u5668\u63d2\u5165\u4ef6\u56fa\u5b9a\uff0c\u901a\u8fc7\u6807\u51c6USB\u63a5\u6536\u5668\u4f20\u8f93\u97f3\u9891\uff0c\u65e0\u9700\u5b9a\u5236\u7535\u5b50\u8bbe\u5907\u6216\u9a71\u52a8\u7a0b\u5e8f\u3002", "result": "\u572810\u7c7b\u6750\u6599\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u523092.9%\u7684\u51c6\u786e\u7387\uff1b\u5728\u62fe\u53d6\u548c\u503e\u5012\u4efb\u52a1\u4e2d\uff0c\u5c06\u6210\u529f\u7387\u4ece0.40\u63d0\u9ad8\u52300.80\uff0c\u5e76\u80fd\u591f\u53ef\u9760\u6267\u884c\u62d4\u63d2\u548c\u57fa\u4e8e\u58f0\u97f3\u7684\u6392\u5e8f\u7b49\u63a5\u89e6\u5bc6\u96c6\u578b\u6280\u80fd\u3002", "conclusion": "\u4e0e\u9ad8\u5206\u8fa8\u7387\u89e6\u89c9\u4f20\u611f\u5668\u76f8\u6bd4\uff0c\u9488\u5f0f\u9ea6\u514b\u98ce\u5728\u7a7a\u95f4\u7ec6\u8282\u4e0a\u6709\u6240\u727a\u7272\uff0c\u4f46\u5728\u6210\u672c\u548c\u96c6\u6210\u4fbf\u5229\u6027\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u4e3a\u4f4e\u6210\u672c\u673a\u5668\u4eba\u8bbe\u7f6e\u4e2d\u90e8\u7f72\u58f0\u5b66\u63a5\u89e6\u4f20\u611f\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\u3002"}}
{"id": "2511.18324", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18324", "abs": "https://arxiv.org/abs/2511.18324", "authors": ["Syed Mohaiminul Hoque", "Naimur Rahman", "Md Sakhawat Hossain"], "title": "Gradient Masters at BLP-2025 Task 1: Advancing Low-Resource NLP for Bengali using Ensemble-Based Adversarial Training for Hate Speech Detection", "comment": "6 pages, 2 figures, 4 tables. Accepted at the Second International Workshop on Bangla Language Processing (BLP-2025) co-located with AACL-IJCNLP 2025. Ranked 6th (Subtask 1A, 73.23% micro F1) and 3rd (Subtask 1B, 73.28% micro F1) on the official leaderboard", "summary": "This paper introduces the approach of \"Gradient Masters\" for BLP-2025 Task 1: \"Bangla Multitask Hate Speech Identification Shared Task\". We present an ensemble-based fine-tuning strategy for addressing subtasks 1A (hate-type classification) and 1B (target group classification) in YouTube comments. We propose a hybrid approach on a Bangla Language Model, which outperformed the baseline models and secured the 6th position in subtask 1A with a micro F1 score of 73.23% and the third position in subtask 1B with 73.28%. We conducted extensive experiments that evaluated the robustness of the model throughout the development and evaluation phases, including comparisons with other Language Model variants, to measure generalization in low-resource Bangla hate speech scenarios and data set coverage. In addition, we provide a detailed analysis of our findings, exploring misclassification patterns in the detection of hate speech.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u5fae\u8c03\u7b56\u7565\u7684\"Gradient Masters\"\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b5f\u52a0\u62c9\u8bed\u591a\u4efb\u52a1\u4ec7\u6068\u8a00\u8bba\u8bc6\u522b\u4efb\u52a1\uff0c\u5728YouTube\u8bc4\u8bba\u7684\u4ec7\u6068\u7c7b\u578b\u5206\u7c7b\u548c\u76ee\u6807\u7fa4\u4f53\u5206\u7c7b\u4e24\u4e2a\u5b50\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u5b5f\u52a0\u62c9\u8bed\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u96c6\u8986\u76d6\u95ee\u9898\uff0c\u63d0\u5347\u4ec7\u6068\u8a00\u8bba\u5206\u7c7b\u7684\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5b5f\u52a0\u62c9\u8bed\u8bed\u8a00\u6a21\u578b\u7684\u6df7\u5408\u96c6\u6210\u5fae\u8c03\u7b56\u7565\uff0c\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u8bc4\u4f30\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5e76\u4e0e\u5176\u4ed6\u8bed\u8a00\u6a21\u578b\u53d8\u4f53\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5728\u5b50\u4efb\u52a11A\u4e2d\u83b7\u5f97\u7b2c6\u540d\uff08\u5faeF1\u5206\u657073.23%\uff09\uff0c\u5728\u5b50\u4efb\u52a11B\u4e2d\u83b7\u5f97\u7b2c3\u540d\uff08\u5faeF1\u5206\u657073.28%\uff09\uff0c\u8d85\u8d8a\u4e86\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u96c6\u6210\u65b9\u6cd5\u5728\u5b5f\u52a0\u62c9\u8bed\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u8bef\u5206\u7c7b\u6a21\u5f0f\u7684\u8be6\u7ec6\u5206\u6790\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u7684\u4ec7\u6068\u8a00\u8bba\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18322", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18322", "abs": "https://arxiv.org/abs/2511.18322", "authors": ["Henrik Krauss", "Johann Licher", "Naoya Takeishi", "Annika Raatz", "Takehisa Yairi"], "title": "Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video", "comment": null, "summary": "Data-driven learning of soft continuum robot (SCR) dynamics from high-dimensional observations offers flexibility but often lacks physical interpretability, while model-based approaches require prior knowledge and can be computationally expensive. We bridge this gap by introducing (1) the Attention Broadcast Decoder (ABCD), a plug-and-play module for autoencoder-based latent dynamics learning that generates pixel-accurate attention maps localizing each latent dimension's contribution while filtering static backgrounds. (2) By coupling these attention maps to 2D oscillator networks, we enable direct on-image visualization of learned dynamics (masses, stiffness, and forces) without prior knowledge. We validate our approach on single- and double-segment SCRs, demonstrating that ABCD-based models significantly improve multi-step prediction accuracy: 5.7x error reduction for Koopman operators and 3.5x for oscillator networks on the two-segment robot. The learned oscillator network autonomously discovers a chain structure of oscillators. Unlike standard methods, ABCD models enable smooth latent space extrapolation beyond training data. This fully data-driven approach yields compact, physically interpretable models suitable for control applications.", "AI": {"tldr": "\u63d0\u51faAttention Broadcast Decoder (ABCD)\u6a21\u5757\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c2D\u632f\u8361\u5668\u7f51\u7edc\uff0c\u4ece\u9ad8\u7ef4\u89c2\u6d4b\u6570\u636e\u4e2d\u5b66\u4e60\u8f6f\u4f53\u8fde\u7eed\u673a\u5668\u4eba\u7684\u7269\u7406\u53ef\u89e3\u91ca\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5b66\u4e60\u8f6f\u4f53\u8fde\u7eed\u673a\u5668\u4eba\u52a8\u529b\u5b66\u7f3a\u4e4f\u7269\u7406\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u9700\u8981\u5148\u9a8c\u77e5\u8bc6\u4e14\u8ba1\u7b97\u6602\u8d35\uff0c\u9700\u8981\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u5f15\u5165ABCD\u6a21\u5757\u4f5c\u4e3a\u81ea\u7f16\u7801\u5668\u6f5c\u5728\u52a8\u529b\u5b66\u5b66\u4e60\u7684\u5373\u63d2\u5373\u7528\u7ec4\u4ef6\uff0c\u751f\u6210\u50cf\u7d20\u7ea7\u6ce8\u610f\u529b\u56fe\u5b9a\u4f4d\u6bcf\u4e2a\u6f5c\u5728\u7ef4\u5ea6\u7684\u8d21\u732e\uff1b\u5c06\u6ce8\u610f\u529b\u56fe\u4e0e2D\u632f\u8361\u5668\u7f51\u7edc\u8026\u5408\uff0c\u5b9e\u73b0\u5b66\u4e60\u52a8\u529b\u5b66\u7684\u76f4\u63a5\u53ef\u89c6\u5316\u3002", "result": "\u5728\u5355\u6bb5\u548c\u53cc\u6bb5\u8f6f\u4f53\u8fde\u7eed\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0cABCD\u6a21\u578b\u663e\u8457\u63d0\u5347\u591a\u6b65\u9884\u6d4b\u7cbe\u5ea6\uff1a\u53cc\u6bb5\u673a\u5668\u4eba\u4e0aKoopman\u7b97\u5b50\u8bef\u5dee\u51cf\u5c115.7\u500d\uff0c\u632f\u8361\u5668\u7f51\u7edc\u8bef\u5dee\u51cf\u5c113.5\u500d\uff1b\u5b66\u4e60\u5230\u7684\u632f\u8361\u5668\u7f51\u7edc\u81ea\u4e3b\u53d1\u73b0\u632f\u8361\u5668\u94fe\u7ed3\u6784\uff1b\u652f\u6301\u8bad\u7ec3\u6570\u636e\u4e4b\u5916\u7684\u5e73\u6ed1\u6f5c\u5728\u7a7a\u95f4\u5916\u63a8\u3002", "conclusion": "\u8fd9\u79cd\u5b8c\u5168\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u4ea7\u751f\u4e86\u7d27\u51d1\u3001\u7269\u7406\u53ef\u89e3\u91ca\u7684\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u63a7\u5236\u5e94\u7528\u3002"}}
{"id": "2511.18335", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18335", "abs": "https://arxiv.org/abs/2511.18335", "authors": ["James Y. Huang", "Wenxuan Zhou", "Nan Xu", "Fei Wang", "Qin Liu", "Sheng Zhang", "Hoifung Poon", "Muhao Chen"], "title": "OmniStruct: Universal Text-to-Structure Generation across Diverse Schemas", "comment": null, "summary": "The ability of Large Language Models (LLMs) to generate structured outputs that follow arbitrary schemas is crucial to a wide range of downstream tasks that require diverse structured representations of results such as information extraction, table generation, and function calling. While modern LLMs excel in generating unstructured responses in natural language, whether this advancement translates to a strong performance on text-to-structure tasks remains unclear. To bridge this gap, we first introduce OmniStruct, a comprehensive benchmark for assessing LLMs' capabilities on diverse text-to-structure tasks such as information extraction, table generation, and function calling. We build OmniStruct by identifying existing datasets across a wide range of tasks that are suitable for a structured answer format, and adapting them under a unified text-to-structure problem setting. To facilitate the development of efficient text-to-structure models, we collect high-quality training data via synthetic task generation. Without using any supervised data for OmniStruct tasks, our experiments demonstrate the possibility of fine-tuning much smaller models on synthetic data into universal structured generation models that can rival the performance of GPT-4o.", "AI": {"tldr": "OmniStruct\u662f\u4e00\u4e2a\u5168\u9762\u7684\u6587\u672c\u5230\u7ed3\u6784\u4efb\u52a1\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u4fe1\u606f\u63d0\u53d6\u3001\u8868\u683c\u751f\u6210\u548c\u51fd\u6570\u8c03\u7528\u7b49\u7ed3\u6784\u5316\u8f93\u51fa\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5408\u6210\u4efb\u52a1\u751f\u6210\u6536\u96c6\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u7814\u7a76\u8868\u660e\u53ef\u4ee5\u5728\u4e0d\u4f7f\u7528\u76d1\u7763\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u66f4\u5c0f\u6a21\u578b\u5fae\u8c03\u4e3a\u901a\u7528\u7ed3\u6784\u5316\u751f\u6210\u6a21\u578b\uff0c\u6027\u80fd\u53ef\u4e0eGPT-4o\u76f8\u5ab2\u7f8e\u3002", "motivation": "\u867d\u7136\u73b0\u4ee3LLMs\u5728\u751f\u6210\u975e\u7ed3\u6784\u5316\u81ea\u7136\u8bed\u8a00\u54cd\u5e94\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u5728\u6587\u672c\u5230\u7ed3\u6784\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u662f\u5426\u540c\u6837\u4f18\u79c0\u4ecd\u4e0d\u6e05\u695a\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6765\u8bc4\u4f30LLMs\u5728\u5404\u79cd\u7ed3\u6784\u5316\u8f93\u51fa\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efaOmniStruct\u57fa\u51c6\uff0c\u8bc6\u522b\u9002\u5408\u7ed3\u6784\u5316\u7b54\u6848\u683c\u5f0f\u7684\u73b0\u6709\u6570\u636e\u96c6\uff0c\u5e76\u5728\u7edf\u4e00\u7684\u6587\u672c\u5230\u7ed3\u6784\u95ee\u9898\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u9002\u914d\u3002\u901a\u8fc7\u5408\u6210\u4efb\u52a1\u751f\u6210\u6536\u96c6\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u4e0d\u4f7f\u7528OmniStruct\u4efb\u52a1\u76d1\u7763\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5fae\u8c03\u66f4\u5c0f\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u901a\u8fc7\u5728\u5408\u6210\u6570\u636e\u4e0a\u5fae\u8c03\u66f4\u5c0f\u6a21\u578b\uff0c\u53ef\u4ee5\u5c06\u5176\u8f6c\u53d8\u4e3a\u901a\u7528\u7ed3\u6784\u5316\u751f\u6210\u6a21\u578b\uff0c\u5176\u6027\u80fd\u80fd\u591f\u4e0eGPT-4o\u76f8\u5ab2\u7f8e\u3002", "conclusion": "OmniStruct\u4e3a\u8bc4\u4f30LLMs\u5728\u6587\u672c\u5230\u7ed3\u6784\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\uff0c\u5e76\u5c55\u793a\u4e86\u901a\u8fc7\u5408\u6210\u6570\u636e\u5fae\u8c03\u5c0f\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u7ed3\u6784\u5316\u751f\u6210\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.18353", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18353", "abs": "https://arxiv.org/abs/2511.18353", "authors": ["Sigrid Helene Strand", "Thomas Wiedemann", "Bram Burczek", "Dmitriy Shutin"], "title": "Enhancing UAV Search under Occlusion using Next Best View Planning", "comment": "Submitted to IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing", "summary": "Search and rescue missions are often critical following sudden natural disasters or in high-risk environmental situations. The most challenging search and rescue missions involve difficult-to-access terrains, such as dense forests with high occlusion. Deploying unmanned aerial vehicles for exploration can significantly enhance search effectiveness, facilitate access to challenging environments, and reduce search time. However, in dense forests, the effectiveness of unmanned aerial vehicles depends on their ability to capture clear views of the ground, necessitating a robust search strategy to optimize camera positioning and perspective. This work presents an optimized planning strategy and an efficient algorithm for the next best view problem in occluded environments. Two novel optimization heuristics, a geometry heuristic, and a visibility heuristic, are proposed to enhance search performance by selecting optimal camera viewpoints. Comparative evaluations in both simulated and real-world settings reveal that the visibility heuristic achieves greater performance, identifying over 90% of hidden objects in simulated forests and offering 10% better detection rates than the geometry heuristic. Additionally, real-world experiments demonstrate that the visibility heuristic provides better coverage under the canopy, highlighting its potential for improving search and rescue missions in occluded environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u906e\u6321\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u641c\u7d22\u6551\u63f4\u4efb\u52a1\u7684\u4f18\u5316\u89c4\u5212\u7b56\u7565\uff0c\u901a\u8fc7\u51e0\u4f55\u542f\u53d1\u5f0f\u548c\u53ef\u89c1\u6027\u542f\u53d1\u5f0f\u4e24\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u6700\u4f73\u89c6\u89d2\u9009\u62e9\u95ee\u9898\uff0c\u53ef\u89c1\u6027\u542f\u53d1\u5f0f\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u90fd\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u5728\u5bc6\u96c6\u68ee\u6797\u7b49\u906e\u6321\u73af\u5883\u4e2d\u8fdb\u884c\u641c\u6551\u4efb\u52a1\u65f6\uff0c\u65e0\u4eba\u673a\u9700\u8981\u4f18\u5316\u76f8\u673a\u4f4d\u7f6e\u548c\u89c6\u89d2\u6765\u83b7\u53d6\u6e05\u6670\u7684\u5730\u9762\u89c6\u56fe\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u906e\u6321\u73af\u5883\u4e2d\u7684\u641c\u7d22\u6548\u679c\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u9896\u7684\u4f18\u5316\u542f\u53d1\u5f0f\u65b9\u6cd5\uff1a\u51e0\u4f55\u542f\u53d1\u5f0f\u548c\u53ef\u89c1\u6027\u542f\u53d1\u5f0f\uff0c\u7528\u4e8e\u9009\u62e9\u6700\u4f18\u76f8\u673a\u89c6\u70b9\uff0c\u5e76\u5f00\u53d1\u4e86\u9ad8\u6548\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u906e\u6321\u73af\u5883\u4e2d\u7684\u6700\u4f73\u89c6\u89d2\u95ee\u9898\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u6bd4\u8f83\u8bc4\u4f30\u663e\u793a\uff0c\u53ef\u89c1\u6027\u542f\u53d1\u5f0f\u8bc6\u522b\u4e86\u6a21\u62df\u68ee\u6797\u4e2d90%\u4ee5\u4e0a\u7684\u9690\u85cf\u7269\u4f53\uff0c\u6bd4\u51e0\u4f55\u542f\u53d1\u5f0f\u7684\u68c0\u6d4b\u7387\u9ad810%\uff0c\u5e76\u4e14\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6811\u51a0\u4e0b\u8986\u76d6\u3002", "conclusion": "\u53ef\u89c1\u6027\u542f\u53d1\u5f0f\u5728\u906e\u6321\u73af\u5883\u4e2d\u5177\u6709\u66f4\u597d\u7684\u641c\u7d22\u6027\u80fd\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u641c\u6551\u4efb\u52a1\u7684\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u5bc6\u96c6\u68ee\u6797\u7b49\u96be\u4ee5\u8fdb\u5165\u7684\u5730\u5f62\u4e2d\u3002"}}
{"id": "2511.18369", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.18369", "abs": "https://arxiv.org/abs/2511.18369", "authors": ["Manon Berriche"], "title": "Tu crois que c'est vrai ? Diversite des regimes d'enonciation face aux fake news et mecanismes d'autoregulation conversationnelle", "comment": "in French language", "summary": "This thesis addresses two paradoxes: (1) why empirical studies find that fake news represent only a small share of the information consulted and shared on social media despite the absence of editorial control or journalistic norms, and (2) how political polarization has intensified even though users do not appear especially receptive to fake news. To investigate these issues, two complementary studies were carried out on Twitter and Facebook, combining quantitative analyses of digital traces with online observation and interviews. This mixed-methods design avoids reducing users to single reactions to identified fake items and instead examines the variety of practices across different interactional situations, online and offline, while recording socio-demographic traits. The first study mapped users who shared at least one item labeled fake by fact-checkers in the French Twittersphere. The second used a corpus of items flagged by Facebook users to study reactions to statements whose epistemic status is uncertain. Three main findings emerge. First, sharing fake news is concentrated among a limited group of users who are not less educated or cognitively disadvantaged but are more politicized and critical of institutions; owing to their high activity and prolific sharing, they can help set the agenda for their political camp. Second, exposed users can deploy varying forms of critical distance depending on their social position and the interactional norms of the situations they inhabit: either discursive caution (prudence \u00e9nonciative) or interventions ('points d'arr\u00eat') that express disagreement or corrections. Third, these forms of critical distance seldom yield genuine deliberative debates or agonistic pluralism; rather, they often produce dialogues of the deaf among a small, particularly active minority.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e24\u4e2a\u6096\u8bba\uff1a\u865a\u5047\u65b0\u95fb\u5728\u793e\u4ea4\u5a92\u4f53\u4e2d\u5360\u6bd4\u5f88\u5c0f\u4f46\u653f\u6cbb\u6781\u5316\u52a0\u5267\u3002\u901a\u8fc7Twitter\u548cFacebook\u7684\u6df7\u5408\u65b9\u6cd5\u7814\u7a76\u53d1\u73b0\uff0c\u865a\u5047\u65b0\u95fb\u5206\u4eab\u96c6\u4e2d\u5728\u5c11\u6570\u9ad8\u5ea6\u653f\u6cbb\u5316\u7684\u7528\u6237\u4e2d\uff0c\u7528\u6237\u4f1a\u6839\u636e\u793e\u4f1a\u4f4d\u7f6e\u91c7\u53d6\u4e0d\u540c\u6279\u5224\u8ddd\u79bb\uff0c\u4f46\u8fd9\u4e9b\u4e92\u52a8\u5f88\u5c11\u4ea7\u751f\u771f\u6b63\u8fa9\u8bba\u3002", "motivation": "\u89e3\u91ca\u4e3a\u4ec0\u4e48\u5728\u7f3a\u4e4f\u7f16\u8f91\u63a7\u5236\u7684\u793e\u4ea4\u5a92\u4f53\u4e0a\u865a\u5047\u65b0\u95fb\u5360\u6bd4\u5f88\u5c0f\uff0c\u4ee5\u53ca\u4e3a\u4ec0\u4e48\u7528\u6237\u5bf9\u865a\u5047\u65b0\u95fb\u4e0d\u7279\u522b\u654f\u611f\u4f46\u653f\u6cbb\u6781\u5316\u4ecd\u5728\u52a0\u5267\u8fd9\u4e24\u4e2a\u6096\u8bba\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u8bbe\u8ba1\uff0c\u7ed3\u5408\u6570\u5b57\u75d5\u8ff9\u5b9a\u91cf\u5206\u6790\u3001\u5728\u7ebf\u89c2\u5bdf\u548c\u8bbf\u8c08\uff0c\u5728Twitter\u548cFacebook\u4e0a\u8fdb\u884c\u4e24\u9879\u4e92\u8865\u7814\u7a76\uff0c\u8003\u5bdf\u4e0d\u540c\u4e92\u52a8\u60c5\u5883\u4e0b\u7684\u7528\u6237\u5b9e\u8df5\u3002", "result": "1) \u865a\u5047\u65b0\u95fb\u5206\u4eab\u96c6\u4e2d\u5728\u5c11\u6570\u9ad8\u5ea6\u653f\u6cbb\u5316\u3001\u6279\u8bc4\u5236\u5ea6\u7684\u7528\u6237\u4e2d\uff1b2) \u7528\u6237\u6839\u636e\u793e\u4f1a\u4f4d\u7f6e\u91c7\u53d6\u4e0d\u540c\u6279\u5224\u8ddd\u79bb\u5f62\u5f0f\uff1b3) \u8fd9\u4e9b\u4e92\u52a8\u5f88\u5c11\u4ea7\u751f\u771f\u6b63\u8fa9\u8bba\uff0c\u66f4\u591a\u662f\"\u804b\u5b50\u5bf9\u8bdd\"\u3002", "conclusion": "\u865a\u5047\u65b0\u95fb\u7684\u5f71\u54cd\u4e0d\u662f\u901a\u8fc7\u5e7f\u6cdb\u4f20\u64ad\uff0c\u800c\u662f\u901a\u8fc7\u5c11\u6570\u9ad8\u5ea6\u6d3b\u8dc3\u7528\u6237\u7684\u8bae\u7a0b\u8bbe\u7f6e\u80fd\u529b\uff0c\u4ee5\u53ca\u7528\u6237\u6279\u5224\u8ddd\u79bb\u5f62\u5f0f\u7684\u793e\u4f1a\u5206\u5316\u6765\u5b9e\u73b0\u7684\uff0c\u8fd9\u89e3\u91ca\u4e86\u653f\u6cbb\u6781\u5316\u7684\u52a0\u5267\u3002"}}
{"id": "2511.18374", "categories": ["cs.RO", "eess.SY", "math.DS"], "pdf": "https://arxiv.org/pdf/2511.18374", "abs": "https://arxiv.org/abs/2511.18374", "authors": ["Jiaxun Sun"], "title": "Explicit Bounds on the Hausdorff Distance for Truncated mRPI Sets via Norm-Dependent Contraction Rates", "comment": null, "summary": "This paper establishes the first explicit and closed-form upper bound on the Hausdorff distance between the truncated minimal robust positively invariant (mRPI) set and its infinite-horizon limit. While existing mRPI approximations guarantee asymptotic convergence through geometric or norm-based arguments, none provides a computable expression that quantifies the truncation error for a given horizon. We show that the error satisfies \\( d_H(\\mathcal{E}_N,\\mathcal{E}_\\infty) \\le r_W\\,\u03b3^{N+1}/(1-\u03b3), \\) where $\u03b3<1$ is the induced-norm contraction factor and $r_W$ depends only on the disturbance set. The bound is fully analytic, requires no iterative set computations, and directly characterizes the decay rate of the truncated Minkowski series. We further demonstrate that the choice of vector norm serves as a design parameter that accelerates convergence, enabling substantially tighter horizon selection for robust invariant-set computations and tube-based MPC. Numerical experiments validate the sharpness, scalability, and practical relevance of the proposed bound.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5efa\u7acb\u4e86\u622a\u65ad\u6700\u5c0f\u9c81\u68d2\u6b63\u4e0d\u53d8\u96c6\u4e0e\u5176\u65e0\u9650\u65f6\u6781\u9650\u4e4b\u95f4Hausdorff\u8ddd\u79bb\u7684\u663e\u5f0f\u95ed\u5f0f\u4e0a\u754c\uff0c\u63d0\u4f9b\u4e86\u53ef\u8ba1\u7b97\u7684\u622a\u65ad\u8bef\u5dee\u8868\u8fbe\u5f0f\u3002", "motivation": "\u73b0\u6709mRPI\u8fd1\u4f3c\u65b9\u6cd5\u901a\u8fc7\u51e0\u4f55\u6216\u8303\u6570\u8bba\u8bc1\u4fdd\u8bc1\u6e10\u8fd1\u6536\u655b\uff0c\u4f46\u90fd\u6ca1\u6709\u63d0\u4f9b\u91cf\u5316\u7ed9\u5b9a\u65f6\u57df\u622a\u65ad\u8bef\u5dee\u7684\u53ef\u8ba1\u7b97\u8868\u8fbe\u5f0f\u3002", "method": "\u8bc1\u660e\u8bef\u5dee\u6ee1\u8db3d_H(\u2130_N,\u2130_\u221e) \u2264 r_W \u03b3^(N+1)/(1-\u03b3)\uff0c\u5176\u4e2d\u03b3<1\u662f\u8bf1\u5bfc\u8303\u6570\u6536\u7f29\u56e0\u5b50\uff0cr_W\u4ec5\u4f9d\u8d56\u4e8e\u6270\u52a8\u96c6\u3002\u8be5\u8fb9\u754c\u5b8c\u5168\u89e3\u6790\uff0c\u65e0\u9700\u8fed\u4ee3\u96c6\u8ba1\u7b97\u3002", "result": "\u8fb9\u754c\u5b8c\u5168\u89e3\u6790\uff0c\u76f4\u63a5\u8868\u5f81\u622a\u65adMinkowski\u7ea7\u6570\u7684\u8870\u51cf\u7387\uff0c\u5411\u91cf\u8303\u6570\u7684\u9009\u62e9\u53ef\u4f5c\u4e3a\u8bbe\u8ba1\u53c2\u6570\u52a0\u901f\u6536\u655b\u3002\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fb9\u754c\u7684\u9510\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u76f8\u5173\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u8fb9\u754c\u663e\u8457\u6539\u8fdb\u4e86\u9c81\u68d2\u4e0d\u53d8\u96c6\u8ba1\u7b97\u548c\u57fa\u4e8e\u7ba1\u7684MPC\u7684\u65f6\u57df\u9009\u62e9\uff0c\u4e3a\u622a\u65ad\u8bef\u5dee\u63d0\u4f9b\u4e86\u9996\u4e2a\u53ef\u8ba1\u7b97\u7684\u663e\u5f0f\u91cf\u5316\u3002"}}
{"id": "2511.18393", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18393", "abs": "https://arxiv.org/abs/2511.18393", "authors": ["Heejoon Koo"], "title": "Towards Robust and Fair Next Visit Diagnosis Prediction under Noisy Clinical Notes with Large Language Models", "comment": "Accepted by the Association for the Advancement of Artificial Intelligence (AAAI) 2026 1st Workshop on Safe, Ethical, Certified, Uncertainty-aware, Robust, and Explainable AI for Health (SECURE-AI4H)", "summary": "A decade of rapid advances in artificial intelligence (AI) has opened new opportunities for clinical decision support systems (CDSS), with large language models (LLMs) demonstrating strong reasoning abilities on timely medical tasks. However, clinical texts are often degraded by human errors or failures in automated pipelines, raising concerns about the reliability and fairness of AI-assisted decision-making. Yet the impact of such degradations remains under-investigated, particularly regarding how noise-induced shifts can heighten predictive uncertainty and unevenly affect demographic subgroups. We present a systematic study of state-of-the-art LLMs under diverse text corruption scenarios, focusing on robustness and equity in next-visit diagnosis prediction. To address the challenge posed by the large diagnostic label space, we introduce a clinically grounded label-reduction scheme and a hierarchical chain-of-thought (CoT) strategy that emulates clinicians' reasoning. Our approach improves robustness and reduces subgroup instability under degraded inputs, advancing the reliable use of LLMs in CDSS. We release code at https://github.com/heejkoo9/NECHOv3.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u5206\u6790\u4e86\u6587\u672c\u9000\u5316\u5bf9LLM\u5728\u4e34\u5e8a\u8bca\u65ad\u9884\u6d4b\u4e2d\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u4e34\u5e8a\u6807\u7b7e\u7f29\u51cf\u548c\u5206\u5c42\u601d\u7ef4\u94fe\u7b56\u7565\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4e34\u5e8a\u6587\u672c\u5e38\u56e0\u4eba\u4e3a\u9519\u8bef\u6216\u81ea\u52a8\u5316\u6d41\u7a0b\u6545\u969c\u800c\u8d28\u91cf\u4e0b\u964d\uff0c\u8fd9\u4f1a\u5f71\u54cdAI\u8f85\u52a9\u51b3\u7b56\u7684\u53ef\u9760\u6027\u548c\u516c\u5e73\u6027\uff0c\u4f46\u76ee\u524d\u5bf9\u6b64\u7c7b\u9000\u5316\u5f71\u54cd\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5173\u4e8e\u566a\u58f0\u5982\u4f55\u589e\u52a0\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5e76\u4e0d\u540c\u5f71\u54cd\u4eba\u53e3\u4e9a\u7fa4\u3002", "method": "\u5f15\u5165\u4e34\u5e8a\u57fa\u7840\u7684\u6807\u7b7e\u7f29\u51cf\u65b9\u6848\u548c\u5206\u5c42\u601d\u7ef4\u94fe\u7b56\u7565\uff0c\u6a21\u62df\u4e34\u5e8a\u533b\u751f\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u591a\u79cd\u6587\u672c\u9000\u5316\u573a\u666f\u4e0b\u6d4b\u8bd5\u6700\u5148\u8fdb\u7684LLM\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9000\u5316\u8f93\u5165\u4e0b\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u4e86\u4e9a\u7fa4\u4e0d\u7a33\u5b9a\u6027\uff0c\u63a8\u8fdb\u4e86LLM\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u4e2d\u7684\u53ef\u9760\u4f7f\u7528\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e34\u5e8a\u6807\u7b7e\u7f29\u51cf\u548c\u5206\u5c42CoT\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u4e34\u5e8a\u6587\u672c\u9000\u5316\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u516c\u5e73\u6027\uff0c\u4e3aCDSS\u4e2d\u53ef\u9760\u4f7f\u7528LLM\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18486", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.18486", "abs": "https://arxiv.org/abs/2511.18486", "authors": ["Jasan Zughaibi", "Denis von Arx", "Maurus Derungs", "Florian Heemeyer", "Luca A. Antonelli", "Quentin Boehler", "Michael Muehlebach", "Bradley J. Nelson"], "title": "Expanding the Workspace of Electromagnetic Navigation Systems Using Dynamic Feedback for Single- and Multi-agent Control", "comment": null, "summary": "Electromagnetic navigation systems (eMNS) enable a number of magnetically guided surgical procedures. A challenge in magnetically manipulating surgical tools is that the effective workspace of an eMNS is often severely constrained by power and thermal limits. We show that system-level control design significantly expands this workspace by reducing the currents needed to achieve a desired motion. We identified five key system approaches that enable this expansion: (i) motion-centric torque/force objectives, (ii) energy-optimal current allocation, (iii) real-time pose estimation, (iv) dynamic feedback, and (v) high-bandwidth eMNS components. As a result, we stabilize a 3D inverted pendulum on an eight-coil OctoMag eMNS with significantly lower currents (0.1-0.2 A vs. 8-14 A), by replacing a field-centric field-alignment strategy with a motion-centric torque/force-based approach. We generalize to multi-agent control by simultaneously stabilizing two inverted pendulums within a shared workspace, exploiting magnetic-field nonlinearity and coil redundancy for independent actuation. A structured analysis compares the electromagnetic workspaces of both paradigms and examines current-allocation strategies that map motion objectives to coil currents. Cross-platform evaluation of the clinically oriented Navion eMNS further demonstrates substantial workspace expansion by maintaining stable balancing at distances up to 50 cm from the coils. The results demonstrate that feedback is a practical path to scalable, efficient, and clinically relevant magnetic manipulation.", "AI": {"tldr": "\u901a\u8fc7\u7cfb\u7edf\u7ea7\u63a7\u5236\u8bbe\u8ba1\u663e\u8457\u6269\u5c55\u7535\u78c1\u5bfc\u822a\u7cfb\u7edf\u7684\u5de5\u4f5c\u7a7a\u95f4\uff0c\u91c7\u7528\u8fd0\u52a8\u4e2d\u5fc3\u7684\u626d\u77e9/\u529b\u76ee\u6807\u3001\u80fd\u91cf\u6700\u4f18\u7535\u6d41\u5206\u914d\u7b49\u4e94\u79cd\u65b9\u6cd5\uff0c\u5728OctoMag\u548cNavion\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u7535\u6d41\u9700\u6c42\u5927\u5e45\u964d\u4f4e\u548c\u7a33\u5b9a\u6027\u63d0\u5347\u3002", "motivation": "\u7535\u78c1\u5bfc\u822a\u7cfb\u7edf\u7684\u6709\u6548\u5de5\u4f5c\u7a7a\u95f4\u5e38\u53d7\u529f\u7387\u548c\u70ed\u9650\u5236\u4e25\u91cd\u7ea6\u675f\uff0c\u9700\u8981\u5bfb\u627e\u65b9\u6cd5\u6765\u6269\u5c55\u5de5\u4f5c\u7a7a\u95f4\u5e76\u964d\u4f4e\u7535\u6d41\u9700\u6c42\u3002", "method": "\u91c7\u7528\u4e94\u79cd\u7cfb\u7edf\u65b9\u6cd5\uff1a\u8fd0\u52a8\u4e2d\u5fc3\u7684\u626d\u77e9/\u529b\u76ee\u6807\u3001\u80fd\u91cf\u6700\u4f18\u7535\u6d41\u5206\u914d\u3001\u5b9e\u65f6\u4f4d\u59ff\u4f30\u8ba1\u3001\u52a8\u6001\u53cd\u9988\u63a7\u5236\u548c\u9ad8\u5e26\u5bbdeMNS\u7ec4\u4ef6\uff0c\u5728OctoMag\u548cNavion\u7cfb\u7edf\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5728OctoMag\u7cfb\u7edf\u4e0a\u7a33\u5b9a3D\u5012\u7acb\u6446\u7684\u7535\u6d41\u4ece8-14A\u964d\u81f30.1-0.2A\uff0c\u5728Navion\u7cfb\u7edf\u4e0a\u80fd\u5728\u8ddd\u79bb\u7ebf\u570850cm\u5904\u4fdd\u6301\u7a33\u5b9a\u5e73\u8861\uff0c\u663e\u8457\u6269\u5c55\u4e86\u5de5\u4f5c\u7a7a\u95f4\u3002", "conclusion": "\u53cd\u9988\u63a7\u5236\u662f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u4e14\u4e34\u5e8a\u76f8\u5173\u7684\u78c1\u64cd\u4f5c\u7684\u5b9e\u9645\u8def\u5f84\uff0c\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u80fd\u663e\u8457\u63d0\u5347\u7535\u78c1\u5bfc\u822a\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2511.18409", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18409", "abs": "https://arxiv.org/abs/2511.18409", "authors": ["Dana Arad", "Yonatan Belinkov", "Hanjie Chen", "Najoung Kim", "Hosein Mohebbi", "Aaron Mueller", "Gabriele Sarti", "Martin Tutek"], "title": "Findings of the BlackboxNLP 2025 Shared Task: Localizing Circuits and Causal Variables in Language Models", "comment": null, "summary": "Mechanistic interpretability (MI) seeks to uncover how language models (LMs) implement specific behaviors, yet measuring progress in MI remains challenging. The recently released Mechanistic Interpretability Benchmark (MIB; Mueller et al., 2025) provides a standardized framework for evaluating circuit and causal variable localization. Building on this foundation, the BlackboxNLP 2025 Shared Task extends MIB into a community-wide reproducible comparison of MI techniques. The shared task features two tracks: circuit localization, which assesses methods that identify causally influential components and interactions driving model behavior, and causal variable localization, which evaluates approaches that map activations into interpretable features. With three teams spanning eight different methods, participants achieved notable gains in circuit localization using ensemble and regularization strategies for circuit discovery. With one team spanning two methods, participants achieved significant gains in causal variable localization using low-dimensional and non-linear projections to featurize activation vectors. The MIB leaderboard remains open; we encourage continued work in this standard evaluation framework to measure progress in MI research going forward.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u57fa\u4e8eMechanistic Interpretability Benchmark (MIB)\u7684BlackboxNLP 2025\u5171\u4eab\u4efb\u52a1\uff0c\u5305\u542b\u7535\u8def\u5b9a\u4f4d\u548c\u56e0\u679c\u53d8\u91cf\u5b9a\u4f4d\u4e24\u4e2a\u8d5b\u9053\uff0c\u5c55\u793a\u4e86\u591a\u79cd\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u4e2d\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u673a\u5236\u53ef\u89e3\u91ca\u6027(MI)\u7814\u7a76\u4e2d\u8fdb\u5c55\u8861\u91cf\u56f0\u96be\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u6765\u6bd4\u8f83\u4e0d\u540cMI\u6280\u672f\u7684\u6548\u679c\u3002", "method": "\u57fa\u4e8eMIB\u57fa\u51c6\u6784\u5efa\u5171\u4eab\u4efb\u52a1\uff0c\u5305\u542b\u7535\u8def\u5b9a\u4f4d(\u8bc6\u522b\u56e0\u679c\u5f71\u54cd\u7ec4\u4ef6\u548c\u4ea4\u4e92)\u548c\u56e0\u679c\u53d8\u91cf\u5b9a\u4f4d(\u5c06\u6fc0\u6d3b\u6620\u5c04\u4e3a\u53ef\u89e3\u91ca\u7279\u5f81)\u4e24\u4e2a\u8d5b\u9053\uff0c\u91c7\u7528\u96c6\u6210\u3001\u6b63\u5219\u5316\u3001\u4f4e\u7ef4\u548c\u975e\u7ebf\u6027\u6295\u5f71\u7b49\u65b9\u6cd5\u3002", "result": "\u5728\u7535\u8def\u5b9a\u4f4d\u4e2d\uff0c8\u79cd\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u548c\u6b63\u5219\u5316\u7b56\u7565\u83b7\u5f97\u663e\u8457\u63d0\u5347\uff1b\u5728\u56e0\u679c\u53d8\u91cf\u5b9a\u4f4d\u4e2d\uff0c2\u79cd\u65b9\u6cd5\u901a\u8fc7\u4f4e\u7ef4\u548c\u975e\u7ebf\u6027\u6295\u5f71\u5b9e\u73b0\u663e\u8457\u589e\u76ca\u3002", "conclusion": "MIB\u6392\u884c\u699c\u4fdd\u6301\u5f00\u653e\uff0c\u9f13\u52b1\u7ee7\u7eed\u4f7f\u7528\u8fd9\u4e00\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\u6765\u8861\u91cfMI\u7814\u7a76\u7684\u8fdb\u5c55\u3002"}}
{"id": "2511.18509", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18509", "abs": "https://arxiv.org/abs/2511.18509", "authors": ["Ziyu Meng", "Tengyu Liu", "Le Ma", "Yingying Wu", "Ran Song", "Wei Zhang", "Siyuan Huang"], "title": "SafeFall: Learning Protective Control for Humanoid Robots", "comment": null, "summary": "Bipedal locomotion makes humanoid robots inherently prone to falls, causing catastrophic damage to the expensive sensors, actuators, and structural components of full-scale robots. To address this critical barrier to real-world deployment, we present \\method, a framework that learns to predict imminent, unavoidable falls and execute protective maneuvers to minimize hardware damage. SafeFall is designed to operate seamlessly alongside existing nominal controller, ensuring no interference during normal operation. It combines two synergistic components: a lightweight, GRU-based fall predictor that continuously monitors the robot's state, and a reinforcement learning policy for damage mitigation. The protective policy remains dormant until the predictor identifies a fall as unavoidable, at which point it activates to take control and execute a damage-minimizing response. This policy is trained with a novel, damage-aware reward function that incorporates the robot's specific structural vulnerabilities, learning to shield critical components like the head and hands while absorbing energy with more robust parts of its body. Validated on a full-scale Unitree G1 humanoid, SafeFall demonstrated significant performance improvements over unprotected falls. It reduced peak contact forces by 68.3\\%, peak joint torques by 78.4\\%, and eliminated 99.3\\% of collisions with vulnerable components. By enabling humanoids to fail safely, SafeFall provides a crucial safety net that allows for more aggressive experiments and accelerates the deployment of these robots in complex, real-world environments.", "AI": {"tldr": "SafeFall\u662f\u4e00\u4e2a\u4fdd\u62a4\u4eba\u5f62\u673a\u5668\u4eba\u514d\u53d7\u6454\u5012\u635f\u574f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u4e0d\u53ef\u907f\u514d\u7684\u6454\u5012\u5e76\u6267\u884c\u4fdd\u62a4\u52a8\u4f5c\u6765\u6700\u5c0f\u5316\u786c\u4ef6\u635f\u4f24", "motivation": "\u53cc\u8db3\u884c\u8d70\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u5bb9\u6613\u6454\u5012\uff0c\u5bfc\u81f4\u6602\u8d35\u7684\u4f20\u611f\u5668\u3001\u6267\u884c\u5668\u548c\u7ed3\u6784\u7ec4\u4ef6\u635f\u574f\uff0c\u8fd9\u662f\u5b9e\u9645\u90e8\u7f72\u7684\u5173\u952e\u969c\u788d", "method": "\u7ed3\u5408\u8f7b\u91cf\u7ea7GRU\u6454\u5012\u9884\u6d4b\u5668\u548c\u5f3a\u5316\u5b66\u4e60\u4fdd\u62a4\u7b56\u7565\uff0c\u5728\u68c0\u6d4b\u5230\u4e0d\u53ef\u907f\u514d\u6454\u5012\u65f6\u6fc0\u6d3b\u4fdd\u62a4\u52a8\u4f5c\uff0c\u4f7f\u7528\u635f\u4f24\u611f\u77e5\u5956\u52b1\u51fd\u6570\u8bad\u7ec3", "result": "\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u5cf0\u503c\u63a5\u89e6\u529b\u51cf\u5c1168.3%\uff0c\u5cf0\u503c\u5173\u8282\u626d\u77e9\u51cf\u5c1178.4%\uff0c99.3%\u7684\u8106\u5f31\u90e8\u4ef6\u78b0\u649e\u88ab\u6d88\u9664", "conclusion": "SafeFall\u4e3a\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u5b89\u5168\u7f51\uff0c\u4f7f\u66f4\u6fc0\u8fdb\u7684\u5b9e\u9a8c\u6210\u4e3a\u53ef\u80fd\uff0c\u52a0\u901f\u4e86\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u90e8\u7f72"}}
{"id": "2511.18411", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18411", "abs": "https://arxiv.org/abs/2511.18411", "authors": ["Sultan Alrashed", "Chadi Helwe", "Francesco Orabona"], "title": "SmolKalam: Ensemble Quality-Filtered Translation at Scale for High Quality Arabic Post-Training Data", "comment": "Work in progress", "summary": "Although the community has tackled the acquisition of high-quality Arabic pretraining data, we still lack large-scale, multi-turn Arabic datasets that include reasoning and tool calling. Naive translation can work at the pretraining scale, but post-training demands much higher quality, which requires a stricter approach to dataset curation. In this work, we introduce SmolKalam, a translation of Smoltalk2 that uses a multi-model ensemble translation pipeline, applies quality filtering, and examines effective translation techniques for traditional decoder-only models through ablations.", "AI": {"tldr": "SmolKalam\u662f\u4e00\u4e2a\u963f\u62c9\u4f2f\u8bed\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u6a21\u578b\u96c6\u6210\u7ffb\u8bd1\u7ba1\u9053\u4eceSmoltalk2\u7ffb\u8bd1\u800c\u6765\uff0c\u5305\u542b\u8d28\u91cf\u8fc7\u6ee4\u548c\u7ffb\u8bd1\u6280\u672f\u6d88\u878d\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u591a\u8f6e\u3001\u5305\u542b\u63a8\u7406\u548c\u5de5\u5177\u8c03\u7528\u7684\u963f\u62c9\u4f2f\u8bed\u6570\u636e\u96c6\uff0c\u800c\u5355\u7eaf\u7684\u7ffb\u8bd1\u65b9\u6cd5\u5728\u9884\u8bad\u7ec3\u540e\u9636\u6bb5\u9700\u8981\u66f4\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u3002", "method": "\u4f7f\u7528\u591a\u6a21\u578b\u96c6\u6210\u7ffb\u8bd1\u7ba1\u9053\uff0c\u5e94\u7528\u8d28\u91cf\u8fc7\u6ee4\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u68c0\u9a8c\u4f20\u7edf\u4ec5\u89e3\u7801\u5668\u6a21\u578b\u7684\u6709\u6548\u7ffb\u8bd1\u6280\u672f\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u9ad8\u8d28\u91cf\u7684\u963f\u62c9\u4f2f\u8bed\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6SmolKalam\u3002", "conclusion": "\u901a\u8fc7\u4e25\u683c\u7684\u7ffb\u8bd1\u6d41\u7a0b\u548c\u8d28\u91cf\u63a7\u5236\uff0c\u53ef\u4ee5\u521b\u5efa\u9ad8\u8d28\u91cf\u7684\u963f\u62c9\u4f2f\u8bed\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u6ee1\u8db3\u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u9700\u6c42\u3002"}}
{"id": "2511.18525", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18525", "abs": "https://arxiv.org/abs/2511.18525", "authors": ["Samarth Chopra", "Jing Liang", "Gershom Seneviratne", "Yonghan Lee", "Jaehoon Choi", "Jianyu An", "Stephen Cheng", "Dinesh Manocha"], "title": "Splatblox: Traversability-Aware Gaussian Splatting for Outdoor Robot Navigation", "comment": "Submitted to ICRA 2026", "summary": "We present Splatblox, a real-time system for autonomous navigation in outdoor environments with dense vegetation, irregular obstacles, and complex terrain. Our method fuses segmented RGB images and LiDAR point clouds using Gaussian Splatting to construct a traversability-aware Euclidean Signed Distance Field (ESDF) that jointly encodes geometry and semantics. Updated online, this field enables semantic reasoning to distinguish traversable vegetation (e.g., tall grass) from rigid obstacles (e.g., trees), while LiDAR ensures 360-degree geometric coverage for extended planning horizons. We validate Splatblox on a quadruped robot and demonstrate transfer to a wheeled platform. In field trials across vegetation-rich scenarios, it outperforms state-of-the-art methods with over 50% higher success rate, 40% fewer freezing incidents, 5% shorter paths, and up to 13% faster time to goal, while supporting long-range missions up to 100 meters. Experiment videos and more details can be found on our project page: https://splatblox.github.io", "AI": {"tldr": "Splatblox\u662f\u4e00\u4e2a\u7528\u4e8e\u6237\u5916\u5bc6\u96c6\u690d\u88ab\u73af\u5883\u7684\u5b9e\u65f6\u81ea\u4e3b\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u878d\u5408RGB\u56fe\u50cf\u548cLiDAR\u70b9\u4e91\u6784\u5efa\u53ef\u901a\u884c\u6027\u611f\u77e5\u7684ESDF\u5730\u56fe\uff0c\u5728\u690d\u88ab\u4e30\u5bcc\u573a\u666f\u4e2d\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6210\u529f\u7387\u63d0\u9ad850%\u4ee5\u4e0a\u3002", "motivation": "\u89e3\u51b3\u6237\u5916\u73af\u5883\u4e2d\u5bc6\u96c6\u690d\u88ab\u3001\u4e0d\u89c4\u5219\u969c\u788d\u7269\u548c\u590d\u6742\u5730\u5f62\u4e0b\u7684\u81ea\u4e3b\u5bfc\u822a\u6311\u6218\uff0c\u9700\u8981\u540c\u65f6\u5904\u7406\u51e0\u4f55\u548c\u8bed\u4e49\u4fe1\u606f\u6765\u533a\u5206\u53ef\u7a7f\u8d8a\u690d\u88ab\u4e0e\u521a\u6027\u969c\u788d\u7269\u3002", "method": "\u4f7f\u7528\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u878d\u5408\u5206\u5272\u7684RGB\u56fe\u50cf\u548cLiDAR\u70b9\u4e91\uff0c\u6784\u5efa\u5728\u7ebf\u66f4\u65b0\u7684\u53ef\u901a\u884c\u6027\u611f\u77e5\u6b27\u51e0\u91cc\u5f97\u7b26\u53f7\u8ddd\u79bb\u573a(ESDF)\uff0c\u7ed3\u5408\u8bed\u4e49\u63a8\u7406\u548c360\u5ea6\u51e0\u4f55\u8986\u76d6\u3002", "result": "\u5728\u690d\u88ab\u4e30\u5bcc\u573a\u666f\u7684\u73b0\u573a\u8bd5\u9a8c\u4e2d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u6210\u529f\u7387\u63d0\u9ad850%\u4ee5\u4e0a\uff0c\u51bb\u7ed3\u4e8b\u4ef6\u51cf\u5c1140%\uff0c\u8def\u5f84\u7f29\u77ed5%\uff0c\u76ee\u6807\u5230\u8fbe\u65f6\u95f4\u52a0\u5feb13%\uff0c\u652f\u6301\u957f\u8fbe100\u7c73\u7684\u8fdc\u7a0b\u4efb\u52a1\u3002", "conclusion": "Splatblox\u7cfb\u7edf\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u81ea\u4e3b\u5bfc\u822a\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4e0d\u540c\u5e73\u53f0\uff08\u56db\u8db3\u548c\u8f6e\u5f0f\u673a\u5668\u4eba\uff09\u4e0a\u7684\u53ef\u8f6c\u79fb\u6027\uff0c\u4e3a\u6237\u5916\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18413", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.18413", "abs": "https://arxiv.org/abs/2511.18413", "authors": ["Yu Xia", "Sungchul Kim", "Tong Yu", "Ryan A. Rossi", "Julian McAuely"], "title": "Multi-Agent Collaborative Filtering: Orchestrating Users and Items for Agentic Recommendations", "comment": null, "summary": "Agentic recommendations cast recommenders as large language model (LLM) agents that can plan, reason, use tools, and interact with users of varying preferences in web applications. However, most existing agentic recommender systems focus on generic single-agent plan-execute workflows or multi-agent task decomposition pipelines. Without recommendation-oriented design, they often underuse the collaborative signals in the user-item interaction history, leading to unsatisfying recommendation results. To address this, we propose the Multi-Agent Collaborative Filtering (MACF) framework for agentic recommendations, drawing an analogy between traditional collaborative filtering algorithms and LLM-based multi-agent collaboration. Specifically, given a target user and query, we instantiate similar users and relevant items as LLM agents with unique profiles. Each agent is able to call retrieval tools, suggest candidate items, and interact with other agents. Different from the static preference aggregation in traditional collaborative filtering, MACF employs a central orchestrator agent to adaptively manage the collaboration between user and item agents via dynamic agent recruitment and personalized collaboration instruction. Experimental results on datasets from three different domains show the advantages of our MACF framework compared to strong agentic recommendation baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u591a\u667a\u80fd\u4f53\u534f\u540c\u8fc7\u6ee4\u6846\u67b6MACF\uff0c\u5c06\u4f20\u7edf\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\u4e0e\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u8fdb\u884c\u7c7b\u6bd4\uff0c\u901a\u8fc7\u52a8\u6001\u667a\u80fd\u4f53\u62db\u52df\u548c\u4e2a\u6027\u5316\u534f\u4f5c\u6307\u4ee4\u6765\u63d0\u5347\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u63a8\u8350\u7cfb\u7edf\u5927\u591a\u5173\u6ce8\u901a\u7528\u5355\u667a\u80fd\u4f53\u6216\u591a\u667a\u80fd\u4f53\u4efb\u52a1\u5206\u89e3\u6d41\u7a0b\uff0c\u7f3a\u4e4f\u63a8\u8350\u5bfc\u5411\u8bbe\u8ba1\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u7528\u6237-\u7269\u54c1\u4ea4\u4e92\u5386\u53f2\u4e2d\u7684\u534f\u540c\u4fe1\u53f7\uff0c\u5bfc\u81f4\u63a8\u8350\u7ed3\u679c\u4e0d\u7406\u60f3\u3002", "method": "\u4e3a\u76ee\u6807\u7528\u6237\u548c\u67e5\u8be2\u5b9e\u4f8b\u5316\u76f8\u4f3c\u7528\u6237\u548c\u76f8\u5173\u7269\u54c1\u4f5c\u4e3a\u5177\u6709\u72ec\u7279\u914d\u7f6e\u6587\u4ef6\u7684LLM\u667a\u80fd\u4f53\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u80fd\u591f\u8c03\u7528\u68c0\u7d22\u5de5\u5177\u3001\u63a8\u8350\u5019\u9009\u7269\u54c1\u5e76\u4e0e\u5176\u4ed6\u667a\u80fd\u4f53\u4ea4\u4e92\uff0c\u901a\u8fc7\u4e2d\u592e\u7f16\u6392\u5668\u667a\u80fd\u4f53\u52a8\u6001\u7ba1\u7406\u7528\u6237\u548c\u7269\u54c1\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u534f\u4f5c\u3002", "result": "\u5728\u4e09\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMACF\u6846\u67b6\u76f8\u6bd4\u5f3a\u5927\u7684\u667a\u80fd\u4f53\u63a8\u8350\u57fa\u7ebf\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "MACF\u6846\u67b6\u901a\u8fc7\u5c06\u4f20\u7edf\u534f\u540c\u8fc7\u6ee4\u4e0eLLM\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u667a\u80fd\u4f53\u63a8\u8350\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2511.18563", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18563", "abs": "https://arxiv.org/abs/2511.18563", "authors": ["Cem Bilaloglu", "Tobias L\u00f6w", "Sylvain Calinon"], "title": "Object-centric Task Representation and Transfer using Diffused Orientation Fields", "comment": null, "summary": "Curved objects pose a fundamental challenge for skill transfer in robotics: unlike planar surfaces, they do not admit a global reference frame. As a result, task-relevant directions such as \"toward\" or \"along\" the surface vary with position and geometry, making object-centric tasks difficult to transfer across shapes. To address this, we introduce an approach using Diffused Orientation Fields (DOF), a smooth representation of local reference frames, for transfer learning of tasks across curved objects. By expressing manipulation tasks in these smoothly varying local frames, we reduce the problem of transferring tasks across curved objects to establishing sparse keypoint correspondences. DOF is computed online from raw point cloud data using diffusion processes governed by partial differential equations, conditioned on keypoints. We evaluate DOF under geometric, topological, and localization perturbations, and demonstrate successful transfer of tasks requiring continuous physical interaction such as inspection, slicing, and peeling across varied objects. We provide our open-source codes at our website https://github.com/idiap/diffused_fields_robotics", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u6269\u6563\u65b9\u5411\u573a(DOF)\u6765\u89e3\u51b3\u66f2\u9762\u7269\u4f53\u4e0a\u6280\u80fd\u8fc1\u79fb\u7684\u6311\u6218\uff0c\u901a\u8fc7\u5c40\u90e8\u53c2\u8003\u5e27\u8868\u793a\u5c06\u4efb\u52a1\u8fc1\u79fb\u95ee\u9898\u7b80\u5316\u4e3a\u7a00\u758f\u5173\u952e\u70b9\u5bf9\u5e94\u95ee\u9898\u3002", "motivation": "\u66f2\u9762\u7269\u4f53\u7f3a\u4e4f\u5168\u5c40\u53c2\u8003\u6846\u67b6\uff0c\u5bfc\u81f4\u4efb\u52a1\u76f8\u5173\u65b9\u5411\u968f\u4f4d\u7f6e\u548c\u51e0\u4f55\u5f62\u72b6\u53d8\u5316\uff0c\u4f7f\u5f97\u9762\u5411\u7269\u4f53\u7684\u4efb\u52a1\u96be\u4ee5\u5728\u4e0d\u540c\u5f62\u72b6\u95f4\u8fc1\u79fb\u3002", "method": "\u4f7f\u7528\u6269\u6563\u65b9\u5411\u573a(DOF)\u4f5c\u4e3a\u5c40\u90e8\u53c2\u8003\u5e27\u7684\u5e73\u6ed1\u8868\u793a\uff0c\u901a\u8fc7\u53d7\u504f\u5fae\u5206\u65b9\u7a0b\u63a7\u5236\u7684\u6269\u6563\u8fc7\u7a0b\u4ece\u539f\u59cb\u70b9\u4e91\u6570\u636e\u5728\u7ebf\u8ba1\u7b97\uff0c\u5e76\u57fa\u4e8e\u5173\u952e\u70b9\u8fdb\u884c\u6761\u4ef6\u5316\u3002", "result": "\u5728\u51e0\u4f55\u3001\u62d3\u6251\u548c\u5b9a\u4f4d\u6270\u52a8\u4e0b\u8bc4\u4f30DOF\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9700\u8981\u8fde\u7eed\u7269\u7406\u4ea4\u4e92\u7684\u4efb\u52a1\uff08\u5982\u68c0\u67e5\u3001\u5207\u5272\u548c\u5265\u76ae\uff09\u5728\u4e0d\u540c\u7269\u4f53\u95f4\u7684\u8fc1\u79fb\u3002", "conclusion": "DOF\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u66f2\u9762\u7269\u4f53\u4e0a\u6280\u80fd\u8fc1\u79fb\u7684\u6839\u672c\u6311\u6218\uff0c\u5c06\u590d\u6742\u4efb\u52a1\u8fc1\u79fb\u7b80\u5316\u4e3a\u5efa\u7acb\u7a00\u758f\u5173\u952e\u70b9\u5bf9\u5e94\u5173\u7cfb\u3002"}}
{"id": "2511.18423", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18423", "abs": "https://arxiv.org/abs/2511.18423", "authors": ["B. Y. Yan", "Chaofan Li", "Hongjin Qian", "Shuqi Lu", "Zheng Liu"], "title": "General Agentic Memory Via Deep Research", "comment": null, "summary": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \\textbf{general agentic memory (GAM)}. GAM follows the principle of \"\\textbf{just-in time (JIT) compilation}\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \\textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \\textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGAM\u7684\u65b0\u578b\u667a\u80fd\u4f53\u8bb0\u5fc6\u6846\u67b6\uff0c\u91c7\u7528\u5373\u65f6\u7f16\u8bd1\u539f\u5219\uff0c\u5728\u8fd0\u884c\u65f6\u4e3a\u5ba2\u6237\u7aef\u521b\u5efa\u4f18\u5316\u4e0a\u4e0b\u6587\uff0c\u540c\u65f6\u79bb\u7ebf\u9636\u6bb5\u4ec5\u4fdd\u7559\u7b80\u5355\u4f46\u6709\u7528\u7684\u8bb0\u5fc6\u3002", "motivation": "\u89e3\u51b3\u9759\u6001\u8bb0\u5fc6\u7cfb\u7edf\u5728\u9884\u5148\u521b\u5efa\u53ef\u7528\u8bb0\u5fc6\u65f6\u4e0d\u53ef\u907f\u514d\u7684\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002", "method": "\u91c7\u7528\u53cc\u7ec4\u4ef6\u8bbe\u8ba1\uff1a1) Memorizer\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8bb0\u5fc6\u7a81\u51fa\u5173\u952e\u5386\u53f2\u4fe1\u606f\uff0c\u540c\u65f6\u5728\u901a\u7528\u9875\u9762\u5b58\u50a8\u4e2d\u7ef4\u62a4\u5b8c\u6574\u5386\u53f2\u4fe1\u606f\uff1b2) Researcher\u6839\u636e\u9884\u6784\u5efa\u7684\u8bb0\u5fc6\u4ece\u9875\u9762\u5b58\u50a8\u4e2d\u68c0\u7d22\u548c\u6574\u5408\u6709\u7528\u4fe1\u606f\u3002", "result": "\u5728\u591a\u79cd\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4efb\u52a1\u5b8c\u6210\u573a\u666f\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u8bb0\u5fc6\u7cfb\u7edf\u5b9e\u73b0\u4e86\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "GAM\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u4f53\u80fd\u529b\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4fc3\u8fdb\u7aef\u5230\u7aef\u6027\u80fd\u4f18\u5316\u3002"}}
{"id": "2511.18604", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2511.18604", "abs": "https://arxiv.org/abs/2511.18604", "authors": ["Hannah Lee", "James D. Motes", "Marco Morales", "Nancy M. Amato"], "title": "An Analysis of Constraint-Based Multi-Agent Pathfinding Algorithms", "comment": null, "summary": "This study informs the design of future multi-agent pathfinding (MAPF) and multi-robot motion planning (MRMP) algorithms by guiding choices based on constraint classification for constraint-based search algorithms. We categorize constraints as conservative or aggressive and provide insights into their search behavior, focusing specifically on vanilla Conflict-Based Search (CBS) and Conflict-Based Search with Priorities (CBSw/P). Under a hybrid grid-roadmap representation with varying resolution, we observe that aggressive (priority constraint) formulations tend to solve more instances as agent count or resolution increases, whereas conservative (motion constraint) formulations yield stronger solution quality when both succeed. Findings are synthesized in a decision flowchart, aiding users in selecting suitable constraints. Recommendations extend to Multi-Robot Motion Planning (MRMP), emphasizing the importance of considering topological features alongside problem, solution, and representation features. A comprehensive exploration of the study, including raw data and map performance, is available in our public GitHub Repository: https://GitHub.com/hannahjmlee/constraint-mapf-analysis", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u57fa\u4e8e\u7ea6\u675f\u5206\u7c7b\u6307\u5bfc\u7ea6\u675f\u641c\u7d22\u7b97\u6cd5\u7684\u9009\u62e9\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212(MAPF)\u548c\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212(MRMP)\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\u3002\u7814\u7a76\u5c06\u7ea6\u675f\u5206\u4e3a\u4fdd\u5b88\u578b\u548c\u6fc0\u8fdb\u578b\uff0c\u5206\u6790\u5176\u641c\u7d22\u884c\u4e3a\uff0c\u7279\u522b\u5173\u6ce8CBS\u548cCBSw/P\u7b97\u6cd5\u3002", "motivation": "\u4e3a\u672a\u6765\u7684MAPF\u548cMRMP\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u6307\u5bfc\uff0c\u5e2e\u52a9\u7528\u6237\u57fa\u4e8e\u7ea6\u675f\u5206\u7c7b\u9009\u62e9\u5408\u9002\u7684\u7b97\u6cd5\uff0c\u8003\u8651\u62d3\u6251\u7279\u5f81\u4e0e\u95ee\u9898\u3001\u89e3\u51b3\u65b9\u6848\u548c\u8868\u793a\u7279\u5f81\u7684\u7ed3\u5408\u3002", "method": "\u4f7f\u7528\u6df7\u5408\u7f51\u683c-\u8def\u7ebf\u56fe\u8868\u793a\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u5206\u8fa8\u7387\u4e0b\u6bd4\u8f83\u4fdd\u5b88\u578b(\u8fd0\u52a8\u7ea6\u675f)\u548c\u6fc0\u8fdb\u578b(\u4f18\u5148\u7ea7\u7ea6\u675f)\u7ea6\u675f\u7684\u641c\u7d22\u884c\u4e3a\uff0c\u91cd\u70b9\u5173\u6ce8CBS\u548cCBSw/P\u7b97\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u6fc0\u8fdb\u578b(\u4f18\u5148\u7ea7\u7ea6\u675f)\u5728\u667a\u80fd\u4f53\u6570\u91cf\u6216\u5206\u8fa8\u7387\u589e\u52a0\u65f6\u80fd\u89e3\u51b3\u66f4\u591a\u5b9e\u4f8b\uff0c\u800c\u4fdd\u5b88\u578b(\u8fd0\u52a8\u7ea6\u675f)\u5728\u4e24\u8005\u90fd\u6210\u529f\u65f6\u80fd\u63d0\u4f9b\u66f4\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u51b3\u7b56\u6d41\u7a0b\u56fe\u5e2e\u52a9\u7528\u6237\u9009\u62e9\u5408\u9002\u7684\u7ea6\u675f\uff0c\u5e76\u5f3a\u8c03\u5728MRMP\u4e2d\u8003\u8651\u62d3\u6251\u7279\u5f81\u7684\u91cd\u8981\u6027\u3002\u6240\u6709\u539f\u59cb\u6570\u636e\u548c\u5730\u56fe\u6027\u80fd\u6570\u636e\u53ef\u5728GitHub\u4ed3\u5e93\u4e2d\u83b7\u53d6\u3002"}}
{"id": "2511.18491", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18491", "abs": "https://arxiv.org/abs/2511.18491", "authors": ["Jos\u00e9 Pombal", "Maya D'Eon", "Nuno M. Guerreiro", "Pedro Henrique Martins", "Ant\u00f3nio Farinhas", "Ricardo Rei"], "title": "MindEval: Benchmarking Language Models on Multi-turn Mental Health Support", "comment": null, "summary": "Demand for mental health support through AI chatbots is surging, though current systems present several limitations, like sycophancy or overvalidation, and reinforcement of maladaptive beliefs. A core obstacle to the creation of better systems is the scarcity of benchmarks that capture the complexity of real therapeutic interactions. Most existing benchmarks either only test clinical knowledge through multiple-choice questions or assess single responses in isolation. To bridge this gap, we present MindEval, a framework designed in collaboration with Ph.D-level Licensed Clinical Psychologists for automatically evaluating language models in realistic, multi-turn mental health therapy conversations. Through patient simulation and automatic evaluation with LLMs, our framework balances resistance to gaming with reproducibility via its fully automated, model-agnostic design. We begin by quantitatively validating the realism of our simulated patients against human-generated text and by demonstrating strong correlations between automatic and human expert judgments. Then, we evaluate 12 state-of-the-art LLMs and show that all models struggle, scoring below 4 out of 6, on average, with particular weaknesses in problematic AI-specific patterns of communication. Notably, reasoning capabilities and model scale do not guarantee better performance, and systems deteriorate with longer interactions or when supporting patients with severe symptoms. We release all code, prompts, and human evaluation data.", "AI": {"tldr": "\u63d0\u51fa\u4e86MindEval\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e0e\u4e34\u5e8a\u5fc3\u7406\u5b66\u5bb6\u5408\u4f5c\u5f00\u53d1\u7684\u81ea\u52a8\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5065\u5eb7\u6cbb\u7597\u5bf9\u8bdd\u4e2d\u8868\u73b0\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u60a3\u8005\u6a21\u62df\u548c\u81ea\u52a8\u8bc4\u4f30\u6765\u6d4b\u8bd5AI\u804a\u5929\u673a\u5668\u4eba\u7684\u6cbb\u7597\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5fc3\u7406\u5065\u5eb7AI\u804a\u5929\u673a\u5668\u4eba\u5b58\u5728\u8bf8\u591a\u5c40\u9650\uff08\u5982\u8c04\u5a9a\u3001\u8fc7\u5ea6\u9a8c\u8bc1\u3001\u5f3a\u5316\u4e0d\u826f\u4fe1\u5ff5\uff09\uff0c\u4e14\u7f3a\u4e4f\u80fd\u6355\u6349\u771f\u5b9e\u6cbb\u7597\u5bf9\u8bdd\u590d\u6742\u6027\u7684\u8bc4\u4f30\u57fa\u51c6\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u901a\u8fc7\u9009\u62e9\u9898\u6d4b\u8bd5\u4e34\u5e8a\u77e5\u8bc6\u6216\u5b64\u7acb\u8bc4\u4f30\u5355\u6b21\u56de\u590d\u3002", "method": "\u4e0e\u535a\u58eb\u7ea7\u6301\u8bc1\u4e34\u5e8a\u5fc3\u7406\u5b66\u5bb6\u5408\u4f5c\u8bbe\u8ba1MindEval\u6846\u67b6\uff0c\u901a\u8fc7\u60a3\u8005\u6a21\u62df\u548c\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u8bc4\u4f30\uff0c\u5728\u771f\u5b9e\u7684\u591a\u8f6e\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\u4e2d\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u3002\u9a8c\u8bc1\u4e86\u6a21\u62df\u60a3\u8005\u7684\u771f\u5b9e\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u81ea\u52a8\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5224\u65ad\u7684\u5f3a\u76f8\u5173\u6027\u3002", "result": "\u8bc4\u4f30\u4e8612\u4e2a\u6700\u5148\u8fdb\u7684LLM\uff0c\u6240\u6709\u6a21\u578b\u5e73\u5747\u5f97\u5206\u4f4e\u4e8e4\u5206\uff08\u6ee1\u52066\u5206\uff09\uff0c\u5728AI\u7279\u6709\u7684\u6c9f\u901a\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u5c24\u5176\u8584\u5f31\u3002\u63a8\u7406\u80fd\u529b\u548c\u6a21\u578b\u89c4\u6a21\u4e0d\u80fd\u4fdd\u8bc1\u66f4\u597d\u8868\u73b0\uff0c\u7cfb\u7edf\u5728\u8f83\u957f\u4ea4\u4e92\u6216\u652f\u6301\u4e25\u91cd\u75c7\u72b6\u60a3\u8005\u65f6\u8868\u73b0\u6076\u5316\u3002", "conclusion": "\u5f53\u524dAI\u7cfb\u7edf\u5728\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u65b9\u9762\u4ecd\u6709\u663e\u8457\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u63a8\u52a8\u6539\u8fdb\u3002MindEval\u4e3a\u81ea\u52a8\u8bc4\u4f30\u6cbb\u7597\u5bf9\u8bdd\u4e2d\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u53d1\u5e03\u4e86\u6240\u6709\u4ee3\u7801\u3001\u63d0\u793a\u548c\u4eba\u5de5\u8bc4\u4f30\u6570\u636e\u3002"}}
{"id": "2511.18606", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18606", "abs": "https://arxiv.org/abs/2511.18606", "authors": ["Kensuke Nakamura", "Arun L. Bishop", "Steven Man", "Aaron M. Johnson", "Zachary Manchester", "Andrea Bajcsy"], "title": "How to Train Your Latent Control Barrier Function: Smooth Safety Filtering Under Hard-to-Model Constraints", "comment": "3 figures, 10 tables, 22 pages", "summary": "Latent safety filters extend Hamilton-Jacobi (HJ) reachability to operate on latent state representations and dynamics learned directly from high-dimensional observations, enabling safe visuomotor control under hard-to-model constraints. However, existing methods implement \"least-restrictive\" filtering that discretely switch between nominal and safety policies, potentially undermining the task performance that makes modern visuomotor policies valuable. While reachability value functions can, in principle, be adapted to be control barrier functions (CBFs) for smooth optimization-based filtering, we theoretically and empirically show that current latent-space learning methods produce fundamentally incompatible value functions. We identify two sources of incompatibility: First, in HJ reachability, failures are encoded via a \"margin function\" in latent space, whose sign indicates whether or not a latent is in the constraint set. However, representing the margin function as a classifier yields saturated value functions that exhibit discontinuous jumps. We prove that the value function's Lipschitz constant scales linearly with the margin function's Lipschitz constant, revealing that smooth CBFs require smooth margins. Second, reinforcement learning (RL) approximations trained solely on safety policy data yield inaccurate value estimates for nominal policy actions, precisely where CBF filtering needs them. We propose the LatentCBF, which addresses both challenges through gradient penalties that lead to smooth margin functions without additional labeling, and a value-training procedure that mixes data from both nominal and safety policy distributions. Experiments on simulated benchmarks and hardware with a vision-based manipulation policy demonstrate that LatentCBF enables smooth safety filtering while doubling the task-completion rate over prior switching methods.", "AI": {"tldr": "LatentCBF\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a7\u5236\u969c\u788d\u51fd\u6570\u7684\u5e73\u6ed1\u5b89\u5168\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6f5c\u5728\u5b89\u5168\u8fc7\u6ee4\u5668\u5728\u540d\u4e49\u7b56\u7565\u548c\u5b89\u5168\u7b56\u7565\u4e4b\u95f4\u79bb\u6563\u5207\u6362\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u68af\u5ea6\u60e9\u7f5a\u548c\u6df7\u5408\u6570\u636e\u8bad\u7ec3\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "motivation": "\u73b0\u6709\u6f5c\u5728\u5b89\u5168\u8fc7\u6ee4\u5668\u91c7\u7528\"\u6700\u5c0f\u9650\u5236\"\u8fc7\u6ee4\uff0c\u5728\u540d\u4e49\u7b56\u7565\u548c\u5b89\u5168\u7b56\u7565\u4e4b\u95f4\u8fdb\u884c\u79bb\u6563\u5207\u6362\uff0c\u8fd9\u4f1a\u635f\u5bb3\u73b0\u4ee3\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u4efb\u52a1\u6027\u80fd\u3002\u867d\u7136\u53ef\u8fbe\u6027\u503c\u51fd\u6570\u7406\u8bba\u4e0a\u53ef\u4ee5\u9002\u914d\u4e3a\u63a7\u5236\u969c\u788d\u51fd\u6570\u8fdb\u884c\u5e73\u6ed1\u8fc7\u6ee4\uff0c\u4f46\u5f53\u524d\u6f5c\u5728\u7a7a\u95f4\u5b66\u4e60\u65b9\u6cd5\u4ea7\u751f\u7684\u503c\u51fd\u6570\u5b58\u5728\u4e0d\u517c\u5bb9\u95ee\u9898\u3002", "method": "\u63d0\u51faLatentCBF\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u68af\u5ea6\u60e9\u7f5a\u83b7\u5f97\u5e73\u6ed1\u7684\u8fb9\u754c\u51fd\u6570\uff0c\u65e0\u9700\u989d\u5916\u6807\u6ce8\uff1b2\uff09\u91c7\u7528\u6df7\u5408\u6570\u636e\u8bad\u7ec3\u7a0b\u5e8f\uff0c\u540c\u65f6\u4f7f\u7528\u540d\u4e49\u7b56\u7565\u548c\u5b89\u5168\u7b56\u7565\u5206\u5e03\u7684\u6570\u636e\u8fdb\u884c\u503c\u51fd\u6570\u8bad\u7ec3\u3002", "result": "\u5728\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0cLatentCBF\u5b9e\u73b0\u4e86\u5e73\u6ed1\u7684\u5b89\u5168\u8fc7\u6ee4\uff0c\u76f8\u6bd4\u4e4b\u524d\u7684\u5207\u6362\u65b9\u6cd5\u5c06\u4efb\u52a1\u5b8c\u6210\u7387\u63d0\u9ad8\u4e86\u4e00\u500d\u3002", "conclusion": "LatentCBF\u6210\u529f\u89e3\u51b3\u4e86\u6f5c\u5728\u5b89\u5168\u8fc7\u6ee4\u5668\u4e2d\u7684\u503c\u51fd\u6570\u4e0d\u517c\u5bb9\u95ee\u9898\uff0c\u901a\u8fc7\u5e73\u6ed1\u7684\u8fb9\u754c\u51fd\u6570\u548c\u6df7\u5408\u6570\u636e\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u65e2\u5b89\u5168\u53c8\u9ad8\u6548\u7684\u63a7\u5236\u6027\u80fd\u3002"}}
{"id": "2511.18499", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18499", "abs": "https://arxiv.org/abs/2511.18499", "authors": ["Tyler Shoemaker"], "title": "For Those Who May Find Themselves on the Red Team", "comment": null, "summary": "This position paper argues that literary scholars must engage with large language model (LLM) interpretability research. While doing so will involve ideological struggle, if not out-right complicity, the necessity of this engagement is clear: the abiding instrumentality of current approaches to interpretability cannot be the only standard by which we measure interpretation with LLMs. One site at which this struggle could take place, I suggest, is the red team.", "AI": {"tldr": "\u6587\u5b66\u5b66\u8005\u5fc5\u987b\u53c2\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7814\u7a76\uff0c\u5c3d\u7ba1\u8fd9\u4f1a\u6d89\u53ca\u610f\u8bc6\u5f62\u6001\u6597\u4e89\u751a\u81f3\u59a5\u534f\uff0c\u4f46\u5f53\u524d\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u7684\u5de5\u5177\u6027\u4e0d\u5e94\u6210\u4e3a\u8861\u91cfLLM\u89e3\u91ca\u7684\u552f\u4e00\u6807\u51c6\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u8fc7\u4e8e\u6ce8\u91cd\u5de5\u5177\u6027\uff0c\u6587\u5b66\u5b66\u8005\u9700\u8981\u53c2\u4e0e\u5176\u4e2d\u4ee5\u5f15\u5165\u66f4\u4e30\u5bcc\u7684\u89e3\u91ca\u6807\u51c6\u3002", "method": "\u63d0\u51fa\u5c06\u7ea2\u961f\u4f5c\u4e3a\u6587\u5b66\u5b66\u8005\u53c2\u4e0eLLM\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7684\u5b9e\u8df5\u573a\u6240\u3002", "result": "\u8bba\u8bc1\u4e86\u6587\u5b66\u5b66\u8005\u53c2\u4e0eLLM\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u7684\u5fc5\u8981\u6027\u548c\u53ef\u884c\u6027\u3002", "conclusion": "\u6587\u5b66\u5b66\u8005\u5e94\u5f53\u79ef\u6781\u4ecb\u5165LLM\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u9886\u57df\uff0c\u901a\u8fc7\u7ea2\u961f\u7b49\u5b9e\u8df5\u573a\u6240\u5c55\u5f00\u610f\u8bc6\u5f62\u6001\u6597\u4e89\uff0c\u63a8\u52a8\u89e3\u91ca\u6807\u51c6\u7684\u591a\u5143\u5316\u3002"}}
{"id": "2511.18617", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18617", "abs": "https://arxiv.org/abs/2511.18617", "authors": ["Litian Gong", "Fatemeh Bahrani", "Yutai Zhou", "Amin Banayeeanzade", "Jiachen Li", "Erdem Biyik"], "title": "AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations", "comment": "8 pages, 6 figures. Code and datasets available at http://autofocus-il.github.io/", "summary": "AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data. Code, datasets, and trained policy videos are available at https://AutoFocus-IL.github.io/.", "AI": {"tldr": "AutoFocus-IL\u662f\u4e00\u79cd\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u65f6\u95f4\u663e\u8457\u6027\u56fe\u6765\u6539\u8fdb\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u6602\u8d35\u7684\u4eba\u5de5\u76d1\u7763\u5373\u53ef\u8ba9\u7b56\u7565\u5173\u6ce8\u4efb\u52a1\u76f8\u5173\u7279\u5f81\uff0c\u5728CARLA\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u663e\u8457\u6027\u6b63\u5219\u5316\u7684\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u76d1\u7763\uff08\u5982\u4eba\u7c7b\u6ce8\u89c6\u6570\u636e\u6216\u624b\u52a8\u6807\u6ce8\uff09\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u8bc6\u522b\u548c\u8ddf\u8e2a\u6f14\u793a\u4e2d\u7684\u5173\u952e\u7269\u4f53\uff0c\u751f\u6210\u65f6\u95f4\u663e\u8457\u6027\u56fe\u6765\u7a81\u51fa\u56e0\u679c\u89c6\u89c9\u4fe1\u53f7\u5e76\u6291\u5236\u5e72\u6270\u7269\uff0c\u7136\u540e\u7528\u8fd9\u4e9b\u56fe\u6765\u6b63\u5219\u5316\u884c\u4e3a\u514b\u9686\u7b56\u7565\u3002", "result": "\u5728CARLA\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cAutoFocus-IL\u4e0d\u4ec5\u4f18\u4e8e\u6807\u51c6\u884c\u4e3a\u514b\u9686\uff0c\u8fd8\u8d85\u8d8a\u4e86\u9700\u8981\u4eba\u7c7b\u76d1\u7763\u7684\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "AutoFocus-IL\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u6602\u8d35\u4eba\u5de5\u76d1\u7763\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u6570\u636e\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2511.18557", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18557", "abs": "https://arxiv.org/abs/2511.18557", "authors": ["Yacouba Diarra", "Nouhoum Souleymane Coulibaly", "Panga Azazia Kamat\u00e9", "Madani Amadou Tall", "Emmanuel \u00c9lis\u00e9 Kon\u00e9", "Aymane Demb\u00e9l\u00e9", "Michael Leventhal"], "title": "Dealing with the Hard Facts of Low-Resource African NLP", "comment": "10 pages, 4 figures", "summary": "Creating speech datasets, models, and evaluation frameworks for low-resource languages remains challenging given the lack of a broad base of pertinent experience to draw from. This paper reports on the field collection of 612 hours of spontaneous speech in Bambara, a low-resource West African language; the semi-automated annotation of that dataset with transcriptions; the creation of several monolingual ultra-compact and small models using the dataset; and the automatic and human evaluation of their output. We offer practical suggestions for data collection protocols, annotation, and model design, as well as evidence for the importance of performing human evaluation. In addition to the main dataset, multiple evaluation datasets, models, and code are made publicly available.", "AI": {"tldr": "\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u73ed\u5df4\u62c9\u8bed\u6536\u96c6\u4e86612\u5c0f\u65f6\u81ea\u53d1\u8bed\u97f3\u6570\u636e\uff0c\u521b\u5efa\u4e86\u534a\u81ea\u52a8\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u591a\u4e2a\u5355\u8bed\u8d85\u7d27\u51d1\u548c\u5c0f\u578b\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u4e86\u81ea\u52a8\u548c\u4eba\u5de5\u8bc4\u4f30\u3002", "motivation": "\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u521b\u5efa\u8bed\u97f3\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u8bc4\u4f30\u6846\u67b6\u5177\u6709\u6311\u6218\u6027\uff0c\u7f3a\u4e4f\u76f8\u5173\u7ecf\u9a8c\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5b9e\u5730\u6536\u96c6\u73ed\u5df4\u62c9\u8bed\u81ea\u53d1\u8bed\u97f3\uff0c\u8fdb\u884c\u534a\u81ea\u52a8\u8f6c\u5f55\u6807\u6ce8\uff0c\u521b\u5efa\u5355\u8bed\u8d85\u7d27\u51d1\u548c\u5c0f\u578b\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u81ea\u52a8\u548c\u4eba\u5de5\u8bc4\u4f30\u3002", "result": "\u6536\u96c6\u4e86612\u5c0f\u65f6\u8bed\u97f3\u6570\u636e\uff0c\u521b\u5efa\u4e86\u591a\u4e2a\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u6570\u636e\u6536\u96c6\u534f\u8bae\u3001\u6807\u6ce8\u548c\u6a21\u578b\u8bbe\u8ba1\u7684\u5b9e\u7528\u5efa\u8bae\uff0c\u5e76\u8bc1\u660e\u4e86\u4eba\u5de5\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u9664\u4e86\u4e3b\u8981\u6570\u636e\u96c6\u5916\uff0c\u8fd8\u516c\u5f00\u4e86\u591a\u4e2a\u8bc4\u4f30\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u4ee3\u7801\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8bed\u97f3\u5904\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u8d44\u6e90\u548c\u65b9\u6cd5\u3002"}}
{"id": "2511.18683", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18683", "abs": "https://arxiv.org/abs/2511.18683", "authors": ["Yinan Dong", "Ziyu Xu", "Tsimafei Lazouski", "Sangli Teng", "Maani Ghaffari"], "title": "Online Learning-Enhanced Lie Algebraic MPC for Robust Trajectory Tracking of Autonomous Surface Vehicles", "comment": null, "summary": "Autonomous surface vehicles (ASVs) are easily influenced by environmental disturbances such as wind and waves, making accurate trajectory tracking a persistent challenge in dynamic marine conditions. In this paper, we propose an efficient controller for trajectory tracking of marine vehicles under unknown disturbances by combining a convex error-state MPC on the Lie group with an online learning module to compensate for these disturbances in real time. This design enables adaptive and robust control while maintaining computational efficiency. Extensive evaluations in numerical simulations, the Virtual RobotX (VRX) simulator, and real-world field experiments demonstrate that our method achieves superior tracking accuracy under various disturbance scenarios compared with existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u674e\u7fa4\u4e0a\u51f8\u8bef\u5dee\u72b6\u6001MPC\u548c\u5728\u7ebf\u5b66\u4e60\u6a21\u5757\u7684\u9ad8\u6548\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u5728\u672a\u77e5\u6270\u52a8\u4e0b\u5b9e\u73b0\u6d77\u6d0b\u8f66\u8f86\u8f68\u8ff9\u8ddf\u8e2a", "motivation": "\u81ea\u4e3b\u6c34\u9762\u8f66\u8f86(ASV)\u5bb9\u6613\u53d7\u5230\u98ce\u6d6a\u7b49\u73af\u5883\u6270\u52a8\u5f71\u54cd\uff0c\u5728\u52a8\u6001\u6d77\u6d0b\u6761\u4ef6\u4e0b\u5b9e\u73b0\u7cbe\u786e\u8f68\u8ff9\u8ddf\u8e2a\u662f\u4e00\u4e2a\u6301\u7eed\u6311\u6218", "method": "\u5c06\u674e\u7fa4\u4e0a\u7684\u51f8\u8bef\u5dee\u72b6\u6001\u6a21\u578b\u9884\u6d4b\u63a7\u5236(MPC)\u4e0e\u5728\u7ebf\u5b66\u4e60\u6a21\u5757\u76f8\u7ed3\u5408\uff0c\u5b9e\u65f6\u8865\u507f\u672a\u77e5\u6270\u52a8", "result": "\u5728\u6570\u503c\u6a21\u62df\u3001VRX\u4eff\u771f\u5668\u548c\u771f\u5b9e\u4e16\u754c\u73b0\u573a\u5b9e\u9a8c\u4e2d\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6270\u52a8\u573a\u666f\u4e0b\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u4e86\u66f4\u4f18\u8d8a\u7684\u8ddf\u8e2a\u7cbe\u5ea6", "conclusion": "\u8be5\u8bbe\u8ba1\u80fd\u591f\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u81ea\u9002\u5e94\u548c\u9c81\u68d2\u63a7\u5236"}}
{"id": "2511.18597", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18597", "abs": "https://arxiv.org/abs/2511.18597", "authors": ["H. M. Shadman Tabib", "Jaber Ahmed Deedar"], "title": "Toward Trustworthy Difficulty Assessments: Large Language Models as Judges in Programming and Synthetic Tasks", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in natural language and code generation, and are increasingly deployed as automatic judges of model outputs and learning activities. Yet, their behavior on structured tasks such as predicting the difficulty of competitive programming problems remains under-explored. We conduct a systematic comparison of GPT-4o, used purely as a natural-language difficulty assessor, against an interpretable Light-GBM ensemble trained on explicit numeric and textual features. On a dataset of 1,825 LeetCode problems labeled Easy, Medium, or Hard, LightGBM attains 86% accuracy, whereas GPT-4o reaches only 37.75%. Detailed analyses, including confusion matrices and SHAP-based interpretability, show that numeric constraints -- such as input size limits and acceptance rates -- play a crucial role in separating Hard problems from easier ones. By contrast, GPT-4o often overlooks these cues and exhibits a strong bias toward simpler categories. We further probe GPT-4o through a synthetic Hard-problem generation protocol. Surprisingly, GPT-4o labels almost all of its own synthetic Hard problems as Medium, contradicting its tendency to downgrade real Hard problems to Easy. Our findings connect to recent work on LLMs-as-judges and automatic difficulty estimation in programming and education, and highlight concrete failure modes that must be addressed before LLM-based judges can be considered trustworthy in competitive programming, educational platforms, or reinforcement-learning pipelines.", "AI": {"tldr": "GPT-4o\u4f5c\u4e3a\u7f16\u7a0b\u95ee\u9898\u96be\u5ea6\u8bc4\u4f30\u5668\u7684\u8868\u73b0\u4e0d\u5982\u57fa\u4e8e\u7279\u5f81\u7684LightGBM\u6a21\u578b\uff0c\u51c6\u786e\u7387\u4ec537.75% vs 86%\uff0c\u4e14\u5b58\u5728\u5bf9\u7b80\u5355\u7c7b\u522b\u7684\u5f3a\u70c8\u504f\u89c1\u3002", "motivation": "\u63a2\u7d22LLM\u5728\u7ed3\u6784\u5316\u4efb\u52a1\uff08\u5982\u9884\u6d4b\u7f16\u7a0b\u95ee\u9898\u96be\u5ea6\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u7ade\u4e89\u6027\u7f16\u7a0b\u548c\u6559\u80b2\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u7cfb\u7edf\u6bd4\u8f83GPT-4o\uff08\u7eaf\u81ea\u7136\u8bed\u8a00\u8bc4\u4f30\u5668\uff09\u4e0e\u57fa\u4e8e\u6570\u503c\u548c\u6587\u672c\u7279\u5f81\u7684LightGBM\u96c6\u6210\u6a21\u578b\uff0c\u57281825\u4e2aLeetCode\u95ee\u9898\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528SHAP\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\u3002", "result": "LightGBM\u8fbe\u523086%\u51c6\u786e\u7387\uff0cGPT-4o\u4ec537.75%\uff1bGPT-4o\u5ffd\u89c6\u6570\u503c\u7ea6\u675f\uff08\u5982\u8f93\u5165\u5927\u5c0f\u9650\u5236\u548c\u63a5\u53d7\u7387\uff09\uff0c\u5bf9\u7b80\u5355\u7c7b\u522b\u6709\u5f3a\u70c8\u504f\u89c1\uff1b\u5728\u5408\u6210\u96be\u9898\u751f\u6210\u5b9e\u9a8c\u4e2d\uff0cGPT-4o\u5c06\u81ea\u5df1\u751f\u6210\u7684\u96be\u9898\u5927\u591a\u6807\u8bb0\u4e3a\u4e2d\u7b49\u96be\u5ea6\u3002", "conclusion": "\u5728\u7ade\u4e89\u6027\u7f16\u7a0b\u3001\u6559\u80b2\u5e73\u53f0\u6216\u5f3a\u5316\u5b66\u4e60\u7ba1\u9053\u4e2d\uff0c\u57fa\u4e8eLLM\u7684\u8bc4\u5224\u5668\u5728\u89e3\u51b3\u5177\u4f53\u5931\u8d25\u6a21\u5f0f\u4e4b\u524d\u5c1a\u4e0d\u53ef\u4fe1\u3002"}}
{"id": "2511.18694", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18694", "abs": "https://arxiv.org/abs/2511.18694", "authors": ["Shuo Wen", "Edwin Meriaux", "Mariana Sosa Guzm\u00e1n", "Zhizun Wang", "Junming Shi", "Gregory Dudek"], "title": "Stable Multi-Drone GNSS Tracking System for Marine Robots", "comment": null, "summary": "Accurate localization is essential for marine robotics, yet Global Navigation Satellite System (GNSS) signals are unreliable or unavailable even at a very short distance below the water surface. Traditional alternatives, such as inertial navigation, Doppler Velocity Loggers (DVL), SLAM, and acoustic methods, suffer from error accumulation, high computational demands, or infrastructure dependence. In this work, we present a scalable multi-drone GNSS-based tracking system for surface and near-surface marine robots. Our approach combines efficient visual detection, lightweight multi-object tracking, GNSS-based triangulation, and a confidence-weighted Extended Kalman Filter (EKF) to provide stable GNSS estimation in real time. We further introduce a cross-drone tracking ID alignment algorithm that enforces global consistency across views, enabling robust multi-robot tracking with redundant aerial coverage. We validate our system in diversified complex settings to show the scalability and robustness of the proposed algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u65e0\u4eba\u673aGNSS\u7684\u6d77\u6d0b\u673a\u5668\u4eba\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u901a\u8fc7\u89c6\u89c9\u68c0\u6d4b\u3001\u591a\u76ee\u6807\u8ddf\u8e2a\u548c\u4e09\u89d2\u5b9a\u4f4d\uff0c\u4e3a\u6c34\u9762\u548c\u8fd1\u6c34\u9762\u673a\u5668\u4eba\u63d0\u4f9b\u5b9e\u65f6\u7a33\u5b9a\u7684GNSS\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u6c34\u4e0bGNSS\u4fe1\u53f7\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u8bef\u5dee\u7d2f\u79ef\u3001\u8ba1\u7b97\u91cf\u5927\u6216\u4f9d\u8d56\u57fa\u7840\u8bbe\u65bd\u7684\u5c40\u9650\u6027\u3002", "method": "\u7ed3\u5408\u9ad8\u6548\u89c6\u89c9\u68c0\u6d4b\u3001\u8f7b\u91cf\u7ea7\u591a\u76ee\u6807\u8ddf\u8e2a\u3001GNSS\u4e09\u89d2\u5b9a\u4f4d\u548c\u7f6e\u4fe1\u5ea6\u52a0\u6743\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\uff0c\u5e76\u5f15\u5165\u8de8\u65e0\u4eba\u673a\u8ddf\u8e2aID\u5bf9\u9f50\u7b97\u6cd5\u786e\u4fdd\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u6837\u5316\u590d\u6742\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u4e3a\u6d77\u6d0b\u673a\u5668\u4eba\u63d0\u4f9b\u53ef\u9760\u7684\u4f4d\u7f6e\u4f30\u8ba1\uff0c\u89e3\u51b3\u4e86\u6c34\u4e0b\u5b9a\u4f4d\u7684\u6311\u6218\u3002"}}
{"id": "2511.18616", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18616", "abs": "https://arxiv.org/abs/2511.18616", "authors": ["Joseph Malone", "Rachith Aiyappa", "Byunghwee Lee", "Haewoon Kwak", "Jisun An", "Yong-Yeol Ahn"], "title": "A Benchmark for Zero-Shot Belief Inference in Large Language Models", "comment": "28 pages, 5 figures", "summary": "Beliefs are central to how humans reason, communicate, and form social connections, yet most computational approaches to studying them remain confined to narrow sociopolitical contexts and rely on fine-tuning for optimal performance. Despite the growing use of large language models (LLMs) across disciplines, how well these systems generalize across diverse belief domains remains unclear. We introduce a systematic, reproducible benchmark that evaluates the ability of LLMs to predict individuals' stances on a wide range of topics in a zero-shot setting using data from an online debate platform. The benchmark includes multiple informational conditions that isolate the contribution of demographic context and known prior beliefs to predictive success. Across several small- to medium-sized models, we find that providing more background information about an individual improves predictive accuracy, but performance varies substantially across belief domains. These findings reveal both the capacity and limitations of current LLMs to emulate human reasoning, advancing the study of machine behavior and offering a scalable framework for modeling belief systems beyond the sociopolitical sphere.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u57fa\u51c6\uff0c\u8bc4\u4f30LLM\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u9884\u6d4b\u4e2a\u4f53\u5bf9\u5404\u79cd\u8bdd\u9898\u7acb\u573a\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u63d0\u4f9b\u66f4\u591a\u80cc\u666f\u4fe1\u606f\u80fd\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u4f46\u6027\u80fd\u5728\u4e0d\u540c\u4fe1\u5ff5\u9886\u57df\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u4fe1\u5ff5\u662f\u4eba\u7c7b\u63a8\u7406\u3001\u6c9f\u901a\u548c\u793e\u4ea4\u7684\u6838\u5fc3\uff0c\u4f46\u73b0\u6709\u8ba1\u7b97\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u793e\u4f1a\u653f\u6cbb\u80cc\u666f\u4e14\u4f9d\u8d56\u5fae\u8c03\u3002\u9700\u8981\u4e86\u89e3LLM\u5728\u4e0d\u540c\u4fe1\u5ff5\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5728\u7ebf\u8fa9\u8bba\u5e73\u53f0\u6570\u636e\uff0c\u6784\u5efa\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u79cd\u4fe1\u606f\u6761\u4ef6\u6765\u5206\u79bb\u4eba\u53e3\u7edf\u8ba1\u80cc\u666f\u548c\u5df2\u77e5\u5148\u9a8c\u4fe1\u5ff5\u5bf9\u9884\u6d4b\u6210\u529f\u7684\u8d21\u732e\u3002", "result": "\u63d0\u4f9b\u66f4\u591a\u4e2a\u4f53\u80cc\u666f\u4fe1\u606f\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u4f46\u4e0d\u540c\u4fe1\u5ff5\u9886\u57df\u7684\u6027\u80fd\u5dee\u5f02\u5f88\u5927\u3002\u4e2d\u5c0f\u578b\u6a21\u578b\u8868\u73b0\u5404\u5f02\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dLLM\u6a21\u62df\u4eba\u7c7b\u63a8\u7406\u7684\u80fd\u529b\u548c\u5c40\u9650\u6027\uff0c\u4e3a\u8d85\u8d8a\u793e\u4f1a\u653f\u6cbb\u9886\u57df\u7684\u4fe1\u5ff5\u7cfb\u7edf\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u6846\u67b6\u3002"}}
{"id": "2511.18702", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.18702", "abs": "https://arxiv.org/abs/2511.18702", "authors": ["Xueyan Oh", "Leonard Loh", "Shaohui Foong", "Zhong Bao Andy Koh", "Kow Leong Ng", "Poh Kang Tan", "Pei Lin Pearlin Toh", "U-Xuan Tan"], "title": "CNN-Based Camera Pose Estimation and Localisation of Scan Images for Aircraft Visual Inspection", "comment": "12 pages, 12 figures", "summary": "General Visual Inspection is a manual inspection process regularly used to detect and localise obvious damage on the exterior of commercial aircraft. There has been increasing demand to perform this process at the boarding gate to minimise the downtime of the aircraft and automating this process is desired to reduce the reliance on human labour. Automating this typically requires estimating a camera's pose with respect to the aircraft for initialisation but most existing localisation methods require infrastructure, which is very challenging in uncontrolled outdoor environments and within the limited turnover time (approximately 2 hours) on an airport tarmac. Additionally, many airlines and airports do not allow contact with the aircraft's surface or using UAVs for inspection between flights, and restrict access to commercial aircraft. Hence, this paper proposes an on-site method that is infrastructure-free and easy to deploy for estimating a pan-tilt-zoom camera's pose and localising scan images. This method initialises using the same pan-tilt-zoom camera used for the inspection task by utilising a Deep Convolutional Neural Network fine-tuned on only synthetic images to predict its own pose. We apply domain randomisation to generate the dataset for fine-tuning the network and modify its loss function by leveraging aircraft geometry to improve accuracy. We also propose a workflow for initialisation, scan path planning, and precise localisation of images captured from a pan-tilt-zoom camera. We evaluate and demonstrate our approach through experiments with real aircraft, achieving root-mean-square camera pose estimation errors of less than 0.24 m and 2 degrees for all real scenes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u57fa\u7840\u8bbe\u65bd\u7684\u73b0\u573a\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1PTZ\u76f8\u673a\u59ff\u6001\u5e76\u5b9a\u4f4d\u626b\u63cf\u56fe\u50cf\uff0c\u901a\u8fc7\u4ec5\u4f7f\u7528\u5408\u6210\u56fe\u50cf\u5fae\u8c03\u7684\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6765\u9884\u6d4b\u76f8\u673a\u59ff\u6001\uff0c\u5728\u771f\u5b9e\u98de\u673a\u4e0a\u5b9e\u73b0\u4e86\u5c0f\u4e8e0.24\u7c73\u548c2\u5ea6\u7684RMS\u59ff\u6001\u4f30\u8ba1\u8bef\u5dee\u3002", "motivation": "\u81ea\u52a8\u5316\u98de\u673a\u5916\u90e8\u76ee\u89c6\u68c0\u67e5\u8fc7\u7a0b\uff0c\u51cf\u5c11\u5bf9\u4eba\u5de5\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u514b\u670d\u5728\u4e0d\u53d7\u63a7\u7684\u5ba4\u5916\u73af\u5883\u548c\u6709\u9650\u5468\u8f6c\u65f6\u95f4\u5185\u57fa\u7840\u8bbe\u65bd\u90e8\u7f72\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u822a\u7a7a\u516c\u53f8\u5bf9\u63a5\u89e6\u98de\u673a\u8868\u9762\u548c\u4f7f\u7528\u65e0\u4eba\u673a\u7684\u9650\u5236\u3002", "method": "\u4f7f\u7528\u4ec5\u57fa\u4e8e\u5408\u6210\u56fe\u50cf\u5fae\u8c03\u7684\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u9884\u6d4bPTZ\u76f8\u673a\u81ea\u8eab\u59ff\u6001\uff0c\u5e94\u7528\u9886\u57df\u968f\u673a\u5316\u751f\u6210\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528\u98de\u673a\u51e0\u4f55\u6539\u8fdb\u635f\u5931\u51fd\u6570\uff0c\u63d0\u51fa\u5305\u62ec\u521d\u59cb\u5316\u3001\u626b\u63cf\u8def\u5f84\u89c4\u5212\u548c\u56fe\u50cf\u7cbe\u786e\u5b9a\u4f4d\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "result": "\u5728\u771f\u5b9e\u98de\u673a\u573a\u666f\u4e2d\uff0c\u6240\u6709\u573a\u666f\u7684\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u5747\u65b9\u6839\u8bef\u5dee\u5747\u5c0f\u4e8e0.24\u7c73\u548c2\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u57fa\u7840\u8bbe\u65bd\u81ea\u7531\u3001\u6613\u4e8e\u90e8\u7f72\u7684\u98de\u673a\u68c0\u67e5\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u673a\u573a\u505c\u673a\u576a\u7684\u6709\u9650\u65f6\u95f4\u5185\u51c6\u786e\u4f30\u8ba1\u76f8\u673a\u59ff\u6001\u5e76\u5b9a\u4f4d\u626b\u63cf\u56fe\u50cf\u3002"}}
{"id": "2511.18618", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18618", "abs": "https://arxiv.org/abs/2511.18618", "authors": ["Mirza Raquib", "Munazer Montasir Akash", "Tawhid Ahmed", "Saydul Akbar Murad", "Farida Siddiqi Prity", "Mohammad Amzad Hossain", "Asif Pervez Polok", "Nick Rahimi"], "title": "A Unified BERT-CNN-BiLSTM Framework for Simultaneous Headline Classification and Sentiment Analysis of Bangla News", "comment": null, "summary": "In our daily lives, newspapers are an essential information source that impacts how the public talks about present-day issues. However, effectively navigating the vast amount of news content from different newspapers and online news portals can be challenging. Newspaper headlines with sentiment analysis tell us what the news is about (e.g., politics, sports) and how the news makes us feel (positive, negative, neutral). This helps us quickly understand the emotional tone of the news. This research presents a state-of-the-art approach to Bangla news headline classification combined with sentiment analysis applying Natural Language Processing (NLP) techniques, particularly the hybrid transfer learning model BERT-CNN-BiLSTM. We have explored a dataset called BAN-ABSA of 9014 news headlines, which is the first time that has been experimented with simultaneously in the headline and sentiment categorization in Bengali newspapers. Over this imbalanced dataset, we applied two experimental strategies: technique-1, where undersampling and oversampling are applied before splitting, and technique-2, where undersampling and oversampling are applied after splitting on the In technique-1 oversampling provided the strongest performance, both headline and sentiment, that is 78.57\\% and 73.43\\% respectively, while technique-2 delivered the highest result when trained directly on the original imbalanced dataset, both headline and sentiment, that is 81.37\\% and 64.46\\% respectively. The proposed model BERT-CNN-BiLSTM significantly outperforms all baseline models in classification tasks, and achieves new state-of-the-art results for Bangla news headline classification and sentiment analysis. These results demonstrate the importance of leveraging both the headline and sentiment datasets, and provide a strong baseline for Bangla text classification in low-resource.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408BERT-CNN-BiLSTM\u6df7\u5408\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\u7684\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u6807\u9898\u5206\u7c7b\u548c\u60c5\u611f\u5206\u6790\u65b9\u6cd5\uff0c\u5728BAN-ABSA\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u62a5\u7eb8\u662f\u91cd\u8981\u7684\u4fe1\u606f\u6765\u6e90\uff0c\u4f46\u6709\u6548\u5bfc\u822a\u5927\u91cf\u65b0\u95fb\u5185\u5bb9\u5177\u6709\u6311\u6218\u6027\u3002\u65b0\u95fb\u6807\u9898\u60c5\u611f\u5206\u6790\u6709\u52a9\u4e8e\u5feb\u901f\u7406\u89e3\u65b0\u95fb\u7684\u60c5\u611f\u57fa\u8c03\uff0c\u7279\u522b\u662f\u5728\u5b5f\u52a0\u62c9\u8bed\u8fd9\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u3002", "method": "\u4f7f\u7528BERT-CNN-BiLSTM\u6df7\u5408\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\uff0c\u57289014\u6761\u5b5f\u52a0\u62c9\u8bed\u65b0\u95fb\u6807\u9898\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u4e24\u79cd\u5b9e\u9a8c\u7b56\u7565\uff1a\u6280\u672f1\uff08\u5206\u5272\u524d\u8fdb\u884c\u6b20\u91c7\u6837\u548c\u8fc7\u91c7\u6837\uff09\u548c\u6280\u672f2\uff08\u5206\u5272\u540e\u8fdb\u884c\u6b20\u91c7\u6837\u548c\u8fc7\u91c7\u6837\uff09\u3002", "result": "\u6280\u672f1\u4e2d\u8fc7\u91c7\u6837\u5728\u6807\u9898\u548c\u60c5\u611f\u5206\u7c7b\u4e0a\u5206\u522b\u8fbe\u523078.57%\u548c73.43%\u7684\u6700\u4f73\u6027\u80fd\uff1b\u6280\u672f2\u4e2d\u76f4\u63a5\u5728\u539f\u59cb\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5206\u522b\u8fbe\u523081.37%\u548c64.46%\u3002BERT-CNN-BiLSTM\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u5b5f\u52a0\u62c9\u8bed\u6587\u672c\u5206\u7c7b\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u51c6\uff0c\u8bc1\u660e\u4e86\u540c\u65f6\u5229\u7528\u6807\u9898\u548c\u60c5\u611f\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027\uff0c\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002"}}
{"id": "2511.18703", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18703", "abs": "https://arxiv.org/abs/2511.18703", "authors": ["Ardalan Tajbakhsh", "Augustinos Saravanos", "James Zhu", "Evangelos A. Theodorou", "Lorenz T. Biegler", "Aaron M. Johnson"], "title": "Asynchronous Distributed Multi-Robot Motion Planning Under Imperfect Communication", "comment": "9 pages, 5 figures", "summary": "This paper addresses the challenge of coordinating multi-robot systems under realistic communication delays using distributed optimization. We focus on consensus ADMM as a scalable framework for generating collision-free, dynamically feasible motion plans in both trajectory optimization and receding-horizon control settings. In practice, however, these algorithms are sensitive to penalty tuning or adaptation schemes (e.g. residual balancing and adaptive parameter heuristics) that do not explicitly consider delays. To address this, we introduce a Delay-Aware ADMM (DA-ADMM) variant that adapts penalty parameters based on real-time delay statistics, allowing agents to down-weight stale information and prioritize recent updates during consensus and dual updates. Through extensive simulations in 2D and 3D environments with double-integrator, Dubins-car, and drone dynamics, we show that DA-ADMM significantly improves robustness, success rate, and solution quality compared to fixed-parameter, residual-balancing, and fixed-constraint baselines. Our results highlight that performance degradation is not solely determined by delay length or frequency, but by the optimizer's ability to contextually reason over delayed information. The proposed DA-ADMM achieves consistently better coordination performance across a wide range of delay conditions, offering a principled and efficient mechanism for resilient multi-robot motion planning under imperfect communication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5ef6\u8fdf\u611f\u77e5ADMM\uff08DA-ADMM\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u5b9e\u65f6\u5ef6\u8fdf\u7edf\u8ba1\u8c03\u6574\u60e9\u7f5a\u53c2\u6570\uff0c\u5728\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u6709\u6548\u5904\u7406\u901a\u4fe1\u5ef6\u8fdf\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u89c4\u5212\u7684\u9c81\u68d2\u6027\u548c\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u5b9e\u9645\u901a\u4fe1\u5ef6\u8fdf\u4e0b\u7684\u534f\u8c03\u95ee\u9898\uff0c\u4f20\u7edfADMM\u7b97\u6cd5\u5bf9\u60e9\u7f5a\u53c2\u6570\u8c03\u4f18\u654f\u611f\uff0c\u4e14\u73b0\u6709\u81ea\u9002\u5e94\u65b9\u6cd5\u672a\u663e\u5f0f\u8003\u8651\u5ef6\u8fdf\u5f71\u54cd\u3002", "method": "\u5f00\u53d1\u4e86\u5ef6\u8fdf\u611f\u77e5ADMM\u53d8\u4f53\uff0c\u6839\u636e\u5b9e\u65f6\u5ef6\u8fdf\u7edf\u8ba1\u8c03\u6574\u60e9\u7f5a\u53c2\u6570\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u964d\u4f4e\u9648\u65e7\u4fe1\u606f\u7684\u6743\u91cd\uff0c\u5728\u5171\u8bc6\u548c\u53cc\u91cd\u66f4\u65b0\u4e2d\u4f18\u5148\u5904\u7406\u6700\u65b0\u66f4\u65b0\u3002", "result": "\u57282D\u548c3D\u73af\u5883\u4e2d\u5bf9\u53cc\u79ef\u5206\u5668\u3001Dubins-car\u548c\u65e0\u4eba\u673a\u52a8\u529b\u5b66\u8fdb\u884c\u5e7f\u6cdb\u4eff\u771f\uff0cDA-ADMM\u76f8\u6bd4\u56fa\u5b9a\u53c2\u6570\u3001\u6b8b\u5dee\u5e73\u8861\u548c\u56fa\u5b9a\u7ea6\u675f\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3001\u6210\u529f\u7387\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u3002", "conclusion": "\u6027\u80fd\u4e0b\u964d\u4e0d\u4ec5\u7531\u5ef6\u8fdf\u957f\u5ea6\u6216\u9891\u7387\u51b3\u5b9a\uff0c\u8fd8\u53d6\u51b3\u4e8e\u4f18\u5316\u5668\u5bf9\u5ef6\u8fdf\u4fe1\u606f\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002DA-ADMM\u5728\u5404\u79cd\u5ef6\u8fdf\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u4e00\u81f4\u7684\u66f4\u597d\u534f\u8c03\u6027\u80fd\uff0c\u4e3a\u4e0d\u5b8c\u7f8e\u901a\u4fe1\u4e0b\u7684\u5f39\u6027\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u9ad8\u6548\u673a\u5236\u3002"}}
{"id": "2511.18619", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18619", "abs": "https://arxiv.org/abs/2511.18619", "authors": ["Maanas Taneja"], "title": "Prompt Optimization as a State-Space Search Problem", "comment": null, "summary": "Language Models are extremely susceptible to performance collapse with even small changes to input prompt strings. Libraries such as DSpy (from Stanford NLP) avoid this problem through demonstration-based prompt optimisation. Inspired by this, I propose an alternative approach that treats prompt optimisation as a classical state-space search problem. I model the prompt space as a graph where nodes represent prompt states and edges correspond to deliberate transformations such as shortening, adding examples, or re- ordering content. Using beam search and random walk algorithms, I systematically explore this space, evaluating candidates on development sets and pruning unpromising branches. Across five NLP tasks (sentiment classification, question answering, summarisation, reason- ing, and natural language inference), I find that even shallow search configurations (beam width=2, depth=2) improve upon seed prompts on development sets. For instance, beam search achieves development accuracy gains from 0.40 to 0.80 on reasoning tasks, though test set improvements are more modest (0.20 to 0.50), indicating overfitting to the develop- ment heuristic. Analysis of successful optimisation paths reveals that transformations that make prompts concise appear most frequently, while verbosity operators are never selected. My results validate prompt optimization as a search problem and suggest that with greater computational resources and improved evaluation metrics, deeper exploration could yield more robust prompts that generalize beyond development sets. Code and implementation are available at [https://github.com/MaanasTaneja/PromptOptimiser].", "AI": {"tldr": "\u5c06\u63d0\u793a\u4f18\u5316\u5efa\u6a21\u4e3a\u72b6\u6001\u7a7a\u95f4\u641c\u7d22\u95ee\u9898\uff0c\u901a\u8fc7\u675f\u641c\u7d22\u548c\u968f\u673a\u6e38\u8d70\u7b97\u6cd5\u5728\u63d0\u793a\u56fe\u4e2d\u63a2\u7d22\u4f18\u5316\u8def\u5f84\uff0c\u5728\u4e94\u4e2aNLP\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u5f00\u53d1\u96c6\u6027\u80fd\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5bf9\u8f93\u5165\u63d0\u793a\u7684\u5fae\u5c0f\u53d8\u5316\u6781\u4e3a\u654f\u611f\uff0c\u73b0\u6709\u65b9\u6cd5\u5982DSpy\u901a\u8fc7\u6f14\u793a\u4f18\u5316\u63d0\u793a\uff0c\u672c\u6587\u63d0\u51fa\u5c06\u63d0\u793a\u4f18\u5316\u89c6\u4e3a\u7ecf\u5178\u72b6\u6001\u7a7a\u95f4\u641c\u7d22\u95ee\u9898\u7684\u65b0\u601d\u8def\u3002", "method": "\u5c06\u63d0\u793a\u7a7a\u95f4\u5efa\u6a21\u4e3a\u56fe\u7ed3\u6784\uff0c\u8282\u70b9\u4ee3\u8868\u63d0\u793a\u72b6\u6001\uff0c\u8fb9\u5bf9\u5e94\u63d0\u793a\u53d8\u6362\u64cd\u4f5c\uff08\u5982\u7f29\u77ed\u3001\u6dfb\u52a0\u793a\u4f8b\u3001\u91cd\u65b0\u6392\u5e8f\u5185\u5bb9\uff09\uff0c\u4f7f\u7528\u675f\u641c\u7d22\u548c\u968f\u673a\u6e38\u8d70\u7b97\u6cd5\u7cfb\u7edf\u63a2\u7d22\u7a7a\u95f4\uff0c\u5728\u5f00\u53d1\u96c6\u4e0a\u8bc4\u4f30\u5019\u9009\u63d0\u793a\u5e76\u526a\u679d\u4e0d\u5177\u524d\u666f\u7684\u5206\u652f\u3002", "result": "\u5728\u4e94\u4e2aNLP\u4efb\u52a1\u4e0a\uff0c\u5373\u4f7f\u6d45\u5c42\u641c\u7d22\u914d\u7f6e\uff08\u675f\u5bbd=2\uff0c\u6df1\u5ea6=2\uff09\u4e5f\u80fd\u5728\u5f00\u53d1\u96c6\u4e0a\u8d85\u8d8a\u79cd\u5b50\u63d0\u793a\u3002\u4f8b\u5982\u63a8\u7406\u4efb\u52a1\u4e2d\u5f00\u53d1\u96c6\u51c6\u786e\u7387\u4ece0.40\u63d0\u5347\u81f30.80\uff0c\u4f46\u6d4b\u8bd5\u96c6\u6539\u8fdb\u8f83\u4e3a\u6709\u9650\uff080.20\u52300.50\uff09\uff0c\u8868\u660e\u5b58\u5728\u5bf9\u5f00\u53d1\u96c6\u542f\u53d1\u5f0f\u7684\u8fc7\u62df\u5408\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u5c06\u63d0\u793a\u4f18\u5316\u4f5c\u4e3a\u641c\u7d22\u95ee\u9898\u7684\u53ef\u884c\u6027\uff0c\u6210\u529f\u4f18\u5316\u8def\u5f84\u5206\u6790\u663e\u793a\u7b80\u6d01\u5316\u53d8\u6362\u6700\u5e38\u7528\uff0c\u800c\u5197\u957f\u64cd\u4f5c\u4ece\u672a\u88ab\u9009\u62e9\u3002\u5efa\u8bae\u901a\u8fc7\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\u548c\u6539\u8fdb\u8bc4\u4f30\u6307\u6807\uff0c\u8fdb\u884c\u66f4\u6df1\u5c42\u63a2\u7d22\u4ee5\u83b7\u5f97\u66f4\u9c81\u68d2\u7684\u63d0\u793a\u3002"}}
{"id": "2511.18708", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18708", "abs": "https://arxiv.org/abs/2511.18708", "authors": ["Yanbin Li", "Canran Xiao", "Shenghai Yuan", "Peilai Yu", "Ziruo Li", "Zhiguo Zhang", "Wenzheng Chi", "Wei Zhang"], "title": "GVD-TG: Topological Graph based on Fast Hierarchical GVD Sampling for Robot Exploration", "comment": "12 pages, 10 figures", "summary": "Topological maps are more suitable than metric maps for robotic exploration tasks. However, real-time updating of accurate and detail-rich environmental topological maps remains a challenge. This paper presents a topological map updating method based on the Generalized Voronoi Diagram (GVD). First, the newly observed areas are denoised to avoid low-efficiency GVD nodes misleading the topological structure. Subsequently, a multi-granularity hierarchical GVD generation method is designed to control the sampling granularity at both global and local levels. This not only ensures the accuracy of the topological structure but also enhances the ability to capture detail features, reduces the probability of path backtracking, and ensures no overlap between GVDs through the maintenance of a coverage map, thereby improving GVD utilization efficiency. Second, a node clustering method with connectivity constraints and a connectivity method based on a switching mechanism are designed to avoid the generation of unreachable nodes and erroneous nodes caused by obstacle attraction. A special cache structure is used to store all connectivity information, thereby improving exploration efficiency. Finally, to address the issue of frontiers misjudgment caused by obstacles within the scope of GVD units, a frontiers extraction method based on morphological dilation is designed to effectively ensure the reachability of frontiers. On this basis, a lightweight cost function is used to assess and switch to the next viewpoint in real time. This allows the robot to quickly adjust its strategy when signs of path backtracking appear, thereby escaping the predicament and increasing exploration flexibility. And the performance of system for exploration task is verified through comparative tests with SOTA methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49Voronoi\u56fe(GVD)\u7684\u62d3\u6251\u5730\u56fe\u66f4\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u5206\u5c42GVD\u751f\u6210\u3001\u8282\u70b9\u805a\u7c7b\u4e0e\u8fde\u901a\u6027\u7ea6\u675f\u3001\u57fa\u4e8e\u5f62\u6001\u5b66\u81a8\u80c0\u7684\u524d\u6cbf\u63d0\u53d6\u7b49\u6280\u672f\uff0c\u5b9e\u73b0\u5b9e\u65f6\u3001\u51c6\u786e\u7684\u73af\u5883\u62d3\u6251\u5730\u56fe\u6784\u5efa\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u5b9e\u65f6\u66f4\u65b0\u51c6\u786e\u4e14\u7ec6\u8282\u4e30\u5bcc\u7684\u73af\u5883\u62d3\u6251\u5730\u56fe\u5728\u673a\u5668\u4eba\u63a2\u7d22\u4efb\u52a1\u4e2d\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3GVD\u8282\u70b9\u8bef\u5bfc\u62d3\u6251\u7ed3\u6784\u3001\u8def\u5f84\u56de\u6eaf\u3001\u4e0d\u53ef\u8fbe\u8282\u70b9\u751f\u6210\u7b49\u95ee\u9898\u3002", "method": "1) \u5bf9\u65b0\u89c2\u6d4b\u533a\u57df\u53bb\u566a\uff1b2) \u591a\u7c92\u5ea6\u5206\u5c42GVD\u751f\u6210\u65b9\u6cd5\u63a7\u5236\u91c7\u6837\u7c92\u5ea6\uff1b3) \u5e26\u8fde\u901a\u6027\u7ea6\u675f\u7684\u8282\u70b9\u805a\u7c7b\u548c\u57fa\u4e8e\u5207\u6362\u673a\u5236\u7684\u8fde\u901a\u65b9\u6cd5\uff1b4) \u57fa\u4e8e\u5f62\u6001\u5b66\u81a8\u80c0\u7684\u524d\u6cbf\u63d0\u53d6\u65b9\u6cd5\uff1b5) \u8f7b\u91cf\u7ea7\u6210\u672c\u51fd\u6570\u5b9e\u65f6\u8bc4\u4f30\u548c\u5207\u6362\u4e0b\u4e00\u4e2a\u89c6\u70b9\u3002", "result": "\u901a\u8fc7\u5bf9\u6bd4\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u63a2\u7d22\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4f18\u4e8eSOTA\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u62d3\u6251\u7ed3\u6784\u51c6\u786e\u6027\u3001\u7ec6\u8282\u7279\u5f81\u6355\u6349\u80fd\u529b\uff0c\u51cf\u5c11\u4e86\u8def\u5f84\u56de\u6eaf\u6982\u7387\uff0c\u63d0\u5347\u4e86\u63a2\u7d22\u6548\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6784\u5efa\u5b9e\u65f6\u66f4\u65b0\u7684\u62d3\u6251\u5730\u56fe\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u63a2\u7d22\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u51fa\u73b0\u8def\u5f84\u56de\u6eaf\u8ff9\u8c61\u65f6\u80fd\u5feb\u901f\u8c03\u6574\u7b56\u7565\u3002"}}
{"id": "2511.18622", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18622", "abs": "https://arxiv.org/abs/2511.18622", "authors": ["Michael J. Bommarito"], "title": "OpenGloss: A Synthetic Encyclopedic Dictionary and Semantic Knowledge Graph", "comment": "30 pages, 5 figures, 8 tables. Dataset available at https://huggingface.co/datasets/mjbommar/opengloss-dictionary", "summary": "We present OpenGloss, a synthetic encyclopedic dictionary and semantic knowledge graph for English that integrates lexicographic definitions, encyclopedic context, etymological histories, and semantic relationships in a unified resource. OpenGloss contains 537K senses across 150K lexemes, on par with WordNet 3.1 and Open English WordNet, while providing more than four times as many sense definitions. These lexemes include 9.1M semantic edges, 1M usage examples, 3M collocations, and 60M words of encyclopedic content.\n  Generated through a multi-agent procedural generation pipeline with schema-validated LLM outputs and automated quality assurance, the entire resource was produced in under one week for under $1,000. This demonstrates that structured generation can create comprehensive lexical resources at cost and time scales impractical for manual curation, enabling rapid iteration as foundation models improve. The resource addresses gaps in pedagogical applications by providing integrated content -- definitions, examples, collocations, encyclopedias, etymology -- that supports both vocabulary learning and natural language processing tasks.\n  As a synthetically generated resource, OpenGloss reflects both the capabilities and limitations of current foundation models. The dataset is publicly available on Hugging Face under CC-BY 4.0, enabling researchers and educators to build upon and adapt this resource.", "AI": {"tldr": "OpenGloss\u662f\u4e00\u4e2a\u5408\u6210\u7684\u82f1\u8bed\u767e\u79d1\u5168\u4e66\u8bcd\u5178\u548c\u8bed\u4e49\u77e5\u8bc6\u56fe\u8c31\uff0c\u6574\u5408\u4e86\u8bcd\u5178\u5b9a\u4e49\u3001\u767e\u79d1\u5168\u4e66\u80cc\u666f\u3001\u8bcd\u6e90\u5386\u53f2\u548c\u8bed\u4e49\u5173\u7cfb\uff0c\u5305\u542b53.7\u4e07\u4e2a\u8bcd\u4e49\u548c150\u4e07\u4e2a\u8bcd\u6761\uff0c\u6210\u672c\u4f4e\u4e8e1000\u7f8e\u5143\u4e14\u5728\u4e00\u5468\u5185\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bcd\u5178\u8d44\u6e90\u5236\u4f5c\u6210\u672c\u9ad8\u3001\u5468\u671f\u957f\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u751f\u6210\u6280\u672f\u521b\u5efa\u7efc\u5408\u7684\u8bcd\u6c47\u8d44\u6e90\uff0c\u586b\u8865\u6559\u5b66\u5e94\u7528\u4e2d\u7684\u7a7a\u767d\uff0c\u652f\u6301\u8bcd\u6c47\u5b66\u4e60\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7a0b\u5e8f\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u6a21\u5f0f\u9a8c\u8bc1\u7684LLM\u8f93\u51fa\u548c\u81ea\u52a8\u5316\u8d28\u91cf\u4fdd\u8bc1\uff0c\u5728\u4f4e\u6210\u672c\u4e0b\u5feb\u901f\u751f\u6210\u7ed3\u6784\u5316\u8bcd\u6c47\u8d44\u6e90\u3002", "result": "\u751f\u6210\u4e86\u5305\u542b53.7\u4e07\u4e2a\u8bcd\u4e49\u3001150\u4e07\u4e2a\u8bcd\u6761\u3001910\u4e07\u4e2a\u8bed\u4e49\u8fb9\u3001100\u4e07\u4e2a\u4f7f\u7528\u793a\u4f8b\u3001300\u4e07\u4e2a\u642d\u914d\u548c6000\u4e07\u8bcd\u767e\u79d1\u5168\u4e66\u5185\u5bb9\u7684\u8d44\u6e90\uff0c\u89c4\u6a21\u4e0eWordNet 3.1\u76f8\u5f53\u4f46\u5b9a\u4e49\u6570\u91cf\u591a\u56db\u500d\u3002", "conclusion": "\u7ed3\u6784\u5316\u751f\u6210\u6280\u672f\u80fd\u591f\u5728\u4f20\u7edf\u4eba\u5de5\u6574\u7406\u4e0d\u53ef\u884c\u7684\u65f6\u95f4\u548c\u6210\u672c\u89c4\u6a21\u4e0a\u521b\u5efa\u5168\u9762\u7684\u8bcd\u6c47\u8d44\u6e90\uff0c\u968f\u7740\u57fa\u7840\u6a21\u578b\u7684\u6539\u8fdb\u53ef\u5b9e\u73b0\u5feb\u901f\u8fed\u4ee3\uff0c\u8be5\u8d44\u6e90\u5df2\u5728Hugging Face\u4e0a\u516c\u5f00\u4f9b\u7814\u7a76\u548c\u6559\u80b2\u4f7f\u7528\u3002"}}
{"id": "2511.18709", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18709", "abs": "https://arxiv.org/abs/2511.18709", "authors": ["Xueyan Oh", "Jonathan Her", "Zhixiang Ong", "Brandon Koh", "Yun Hann Tan", "U-Xuan Tan"], "title": "Autonomous Surface Selection For Manipulator-Based UV Disinfection In Hospitals Using Foundation Models", "comment": "7 pages, 7 figures; This paper has been accepted by IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)", "summary": "Ultraviolet (UV) germicidal radiation is an established non-contact method for surface disinfection in medical environments. Traditional approaches require substantial human intervention to define disinfection areas, complicating automation, while deep learning-based methods often need extensive fine-tuning and large datasets, which can be impractical for large-scale deployment. Additionally, these methods often do not address scene understanding for partial surface disinfection, which is crucial for avoiding unintended UV exposure. We propose a solution that leverages foundation models to simplify surface selection for manipulator-based UV disinfection, reducing human involvement and removing the need for model training. Additionally, we propose a VLM-assisted segmentation refinement to detect and exclude thin and small non-target objects, showing that this reduces mis-segmentation errors. Our approach achieves over 92\\% success rate in correctly segmenting target and non-target surfaces, and real-world experiments with a manipulator and simulated UV light demonstrate its practical potential for real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u7d2b\u5916\u7ebf\u6d88\u6bd2\u8868\u9762\u9009\u62e9\u65b9\u6cd5\uff0c\u65e0\u9700\u6a21\u578b\u8bad\u7ec3\uff0c\u901a\u8fc7VLM\u8f85\u52a9\u5206\u5272\u7ec6\u5316\u51cf\u5c11\u8bef\u5206\u5272\u9519\u8bef\uff0c\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u7d2b\u5916\u7ebf\u6d88\u6bd2\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u5de5\u5e72\u9884\u5b9a\u4e49\u6d88\u6bd2\u533a\u57df\uff0c\u800c\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u5fae\u8c03\u548c\u6570\u636e\u96c6\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u90e8\u5206\u8868\u9762\u6d88\u6bd2\u7684\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002", "method": "\u5229\u7528\u57fa\u7840\u6a21\u578b\u7b80\u5316\u673a\u68b0\u81c2\u7d2b\u5916\u7ebf\u6d88\u6bd2\u7684\u8868\u9762\u9009\u62e9\uff0c\u51cf\u5c11\u4eba\u5de5\u53c2\u4e0e\uff0c\u65e0\u9700\u6a21\u578b\u8bad\u7ec3\uff1b\u91c7\u7528VLM\u8f85\u52a9\u5206\u5272\u7ec6\u5316\u6765\u68c0\u6d4b\u548c\u6392\u9664\u7ec6\u5c0f\u975e\u76ee\u6807\u7269\u4f53\u3002", "result": "\u76ee\u6807\u548c\u975e\u76ee\u6807\u8868\u9762\u6b63\u786e\u5206\u5272\u6210\u529f\u7387\u8d85\u8fc792%\uff0c\u771f\u5b9e\u673a\u68b0\u81c2\u548c\u6a21\u62df\u7d2b\u5916\u7ebf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7d2b\u5916\u7ebf\u6d88\u6bd2\u81ea\u52a8\u5316\u4e2d\u7684\u8868\u9762\u9009\u62e9\u95ee\u9898\uff0c\u51cf\u5c11\u4e86\u4eba\u5de5\u5e72\u9884\u548c\u6a21\u578b\u8bad\u7ec3\u9700\u6c42\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2511.18635", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.18635", "abs": "https://arxiv.org/abs/2511.18635", "authors": ["Shireen Chand", "Faith Baca", "Emilio Ferrara"], "title": "No Free Lunch in Language Model Bias Mitigation? Targeted Bias Reduction Can Exacerbate Unmitigated LLM Biases", "comment": null, "summary": "Large Language Models (LLMs) inherit societal biases from their training data, potentially leading to harmful or unfair outputs. While various techniques aim to mitigate these biases, their effects are often evaluated only along the dimension of the bias being targeted. This work investigates the cross-category consequences of targeted bias mitigation. We study four bias mitigation techniques applied across ten models from seven model families, and we explore racial, religious, profession- and gender-related biases. We measure the impact of debiasing on model coherence and stereotypical preference using the StereoSet benchmark. Our results consistently show that while targeted mitigation can sometimes reduce bias in the intended dimension, it frequently leads to unintended and often negative consequences in others, such as increasing model bias and decreasing general coherence. These findings underscore the critical need for robust, multi-dimensional evaluation tools when examining and developing bias mitigation strategies to avoid inadvertently shifting or worsening bias along untargeted axes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u9488\u5bf9\u7279\u5b9a\u504f\u89c1\u8fdb\u884c\u7f13\u89e3\u65f6\u53ef\u80fd\u4ea7\u751f\u7684\u8de8\u7c7b\u522b\u540e\u679c\uff0c\u53d1\u73b0\u867d\u7136\u76ee\u6807\u504f\u89c1\u53ef\u80fd\u51cf\u5c11\uff0c\u4f46\u7ecf\u5e38\u4f1a\u5728\u5176\u4ed6\u7ef4\u5ea6\u4ea7\u751f\u610f\u5916\u7684\u8d1f\u9762\u540e\u679c\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u7ee7\u627f\u4e86\u793e\u4f1a\u504f\u89c1\uff0c\u53ef\u80fd\u5bfc\u81f4\u6709\u5bb3\u6216\u4e0d\u516c\u5e73\u7684\u8f93\u51fa\u3002\u73b0\u6709\u504f\u89c1\u7f13\u89e3\u6280\u672f\u901a\u5e38\u53ea\u8bc4\u4f30\u76ee\u6807\u504f\u89c1\u7ef4\u5ea6\uff0c\u800c\u5ffd\u7565\u4e86\u8de8\u7c7b\u522b\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4e86\u56db\u79cd\u504f\u89c1\u7f13\u89e3\u6280\u672f\u5e94\u7528\u4e8e\u5341\u4e2a\u6a21\u578b\uff08\u6765\u81ea\u4e03\u4e2a\u6a21\u578b\u5bb6\u65cf\uff09\uff0c\u63a2\u7d22\u4e86\u79cd\u65cf\u3001\u5b97\u6559\u3001\u804c\u4e1a\u548c\u6027\u522b\u76f8\u5173\u7684\u504f\u89c1\uff0c\u4f7f\u7528StereoSet\u57fa\u51c6\u8bc4\u4f30\u504f\u89c1\u7f13\u89e3\u5bf9\u6a21\u578b\u8fde\u8d2f\u6027\u548c\u523b\u677f\u5370\u8c61\u504f\u597d\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u9488\u5bf9\u6027\u504f\u89c1\u7f13\u89e3\u867d\u7136\u6709\u65f6\u80fd\u51cf\u5c11\u76ee\u6807\u7ef4\u5ea6\u7684\u504f\u89c1\uff0c\u4f46\u7ecf\u5e38\u5728\u5176\u4ed6\u7ef4\u5ea6\u4ea7\u751f\u610f\u5916\u4e14\u5f80\u5f80\u662f\u8d1f\u9762\u7684\u540e\u679c\uff0c\u5982\u589e\u52a0\u6a21\u578b\u504f\u89c1\u548c\u964d\u4f4e\u4e00\u822c\u8fde\u8d2f\u6027\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86\u5728\u68c0\u67e5\u548c\u5f00\u53d1\u504f\u89c1\u7f13\u89e3\u7b56\u7565\u65f6\uff0c\u9700\u8981\u5f3a\u5927\u7684\u591a\u7ef4\u5ea6\u8bc4\u4f30\u5de5\u5177\uff0c\u4ee5\u907f\u514d\u65e0\u610f\u4e2d\u5728\u672a\u9488\u5bf9\u6027\u7ef4\u5ea6\u4e0a\u8f6c\u79fb\u6216\u52a0\u5267\u504f\u89c1\u3002"}}
{"id": "2511.18712", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18712", "abs": "https://arxiv.org/abs/2511.18712", "authors": ["Tianyu Wang", "Chunxiang Yan", "Xuanhong Liao", "Tao Zhang", "Ping Wang", "Cong Wen", "Dingchuan Liu", "Haowen Yu", "Ximin Lyu"], "title": "Head Stabilization for Wheeled Bipedal Robots via Force-Estimation-Based Admittance Control", "comment": null, "summary": "Wheeled bipedal robots are emerging as flexible platforms for field exploration. However, head instability induced by uneven terrain can degrade the accuracy of onboard sensors or damage fragile payloads. Existing research primarily focuses on stabilizing the mobile platform but overlooks active stabilization of the head in the world frame, resulting in vertical oscillations that undermine overall stability. To address this challenge, we developed a model-based ground force estimation method for our 6-degree-of-freedom wheeled bipedal robot. Leveraging these force estimates, we implemented an admittance control algorithm to enhance terrain adaptability. Simulation experiments validated the real-time performance of the force estimator and the robot's robustness when traversing uneven terrain.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u578b\u7684\u5730\u9762\u529b\u4f30\u8ba1\u65b9\u6cd5\u548c\u5bfc\u7eb3\u63a7\u5236\u7b97\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u7684\u5934\u90e8\u7a33\u5b9a\u6027\u3002", "motivation": "\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u4f5c\u4e3a\u7075\u6d3b\u7684\u73b0\u573a\u63a2\u7d22\u5e73\u53f0\uff0c\u4f46\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u5934\u90e8\u4e0d\u7a33\u5b9a\u4f1a\u964d\u4f4e\u4f20\u611f\u5668\u7cbe\u5ea6\u6216\u635f\u574f\u6709\u6548\u8f7d\u8377\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5e73\u53f0\u7a33\u5b9a\u800c\u5ffd\u7565\u4e86\u4e16\u754c\u5750\u6807\u7cfb\u4e2d\u7684\u5934\u90e8\u4e3b\u52a8\u7a33\u5b9a\u3002", "method": "\u4e3a6\u81ea\u7531\u5ea6\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u5f00\u53d1\u4e86\u57fa\u4e8e\u6a21\u578b\u7684\u5730\u9762\u529b\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u529b\u4f30\u8ba1\u5b9e\u73b0\u5bfc\u7eb3\u63a7\u5236\u7b97\u6cd5\u4ee5\u589e\u5f3a\u5730\u5f62\u9002\u5e94\u6027\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u529b\u4f30\u8ba1\u5668\u7684\u5b9e\u65f6\u6027\u80fd\u548c\u673a\u5668\u4eba\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u7a7f\u8d8a\u65f6\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u8f6e\u5f0f\u53cc\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u5e73\u5766\u5730\u5f62\u4e0a\u7684\u5934\u90e8\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u9ad8\u6574\u4f53\u7a33\u5b9a\u6027\u3002"}}
{"id": "2511.18649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18649", "abs": "https://arxiv.org/abs/2511.18649", "authors": ["Goun Pyeon", "Inbum Heo", "Jeesu Jung", "Taewook Hwang", "Hyuk Namgoong", "Hyein Seo", "Yerim Han", "Eunbin Kim", "Hyeonseok Kang", "Sangkeun Jung"], "title": "Evaluating Large Language Models on the 2026 Korean CSAT Mathematics Exam: Measuring Mathematical Ability in a Zero-Data-Leakage Setting", "comment": "52 pages, Korean", "summary": "This study systematically evaluated the mathematical reasoning capabilities of Large Language Models (LLMs) using the 2026 Korean College Scholastic Ability Test (CSAT) Mathematics section, ensuring a completely contamination-free evaluation environment. To address data leakage issues in existing benchmarks, we digitized all 46 questions (22 common and 24 elective) within two hours of the exam's public release, eliminating any possibility of inclusion in model training data. We conducted comprehensive evaluations of 24 state-of-the-art LLMs across varying input modalities (text, image, text+figure) and prompt languages (Korean, English).\n  GPT-5 Codex achieved the only perfect score (100 points) with text input and Korean prompts, while Grok 4, GPT-5, and Deepseek R1 scored above 95 points. Notably, gpt-oss-20B achieved 95.7 points despite its relatively small size, demonstrating high cost-effectiveness. Problem-specific analysis revealed geometry as the weakest domain (77.7% average) with significant performance degradation on 4-point high-difficulty problems. Text input consistently outperformed image input, while prompt language effects varied by model scale.\n  In reasoning enhancement experiments with GPT-5 series, increased reasoning intensity improved performance (from 82.6 to 100 points) but quadrupled token usage and drastically reduced efficiency, suggesting that models with minimal reasoning may be more practical. This research contributes: (1) implementation of a completely unexposed evaluation environment, (2) a real-exam-based LLM assessment framework, and (3) a practical evaluation perspective integrating performance, cost, and time considerations. Detailed results and model comparisons are available at the 2026 Korean CSAT LLM Evaluation Leaderboard (https://isoft.cnu.ac.kr/csat2026/).", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u75282026\u5e74\u97e9\u56fd\u9ad8\u8003\u6570\u5b66\u8bd5\u5377\u5bf924\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6570\u5b66\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\uff0cGPT-5 Codex\u83b7\u5f97\u6ee1\u5206\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u8f93\u5165\u65b9\u5f0f\u548c\u63a8\u7406\u5f3a\u5ea6\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u786e\u4fdd\u5728\u5b8c\u5168\u672a\u88ab\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u7684\u73af\u5883\u4e2d\u8bc4\u4f30LLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5728\u8003\u8bd5\u516c\u5f00\u540e2\u5c0f\u65f6\u5185\u6570\u5b57\u5316\u6240\u670946\u9053\u9898\u76ee\uff0c\u8bc4\u4f3024\u4e2a\u6700\u5148\u8fdbLLMs\u5728\u4e0d\u540c\u8f93\u5165\u6a21\u6001\uff08\u6587\u672c\u3001\u56fe\u50cf\u3001\u6587\u672c+\u56fe\u5f62\uff09\u548c\u63d0\u793a\u8bed\u8a00\uff08\u97e9\u8bed\u3001\u82f1\u8bed\uff09\u4e0b\u7684\u8868\u73b0\u3002", "result": "GPT-5 Codex\u83b7\u5f97\u552f\u4e00\u6ee1\u5206\uff0c\u51e0\u4f55\u662f\u6700\u8584\u5f31\u9886\u57df\uff08\u5e73\u574777.7%\uff09\uff0c\u6587\u672c\u8f93\u5165\u4f18\u4e8e\u56fe\u50cf\u8f93\u5165\uff0c\u589e\u52a0\u63a8\u7406\u5f3a\u5ea6\u80fd\u63d0\u5347\u6027\u80fd\u4f46\u5927\u5e45\u964d\u4f4e\u6548\u7387\u3002", "conclusion": "\u672c\u7814\u7a76\u5b9e\u73b0\u4e86\u5b8c\u5168\u672a\u66b4\u9732\u7684\u8bc4\u4f30\u73af\u5883\uff0c\u5efa\u7acb\u4e86\u57fa\u4e8e\u771f\u5b9e\u8003\u8bd5\u7684LLM\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u7efc\u5408\u8003\u8651\u6027\u80fd\u3001\u6210\u672c\u548c\u65f6\u95f4\u56e0\u7d20\u7684\u5b9e\u7528\u8bc4\u4f30\u89c6\u89d2\u3002"}}
{"id": "2511.18718", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18718", "abs": "https://arxiv.org/abs/2511.18718", "authors": ["Omar Garib", "Jayaprakash D. Kambhampaty", "Olivia J. Pinon Fischer", "Dimitri N. Mavris"], "title": "AIRHILT: A Human-in-the-Loop Testbed for Multimodal Conflict Detection in Aviation", "comment": "9 pages, 4 figures, 1 table, 1 algorithm", "summary": "We introduce AIRHILT (Aviation Integrated Reasoning, Human-in-the-Loop Testbed), a modular and lightweight simulation environment designed to evaluate multimodal pilot and air traffic control (ATC) assistance systems for aviation conflict detection. Built on the open-source Godot engine, AIRHILT synchronizes pilot and ATC radio communications, visual scene understanding from camera streams, and ADS-B surveillance data within a unified, scalable platform. The environment supports pilot- and controller-in-the-loop interactions, providing a comprehensive scenario suite covering both terminal area and en route operational conflicts, including communication errors and procedural mistakes. AIRHILT offers standardized JSON-based interfaces that enable researchers to easily integrate, swap, and evaluate automatic speech recognition (ASR), visual detection, decision-making, and text-to-speech (TTS) models. We demonstrate AIRHILT through a reference pipeline incorporating fine-tuned Whisper ASR, YOLO-based visual detection, ADS-B-based conflict logic, and GPT-OSS-20B structured reasoning, and present preliminary results from representative runway-overlap scenarios, where the assistant achieves an average time-to-first-warning of approximately 7.7 s, with average ASR and vision latencies of approximately 5.9 s and 0.4 s, respectively. The AIRHILT environment and scenario suite are openly available, supporting reproducible research on multimodal situational awareness and conflict detection in aviation; code and scenarios are available at https://github.com/ogarib3/airhilt.", "AI": {"tldr": "AIRHILT\u662f\u4e00\u4e2a\u57fa\u4e8eGodot\u5f15\u64ce\u7684\u8f7b\u91cf\u7ea7\u6a21\u62df\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30\u822a\u7a7a\u51b2\u7a81\u68c0\u6d4b\u4e2d\u7684\u591a\u6a21\u6001\u98de\u884c\u5458\u548c\u7a7a\u7ba1\u8f85\u52a9\u7cfb\u7edf\uff0c\u652f\u6301\u4eba\u673a\u4ea4\u4e92\u548c\u6807\u51c6\u5316\u63a5\u53e3\u3002", "motivation": "\u9700\u8981\u7edf\u4e00\u7684\u5e73\u53f0\u6765\u8bc4\u4f30\u822a\u7a7a\u51b2\u7a81\u68c0\u6d4b\u4e2d\u7684\u591a\u6a21\u6001\u8f85\u52a9\u7cfb\u7edf\uff0c\u6574\u5408\u65e0\u7ebf\u7535\u901a\u4fe1\u3001\u89c6\u89c9\u573a\u666f\u7406\u89e3\u548cADS-B\u76d1\u89c6\u6570\u636e\u3002", "method": "\u57fa\u4e8e\u5f00\u6e90Godot\u5f15\u64ce\u6784\u5efa\uff0c\u540c\u6b65\u98de\u884c\u5458\u548c\u7a7a\u7ba1\u65e0\u7ebf\u7535\u901a\u4fe1\u3001\u6444\u50cf\u5934\u6d41\u89c6\u89c9\u7406\u89e3\u548cADS-B\u6570\u636e\uff0c\u63d0\u4f9b\u6807\u51c6\u5316JSON\u63a5\u53e3\u96c6\u6210ASR\u3001\u89c6\u89c9\u68c0\u6d4b\u3001\u51b3\u7b56\u548cTTS\u6a21\u578b\u3002", "result": "\u5728\u4ee3\u8868\u6027\u8dd1\u9053\u91cd\u53e0\u573a\u666f\u4e2d\uff0c\u8f85\u52a9\u7cfb\u7edf\u5e73\u5747\u9996\u6b21\u8b66\u544a\u65f6\u95f4\u4e3a7.7\u79d2\uff0cASR\u548c\u89c6\u89c9\u5ef6\u8fdf\u5206\u522b\u4e3a5.9\u79d2\u548c0.4\u79d2\u3002", "conclusion": "AIRHILT\u4e3a\u822a\u7a7a\u591a\u6a21\u6001\u6001\u52bf\u611f\u77e5\u548c\u51b2\u7a81\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u7814\u7a76\u73af\u5883\uff0c\u4ee3\u7801\u548c\u573a\u666f\u5df2\u5f00\u6e90\u3002"}}
{"id": "2511.18659", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18659", "abs": "https://arxiv.org/abs/2511.18659", "authors": ["Jie He", "Richard He Bai", "Sinead Williamson", "Jeff Z. Pan", "Navdeep Jaitly", "Yizhe Zhang"], "title": "CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning", "comment": null, "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.", "AI": {"tldr": "CLaRa\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165\u538b\u7f29\u548c\u8054\u5408\u4f18\u5316\u5728\u5171\u4eab\u8fde\u7eed\u7a7a\u95f4\u4e2d\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u548c\u68c0\u7d22-\u751f\u6210\u5206\u79bb\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u548c\u68c0\u7d22-\u751f\u6210\u6a21\u5757\u5206\u79bb\u4f18\u5316\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u53d7\u9650\u3002", "method": "\u63d0\u51faCLaRa\u6846\u67b6\uff0c\u4f7f\u7528SCP\u6570\u636e\u5408\u6210\u65b9\u6cd5\u751f\u6210\u8bed\u4e49\u4e30\u5bcc\u7684\u538b\u7f29\u5411\u91cf\uff0c\u901a\u8fc7\u53ef\u5fae\u5206top-k\u4f30\u8ba1\u5668\u5b9e\u73b0\u91cd\u6392\u5e8f\u5668\u548c\u751f\u6210\u5668\u7684\u7aef\u5230\u7aef\u8054\u5408\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCLaRa\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u538b\u7f29\u548c\u91cd\u6392\u5e8f\u6027\u80fd\uff0c\u901a\u5e38\u8d85\u8d8a\u57fa\u4e8e\u6587\u672c\u7684\u5fae\u8c03\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CLaRa\u901a\u8fc7\u7edf\u4e00\u8fde\u7eed\u7a7a\u95f4\u4e2d\u7684\u8054\u5408\u4f18\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u8bc1\u660e\u4e86\u7406\u8bba\u4e0a\u7684\u68c0\u7d22\u76f8\u5173\u6027\u4e0e\u7b54\u6848\u8d28\u91cf\u5bf9\u9f50\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.18756", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18756", "abs": "https://arxiv.org/abs/2511.18756", "authors": ["Xueyu Du", "Lilian Zhang", "Fuan Duan", "Xincan Luo", "Maosong Wang", "Wenqi Wu", "JunMao"], "title": "SP-VINS: A Hybrid Stereo Visual Inertial Navigation System based on Implicit Environmental Map", "comment": null, "summary": "Filter-based visual inertial navigation system (VINS) has attracted mobile-robot researchers for the good balance between accuracy and efficiency, but its limited mapping quality hampers long-term high-accuracy state estimation. To this end, we first propose a novel filter-based stereo VINS, differing from traditional simultaneous localization and mapping (SLAM) systems based on 3D map, which performs efficient loop closure constraints with implicit environmental map composed of keyframes and 2D keypoints. Secondly, we proposed a hybrid residual filter framework that combines landmark reprojection and ray constraints to construct a unified Jacobian matrix for measurement updates. Finally, considering the degraded environment, we incorporated the camera-IMU extrinsic parameters into visual description to achieve online calibration. Benchmark experiments demonstrate that the proposed SP-VINS achieves high computational efficiency while maintaining long-term high-accuracy localization performance, and is superior to existing state-of-the-art (SOTA) methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6ee4\u6ce2\u5668\u7684\u7acb\u4f53\u89c6\u89c9\u60ef\u6027\u5bfc\u822a\u7cfb\u7edfSP-VINS\uff0c\u901a\u8fc7\u9690\u5f0f\u73af\u5883\u5730\u56fe\u5b9e\u73b0\u9ad8\u6548\u95ed\u73af\u7ea6\u675f\uff0c\u7ed3\u5408\u6df7\u5408\u6b8b\u5dee\u6ee4\u6ce2\u6846\u67b6\u548c\u5728\u7ebf\u5916\u53c2\u6807\u5b9a\uff0c\u5728\u4fdd\u6301\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u957f\u671f\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002", "motivation": "\u57fa\u4e8e\u6ee4\u6ce2\u5668\u7684VINS\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u4f46\u5176\u6709\u9650\u7684\u5efa\u56fe\u8d28\u91cf\u9650\u5236\u4e86\u957f\u671f\u9ad8\u7cbe\u5ea6\u72b6\u6001\u4f30\u8ba1\u80fd\u529b\u3002", "method": "1) \u63d0\u51fa\u57fa\u4e8e\u5173\u952e\u5e27\u548c2D\u5173\u952e\u70b9\u7684\u9690\u5f0f\u73af\u5883\u5730\u56fe\u5b9e\u73b0\u9ad8\u6548\u95ed\u73af\u7ea6\u675f\uff1b2) \u8bbe\u8ba1\u7ed3\u5408\u5730\u6807\u91cd\u6295\u5f71\u548c\u5c04\u7ebf\u7ea6\u675f\u7684\u6df7\u5408\u6b8b\u5dee\u6ee4\u6ce2\u6846\u67b6\uff1b3) \u5c06\u76f8\u673a-IMU\u5916\u53c2\u878d\u5165\u89c6\u89c9\u63cf\u8ff0\u5b9e\u73b0\u5728\u7ebf\u6807\u5b9a\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u8868\u660eSP-VINS\u5728\u4fdd\u6301\u9ad8\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u957f\u671f\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684SP-VINS\u7cfb\u7edf\u5728\u6ee4\u6ce2\u6846\u67b6\u4e0b\u5b9e\u73b0\u4e86\u7c7b\u4f3cSLAM\u7684\u5efa\u56fe\u80fd\u529b\uff0c\u4e3a\u79fb\u52a8\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u957f\u671f\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18696", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18696", "abs": "https://arxiv.org/abs/2511.18696", "authors": ["Wangjiaxuan Xin"], "title": "Empathetic Cascading Networks: A Multi-Stage Prompting Technique for Reducing Social Biases in Large Language Models", "comment": null, "summary": "This report presents the Empathetic Cascading Networks (ECN) framework, a multi-stage prompting method designed to enhance the empathetic and inclusive capabilities of large language models. ECN employs four stages: Perspective Adoption, Emotional Resonance, Reflective Understanding, and Integrative Synthesis, to guide models toward generating emotionally resonant and contextually aware responses. Experimental results demonstrate that ECN achieves the highest Empathy Quotient (EQ) scores across GPT-3.5-turbo and GPT-4, while maintaining competitive Regard and Perplexity metrics. These findings emphasize ECN's potential for applications requiring empathy and inclusivity in conversational AI.", "AI": {"tldr": "ECN\u6846\u67b6\u901a\u8fc7\u56db\u9636\u6bb5\u63d0\u793a\u65b9\u6cd5\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5171\u60c5\u80fd\u529b\uff0c\u5728GPT-3.5-turbo\u548cGPT-4\u4e0a\u83b7\u5f97\u6700\u9ad8\u5171\u60c5\u5546\u6570\u5f97\u5206", "motivation": "\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bddAI\u4e2d\u7684\u5171\u60c5\u548c\u5305\u5bb9\u80fd\u529b", "method": "\u4f7f\u7528\u56db\u9636\u6bb5\u63d0\u793a\u65b9\u6cd5\uff1a\u89c6\u89d2\u91c7\u7eb3\u3001\u60c5\u611f\u5171\u9e23\u3001\u53cd\u601d\u7406\u89e3\u3001\u6574\u5408\u7efc\u5408", "result": "ECN\u5728GPT-3.5-turbo\u548cGPT-4\u4e0a\u83b7\u5f97\u6700\u9ad8\u5171\u60c5\u5546\u6570\u5f97\u5206\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u5c0a\u91cd\u5ea6\u548c\u56f0\u60d1\u5ea6\u6307\u6807", "conclusion": "ECN\u5728\u9700\u8981\u5171\u60c5\u548c\u5305\u5bb9\u6027\u7684\u5bf9\u8bddAI\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u6f5c\u529b"}}
{"id": "2511.18810", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18810", "abs": "https://arxiv.org/abs/2511.18810", "authors": ["Yuxia Fu", "Zhizhen Zhang", "Yuqi Zhang", "Zijian Wang", "Zi Huang", "Yadan Luo"], "title": "MergeVLA: Cross-Skill Model Merging Toward a Generalist Vision-Language-Action Agent", "comment": null, "summary": "Recent Vision-Language-Action (VLA) models reformulate vision-language models by tuning them with millions of robotic demonstrations. While they perform well when fine-tuned for a single embodiment or task family, extending them to multi-skill settings remains challenging: directly merging VLA experts trained on different tasks results in near-zero success rates. This raises a fundamental question: what prevents VLAs from mastering multiple skills within one model? With an empirical decomposition of learnable parameters during VLA fine-tuning, we identify two key sources of non-mergeability: (1) Finetuning drives LoRA adapters in the VLM backbone toward divergent, task-specific directions beyond the capacity of existing merging methods to unify. (2) Action experts develop inter-block dependencies through self-attention feedback, causing task information to spread across layers and preventing modular recombination. To address these challenges, we present MergeVLA, a merging-oriented VLA architecture that preserves mergeability by design. MergeVLA introduces sparsely activated LoRA adapters via task masks to retain consistent parameters and reduce irreconcilable conflicts in the VLM. Its action expert replaces self-attention with cross-attention-only blocks to keep specialization localized and composable. When the task is unknown, it uses a test-time task router to adaptively select the appropriate task mask and expert head from the initial observation, enabling unsupervised task inference. Across LIBERO, LIBERO-Plus, RoboTwin, and multi-task experiments on the real SO101 robotic arm, MergeVLA achieves performance comparable to or even exceeding individually finetuned experts, demonstrating robust generalization across tasks, embodiments, and environments.", "AI": {"tldr": "MergeVLA\u662f\u4e00\u4e2a\u53ef\u5408\u5e76\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u7a00\u758f\u6fc0\u6d3b\u7684LoRA\u9002\u914d\u5668\u548c\u4ec5\u4ea4\u53c9\u6ce8\u610f\u529b\u5757\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u591a\u6280\u80fdVLA\u6a21\u578b\u5408\u5e76\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u8de8\u4efb\u52a1\u3001\u5177\u8eab\u548c\u73af\u5883\u7684\u7a33\u5065\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u5728\u5355\u4e00\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u591a\u6280\u80fd\u8bbe\u7f6e\u4e2d\u76f4\u63a5\u5408\u5e76\u4e13\u5bb6\u6a21\u578b\u4f1a\u5bfc\u81f4\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u4ec0\u4e48\u963b\u6b62\u4e86VLA\u5728\u4e00\u4e2a\u6a21\u578b\u4e2d\u638c\u63e1\u591a\u79cd\u6280\u80fd\uff1f", "method": "\u63d0\u51faMergeVLA\u67b6\u6784\uff1a1) \u4f7f\u7528\u4efb\u52a1\u63a9\u7801\u7684\u7a00\u758f\u6fc0\u6d3bLoRA\u9002\u914d\u5668\u4fdd\u6301\u53c2\u6570\u4e00\u81f4\u6027\uff1b2) \u7528\u4ec5\u4ea4\u53c9\u6ce8\u610f\u529b\u5757\u66ff\u6362\u81ea\u6ce8\u610f\u529b\uff0c\u4f7f\u4e13\u4e1a\u5316\u4fdd\u6301\u5c40\u90e8\u5316\u548c\u53ef\u7ec4\u5408\uff1b3) \u6d4b\u8bd5\u65f6\u4efb\u52a1\u8def\u7531\u5668\u6839\u636e\u521d\u59cb\u89c2\u5bdf\u81ea\u9002\u5e94\u9009\u62e9\u4efb\u52a1\u63a9\u7801\u548c\u4e13\u5bb6\u5934\u3002", "result": "\u5728LIBERO\u3001LIBERO-Plus\u3001RoboTwin\u548c\u771f\u5b9eSO101\u673a\u68b0\u81c2\u4e0a\u7684\u591a\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cMergeVLA\u5b9e\u73b0\u4e86\u4e0e\u5355\u72ec\u5fae\u8c03\u4e13\u5bb6\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u8de8\u4efb\u52a1\u3001\u5177\u8eab\u548c\u73af\u5883\u7684\u7a33\u5065\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "MergeVLA\u901a\u8fc7\u67b6\u6784\u8bbe\u8ba1\u4fdd\u7559\u4e86\u53ef\u5408\u5e76\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u591a\u6280\u80fd\u8bbe\u7f6e\u4e2d\u7684\u5408\u5e76\u6311\u6218\uff0c\u4e3a\u6784\u5efa\u901a\u7528\u591a\u6280\u80fd\u673a\u5668\u4eba\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.18743", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18743", "abs": "https://arxiv.org/abs/2511.18743", "authors": ["Yu Lei", "Shuzheng Si", "Wei Wang", "Yifei Wu", "Gang Chen", "Fanchao Qi", "Maosong Sun"], "title": "RhinoInsight: Improving Deep Research through Control Mechanisms for Model Behavior and Context", "comment": null, "summary": "Large language models are evolving from single-turn responders into tool-using agents capable of sustained reasoning and decision-making for deep research. Prevailing systems adopt a linear pipeline of plan to search to write to a report, which suffers from error accumulation and context rot due to the lack of explicit control over both model behavior and context. We introduce RhinoInsight, a deep research framework that adds two control mechanisms to enhance robustness, traceability, and overall quality without parameter updates. First, a Verifiable Checklist module transforms user requirements into traceable and verifiable sub-goals, incorporates human or LLM critics for refinement, and compiles a hierarchical outline to anchor subsequent actions and prevent non-executable planning. Second, an Evidence Audit module structures search content, iteratively updates the outline, and prunes noisy context, while a critic ranks and binds high-quality evidence to drafted content to ensure verifiability and reduce hallucinations. Our experiments demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.", "AI": {"tldr": "RhinoInsight\u662f\u4e00\u4e2a\u6df1\u5ea6\u7814\u7a76\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9a8c\u8bc1\u6e05\u5355\u548c\u8bc1\u636e\u5ba1\u8ba1\u4e24\u4e2a\u63a7\u5236\u673a\u5236\uff0c\u589e\u5f3aLLM\u4ee3\u7406\u5728\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u3001\u53ef\u8ffd\u6eaf\u6027\u548c\u8d28\u91cf\uff0c\u65e0\u9700\u53c2\u6570\u66f4\u65b0\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u91c7\u7528\u7ebf\u6027\u7ba1\u9053\uff08\u89c4\u5212-\u641c\u7d22-\u5199\u4f5c-\u62a5\u544a\uff09\u5b58\u5728\u9519\u8bef\u7d2f\u79ef\u548c\u4e0a\u4e0b\u6587\u8150\u5316\u95ee\u9898\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u884c\u4e3a\u548c\u4e0a\u4e0b\u6587\u7684\u663e\u5f0f\u63a7\u5236\u3002", "method": "1. \u53ef\u9a8c\u8bc1\u6e05\u5355\u6a21\u5757\uff1a\u5c06\u7528\u6237\u9700\u6c42\u8f6c\u5316\u4e3a\u53ef\u8ffd\u6eaf\u7684\u5b50\u76ee\u6807\uff0c\u901a\u8fc7\u6279\u8bc4\u8005\u7cbe\u70bc\u5e76\u751f\u6210\u5c42\u6b21\u5316\u5927\u7eb2\uff1b2. \u8bc1\u636e\u5ba1\u8ba1\u6a21\u5757\uff1a\u7ed3\u6784\u5316\u641c\u7d22\u5185\u5bb9\uff0c\u8fed\u4ee3\u66f4\u65b0\u5927\u7eb2\uff0c\u4fee\u526a\u566a\u58f0\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u6279\u8bc4\u8005\u6392\u540d\u548c\u7ed1\u5b9a\u9ad8\u8d28\u91cf\u8bc1\u636e\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRhinoInsight\u5728\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5728\u6df1\u5ea6\u641c\u7d22\u4efb\u52a1\u4e0a\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "RhinoInsight\u901a\u8fc7\u6dfb\u52a0\u63a7\u5236\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u7684\u8d28\u91cf\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2511.18857", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18857", "abs": "https://arxiv.org/abs/2511.18857", "authors": ["Changsheng Luo", "Yushi Wang", "Wenhan Cai", "Mingguo Zhao"], "title": "AutoOdom: Learning Auto-regressive Proprioceptive Odometry for Legged Locomotion", "comment": null, "summary": "Accurate proprioceptive odometry is fundamental for legged robot navigation in GPS-denied and visually degraded environments where conventional visual odometry systems fail. Current approaches face critical limitations: analytical filtering methods suffer from modeling uncertainties and cumulative drift, hybrid learning-filtering approaches remain constrained by their analytical components, while pure learning-based methods struggle with simulation-to-reality transfer and demand extensive real-world data collection. This paper introduces AutoOdom, a novel autoregressive proprioceptive odometry system that overcomes these challenges through an innovative two-stage training paradigm. Stage 1 employs large-scale simulation data to learn complex nonlinear dynamics and rapidly changing contact states inherent in legged locomotion, while Stage 2 introduces an autoregressive enhancement mechanism using limited real-world data to effectively bridge the sim-to-real gap. The key innovation lies in our autoregressive training approach, where the model learns from its own predictions to develop resilience against sensor noise and improve robustness in highly dynamic environments. Comprehensive experimental validation on the Booster T1 humanoid robot demonstrates that AutoOdom significantly outperforms state-of-the-art methods across all evaluation metrics, achieving 57.2% improvement in absolute trajectory error, 59.2% improvement in Umeyama-aligned error, and 36.2% improvement in relative pose error compared to the Legolas baseline. Extensive ablation studies provide critical insights into sensor modality selection and temporal modeling, revealing counterintuitive findings about IMU acceleration data and validating our systematic design choices for robust proprioceptive odometry in challenging locomotion scenarios.", "AI": {"tldr": "AutoOdom\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u56de\u5f52\u672c\u4f53\u611f\u77e5\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\u89e3\u51b3\u8db3\u5f0f\u673a\u5668\u4eba\u5728GPS\u7f3a\u5931\u548c\u89c6\u89c9\u9000\u5316\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u8db3\u5f0f\u673a\u5668\u4eba\u5728GPS\u7f3a\u5931\u548c\u89c6\u89c9\u9000\u5316\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u5b9a\u4f4d\u95ee\u9898\uff0c\u514b\u670d\u4f20\u7edf\u89c6\u89c9\u91cc\u7a0b\u8ba1\u5931\u6548\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\u5728\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u3001\u7d2f\u79ef\u6f02\u79fb\u3001\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u7b49\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u5927\u89c4\u6a21\u4eff\u771f\u6570\u636e\u5b66\u4e60\u8db3\u5f0f\u8fd0\u52a8\u7684\u590d\u6742\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u5feb\u901f\u53d8\u5316\u7684\u63a5\u89e6\u72b6\u6001\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u6709\u9650\u771f\u5b9e\u4e16\u754c\u6570\u636e\u5f15\u5165\u81ea\u56de\u5f52\u589e\u5f3a\u673a\u5236\uff0c\u901a\u8fc7\u6a21\u578b\u4ece\u81ea\u8eab\u9884\u6d4b\u4e2d\u5b66\u4e60\u6765\u5f25\u5408\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "result": "\u5728Booster T1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u76f8\u6bd4Legolas\u57fa\u7ebf\uff0cAutoOdom\u5728\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\u4e0a\u63d0\u534757.2%\uff0c\u5728Umeyama\u5bf9\u9f50\u8bef\u5dee\u4e0a\u63d0\u534759.2%\uff0c\u5728\u76f8\u5bf9\u4f4d\u59ff\u8bef\u5dee\u4e0a\u63d0\u534736.2%\u3002", "conclusion": "AutoOdom\u901a\u8fc7\u521b\u65b0\u7684\u81ea\u56de\u5f52\u8bad\u7ec3\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u672c\u4f53\u611f\u77e5\u91cc\u7a0b\u8ba1\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u8db3\u5f0f\u673a\u5668\u4eba\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18749", "categories": ["cs.CL", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2511.18749", "abs": "https://arxiv.org/abs/2511.18749", "authors": ["Matthew R. DeVerna", "Kai-Cheng Yang", "Harry Yaojun Yan", "Filippo Menczer"], "title": "Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search", "comment": null, "summary": "Large language models (LLMs) have raised hopes for automated end-to-end fact-checking, but prior studies report mixed results. As mainstream chatbots increasingly ship with reasoning capabilities and web search tools -- and millions of users already rely on them for verification -- rigorous evaluation is urgent. We evaluate 15 recent LLMs from OpenAI, Google, Meta, and DeepSeek on more than 6,000 claims fact-checked by PolitiFact, comparing standard models with reasoning- and web-search variants. Standard models perform poorly, reasoning offers minimal benefits, and web search provides only moderate gains, despite fact-checks being available on the web. In contrast, a curated RAG system using PolitiFact summaries improved macro F1 by 233% on average across model variants. These findings suggest that giving models access to curated high-quality context is a promising path for automated fact-checking.", "AI": {"tldr": "\u8bc4\u4f3015\u4e2a\u4e3b\u6d41LLM\u57286000\u591a\u4e2a\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6807\u51c6\u6a21\u578b\u8868\u73b0\u5dee\uff0c\u63a8\u7406\u80fd\u529b\u5e2e\u52a9\u6709\u9650\uff0c\u7f51\u7edc\u641c\u7d22\u4ec5\u63d0\u4f9b\u4e2d\u7b49\u63d0\u5347\uff0c\u800c\u4f7f\u7528\u9ad8\u8d28\u91cf\u4e0a\u4e0b\u6587\u68c0\u7d22\u7684RAG\u7cfb\u7edf\u663e\u8457\u6539\u5584\u6027\u80fd\u3002", "motivation": "\u968f\u7740LLM\u5177\u5907\u63a8\u7406\u548c\u7f51\u7edc\u641c\u7d22\u80fd\u529b\u5e76\u88ab\u5e7f\u6cdb\u7528\u4e8e\u4e8b\u5b9e\u6838\u67e5\uff0c\u6025\u9700\u5bf9\u5176\u6027\u80fd\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\u3002", "method": "\u5728PolitiFact\u76846000\u591a\u4e2a\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u4e0a\u6d4b\u8bd515\u4e2aLLM\uff0c\u6bd4\u8f83\u6807\u51c6\u6a21\u578b\u3001\u63a8\u7406\u53d8\u4f53\u548c\u7f51\u7edc\u641c\u7d22\u53d8\u4f53\u7684\u8868\u73b0\u3002", "result": "\u6807\u51c6\u6a21\u578b\u8868\u73b0\u5dee\uff0c\u63a8\u7406\u80fd\u529b\u5e2e\u52a9\u6709\u9650\uff0c\u7f51\u7edc\u641c\u7d22\u4ec5\u63d0\u4f9b\u4e2d\u7b49\u63d0\u5347\uff0c\u800cRAG\u7cfb\u7edf\u5c06\u5b8f\u89c2F1\u5e73\u5747\u63d0\u9ad8\u4e86233%\u3002", "conclusion": "\u4e3a\u6a21\u578b\u63d0\u4f9b\u9ad8\u8d28\u91cf\u4e0a\u4e0b\u6587\u662f\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u7684\u6709\u524d\u666f\u8def\u5f84\uff0c\u800c\u975e\u4f9d\u8d56\u6a21\u578b\u81ea\u8eab\u7684\u63a8\u7406\u6216\u7f51\u7edc\u641c\u7d22\u80fd\u529b\u3002"}}
{"id": "2511.18878", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18878", "abs": "https://arxiv.org/abs/2511.18878", "authors": ["Suzie Kim", "Hye-Bin Shin", "Hyo-Jeong Jang"], "title": "Accelerating Reinforcement Learning via Error-Related Human Brain Signals", "comment": null, "summary": "In this work, we investigate how implicit neural feed back can accelerate reinforcement learning in complex robotic manipulation settings. While prior electroencephalogram (EEG) guided reinforcement learning studies have primarily focused on navigation or low-dimensional locomotion tasks, we aim to understand whether such neural evaluative signals can improve policy learning in high-dimensional manipulation tasks involving obstacles and precise end-effector control. We integrate error related potentials decoded from offline-trained EEG classifiers into reward shaping and systematically evaluate the impact of human-feedback weighting. Experiments on a 7-DoF manipulator in an obstacle-rich reaching environment show that neural feedback accelerates reinforcement learning and, depending on the human-feedback weighting, can yield task success rates that at times exceed those of sparse-reward baselines. Moreover, when applying the best-performing feedback weighting across all sub jects, we observe consistent acceleration of reinforcement learning relative to the sparse-reward setting. Furthermore, leave-one subject-out evaluations confirm that the proposed framework remains robust despite the intrinsic inter-individual variability in EEG decodability. Our findings demonstrate that EEG-based reinforcement learning can scale beyond locomotion tasks and provide a viable pathway for human-aligned manipulation skill acquisition.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u5982\u4f55\u5229\u7528\u9690\u5f0f\u795e\u7ecf\u53cd\u9988\u52a0\u901f\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7EEG\u89e3\u7801\u7684\u9519\u8bef\u76f8\u5173\u7535\u4f4d\u8fdb\u884c\u5956\u52b1\u5851\u5f62\uff0c\u57287\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u969c\u788d\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u52a0\u901f\u5b66\u4e60\u5e76\u63d0\u9ad8\u6210\u529f\u7387\u3002", "motivation": "\u5148\u524d\u57fa\u4e8e\u8111\u7535\u56fe\uff08EEG\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5bfc\u822a\u6216\u4f4e\u7ef4\u8fd0\u52a8\u4efb\u52a1\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u795e\u7ecf\u8bc4\u4f30\u4fe1\u53f7\u662f\u5426\u80fd\u5728\u6d89\u53ca\u969c\u788d\u7269\u548c\u7cbe\u786e\u672b\u7aef\u6267\u884c\u5668\u63a7\u5236\u7684\u9ad8\u7ef4\u64cd\u4f5c\u4efb\u52a1\u4e2d\u6539\u8fdb\u7b56\u7565\u5b66\u4e60\u3002", "method": "\u5c06\u79bb\u7ebf\u8bad\u7ec3\u7684EEG\u5206\u7c7b\u5668\u89e3\u7801\u7684\u9519\u8bef\u76f8\u5173\u7535\u4f4d\u6574\u5408\u5230\u5956\u52b1\u5851\u5f62\u4e2d\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4eba\u7c7b\u53cd\u9988\u6743\u91cd\u7684\u5f71\u54cd\uff0c\u57287\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u7684\u969c\u788d\u4e30\u5bcc\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u795e\u7ecf\u53cd\u9988\u52a0\u901f\u4e86\u5f3a\u5316\u5b66\u4e60\uff0c\u6839\u636e\u4eba\u7c7b\u53cd\u9988\u6743\u91cd\u7684\u4e0d\u540c\uff0c\u4efb\u52a1\u6210\u529f\u7387\u6709\u65f6\u8d85\u8fc7\u7a00\u758f\u5956\u52b1\u57fa\u7ebf\u3002\u6700\u4f73\u53cd\u9988\u6743\u91cd\u5728\u6240\u6709\u53d7\u8bd5\u8005\u4e2d\u4e00\u81f4\u52a0\u901f\u4e86\u5f3a\u5316\u5b66\u4e60\uff0c\u7559\u4e00\u53d7\u8bd5\u8005\u9a8c\u8bc1\u8868\u660e\u6846\u67b6\u5bf9EEG\u89e3\u7801\u80fd\u529b\u7684\u4e2a\u4f53\u5dee\u5f02\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "\u57fa\u4e8eEEG\u7684\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u6269\u5c55\u5230\u8fd0\u52a8\u4efb\u52a1\u4e4b\u5916\uff0c\u4e3a\u4eba\u7c7b\u5bf9\u9f50\u7684\u64cd\u4f5c\u6280\u80fd\u83b7\u53d6\u63d0\u4f9b\u4e86\u53ef\u884c\u9014\u5f84\u3002"}}
{"id": "2511.18751", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18751", "abs": "https://arxiv.org/abs/2511.18751", "authors": ["Daiqing Wu", "Dongbao Yang", "Can Ma", "Yu Zhou"], "title": "Robust Multimodal Sentiment Analysis with Distribution-Based Feature Recovery and Fusion", "comment": "Accepted by ACM MM 2024", "summary": "As posts on social media increase rapidly, analyzing the sentiments embedded in image-text pairs has become a popular research topic in recent years. Although existing works achieve impressive accomplishments in simultaneously harnessing image and text information, they lack the considerations of possible low-quality and missing modalities. In real-world applications, these issues might frequently occur, leading to urgent needs for models capable of predicting sentiment robustly. Therefore, we propose a Distribution-based feature Recovery and Fusion (DRF) method for robust multimodal sentiment analysis of image-text pairs. Specifically, we maintain a feature queue for each modality to approximate their feature distributions, through which we can simultaneously handle low-quality and missing modalities in a unified framework. For low-quality modalities, we reduce their contributions to the fusion by quantitatively estimating modality qualities based on the distributions. For missing modalities, we build inter-modal mapping relationships supervised by samples and distributions, thereby recovering the missing modalities from available ones. In experiments, two disruption strategies that corrupt and discard some modalities in samples are adopted to mimic the low-quality and missing modalities in various real-world scenarios. Through comprehensive experiments on three publicly available image-text datasets, we demonstrate the universal improvements of DRF compared to SOTA methods under both two strategies, validating its effectiveness in robust multimodal sentiment analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u7684\u7279\u5f81\u6062\u590d\u4e0e\u878d\u5408\u65b9\u6cd5(DRF)\uff0c\u7528\u4e8e\u5904\u7406\u56fe\u50cf-\u6587\u672c\u5bf9\u4e2d\u4f4e\u8d28\u91cf\u548c\u7f3a\u5931\u6a21\u6001\u7684\u9c81\u68d2\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u540c\u65f6\u5229\u7528\u56fe\u50cf\u548c\u6587\u672c\u4fe1\u606f\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6210\u679c\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u53ef\u80fd\u5b58\u5728\u7684\u4f4e\u8d28\u91cf\u548c\u7f3a\u5931\u6a21\u6001\u7684\u8003\u8651\u3002\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u8fd9\u4e9b\u95ee\u9898\u9891\u7e41\u53d1\u751f\uff0c\u8feb\u5207\u9700\u8981\u80fd\u591f\u9c81\u68d2\u9884\u6d4b\u60c5\u611f\u7684\u6a21\u578b\u3002", "method": "\u4e3a\u6bcf\u4e2a\u6a21\u6001\u7ef4\u62a4\u7279\u5f81\u961f\u5217\u4ee5\u8fd1\u4f3c\u5176\u7279\u5f81\u5206\u5e03\uff0c\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u540c\u65f6\u5904\u7406\u4f4e\u8d28\u91cf\u548c\u7f3a\u5931\u6a21\u6001\u3002\u5bf9\u4e8e\u4f4e\u8d28\u91cf\u6a21\u6001\uff0c\u57fa\u4e8e\u5206\u5e03\u5b9a\u91cf\u4f30\u8ba1\u6a21\u6001\u8d28\u91cf\u5e76\u51cf\u5c11\u5176\u5bf9\u878d\u5408\u7684\u8d21\u732e\uff1b\u5bf9\u4e8e\u7f3a\u5931\u6a21\u6001\uff0c\u901a\u8fc7\u6837\u672c\u548c\u5206\u5e03\u76d1\u7763\u5efa\u7acb\u6a21\u6001\u95f4\u6620\u5c04\u5173\u7cfb\uff0c\u4ece\u53ef\u7528\u6a21\u6001\u6062\u590d\u7f3a\u5931\u6a21\u6001\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u56fe\u50cf-\u6587\u672c\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cDRF\u76f8\u6bd4SOTA\u65b9\u6cd5\u5728\u4e24\u79cd\u7834\u574f\u7b56\u7565\u4e0b\u5747\u53d6\u5f97\u666e\u904d\u6539\u8fdb\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u9c81\u68d2\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "DRF\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u4f4e\u8d28\u91cf\u548c\u7f3a\u5931\u6a21\u6001\u95ee\u9898\uff0c\u4e3a\u9c81\u68d2\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.18910", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.18910", "abs": "https://arxiv.org/abs/2511.18910", "authors": ["Samuel Cerezo", "Seong Hun Lee", "Javier Civera"], "title": "An Efficient Closed-Form Solution to Full Visual-Inertial State Initialization", "comment": "8 pages, 2 figures, 10 tables. Submitted to RA-L", "summary": "In this letter, we present a closed-form initialization method that recovers the full visual-inertial state without nonlinear optimization. Unlike previous approaches that rely on iterative solvers, our formulation yields analytical, easy-to-implement, and numerically stable solutions for reliable start-up. Our method builds on small-rotation and constant-velocity approximations, which keep the formulation compact while preserving the essential coupling between motion and inertial measurements. We further propose an observability-driven, two-stage initialization scheme that balances accuracy with initialization latency. Extensive experiments on the EuRoC dataset validate our assumptions: our method achieves 10-20% lower initialization error than optimization-based approaches, while using 4x shorter initialization windows and reducing computational cost by 5x.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u95ed\u5f0f\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u65e0\u9700\u975e\u7ebf\u6027\u4f18\u5316\u5373\u53ef\u6062\u590d\u5b8c\u6574\u7684\u89c6\u89c9-\u60ef\u6027\u72b6\u6001\uff0c\u76f8\u6bd4\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u4f4e\u7684\u521d\u59cb\u5316\u8bef\u5dee\u3001\u66f4\u77ed\u7684\u521d\u59cb\u5316\u7a97\u53e3\u548c\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8fed\u4ee3\u6c42\u89e3\u5668\uff0c\u9700\u8981\u975e\u7ebf\u6027\u4f18\u5316\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u521d\u59cb\u5316\u5ef6\u8fdf\u957f\u3002\u9700\u8981\u4e00\u79cd\u89e3\u6790\u3001\u6613\u5b9e\u73b0\u4e14\u6570\u503c\u7a33\u5b9a\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u6765\u5b9e\u73b0\u53ef\u9760\u542f\u52a8\u3002", "method": "\u57fa\u4e8e\u5c0f\u65cb\u8f6c\u548c\u6052\u5b9a\u901f\u5ea6\u8fd1\u4f3c\uff0c\u4fdd\u6301\u516c\u5f0f\u7d27\u51d1\u540c\u65f6\u4fdd\u7559\u8fd0\u52a8\u4e0e\u60ef\u6027\u6d4b\u91cf\u4e4b\u95f4\u7684\u57fa\u672c\u8026\u5408\u3002\u63d0\u51fa\u53ef\u89c2\u6d4b\u6027\u9a71\u52a8\u7684\u4e24\u9636\u6bb5\u521d\u59cb\u5316\u65b9\u6848\uff0c\u5e73\u8861\u7cbe\u5ea6\u4e0e\u521d\u59cb\u5316\u5ef6\u8fdf\u3002", "result": "\u5728EuRoC\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff1a\u76f8\u6bd4\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u521d\u59cb\u5316\u8bef\u5dee\u964d\u4f4e10-20%\uff0c\u521d\u59cb\u5316\u7a97\u53e3\u7f29\u77ed4\u500d\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e5\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u89e3\u6790\u3001\u6613\u5b9e\u73b0\u4e14\u6570\u503c\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u89c6\u89c9-\u60ef\u6027\u7cfb\u7edf\u7684\u53ef\u9760\u542f\u52a8\u3002"}}
{"id": "2511.18774", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18774", "abs": "https://arxiv.org/abs/2511.18774", "authors": ["Bashar Talafha", "Amin Abu Alhassan", "Muhammad Abdul-Mageed"], "title": "Context-Aware Whisper for Arabic ASR Under Linguistic Varieties", "comment": null, "summary": "Low-resource ASR remains a challenging problem, especially for languages like Arabic that exhibit wide dialectal variation and limited labeled data. We propose context-aware prompting strategies to adapt OpenAI's Whisper for Arabic speech recognition without retraining. Our methods include decoder prompting with first-pass transcriptions or retrieved utterances, and encoder prefixing using speech synthesized in the target speaker's voice. We introduce techniques such as prompt reordering, speaker-aware prefix synthesis, and modality-specific retrieval (lexical, semantic, acoustic) to improve transcription in real-world, zero-shot settings. Evaluated on nine Arabic linguistic conditions, our approach reduces WER by up to 22.3% on Modern Standard Arabic and 9.2% on dialectal speech, significantly mitigating hallucinations and speaker mismatch.", "AI": {"tldr": "\u63d0\u51fa\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\u7b56\u7565\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u9002\u914dWhisper\u7528\u4e8e\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u8bc6\u522b\uff0c\u5728\u591a\u79cd\u963f\u62c9\u4f2f\u8bed\u6761\u4ef6\u4e0b\u663e\u8457\u964d\u4f4e\u8bcd\u9519\u8bef\u7387", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684ASR\u6311\u6218\uff0c\u7279\u522b\u662f\u65b9\u8a00\u53d8\u5f02\u5927\u548c\u6807\u6ce8\u6570\u636e\u6709\u9650\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u89e3\u7801\u5668\u63d0\u793a\uff08\u9996\u904d\u8f6c\u5f55\u6216\u68c0\u7d22\u8bed\u53e5\uff09\u548c\u7f16\u7801\u5668\u524d\u7f00\uff08\u76ee\u6807\u8bf4\u8bdd\u4eba\u8bed\u97f3\u5408\u6210\uff09\uff0c\u5305\u62ec\u63d0\u793a\u91cd\u6392\u5e8f\u3001\u8bf4\u8bdd\u4eba\u611f\u77e5\u524d\u7f00\u5408\u6210\u548c\u6a21\u6001\u7279\u5b9a\u68c0\u7d22\u6280\u672f", "result": "\u5728\u4e5d\u79cd\u963f\u62c9\u4f2f\u8bed\u6761\u4ef6\u4e0b\uff0c\u73b0\u4ee3\u6807\u51c6\u963f\u62c9\u4f2f\u8bedWER\u964d\u4f4e22.3%\uff0c\u65b9\u8a00\u8bed\u97f3\u964d\u4f4e9.2%\uff0c\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u548c\u8bf4\u8bdd\u4eba\u4e0d\u5339\u914d", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u63d0\u793a\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90\u963f\u62c9\u4f2f\u8bedASR\u6027\u80fd\uff0c\u65e0\u9700\u6a21\u578b\u91cd\u8bad\u7ec3"}}
{"id": "2511.18950", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18950", "abs": "https://arxiv.org/abs/2511.18950", "authors": ["Juntao Gao", "Feiyang Ye", "Jing Zhang", "Wenjing Qian"], "title": "Compressor-VLA: Instruction-Guided Visual Token Compression for Efficient Robotic Manipulation", "comment": "11 pages, 5 figures", "summary": "Vision-Language-Action (VLA) models have emerged as a powerful paradigm in Embodied AI. However, the significant computational overhead of processing redundant visual tokens remains a critical bottleneck for real-time robotic deployment. While standard token pruning techniques can alleviate this, these task-agnostic methods struggle to preserve task-critical visual information. To address this challenge, simultaneously preserving both the holistic context and fine-grained details for precise action, we propose Compressor-VLA, a novel hybrid instruction-conditioned token compression framework designed for efficient, task-oriented compression of visual information in VLA models. The proposed Compressor-VLA framework consists of two token compression modules: a Semantic Task Compressor (STC) that distills holistic, task-relevant context, and a Spatial Refinement Compressor (SRC) that preserves fine-grained spatial details. This compression is dynamically modulated by the natural language instruction, allowing for the adaptive condensation of task-relevant visual information. Experimentally, extensive evaluations demonstrate that Compressor-VLA achieves a competitive success rate on the LIBERO benchmark while reducing FLOPs by 59% and the visual token count by over 3x compared to its baseline. The real-robot deployments on a dual-arm robot platform validate the model's sim-to-real transferability and practical applicability. Moreover, qualitative analyses reveal that our instruction guidance dynamically steers the model's perceptual focus toward task-relevant objects, thereby validating the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86Compressor-VLA\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u4efb\u52a1\u538b\u7f29\u5668\u548c\u7a7a\u95f4\u7ec6\u5316\u538b\u7f29\u5668\u5b9e\u73b0\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u7684\u9ad8\u6548\u4efb\u52a1\u5bfc\u5411\u538b\u7f29\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5728\u5177\u8eabAI\u4e2d\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u73b0\u6709\u4efb\u52a1\u65e0\u5173\u7684token\u526a\u679d\u65b9\u6cd5\u96be\u4ee5\u4fdd\u7559\u4efb\u52a1\u5173\u952e\u89c6\u89c9\u4fe1\u606f\uff0c\u9700\u8981\u540c\u65f6\u4fdd\u6301\u6574\u4f53\u4e0a\u4e0b\u6587\u548c\u7ec6\u7c92\u5ea6\u7ec6\u8282\u4ee5\u5b9e\u73b0\u7cbe\u786e\u52a8\u4f5c\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6307\u4ee4\u6761\u4ef6token\u538b\u7f29\u6846\u67b6\uff0c\u5305\u542b\u8bed\u4e49\u4efb\u52a1\u538b\u7f29\u5668\uff08\u63d0\u53d6\u6574\u4f53\u4efb\u52a1\u76f8\u5173\u4e0a\u4e0b\u6587\uff09\u548c\u7a7a\u95f4\u7ec6\u5316\u538b\u7f29\u5668\uff08\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u7ec6\u8282\uff09\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u52a8\u6001\u8c03\u8282\u538b\u7f29\u8fc7\u7a0b\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u7ade\u4e89\u6027\u6210\u529f\u7387\uff0cFLOPs\u51cf\u5c1159%\uff0c\u89c6\u89c9token\u6570\u91cf\u51cf\u5c113\u500d\u4ee5\u4e0a\uff0c\u771f\u5b9e\u673a\u5668\u4eba\u90e8\u7f72\u9a8c\u8bc1\u4e86\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "Compressor-VLA\u6846\u67b6\u901a\u8fc7\u6307\u4ee4\u5f15\u5bfc\u52a8\u6001\u8c03\u6574\u611f\u77e5\u7126\u70b9\u5230\u4efb\u52a1\u76f8\u5173\u5bf9\u8c61\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4efb\u52a1\u5bfc\u5411\u89c6\u89c9\u4fe1\u606f\u538b\u7f29\uff0c\u4e3a\u5b9e\u65f6\u673a\u5668\u4eba\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.18808", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18808", "abs": "https://arxiv.org/abs/2511.18808", "authors": ["Cao Linxiao", "Wang Ruitao", "Li Jindong", "Zhou Zhipeng", "Yang Menglin"], "title": "HyperbolicRAG: Enhancing Retrieval-Augmented Generation with Hyperbolic Representations", "comment": "12 pages", "summary": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to access external knowledge, helping mitigate hallucinations and enhance domain-specific expertise. Graph-based RAG enhances structural reasoning by introducing explicit relational organization that enables information propagation across semantically connected text units. However, these methods typically rely on Euclidean embeddings that capture semantic similarity but lack a geometric notion of hierarchical depth, limiting their ability to represent abstraction relationships inherent in complex knowledge graphs. To capture both fine-grained semantics and global hierarchy, we propose HyperbolicRAG, a retrieval framework that integrates hyperbolic geometry into graph-based RAG. HyperbolicRAG introduces three key designs: (1) a depth-aware representation learner that embeds nodes within a shared Poincare manifold to align semantic similarity with hierarchical containment, (2) an unsupervised contrastive regularization that enforces geometric consistency across abstraction levels, and (3) a mutual-ranking fusion mechanism that jointly exploits retrieval signals from Euclidean and hyperbolic spaces, emphasizing cross-space agreement during inference. Extensive experiments across multiple QA benchmarks demonstrate that HyperbolicRAG outperforms competitive baselines, including both standard RAG and graph-augmented baselines.", "AI": {"tldr": "HyperbolicRAG\u662f\u4e00\u4e2a\u5c06\u53cc\u66f2\u51e0\u4f55\u878d\u5165\u57fa\u4e8e\u56fe\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u66f2\u5d4c\u5165\u6355\u6349\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u5c42\u6b21\u7ed3\u6784\u5173\u7cfb\uff0c\u63d0\u5347\u8bed\u4e49\u68c0\u7d22\u6548\u679c", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684RAG\u65b9\u6cd5\u4f9d\u8d56\u6b27\u51e0\u91cc\u5f97\u5d4c\u5165\uff0c\u867d\u7136\u80fd\u6355\u6349\u8bed\u4e49\u76f8\u4f3c\u6027\u4f46\u7f3a\u4e4f\u5bf9\u5c42\u6b21\u6df1\u5ea6\u5173\u7cfb\u7684\u51e0\u4f55\u8868\u793a\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u77e5\u8bc6\u56fe\u8c31\u4e2d\u8868\u793a\u62bd\u8c61\u5173\u7cfb\u7684\u80fd\u529b", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u8bbe\u8ba1\uff1a1) \u5728\u5171\u4eab\u5e9e\u52a0\u83b1\u6d41\u5f62\u4e2d\u7684\u6df1\u5ea6\u611f\u77e5\u8868\u793a\u5b66\u4e60\u5668\uff1b2) \u8de8\u62bd\u8c61\u5c42\u6b21\u5f3a\u5236\u51e0\u4f55\u4e00\u81f4\u6027\u7684\u65e0\u76d1\u7763\u5bf9\u6bd4\u6b63\u5219\u5316\uff1b3) \u8054\u5408\u5229\u7528\u6b27\u51e0\u91cc\u5f97\u548c\u53cc\u66f2\u7a7a\u95f4\u68c0\u7d22\u4fe1\u53f7\u7684\u4e92\u6392\u540d\u878d\u5408\u673a\u5236", "result": "\u5728\u591a\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cHyperbolicRAG\u4f18\u4e8e\u5305\u62ec\u6807\u51c6RAG\u548c\u56fe\u589e\u5f3a\u57fa\u7ebf\u5728\u5185\u7684\u7ade\u4e89\u57fa\u7ebf", "conclusion": "\u53cc\u66f2\u51e0\u4f55\u80fd\u591f\u6709\u6548\u589e\u5f3a\u56fe\u57faRAG\u7684\u8868\u793a\u80fd\u529b\uff0c\u901a\u8fc7\u540c\u65f6\u6355\u6349\u7ec6\u7c92\u5ea6\u8bed\u4e49\u548c\u5168\u5c40\u5c42\u6b21\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd"}}
{"id": "2511.19011", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19011", "abs": "https://arxiv.org/abs/2511.19011", "authors": ["Jiale Zhang", "Yeqiang Qian", "Tong Qin", "Mingyang Jiang", "Siyuan Chen", "Ming Yang"], "title": "End-to-end Autonomous Vehicle Following System using Monocular Fisheye Camera", "comment": null, "summary": "The increase in vehicle ownership has led to increased traffic congestion, more accidents, and higher carbon emissions. Vehicle platooning is a promising solution to address these issues by improving road capacity and reducing fuel consumption. However, existing platooning systems face challenges such as reliance on lane markings and expensive high-precision sensors, which limits their general applicability. To address these issues, we propose a vehicle following framework that expands its capability from restricted scenarios to general scenario applications using only a camera. This is achieved through our newly proposed end-to-end method, which improves overall driving performance. The method incorporates a semantic mask to address causal confusion in multi-frame data fusion. Additionally, we introduce a dynamic sampling mechanism to precisely track the trajectories of preceding vehicles. Extensive closed-loop validation in real-world vehicle experiments demonstrates the system's ability to follow vehicles in various scenarios, outperforming traditional multi-stage algorithms. This makes it a promising solution for cost-effective autonomous vehicle platooning. A complete real-world vehicle experiment is available at https://youtu.be/zL1bcVb9kqQ.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f7f\u7528\u6444\u50cf\u5934\u7684\u8f66\u8f86\u8ddf\u968f\u6846\u67b6\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u65b9\u6cd5\u63d0\u5347\u9a7e\u9a76\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7f16\u961f\u7cfb\u7edf\u5bf9\u8f66\u9053\u7ebf\u548c\u6602\u8d35\u4f20\u611f\u5668\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "\u8f66\u8f86\u4fdd\u6709\u91cf\u589e\u52a0\u5bfc\u81f4\u4ea4\u901a\u62e5\u5835\u3001\u4e8b\u6545\u548c\u78b3\u6392\u653e\u95ee\u9898\u52a0\u5267\uff0c\u8f66\u8f86\u7f16\u961f\u662f\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u6709\u524d\u666f\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u8f66\u9053\u7ebf\u548c\u6602\u8d35\u4f20\u611f\u5668\uff0c\u9650\u5236\u4e86\u901a\u7528\u6027\u3002", "method": "\u4f7f\u7528\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u7ed3\u5408\u8bed\u4e49\u63a9\u7801\u89e3\u51b3\u591a\u5e27\u6570\u636e\u878d\u5408\u4e2d\u7684\u56e0\u679c\u6df7\u6dc6\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u91c7\u6837\u673a\u5236\u7cbe\u786e\u8ddf\u8e2a\u524d\u8f66\u8f68\u8ff9\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u8f66\u8f86\u5b9e\u9a8c\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u95ed\u73af\u9a8c\u8bc1\uff0c\u8bc1\u660e\u7cfb\u7edf\u80fd\u5728\u5404\u79cd\u573a\u666f\u4e0b\u8ddf\u968f\u8f66\u8f86\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u591a\u9636\u6bb5\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u662f\u6210\u672c\u6548\u76ca\u9ad8\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7f16\u961f\u7684\u6709\u524d\u666f\u89e3\u51b3\u65b9\u6848\uff0c\u4ec5\u9700\u6444\u50cf\u5934\u5373\u53ef\u5b9e\u73b0\u901a\u7528\u573a\u666f\u5e94\u7528\u3002"}}
{"id": "2511.18832", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18832", "abs": "https://arxiv.org/abs/2511.18832", "authors": ["Kaize Shi", "Xueyao Sun", "Xiaohui Tao", "Lin Li", "Qika Lin", "Guandong Xu"], "title": "Concept than Document: Context Compression via AMR-based Conceptual Entropy", "comment": null, "summary": "Large Language Models (LLMs) face information overload when handling long contexts, particularly in Retrieval-Augmented Generation (RAG) where extensive supporting documents often introduce redundant content. This issue not only weakens reasoning accuracy but also increases computational overhead. We propose an unsupervised context compression framework that exploits Abstract Meaning Representation (AMR) graphs to preserve semantically essential information while filtering out irrelevant text. By quantifying node-level entropy within AMR graphs, our method estimates the conceptual importance of each node, enabling the retention of core semantics. Specifically, we construct AMR graphs from raw contexts, compute the conceptual entropy of each node, and screen significant informative nodes to form a condensed and semantically focused context than raw documents. Experiments on the PopQA and EntityQuestions datasets show that our method outperforms vanilla and other baselines, achieving higher accuracy while substantially reducing context length. To the best of our knowledge, this is the first work introducing AMR-based conceptual entropy for context compression, demonstrating the potential of stable linguistic features in context engineering.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u62bd\u8c61\u610f\u4e49\u8868\u793a(AMR)\u56fe\u7684\u65e0\u76d1\u7763\u4e0a\u4e0b\u6587\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u8282\u70b9\u7ea7\u71b5\u91cf\u5316\u6982\u5ff5\u91cd\u8981\u6027\uff0c\u4fdd\u7559\u6838\u5fc3\u8bed\u4e49\u540c\u65f6\u8fc7\u6ee4\u5197\u4f59\u5185\u5bb9\uff0c\u5728\u95ee\u7b54\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u51c6\u786e\u7387\u548c\u66f4\u77ed\u4e0a\u4e0b\u6587\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u9762\u4e34\u4fe1\u606f\u8fc7\u8f7d\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\uff0c\u5927\u91cf\u652f\u6301\u6587\u6863\u5e38\u5305\u542b\u5197\u4f59\u5185\u5bb9\uff0c\u8fd9\u4f1a\u964d\u4f4e\u63a8\u7406\u51c6\u786e\u6027\u5e76\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u6784\u5efaAMR\u56fe\uff0c\u8ba1\u7b97\u8282\u70b9\u6982\u5ff5\u71b5\u6765\u4f30\u8ba1\u6bcf\u4e2a\u8282\u70b9\u7684\u6982\u5ff5\u91cd\u8981\u6027\uff0c\u7b5b\u9009\u91cd\u8981\u4fe1\u606f\u8282\u70b9\u5f62\u6210\u538b\u7f29\u7684\u8bed\u4e49\u805a\u7126\u4e0a\u4e0b\u6587\u3002", "result": "\u5728PopQA\u548cEntityQuestions\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u5728\u663e\u8457\u51cf\u5c11\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u540c\u65f6\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u5c06\u57fa\u4e8eAMR\u7684\u6982\u5ff5\u71b5\u5f15\u5165\u4e0a\u4e0b\u6587\u538b\u7f29\u7684\u5de5\u4f5c\uff0c\u5c55\u793a\u4e86\u7a33\u5b9a\u8bed\u8a00\u7279\u5f81\u5728\u4e0a\u4e0b\u6587\u5de5\u7a0b\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.19031", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19031", "abs": "https://arxiv.org/abs/2511.19031", "authors": ["Haihang Wu", "Yuchen Zhou"], "title": "Multi-Agent Monocular Dense SLAM With 3D Reconstruction Priors", "comment": null, "summary": "Monocular Simultaneous Localization and Mapping (SLAM) aims to estimate a robot's pose while simultaneously reconstructing an unknown 3D scene using a single camera. While existing monocular SLAM systems generate detailed 3D geometry through dense scene representations, they are computationally expensive due to the need for iterative optimization. To address this challenge, MASt3R-SLAM utilizes learned 3D reconstruction priors, enabling more efficient and accurate estimation of both 3D structures and camera poses. However, MASt3R-SLAM is limited to single-agent operation. In this paper, we extend MASt3R-SLAM to introduce the first multi-agent monocular dense SLAM system. Each agent performs local SLAM using a 3D reconstruction prior, and their individual maps are fused into a globally consistent map through a loop-closure-based map fusion mechanism. Our approach improves computational efficiency compared to state-of-the-art methods, while maintaining similar mapping accuracy when evaluated on real-world datasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u591a\u667a\u80fd\u4f53\u5355\u76ee\u7a20\u5bc6SLAM\u7cfb\u7edf\uff0c\u901a\u8fc73D\u91cd\u5efa\u5148\u9a8c\u548c\u57fa\u4e8e\u95ed\u73af\u7684\u5730\u56fe\u878d\u5408\u673a\u5236\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u76eeSLAM\u7cfb\u7edf\u867d\u7136\u80fd\u751f\u6210\u8be6\u7ec6\u76843D\u51e0\u4f55\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u4e14MASt3R-SLAM\u4ec5\u9650\u4e8e\u5355\u667a\u80fd\u4f53\u64cd\u4f5c\u3002\u9700\u8981\u6269\u5c55\u5230\u591a\u667a\u80fd\u4f53\u573a\u666f\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u534f\u4f5c\u5efa\u56fe\u3002", "method": "\u6bcf\u4e2a\u667a\u80fd\u4f53\u4f7f\u75283D\u91cd\u5efa\u5148\u9a8c\u8fdb\u884c\u5c40\u90e8SLAM\uff0c\u901a\u8fc7\u57fa\u4e8e\u95ed\u73af\u7684\u5730\u56fe\u878d\u5408\u673a\u5236\u5c06\u4e2a\u4f53\u5730\u56fe\u878d\u5408\u6210\u5168\u5c40\u4e00\u81f4\u7684\u5730\u56fe\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u76f8\u4f3c\u5efa\u56fe\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u6210\u529f\u5b9e\u73b0\u4e86\u9996\u4e2a\u591a\u667a\u80fd\u4f53\u5355\u76ee\u7a20\u5bc6SLAM\u7cfb\u7edf\uff0c\u8bc1\u660e\u4e863D\u91cd\u5efa\u5148\u9a8c\u548c\u5730\u56fe\u878d\u5408\u673a\u5236\u5728\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5efa\u56fe\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.18843", "categories": ["cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.18843", "abs": "https://arxiv.org/abs/2511.18843", "authors": ["Heger Arfaoui", "Mohammed Iheb Hergli", "Beya Benzina", "Slimane BenMiled"], "title": "A Reproducible Framework for Neural Topic Modeling in Focus Group Analysis", "comment": null, "summary": "Focus group discussions generate rich qualitative data but their analysis traditionally relies on labor-intensive manual coding that limits scalability and reproducibility. We present a rigorous, reproducible computational framework for applying neural topic modeling to focus group transcripts, addressing fundamental methodological challenges: hyperparameter sensitivity, model stability, and validation of interpretability. Using BERTopic applied to ten focus groups exploring HPV vaccine perceptions in Tunisia (1,076 utterances), we conducted systematic evaluation across 27 hyperparameter configurations, assessed stability through bootstrap resampling with 30 replicates per configuration, and validated interpretability through formal human evaluation by three domain experts. Our analysis demonstrates substantial sensitivity to hyperparameter choices and reveals that metric selection for stability assessment must align with analytical goals. A hierarchical merging strategy (extracting fine-grained topics for stability then consolidating for interpretability) effectively navigates the stability-coherence tradeoff, achieving coherence of 0.558 compared to 0.539 for direct extraction. Human validation confirmed topic quality with very good inter-rater reliability (ICC = 0.79, weighted Cohen's kappa = 0.578). Our framework provides practical guidelines that researchers can adapt to their own qualitative research contexts. All code, data processing scripts, and evaluation protocols are publicly available to support reproduction and extension of this work.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u7126\u70b9\u5c0f\u7ec4\u8ba8\u8bba\u6587\u672c\u7684\u8ba1\u7b97\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7BERTopic\u4e3b\u9898\u5efa\u6a21\u89e3\u51b3\u8d85\u53c2\u6570\u654f\u611f\u6027\u3001\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u53ef\u89e3\u91ca\u6027\u9a8c\u8bc1\u7b49\u6311\u6218\uff0c\u5728\u7a81\u5c3c\u65afHPV\u75ab\u82d7\u8ba4\u77e5\u7814\u7a76\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u7126\u70b9\u5c0f\u7ec4\u6570\u636e\u5206\u6790\u4f9d\u8d56\u4eba\u5de5\u7f16\u7801\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002\u9700\u8981\u5f00\u53d1\u53ef\u91cd\u590d\u7684\u8ba1\u7b97\u6846\u67b6\u6765\u5904\u7406\u5b9a\u6027\u6587\u672c\u6570\u636e\u3002", "method": "\u4f7f\u7528BERTopic\u5bf910\u4e2a\u7126\u70b9\u5c0f\u7ec4\u76841076\u6761\u8bdd\u8bed\u8fdb\u884c\u4e3b\u9898\u5efa\u6a21\uff0c\u7cfb\u7edf\u8bc4\u4f3027\u79cd\u8d85\u53c2\u6570\u914d\u7f6e\uff0c\u901a\u8fc730\u6b21bootstrap\u91cd\u91c7\u6837\u8bc4\u4f30\u7a33\u5b9a\u6027\uff0c\u5e76\u75313\u540d\u9886\u57df\u4e13\u5bb6\u8fdb\u884c\u4eba\u5de5\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u4e3b\u9898\u5efa\u6a21\u5bf9\u8d85\u53c2\u6570\u9009\u62e9\u654f\u611f\uff0c\u5206\u5c42\u5408\u5e76\u7b56\u7565\u5728\u7a33\u5b9a\u6027-\u8fde\u8d2f\u6027\u6743\u8861\u4e2d\u8868\u73b0\u826f\u597d\uff08\u8fde\u8d2f\u60270.558 vs 0.539\uff09\uff0c\u4eba\u5de5\u9a8c\u8bc1\u663e\u793a\u826f\u597d\u8bc4\u5206\u8005\u95f4\u4fe1\u5ea6\uff08ICC=0.79\uff0c\u52a0\u6743Cohen's kappa=0.578\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9a\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\uff0c\u6240\u6709\u4ee3\u7801\u3001\u6570\u636e\u5904\u7406\u811a\u672c\u548c\u8bc4\u4f30\u534f\u8bae\u90fd\u5df2\u516c\u5f00\uff0c\u652f\u6301\u7814\u7a76\u7684\u590d\u73b0\u548c\u6269\u5c55\u3002"}}
{"id": "2511.19094", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19094", "abs": "https://arxiv.org/abs/2511.19094", "authors": ["David Bricher", "Andreas Mueller"], "title": "Analysis of Deep-Learning Methods in an ISO/TS 15066-Compliant Human-Robot Safety Framework", "comment": "MDPI Sensors, published 22 November 2025", "summary": "Over the last years collaborative robots have gained great success in manufacturing applications where human and robot work together in close proximity. However, current ISO/TS-15066-compliant implementations often limit the efficiency of collaborative tasks due to conservative speed restrictions. For this reason, this paper introduces a deep-learning-based human-robot-safety framework (HRSF) that aims at a dynamical adaptation of robot velocities depending on the separation distance between human and robot while respecting maximum biomechanical force and pressure limits. The applicability of the framework was investigated for four different deep learning approaches that can be used for human body extraction: human body recognition, human body segmentation, human pose estimation, and human body part segmentation. Unlike conventional industrial safety systems, the proposed HRSF differentiates individual human body parts from other objects, enabling optimized robot process execution. Experiments demonstrated a quantitative reduction in cycle time of up to 15% compared to conventional safety technology.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4eba\u673a\u5b89\u5168\u6846\u67b6(HRSF)\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u901f\u5ea6\u6765\u63d0\u5347\u534f\u4f5c\u6548\u7387\uff0c\u76f8\u6bd4\u4f20\u7edf\u5b89\u5168\u6280\u672f\u53ef\u51cf\u5c1115%\u7684\u5468\u671f\u65f6\u95f4\u3002", "motivation": "\u5f53\u524d\u7b26\u5408ISO/TS-15066\u6807\u51c6\u7684\u534f\u4f5c\u673a\u5668\u4eba\u5b9e\u73b0\u7531\u4e8e\u4fdd\u5b88\u7684\u901f\u5ea6\u9650\u5236\uff0c\u9650\u5236\u4e86\u534f\u4f5c\u4efb\u52a1\u7684\u6548\u7387\u3002", "method": "\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u56db\u79cd\u4eba\u4f53\u63d0\u53d6\u65b9\u6cd5\uff08\u4eba\u4f53\u8bc6\u522b\u3001\u4eba\u4f53\u5206\u5272\u3001\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u3001\u4eba\u4f53\u90e8\u4f4d\u5206\u5272\uff09\u6765\u52a8\u6001\u9002\u5e94\u673a\u5668\u4eba\u901f\u5ea6\uff0c\u540c\u65f6\u9075\u5b88\u6700\u5927\u751f\u7269\u529b\u5b66\u529b\u548c\u538b\u529b\u9650\u5236\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u76f8\u6bd4\u4f20\u7edf\u5b89\u5168\u6280\u672f\uff0c\u5468\u671f\u65f6\u95f4\u6700\u591a\u53ef\u51cf\u5c1115%\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u533a\u5206\u4eba\u4f53\u90e8\u4f4d\u4e0e\u5176\u4ed6\u7269\u4f53\uff0c\u5b9e\u73b0\u4f18\u5316\u7684\u673a\u5668\u4eba\u6d41\u7a0b\u6267\u884c\uff0c\u63d0\u9ad8\u534f\u4f5c\u6548\u7387\u3002"}}
{"id": "2511.18848", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18848", "abs": "https://arxiv.org/abs/2511.18848", "authors": ["V\u00e1clav Tran", "Jakub \u0160m\u00edd", "Ladislav Lenc", "Jean-Pierre Salmon", "Pavel Kr\u00e1l"], "title": "Large Language Models for the Summarization of Czech Documents: From History to the Present", "comment": null, "summary": "Text summarization is the task of automatically condensing longer texts into shorter, coherent summaries while preserving the original meaning and key information. Although this task has been extensively studied in English and other high-resource languages, Czech summarization, particularly in the context of historical documents, remains underexplored. This is largely due to the inherent linguistic complexity of Czech and the lack of high-quality annotated datasets.\n  In this work, we address this gap by leveraging the capabilities of Large Language Models (LLMs), specifically Mistral and mT5, which have demonstrated strong performance across a wide range of natural language processing tasks and multilingual settings. In addition, we also propose a translation-based approach that first translates Czech texts into English, summarizes them using an English-language model, and then translates the summaries back into Czech. Our study makes the following main contributions: We demonstrate that LLMs achieve new state-of-the-art results on the SumeCzech dataset, a benchmark for modern Czech text summarization, showing the effectiveness of multilingual LLMs even for morphologically rich, medium-resource languages like Czech. We introduce a new dataset, Posel od \u010cerchova, designed for the summarization of historical Czech texts. This dataset is derived from digitized 19th-century publications and annotated for abstractive summarization. We provide initial baselines using modern LLMs to facilitate further research in this underrepresented area.\n  By combining cutting-edge models with both modern and historical Czech datasets, our work lays the foundation for further progress in Czech summarization and contributes valuable resources for future research in Czech historical document processing and low-resource summarization more broadly.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Mistral\u548cmT5\uff09\u548c\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u6377\u514b\u8bed\u6587\u672c\u6458\u8981\u7684\u7a7a\u767d\uff0c\u7279\u522b\u662f\u5728\u5386\u53f2\u6587\u732e\u9886\u57df\uff0c\u5e76\u521b\u5efa\u4e86\u65b0\u7684\u5386\u53f2\u6377\u514b\u8bed\u6458\u8981\u6570\u636e\u96c6\u3002", "motivation": "\u6377\u514b\u8bed\u6458\u8981\uff0c\u7279\u522b\u662f\u5386\u53f2\u6587\u732e\u6458\u8981\uff0c\u7531\u4e8e\u8bed\u8a00\u590d\u6742\u6027\u548c\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u800c\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08Mistral\u548cmT5\uff09\u8fdb\u884c\u76f4\u63a5\u6458\u8981\uff0c\u4ee5\u53ca\u7ffb\u8bd1\u65b9\u6cd5\uff1a\u5148\u5c06\u6377\u514b\u8bed\u6587\u672c\u7ffb\u8bd1\u6210\u82f1\u8bed\uff0c\u7528\u82f1\u8bed\u6a21\u578b\u6458\u8981\uff0c\u518d\u7ffb\u8bd1\u56de\u6377\u514b\u8bed\u3002", "result": "\u5728SumeCzech\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u521b\u5efa\u4e86\u65b0\u7684\u5386\u53f2\u6377\u514b\u8bed\u6458\u8981\u6570\u636e\u96c6Posel od \u010cerchova\uff0c\u5e76\u4e3a\u8be5\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u6377\u514b\u8bed\u6458\u8981\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u6377\u514b\u5386\u53f2\u6587\u732e\u5904\u7406\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u6458\u8981\u63d0\u4f9b\u4e86\u5b9d\u8d35\u8d44\u6e90\u3002"}}
{"id": "2511.19135", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19135", "abs": "https://arxiv.org/abs/2511.19135", "authors": ["Pascal Goldschmid", "Aamir Ahmad"], "title": "Autonomous Docking of Multi-Rotor UAVs on Blimps under the Influence of Wind Gusts", "comment": "13 pages, 8 figures, 8 tables", "summary": "Multi-rotor UAVs face limited flight time due to battery constraints. Autonomous docking on blimps with onboard battery recharging and data offloading offers a promising solution for extended UAV missions. However, the vulnerability of blimps to wind gusts causes trajectory deviations, requiring precise, obstacle-aware docking strategies. To this end, this work introduces two key novelties: (i) a temporal convolutional network that predicts blimp responses to wind gusts, enabling rapid gust detection and estimation of points where the wind gust effect has subsided; (ii) a model predictive controller (MPC) that leverages these predictions to compute collision-free trajectories for docking, enabled by a novel obstacle avoidance method for close-range manoeuvres near the blimp. Simulation results show our method outperforms a baseline constant-velocity model of the blimp significantly across different scenarios. We further validate the approach in real-world experiments, demonstrating the first autonomous multi-rotor docking control strategy on blimps shown outside simulation. Source code is available here https://github.com/robot-perception-group/multi_rotor_airship_docking.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u98de\u8247\u4e0a\u81ea\u4e3b\u5bf9\u63a5\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\u9884\u6d4b\u98de\u8247\u5bf9\u98ce\u9635\u98ce\u7684\u54cd\u5e94\uff0c\u5e76\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u5b9e\u73b0\u907f\u969c\u5bf9\u63a5\u3002", "motivation": "\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\u53d7\u9650\u4e8e\u7535\u6c60\u7eed\u822a\u65f6\u95f4\uff0c\u901a\u8fc7\u5728\u98de\u8247\u4e0a\u8fdb\u884c\u81ea\u4e3b\u5bf9\u63a5\u53ef\u4ee5\u5b9e\u73b0\u7535\u6c60\u5145\u7535\u548c\u6570\u636e\u4f20\u8f93\uff0c\u4ece\u800c\u5ef6\u957f\u4efb\u52a1\u65f6\u95f4\u3002\u4f46\u98de\u8247\u6613\u53d7\u98ce\u9635\u98ce\u5f71\u54cd\u5bfc\u81f4\u8f68\u8ff9\u504f\u5dee\uff0c\u9700\u8981\u7cbe\u786e\u7684\u907f\u969c\u5bf9\u63a5\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u65f6\u95f4\u5377\u79ef\u7f51\u7edc\u9884\u6d4b\u98de\u8247\u5bf9\u98ce\u9635\u98ce\u7684\u54cd\u5e94\uff0c\u5feb\u901f\u68c0\u6d4b\u98ce\u9635\u98ce\u5e76\u4f30\u8ba1\u98ce\u6548\u5e94\u6d88\u9000\u70b9\uff1b\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u8ba1\u7b97\u65e0\u78b0\u649e\u5bf9\u63a5\u8f68\u8ff9\uff0c\u91c7\u7528\u65b0\u578b\u8fd1\u8ddd\u79bb\u907f\u969c\u65b9\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6052\u5b9a\u901f\u5ea6\u6a21\u578b\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u9996\u4e2a\u5728\u98de\u8247\u4e0a\u7684\u81ea\u4e3b\u591a\u65cb\u7ffc\u5bf9\u63a5\u63a7\u5236\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u98de\u8247\u4e0a\u7684\u81ea\u4e3b\u5bf9\u63a5\uff0c\u89e3\u51b3\u4e86\u98ce\u9635\u98ce\u5f71\u54cd\u4e0b\u7684\u7cbe\u786e\u5bf9\u63a5\u95ee\u9898\uff0c\u4e3a\u5ef6\u957f\u65e0\u4eba\u673a\u4efb\u52a1\u65f6\u95f4\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2511.18850", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18850", "abs": "https://arxiv.org/abs/2511.18850", "authors": ["Fengyuan Liu", "Huang Yi", "Sichun Luo", "Yuqi Wang", "Yazheng Yang", "Xinye Li", "Zefa Hu", "Junlan Feng", "Qi Liu"], "title": "Cognitive Alpha Mining via LLM-Driven Code-Based Evolution", "comment": null, "summary": "Discovering effective predictive signals, or ``alphas,'' from financial data with high dimensionality and extremely low signal-to-noise ratio remains a difficult open problem. Despite progress in deep learning, genetic programming, and, more recently, large language model (LLM)--based factor generation, existing approaches still explore only a narrow region of the vast alpha search space. Neural models tend to produce opaque and fragile patterns, while symbolic or formula-based methods often yield redundant or economically ungrounded expressions that generalize poorly. Although different in form, these paradigms share a key limitation: none can conduct broad, structured, and human-like exploration that balances logical consistency with creative leaps. To address this gap, we introduce the Cognitive Alpha Mining Framework (CogAlpha), which combines code-level alpha representation with LLM-driven reasoning and evolutionary search. Treating LLMs as adaptive cognitive agents, our framework iteratively refines, mutates, and recombines alpha candidates through multi-stage prompts and financial feedback. This synergistic design enables deeper thinking, richer structural diversity, and economically interpretable alpha discovery, while greatly expanding the effective search space. Experiments on A-share equities demonstrate that CogAlpha consistently discovers alphas with superior predictive accuracy, robustness, and generalization over existing methods. Our results highlight the promise of aligning evolutionary optimization with LLM-based reasoning for automated and explainable alpha discovery. All source code will be released.", "AI": {"tldr": "\u63d0\u51fa\u4e86CogAlpha\u6846\u67b6\uff0c\u7ed3\u5408\u4ee3\u7801\u7ea7alpha\u8868\u793a\u3001LLM\u9a71\u52a8\u63a8\u7406\u548c\u8fdb\u5316\u641c\u7d22\uff0c\u7528\u4e8e\u4ece\u9ad8\u7ef4\u91d1\u878d\u6570\u636e\u4e2d\u53d1\u73b0\u6709\u6548\u7684\u9884\u6d4b\u4fe1\u53f7\uff08alpha\u56e0\u5b50\uff09\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u6df1\u5ea6\u5b66\u4e60\u3001\u9057\u4f20\u7f16\u7a0b\u3001LLM\u56e0\u5b50\u751f\u6210\uff09\u5728\u5e9e\u5927\u7684alpha\u641c\u7d22\u7a7a\u95f4\u4e2d\u63a2\u7d22\u8303\u56f4\u6709\u9650\uff0c\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4ea7\u751f\u4e0d\u900f\u660e\u548c\u8106\u5f31\u7684\u6a21\u5f0f\uff0c\u7b26\u53f7\u65b9\u6cd5\u4ea7\u751f\u5197\u4f59\u6216\u7ecf\u6d4e\u610f\u4e49\u4e0d\u8db3\u7684\u8868\u8fbe\u5f0f\uff0c\u7f3a\u4e4f\u5e73\u8861\u903b\u8f91\u4e00\u81f4\u6027\u548c\u521b\u9020\u6027\u7684\u4eba\u7c7b\u5f0f\u63a2\u7d22\u3002", "method": "\u5c06LLM\u4f5c\u4e3a\u81ea\u9002\u5e94\u8ba4\u77e5\u4ee3\u7406\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u63d0\u793a\u548c\u91d1\u878d\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u3001\u53d8\u5f02\u548c\u91cd\u7ec4alpha\u5019\u9009\uff0c\u7ed3\u5408\u4ee3\u7801\u7ea7alpha\u8868\u793a\u4e0e\u8fdb\u5316\u641c\u7d22\u3002", "result": "\u5728A\u80a1\u80a1\u7968\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCogAlpha\u80fd\u591f\u6301\u7eed\u53d1\u73b0\u5177\u6709\u66f4\u4f18\u9884\u6d4b\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u7684alpha\u56e0\u5b50\u3002", "conclusion": "\u5c06\u8fdb\u5316\u4f18\u5316\u4e0e\u57fa\u4e8eLLM\u7684\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u6709\u671b\u5b9e\u73b0\u81ea\u52a8\u5316\u548c\u53ef\u89e3\u91ca\u7684alpha\u53d1\u73b0\u3002"}}
{"id": "2511.19201", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19201", "abs": "https://arxiv.org/abs/2511.19201", "authors": ["Ann-Sophia M\u00fcller", "Moonkwang Jeong", "Jiyuan Tian", "Meng Zhang", "Tian Qiu"], "title": "Efficient Optimization of a Permanent Magnet Array for a Stable 2D Trap", "comment": "6 pages, 6 figures, IEEE International Conference on Robotics and Automation (ICRA)", "summary": "Untethered magnetic manipulation of biomedical millirobots has a high potential for minimally invasive surgical applications. However, it is still challenging to exert high actuation forces on the small robots over a large distance. Permanent magnets offer stronger magnetic torques and forces than electromagnetic coils, however, feedback control is more difficult. As proven by Earnshaw's theorem, it is not possible to achieve a stable magnetic trap in 3D by static permanent magnets. Here, we report a stable 2D magnetic force trap by an array of permanent magnets to control a millirobot. The trap is located in an open space with a tunable distance to the magnet array in the range of 20 - 120mm, which is relevant to human anatomical scales. The design is achieved by a novel GPU-accelerated optimization algorithm that uses mean squared error (MSE) and Adam optimizer to efficiently compute the optimal angles for any number of magnets in the array. The algorithm is verified using numerical simulation and physical experiments with an array of two magnets. A millirobot is successfully trapped and controlled to follow a complex trajectory. The algorithm demonstrates high scalability by optimizing the angles for 100 magnets in under three seconds. Moreover, the optimization workflow can be adapted to optimize a permanent magnet array to achieve the desired force vector fields.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6c38\u78c1\u4f53\u9635\u5217\u5b9e\u73b0\u7a33\u5b9a2D\u78c1\u529b\u9677\u9631\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63a7\u5236\u533b\u7597\u5fae\u578b\u673a\u5668\u4eba\uff0c\u901a\u8fc7GPU\u52a0\u901f\u4f18\u5316\u7b97\u6cd5\u8ba1\u7b97\u78c1\u4f53\u6700\u4f18\u89d2\u5ea6\uff0c\u5b9e\u73b0\u4e86\u572820-120mm\u8ddd\u79bb\u8303\u56f4\u5185\u7684\u7a33\u5b9a\u63a7\u5236\u3002", "motivation": "\u89e3\u51b3\u5728\u533b\u7597\u5e94\u7528\u4e2d\u8fdc\u8ddd\u79bb\u5bf9\u5fae\u578b\u673a\u5668\u4eba\u65bd\u52a0\u9ad8\u9a71\u52a8\u529b\u7684\u6311\u6218\uff0c\u540c\u65f6\u514b\u670d\u6c38\u78c1\u4f53\u96be\u4ee5\u5b9e\u73b0\u7a33\u5b9a3D\u78c1\u529b\u9677\u9631\u7684\u9650\u5236\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5747\u65b9\u8bef\u5dee\u548cAdam\u4f18\u5316\u5668\u7684GPU\u52a0\u901f\u4f18\u5316\u7b97\u6cd5\uff0c\u8ba1\u7b97\u6c38\u78c1\u4f53\u9635\u5217\u4e2d\u78c1\u4f53\u7684\u6700\u4f18\u89d2\u5ea6\uff0c\u5b9e\u73b02D\u78c1\u529b\u9677\u9631\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u572820-120mm\u8ddd\u79bb\u8303\u56f4\u5185\u7684\u7a33\u5b9a\u78c1\u529b\u9677\u9631\uff0c\u80fd\u591f\u63a7\u5236\u5fae\u578b\u673a\u5668\u4eba\u6cbf\u590d\u6742\u8f68\u8ff9\u8fd0\u52a8\uff0c\u7b97\u6cd5\u5177\u6709\u9ad8\u53ef\u6269\u5c55\u6027\uff0c\u53ef\u57283\u79d2\u5185\u4f18\u5316100\u4e2a\u78c1\u4f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u533b\u7597\u5fae\u578b\u673a\u5668\u4eba\u7684\u7cbe\u786e\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u5316\u6d41\u7a0b\u53ef\u9002\u5e94\u4e0d\u540c\u78c1\u529b\u573a\u9700\u6c42\u3002"}}
{"id": "2511.18852", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18852", "abs": "https://arxiv.org/abs/2511.18852", "authors": ["Masoomali Fatehkia", "Enes Altinisik", "Husrev Taha Sencar"], "title": "FanarGuard: A Culturally-Aware Moderation Filter for Arabic Language Models", "comment": null, "summary": "Content moderation filters are a critical safeguard against alignment failures in language models. Yet most existing filters focus narrowly on general safety and overlook cultural context. In this work, we introduce FanarGuard, a bilingual moderation filter that evaluates both safety and cultural alignment in Arabic and English. We construct a dataset of over 468K prompt and response pairs, drawn from synthetic and public datasets, scored by a panel of LLM judges on harmlessness and cultural awareness, and use it to train two filter variants. To rigorously evaluate cultural alignment, we further develop the first benchmark targeting Arabic cultural contexts, comprising over 1k norm-sensitive prompts with LLM-generated responses annotated by human raters. Results show that FanarGuard achieves stronger agreement with human annotations than inter-annotator reliability, while matching the performance of state-of-the-art filters on safety benchmarks. These findings highlight the importance of integrating cultural awareness into moderation and establish FanarGuard as a practical step toward more context-sensitive safeguards.", "AI": {"tldr": "FanarGuard\u662f\u4e00\u4e2a\u53cc\u8bed\u5185\u5bb9\u5ba1\u6838\u8fc7\u6ee4\u5668\uff0c\u4e13\u95e8\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u8bbe\u8ba1\uff0c\u540c\u65f6\u8bc4\u4f30\u5b89\u5168\u6027\u548c\u6587\u5316\u5bf9\u9f50\u6027\u3002\u5b83\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u5728\u6587\u5316\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4eba\u7c7b\u6807\u6ce8\u8005\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u5728\u5b89\u5168\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u5185\u5bb9\u5ba1\u6838\u8fc7\u6ee4\u5668\u4e3b\u8981\u5173\u6ce8\u4e00\u822c\u5b89\u5168\u6027\u800c\u5ffd\u89c6\u6587\u5316\u80cc\u666f\uff0c\u7279\u522b\u662f\u5728\u963f\u62c9\u4f2f\u8bed\u7b49\u975e\u82f1\u8bed\u8bed\u5883\u4e2d\u7f3a\u4e4f\u6587\u5316\u654f\u611f\u6027\u7684\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u5305\u542b46.8\u4e07\u6761\u63d0\u793a-\u54cd\u5e94\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u7531LLM\u8bc4\u59d4\u5728\u65e0\u5bb3\u6027\u548c\u6587\u5316\u610f\u8bc6\u65b9\u9762\u8bc4\u5206\uff0c\u8bad\u7ec3\u4e24\u79cd\u8fc7\u6ee4\u5668\u53d8\u4f53\u3002\u5f00\u53d1\u9996\u4e2a\u9488\u5bf9\u963f\u62c9\u4f2f\u6587\u5316\u80cc\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1000\u591a\u4e2a\u89c4\u8303\u654f\u611f\u63d0\u793a\u548c\u4eba\u5de5\u6807\u6ce8\u7684LLM\u751f\u6210\u54cd\u5e94\u3002", "result": "FanarGuard\u5728\u6587\u5316\u5bf9\u9f50\u65b9\u9762\u4e0e\u4eba\u7c7b\u6807\u6ce8\u7684\u4e00\u81f4\u6027\u4f18\u4e8e\u6807\u6ce8\u8005\u95f4\u53ef\u9760\u6027\uff0c\u5728\u5b89\u5168\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u6700\u5148\u8fdb\u8fc7\u6ee4\u5668\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u5c06\u6587\u5316\u610f\u8bc6\u6574\u5408\u5230\u5185\u5bb9\u5ba1\u6838\u4e2d\u7684\u91cd\u8981\u6027\uff0cFanarGuard\u662f\u5b9e\u73b0\u66f4\u4e0a\u4e0b\u6587\u654f\u611f\u4fdd\u969c\u63aa\u65bd\u7684\u5b9e\u9645\u6b65\u9aa4\u3002"}}
{"id": "2511.19204", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2511.19204", "abs": "https://arxiv.org/abs/2511.19204", "authors": ["Fabian Schramm", "Pierre Fabre", "Nicolas Perrin-Gilbert", "Justin Carpentier"], "title": "Reference-Free Sampling-Based Model Predictive Control", "comment": null, "summary": "We present a sampling-based model predictive control (MPC) framework that enables emergent locomotion without relying on handcrafted gait patterns or predefined contact sequences. Our method discovers diverse motion patterns, ranging from trotting to galloping, robust standing policies, jumping, and handstand balancing, purely through the optimization of high-level objectives. Building on model predictive path integral (MPPI), we propose a dual-space spline parameterization that operates on position and velocity control points. Our approach enables contact-making and contact-breaking strategies that adapt automatically to task requirements, requiring only a limited number of sampled trajectories. This sample efficiency allows us to achieve real-time control on standard CPU hardware, eliminating the need for GPU acceleration typically required by other state-of-the-art MPPI methods. We validate our approach on the Go2 quadrupedal robot, demonstrating various emergent gaits and basic jumping capabilities. In simulation, we further showcase more complex behaviors, such as backflips, dynamic handstand balancing and locomotion on a Humanoid, all without requiring reference tracking or offline pre-training.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u91c7\u6837\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u65e0\u9700\u9884\u8bbe\u6b65\u6001\u6a21\u5f0f\u6216\u63a5\u89e6\u5e8f\u5217\uff0c\u901a\u8fc7\u4f18\u5316\u9ad8\u5c42\u76ee\u6807\u81ea\u52a8\u53d1\u73b0\u591a\u6837\u5316\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5305\u62ec\u5c0f\u8dd1\u3001\u75be\u8dd1\u3001\u8df3\u8dc3\u7b49\uff0c\u5b9e\u73b0\u5b9e\u65f6CPU\u63a7\u5236\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u8bbe\u8ba1\u7684\u6b65\u6001\u6a21\u5f0f\u548c\u9884\u5b9a\u4e49\u7684\u63a5\u89e6\u5e8f\u5217\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u591a\u6837\u6027\u548c\u9002\u5e94\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7eaf\u4f18\u5316\u65b9\u6cd5\u81ea\u52a8\u53d1\u73b0\u590d\u6742\u8fd0\u52a8\u884c\u4e3a\u3002", "method": "\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206(MPPI)\uff0c\u63d0\u51fa\u53cc\u7a7a\u95f4\u6837\u6761\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u5728\u4f4d\u7f6e\u548c\u901f\u5ea6\u63a7\u5236\u70b9\u4e0a\u64cd\u4f5c\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u63a5\u89e6\u7b56\u7565\uff0c\u4ec5\u9700\u6709\u9650\u91c7\u6837\u8f68\u8ff9\u5373\u53ef\u9ad8\u6548\u63a7\u5236\u3002", "result": "\u5728Go2\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u591a\u79cd\u6d8c\u73b0\u6b65\u6001\u548c\u57fa\u672c\u8df3\u8dc3\u80fd\u529b\uff0c\u4eff\u771f\u4e2d\u5c55\u793a\u4e86\u540e\u7a7a\u7ffb\u3001\u52a8\u6001\u5012\u7acb\u5e73\u8861\u7b49\u590d\u6742\u884c\u4e3a\uff0c\u65e0\u9700\u53c2\u8003\u8ddf\u8e2a\u6216\u79bb\u7ebf\u9884\u8bad\u7ec3\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65e0\u9700\u9884\u8bbe\u6a21\u5f0f\u7684\u8fd0\u52a8\u63a7\u5236\uff0c\u5728\u6807\u51c6CPU\u4e0a\u8fbe\u5230\u5b9e\u65f6\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u901a\u8fc7\u4f18\u5316\u9ad8\u5c42\u76ee\u6807\u81ea\u52a8\u53d1\u73b0\u591a\u6837\u5316\u8fd0\u52a8\u6a21\u5f0f\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.18860", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18860", "abs": "https://arxiv.org/abs/2511.18860", "authors": ["Xingyu Huang", "Fei Jiang", "Jianli Xiao"], "title": "Generating Reading Comprehension Exercises with Large Language Models for Educational Applications", "comment": null, "summary": "With the rapid development of large language models (LLMs), the applications of LLMs have grown substantially. In the education domain, LLMs demonstrate significant potential, particularly in automatic text generation, which enables the creation of intelligent and adaptive learning content. This paper proposes a new LLMs framework, which is named as Reading Comprehension Exercise Generation (RCEG). It can generate high-quality and personalized English reading comprehension exercises automatically. Firstly, RCEG uses fine-tuned LLMs to generate content candidates. Then, it uses a discriminator to select the best candidate. Finally, the quality of the generated content has been improved greatly. To evaluate the performance of RCEG, a dedicated dataset for English reading comprehension is constructed to perform the experiments, and comprehensive evaluation metrics are used to analyze the experimental results. These metrics include content diversity, factual accuracy, linguistic toxicity, and pedagogical alignment. Experimental results show that RCEG significantly improves the relevance and cognitive appropriateness of the generated exercises.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aRCEG\u7684LLM\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u4e2a\u6027\u5316\u7684\u82f1\u8bed\u9605\u8bfb\u7406\u89e3\u7ec3\u4e60\uff0c\u901a\u8fc7\u5fae\u8c03LLM\u751f\u6210\u5019\u9009\u5185\u5bb9\uff0c\u518d\u4f7f\u7528\u5224\u522b\u5668\u9009\u62e9\u6700\u4f73\u5019\u9009\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u5185\u5bb9\u7684\u8d28\u91cf\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5728\u6559\u80b2\u9886\u57df\u7279\u522b\u662f\u81ea\u52a8\u6587\u672c\u751f\u6210\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u521b\u5efa\u667a\u80fd\u548c\u81ea\u9002\u5e94\u7684\u5b66\u4e60\u5185\u5bb9\u3002", "method": "RCEG\u6846\u67b6\u9996\u5148\u4f7f\u7528\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\u5019\u9009\uff0c\u7136\u540e\u4f7f\u7528\u5224\u522b\u5668\u9009\u62e9\u6700\u4f73\u5019\u9009\uff0c\u6700\u540e\u901a\u8fc7\u8d28\u91cf\u63d0\u5347\u673a\u5236\u5927\u5e45\u6539\u5584\u751f\u6210\u5185\u5bb9\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRCEG\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u6210\u7ec3\u4e60\u7684\u76f8\u5173\u6027\u548c\u8ba4\u77e5\u9002\u5f53\u6027\uff0c\u5728\u5185\u5bb9\u591a\u6837\u6027\u3001\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u8bed\u8a00\u6bd2\u6027\u548c\u6559\u5b66\u5bf9\u9f50\u7b49\u7efc\u5408\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "RCEG\u6846\u67b6\u80fd\u591f\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u82f1\u8bed\u9605\u8bfb\u7406\u89e3\u7ec3\u4e60\uff0c\u5728\u6559\u80b2\u9886\u57df\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2511.19211", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19211", "abs": "https://arxiv.org/abs/2511.19211", "authors": ["Prabhat Kumar", "Chandra Prakash", "Josh Pinskier", "David Howard", "Matthijs Langelaar"], "title": "Soft pneumatic grippers: Topology optimization, 3D-printing and experimental validation", "comment": "9 Figures", "summary": "This paper presents a systematic topology optimization framework for designing a soft pneumatic gripper (SPG), explicitly considering the design-dependent nature of the actuating load. The load is modeled using Darcy's law with an added drainage term. A 2D soft arm unit is optimized by formulating it as a compliant mechanism design problem using the robust formulation. The problem is posed as a min-max optimization, where the output deformations of blueprint and eroded designs are considered. A volume constraint is imposed on the blueprint part, while a strain-energy constraint is enforced on the eroded part. The MMA is employed to solve the optimization problem and obtain the optimized soft unit. Finite element analysis with the Ogden material model confirms that the optimized 2D unit outperforms a conventional rectangular design under pneumatic loading. The optimized 2D unit is extruded to obtain a 3D module, and ten such units are assembled to create a soft arm. Deformation profiles of the optimized arm are analysed under different pressure loads. Four arms are 3D-printed and integrated with a supporting structure to realize the proposed SPG. The gripping performance of the SPG is demonstrated on objects with different weights, sizes, stiffness, and shapes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u8bbe\u8ba1\u4f9d\u8d56\u8f7d\u8377\u7684\u8f6f\u4f53\u6c14\u52a8\u6293\u53d6\u5668\u7cfb\u7edf\u62d3\u6251\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc72D\u8f6f\u81c2\u5355\u5143\u4f18\u5316\u548c3D\u7ec4\u88c5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e0d\u540c\u7269\u4f53\u7684\u6293\u53d6\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8f6f\u4f53\u6c14\u52a8\u6293\u53d6\u5668\u8bbe\u8ba1\u672a\u5145\u5206\u8003\u8651\u8bbe\u8ba1\u4f9d\u8d56\u7684\u9a71\u52a8\u8f7d\u8377\u7279\u6027\uff0c\u9700\u8981\u5f00\u53d1\u7cfb\u7edf\u5316\u7684\u62d3\u6251\u4f18\u5316\u65b9\u6cd5\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "\u4f7f\u7528Darcy\u5b9a\u5f8b\u5efa\u6a21\u6c14\u52a8\u8f7d\u8377\uff0c\u91c7\u7528\u7a33\u5065\u516c\u5f0f\u5c062D\u8f6f\u81c2\u5355\u5143\u8bbe\u8ba1\u8868\u8ff0\u4e3a\u67d4\u987a\u673a\u6784\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7MMA\u7b97\u6cd5\u6c42\u89e3\uff0c\u6700\u540e\u5c06\u4f18\u5316\u5355\u5143\u6324\u51fa\u4e3a3D\u6a21\u5757\u5e76\u7ec4\u88c5\u6210\u6293\u53d6\u5668\u3002", "result": "\u4f18\u5316\u76842D\u5355\u5143\u5728\u6c14\u52a8\u8f7d\u8377\u4e0b\u6027\u80fd\u4f18\u4e8e\u4f20\u7edf\u77e9\u5f62\u8bbe\u8ba1\uff0c\u7ec4\u88c5\u7684\u8f6f\u81c2\u5728\u4e0d\u540c\u538b\u529b\u4e0b\u5448\u73b0\u826f\u597d\u53d8\u5f62\u7279\u6027\uff0c3D\u6253\u5370\u7684\u6293\u53d6\u5668\u80fd\u591f\u6709\u6548\u6293\u53d6\u4e0d\u540c\u91cd\u91cf\u3001\u5c3a\u5bf8\u3001\u521a\u5ea6\u548c\u5f62\u72b6\u7684\u7269\u4f53\u3002", "conclusion": "\u63d0\u51fa\u7684\u62d3\u6251\u4f18\u5316\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u8f6f\u4f53\u6c14\u52a8\u6293\u53d6\u5668\u7684\u8bbe\u8ba1\uff0c\u9a8c\u8bc1\u4e86\u8003\u8651\u8bbe\u8ba1\u4f9d\u8d56\u8f7d\u8377\u7684\u4f18\u5316\u65b9\u6cd5\u5728\u8f6f\u4f53\u673a\u5668\u4eba\u8bbe\u8ba1\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2511.18864", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18864", "abs": "https://arxiv.org/abs/2511.18864", "authors": ["Yang Xiang", "Yixin Ji", "Juntao Li", "Min Zhang"], "title": "Think Before You Prune: Selective Self-Generated Calibration for Pruning Large Reasoning Models", "comment": "Under Review", "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex reasoning benchmarks. However, their long chain-of-thought reasoning processes incur significant inference overhead. Pruning has emerged as a promising approach to reducing computational costs. However, existing efforts have primarily focused on large language models (LLMs), while pruning LRMs remains unexplored. In this work, we conduct the first empirical study on pruning LRMs and show that directly applying existing pruning techniques fails to yield satisfactory results. Our findings indicate that using self-generated reasoning data for calibration can substantially improve pruning performance. We further investigate how the difficulty and length of reasoning data affect pruning outcomes. Our analysis reveals that challenging and moderately long self-generated reasoning data serve as ideal calibration data. Based on these insights, we propose a Selective Self-Generated Reasoning (SSGR) data construction strategy to provide effective calibration data for pruning LRMs. Experimental results on the DeepSeek-R1-Distill model series validate that our strategy improves the reasoning ability of pruned LRMs by 10%-13% compared to general pruning methods.", "AI": {"tldr": "\u9996\u6b21\u5bf9\u5927\u578b\u63a8\u7406\u6a21\u578b\u8fdb\u884c\u526a\u679d\u7814\u7a76\uff0c\u53d1\u73b0\u4f7f\u7528\u81ea\u751f\u6210\u63a8\u7406\u6570\u636e\u4f5c\u4e3a\u6821\u51c6\u6570\u636e\u80fd\u663e\u8457\u63d0\u5347\u526a\u679d\u6548\u679c\uff0c\u63d0\u51fa\u9009\u62e9\u6027\u81ea\u751f\u6210\u63a8\u7406\u6570\u636e\u6784\u5efa\u7b56\u7565\uff0c\u76f8\u6bd4\u901a\u7528\u526a\u679d\u65b9\u6cd5\u63d0\u5347\u63a8\u7406\u80fd\u529b10%-13%\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u957f\u94fe\u63a8\u7406\u8fc7\u7a0b\u5e26\u6765\u663e\u8457\u63a8\u7406\u5f00\u9500\u3002\u526a\u679d\u662f\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u7684\u6709\u524d\u666f\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u526a\u679d\u5c1a\u672a\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u9009\u62e9\u6027\u81ea\u751f\u6210\u63a8\u7406\u6570\u636e\u6784\u5efa\u7b56\u7565\uff0c\u4f7f\u7528\u5177\u6709\u6311\u6218\u6027\u548c\u4e2d\u7b49\u957f\u5ea6\u7684\u81ea\u751f\u6210\u63a8\u7406\u6570\u636e\u4f5c\u4e3a\u526a\u679d\u6821\u51c6\u6570\u636e\uff0c\u7814\u7a76\u63a8\u7406\u6570\u636e\u96be\u5ea6\u548c\u957f\u5ea6\u5bf9\u526a\u679d\u6548\u679c\u7684\u5f71\u54cd\u3002", "result": "\u5728DeepSeek-R1-Distill\u6a21\u578b\u7cfb\u5217\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7b56\u7565\u76f8\u6bd4\u901a\u7528\u526a\u679d\u65b9\u6cd5\u5c06\u526a\u679d\u540e\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u4e8610%-13%\u3002", "conclusion": "\u81ea\u751f\u6210\u63a8\u7406\u6570\u636e\u662f\u5927\u578b\u63a8\u7406\u6a21\u578b\u526a\u679d\u7684\u6709\u6548\u6821\u51c6\u6570\u636e\uff0c\u6311\u6218\u6027\u548c\u4e2d\u7b49\u957f\u5ea6\u7684\u63a8\u7406\u6570\u636e\u662f\u6700\u4f73\u9009\u62e9\uff0c\u9009\u62e9\u6027\u81ea\u751f\u6210\u63a8\u7406\u6570\u636e\u6784\u5efa\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u526a\u679d\u6548\u679c\u3002"}}
{"id": "2511.19236", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19236", "abs": "https://arxiv.org/abs/2511.19236", "authors": ["Yuxuan Wang", "Haobin Jiang", "Shiqing Yao", "Ziluo Ding", "Zongqing Lu"], "title": "SENTINEL: A Fully End-to-End Language-Action Model for Humanoid Whole Body Control", "comment": "23 pages, 8 figures, 11 tables", "summary": "Existing humanoid control systems often rely on teleoperation or modular generation pipelines that separate language understanding from physical execution. However, the former is entirely human-driven, and the latter lacks tight alignment between language commands and physical behaviors. In this paper, we present SENTINEL, a fully end-to-end language-action model for humanoid whole-body control. We construct a large-scale dataset by tracking human motions in simulation using a pretrained whole body controller, combined with their text annotations. The model directly maps language commands and proprioceptive inputs to low-level actions without any intermediate representation. The model generates action chunks using flow matching, which can be subsequently refined by a residual action head for real-world deployment. Our method exhibits strong semantic understanding and stable execution on humanoid robots in both simulation and real-world deployment, and also supports multi-modal extensions by converting inputs into texts.", "AI": {"tldr": "SENTINEL\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u63a7\u5236\uff0c\u76f4\u63a5\u6620\u5c04\u8bed\u8a00\u547d\u4ee4\u548c\u672c\u4f53\u611f\u53d7\u8f93\u5165\u5230\u4f4e\u7ea7\u52a8\u4f5c\uff0c\u65e0\u9700\u4e2d\u95f4\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u7cfb\u7edf\u4f9d\u8d56\u9065\u64cd\u4f5c\u6216\u6a21\u5757\u5316\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u524d\u8005\u5b8c\u5168\u7531\u4eba\u7c7b\u9a71\u52a8\uff0c\u540e\u8005\u7f3a\u4e4f\u8bed\u8a00\u547d\u4ee4\u4e0e\u7269\u7406\u884c\u4e3a\u4e4b\u95f4\u7684\u7d27\u5bc6\u5bf9\u9f50\u3002", "method": "\u901a\u8fc7\u5728\u4eff\u771f\u4e2d\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u5168\u8eab\u63a7\u5236\u5668\u8ddf\u8e2a\u4eba\u7c7b\u8fd0\u52a8\u5e76\u914d\u4ee5\u6587\u672c\u6807\u6ce8\u6784\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002\u6a21\u578b\u4f7f\u7528\u6d41\u5339\u914d\u751f\u6210\u52a8\u4f5c\u5757\uff0c\u5e76\u901a\u8fc7\u6b8b\u5dee\u52a8\u4f5c\u5934\u8fdb\u884c\u7ec6\u5316\u4ee5\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4eba\u5f62\u673a\u5668\u4eba\u7684\u4eff\u771f\u548c\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8bed\u4e49\u7406\u89e3\u548c\u7a33\u5b9a\u6267\u884c\u80fd\u529b\uff0c\u5e76\u652f\u6301\u901a\u8fc7\u5c06\u8f93\u5165\u8f6c\u6362\u4e3a\u6587\u672c\u6765\u5b9e\u73b0\u591a\u6a21\u6001\u6269\u5c55\u3002", "conclusion": "SENTINEL\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u5168\u7aef\u5230\u7aef\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u8bed\u8a00\u547d\u4ee4\u4e0e\u7269\u7406\u884c\u4e3a\u4e4b\u95f4\u7684\u7d27\u5bc6\u5bf9\u9f50\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002"}}
{"id": "2511.18889", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18889", "abs": "https://arxiv.org/abs/2511.18889", "authors": ["Jingqian Zhao", "Bingbing Wang", "Geng Tu", "Yice Zhang", "Qianlong Wang", "Bin Liang", "Jing Li", "Ruifeng Xu"], "title": "CoreEval: Automatically Building Contamination-Resilient Datasets with Real-World Knowledge toward Reliable LLM Evaluation", "comment": "ACL'25", "summary": "Data contamination poses a significant challenge to the fairness of LLM evaluations in natural language processing tasks by inadvertently exposing models to test data during training. Current studies attempt to mitigate this issue by modifying existing datasets or generating new ones from freshly collected information. However, these methods fall short of ensuring contamination-resilient evaluation, as they fail to fully eliminate pre-existing knowledge from models or preserve the semantic complexity of the original datasets. To address these limitations, we propose \\textbf{CoreEval}, a \\textbf{Co}ntamination-\\textbf{re}silient \\textbf{Eval}uation strategy for automatically updating data with real-world knowledge. This approach begins by extracting entity relationships from the original data and leveraging the GDELT database to retrieve relevant, up-to-date knowledge. The retrieved knowledge is then recontextualized and integrated with the original data, which is refined and restructured to ensure semantic coherence and enhanced task relevance. Ultimately, a robust data reflection mechanism is employed to iteratively verify and refine labels, ensuring consistency between the updated and original datasets. Extensive experiments on updated datasets validate the robustness of CoreEval, demonstrating its effectiveness in mitigating performance overestimation caused by data contamination.", "AI": {"tldr": "CoreEval\u662f\u4e00\u79cd\u6297\u6570\u636e\u6c61\u67d3\u8bc4\u4f30\u7b56\u7565\uff0c\u901a\u8fc7\u4eceGDELT\u6570\u636e\u5e93\u83b7\u53d6\u6700\u65b0\u77e5\u8bc6\u6765\u66f4\u65b0\u6570\u636e\uff0c\u89e3\u51b3LLM\u8bc4\u4f30\u4e2d\u7684\u6d4b\u8bd5\u6570\u636e\u6cc4\u9732\u95ee\u9898\u3002", "motivation": "\u6570\u636e\u6c61\u67d3\u5bfc\u81f4LLM\u8bc4\u4f30\u4e0d\u516c\u5e73\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u5168\u6d88\u9664\u6a21\u578b\u9884\u8bad\u7ec3\u77e5\u8bc6\u6216\u4fdd\u6301\u539f\u59cb\u6570\u636e\u96c6\u7684\u8bed\u4e49\u590d\u6742\u6027\u3002", "method": "\u4ece\u539f\u59cb\u6570\u636e\u63d0\u53d6\u5b9e\u4f53\u5173\u7cfb\uff0c\u4f7f\u7528GDELT\u6570\u636e\u5e93\u68c0\u7d22\u6700\u65b0\u77e5\u8bc6\uff0c\u91cd\u65b0\u8bed\u5883\u5316\u5e76\u6574\u5408\u77e5\u8bc6\uff0c\u901a\u8fc7\u6570\u636e\u53cd\u5c04\u673a\u5236\u8fed\u4ee3\u9a8c\u8bc1\u6807\u7b7e\u3002", "result": "\u5728\u66f4\u65b0\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86CoreEval\u7684\u9c81\u68d2\u6027\uff0c\u6709\u6548\u51cf\u8f7b\u4e86\u7531\u6570\u636e\u6c61\u67d3\u5f15\u8d77\u7684\u6027\u80fd\u9ad8\u4f30\u3002", "conclusion": "CoreEval\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6297\u6570\u636e\u6c61\u67d3\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u77e5\u8bc6\u66f4\u65b0\u786e\u4fdd\u8bc4\u4f30\u7684\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2511.19315", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19315", "abs": "https://arxiv.org/abs/2511.19315", "authors": ["Weiliang Tang", "Jialin Gao", "Jia-Hui Pan", "Gang Wang", "Li Erran Li", "Yunhui Liu", "Mingyu Ding", "Pheng-Ann Heng", "Chi-Wing Fu"], "title": "Rethinking Intermediate Representation for VLM-based Robot Manipulation", "comment": null, "summary": "Vision-Language Model (VLM) is an important component to enable robust robot manipulation. Yet, using it to translate human instructions into an action-resolvable intermediate representation often needs a tradeoff between VLM-comprehensibility and generalizability. Inspired by context-free grammar, we design the Semantic Assembly representation named SEAM, by decomposing the intermediate representation into vocabulary and grammar. Doing so leads us to a concise vocabulary of semantically-rich operations and a VLM-friendly grammar for handling diverse unseen tasks. In addition, we design a new open-vocabulary segmentation paradigm with a retrieval-augmented few-shot learning strategy to localize fine-grained object parts for manipulation, effectively with the shortest inference time over all state-of-the-art parallel works. Also, we formulate new metrics for action-generalizability and VLM-comprehensibility, demonstrating the compelling performance of SEAM over mainstream representations on both aspects. Extensive real-world experiments further manifest its SOTA performance under varying settings and tasks.", "AI": {"tldr": "\u63d0\u51faSEAM\u8bed\u4e49\u7ec4\u88c5\u8868\u793a\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u4e2d\u95f4\u8868\u793a\u4e3a\u8bcd\u6c47\u8868\u548c\u8bed\u6cd5\uff0c\u5b9e\u73b0VLM\u53cb\u597d\u4e14\u901a\u7528\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u8868\u793a\uff0c\u7ed3\u5408\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u548c\u68c0\u7d22\u589e\u5f3a\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u5728\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3VLM\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7ffb\u8bd1\u4eba\u7c7b\u6307\u4ee4\u65f6\uff0c\u5728VLM\u53ef\u7406\u89e3\u6027\u548c\u901a\u7528\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1SEAM\u8868\u793a\u6cd5\uff0c\u5c06\u4e2d\u95f4\u8868\u793a\u5206\u89e3\u4e3a\u8bed\u4e49\u4e30\u5bcc\u7684\u64cd\u4f5c\u8bcd\u6c47\u8868\u548cVLM\u53cb\u597d\u7684\u8bed\u6cd5\uff1b\u63d0\u51fa\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u8303\u5f0f\uff0c\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u7684\u5c11\u6837\u672c\u5b66\u4e60\u7b56\u7565\u6765\u5b9a\u4f4d\u7ec6\u7c92\u5ea6\u7269\u4f53\u90e8\u4ef6\u3002", "result": "\u5728\u52a8\u4f5c\u901a\u7528\u6027\u548cVLM\u53ef\u7406\u89e3\u6027\u65b9\u9762\u4f18\u4e8e\u4e3b\u6d41\u8868\u793a\u65b9\u6cd5\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u4e14\u63a8\u7406\u65f6\u95f4\u6700\u77ed\u3002", "conclusion": "SEAM\u8868\u793a\u6cd5\u901a\u8fc7\u8bcd\u6c47-\u8bed\u6cd5\u5206\u89e3\u6709\u6548\u5e73\u8861\u4e86VLM\u53ef\u7406\u89e3\u6027\u548c\u4efb\u52a1\u901a\u7528\u6027\uff0c\u7ed3\u5408\u9ad8\u6548\u7684\u7269\u4f53\u90e8\u4ef6\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u4e2d\u95f4\u8868\u793a\u65b9\u6848\u3002"}}
{"id": "2511.18891", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18891", "abs": "https://arxiv.org/abs/2511.18891", "authors": ["Adam Rychert", "Gasper Spagnolo", "Evgenii Posashkov"], "title": "Reproducibility Study of Large Language Model Bayesian Optimization", "comment": "7 pages, 8 figures. Reproducibility study of the LLAMBO framework (ICLR 2024). Code: https://github.com/spagnoloG/llambo-reproducibility", "summary": "In this reproducibility study, we revisit the LLAMBO framework of Daxberger et al. (2024), a prompting-based Bayesian optimization (BO) method that uses large language models as discriminative surrogates and acquisition optimizers via text-only interactions. We replicate the core Bayesmark and HPOBench experiments under the original evaluation protocol, but replace GPT-3.5 with the open-weight Llama 3.1 70B model used for all text encoding components.\n  Our results broadly confirm the main claims of LLAMBO. Contextual warm starting via textual problem and hyperparameter descriptions substantially improves early regret behaviour and reduces variance across runs. LLAMBO's discriminative surrogate is weaker than GP or SMAC as a pure single task regressor, yet benefits from cross task semantic priors induced by the language model. Ablations that remove textual context markedly degrade predictive accuracy and calibration, while the LLAMBO candidate sampler consistently generates higher quality and more diverse proposals than TPE or random sampling. Experiments with smaller backbones (Gemma 27B, Llama 3.1 8B) yield unstable or invalid predictions, suggesting insufficient capacity for reliable surrogate behaviour.\n  Overall, our study shows that the LLAMBO architecture is robust to changing the language model backbone and remains effective when instantiated with Llama 3.1 70B.", "AI": {"tldr": "LLAMBO\u6846\u67b6\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u5224\u522b\u5f0f\u4ee3\u7406\u548c\u91c7\u96c6\u4f18\u5316\u5668\uff0c\u901a\u8fc7\u7eaf\u6587\u672c\u4ea4\u4e92\u5b9e\u73b0\u3002\u590d\u73b0\u7814\u7a76\u8bc1\u5b9e\u4e86\u5176\u4e3b\u8981\u4e3b\u5f20\uff1a\u4e0a\u4e0b\u6587\u9884\u70ed\u542f\u52a8\u6539\u5584\u4e86\u65e9\u671f\u9057\u61be\u884c\u4e3a\uff0c\u51cf\u5c11\u4e86\u65b9\u5dee\uff1b\u5224\u522b\u5f0f\u4ee3\u7406\u867d\u5f31\u4e8e\u4f20\u7edf\u65b9\u6cd5\u4f46\u53d7\u76ca\u4e8e\u8de8\u4efb\u52a1\u8bed\u4e49\u5148\u9a8c\uff1b\u79fb\u9664\u6587\u672c\u4e0a\u4e0b\u6587\u4f1a\u663e\u8457\u964d\u4f4e\u6027\u80fd\uff1b\u5019\u9009\u91c7\u6837\u5668\u4f18\u4e8eTPE\u548c\u968f\u673a\u91c7\u6837\u3002", "motivation": "\u91cd\u65b0\u9a8c\u8bc1LLAMBO\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u4f7f\u7528\u5f00\u6e90\u7684Llama 3.1 70B\u6a21\u578b\u66ff\u4ee3GPT-3.5\uff0c\u6d4b\u8bd5\u8be5\u67b6\u6784\u5bf9\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u9aa8\u5e72\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5728\u539f\u59cb\u8bc4\u4f30\u534f\u8bae\u4e0b\u590d\u73b0\u6838\u5fc3Bayesmark\u548cHPOBench\u5b9e\u9a8c\uff0c\u5c06GPT-3.5\u66ff\u6362\u4e3aLlama 3.1 70B\u6a21\u578b\u7528\u4e8e\u6240\u6709\u6587\u672c\u7f16\u7801\u7ec4\u4ef6\uff0c\u5e76\u8fdb\u884c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u5404\u7ec4\u4ef6\u91cd\u8981\u6027\u3002", "result": "\u7ed3\u679c\u8bc1\u5b9e\u4e86LLAMBO\u7684\u4e3b\u8981\u4e3b\u5f20\uff1a\u4e0a\u4e0b\u6587\u9884\u70ed\u542f\u52a8\u6539\u5584\u4e86\u65e9\u671f\u9057\u61be\u884c\u4e3a\u548c\u65b9\u5dee\uff1b\u5224\u522b\u5f0f\u4ee3\u7406\u53d7\u76ca\u4e8e\u8de8\u4efb\u52a1\u8bed\u4e49\u5148\u9a8c\uff1b\u79fb\u9664\u6587\u672c\u4e0a\u4e0b\u6587\u4f1a\u964d\u4f4e\u9884\u6d4b\u51c6\u786e\u6027\u548c\u6821\u51c6\uff1b\u5019\u9009\u91c7\u6837\u5668\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u548c\u591a\u6837\u5316\u7684\u63d0\u8bae\uff1b\u8f83\u5c0f\u6a21\u578b\u4ea7\u751f\u4e0d\u7a33\u5b9a\u9884\u6d4b\u3002", "conclusion": "LLAMBO\u67b6\u6784\u5bf9\u66f4\u6362\u8bed\u8a00\u6a21\u578b\u9aa8\u5e72\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f7f\u7528Llama 3.1 70B\u65f6\u4ecd\u4fdd\u6301\u6709\u6548\u6027\uff0c\u4f46\u9700\u8981\u8db3\u591f\u5bb9\u91cf\u7684\u6a21\u578b\u624d\u80fd\u5b9e\u73b0\u53ef\u9760\u7684\u4ee3\u7406\u884c\u4e3a\u3002"}}
{"id": "2511.19377", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2511.19377", "abs": "https://arxiv.org/abs/2511.19377", "authors": ["Mamoon Aamir", "Mariyam Sattar", "Naveed Ur Rehman Junejo", "Aqsa Zafar Abbasi"], "title": "Deployment Dynamics and Optimization of Novel Space Antenna Deployable Mechanism", "comment": null, "summary": "Given the increasing need for large aperture antennas in space missions, the difficulty of fitting such structures into small launch vehicles has prompted the design of deployable antenna systems. The thesis introduces a new Triple Scissors Deployable Truss Mechanism (TSDTM) for space antenna missions. The new mechanism is to be stowed during launch and efficiently deploy in orbit, offering maximum aperture size while taking up minimal launch volume. The thesis covers the entire design process from geometric modeling, kinematic analysis with screw theory and Newtonian approaches, dynamic analysis by eigenvalue and simulation methods, and verification with SolidWorks. In addition, optimization routines were coded based on Support Vector Machines for material choice in LEO environments and machine learning method for geometric setup. The TSDTM presented has enhanced structural dynamics with good comparison between simulation and analytical predictions. The structure optimized proved highly accurate, with a deviation of just 1.94% between machine learning-predicted and simulated natural frequencies, demonstrating the potential of incorporating AI-based methods in space structural design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4e09\u91cd\u526a\u5200\u53ef\u5c55\u5f00\u6841\u67b6\u673a\u6784(TSDTM)\uff0c\u7528\u4e8e\u7a7a\u95f4\u5929\u7ebf\u4efb\u52a1\uff0c\u8be5\u673a\u6784\u5728\u53d1\u5c04\u65f6\u53ef\u6536\u8d77\uff0c\u5728\u8f68\u9053\u4e0a\u9ad8\u6548\u5c55\u5f00\uff0c\u5b9e\u73b0\u6700\u5927\u5b54\u5f84\u5c3a\u5bf8\u548c\u6700\u5c0f\u53d1\u5c04\u4f53\u79ef\u3002", "motivation": "\u968f\u7740\u7a7a\u95f4\u4efb\u52a1\u5bf9\u5927\u5b54\u5f84\u5929\u7ebf\u9700\u6c42\u7684\u589e\u52a0\uff0c\u5c06\u8fd9\u4e9b\u7ed3\u6784\u88c5\u5165\u5c0f\u578b\u8fd0\u8f7d\u706b\u7bad\u7684\u56f0\u96be\u4fc3\u4f7f\u4e86\u53ef\u5c55\u5f00\u5929\u7ebf\u7cfb\u7edf\u7684\u8bbe\u8ba1\u3002", "method": "\u6db5\u76d6\u4ece\u51e0\u4f55\u5efa\u6a21\u3001\u4f7f\u7528\u87ba\u65cb\u7406\u8bba\u548c\u725b\u987f\u65b9\u6cd5\u7684\u8fd0\u52a8\u5b66\u5206\u6790\u3001\u901a\u8fc7\u7279\u5f81\u503c\u548c\u4eff\u771f\u65b9\u6cd5\u7684\u52a8\u529b\u5b66\u5206\u6790\uff0c\u5230SolidWorks\u9a8c\u8bc1\u7684\u5b8c\u6574\u8bbe\u8ba1\u8fc7\u7a0b\u3002\u57fa\u4e8e\u652f\u6301\u5411\u91cf\u673a\u7f16\u5199\u4e86\u4f18\u5316\u7a0b\u5e8f\u7528\u4e8eLEO\u73af\u5883\u6750\u6599\u9009\u62e9\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u51e0\u4f55\u8bbe\u7f6e\u3002", "result": "\u63d0\u51fa\u7684TSDTM\u5177\u6709\u589e\u5f3a\u7684\u7ed3\u6784\u52a8\u529b\u5b66\u6027\u80fd\uff0c\u4eff\u771f\u4e0e\u89e3\u6790\u9884\u6d4b\u7ed3\u679c\u826f\u597d\u543b\u5408\u3002\u4f18\u5316\u7ed3\u6784\u975e\u5e38\u51c6\u786e\uff0c\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u4e0e\u6a21\u62df\u56fa\u6709\u9891\u7387\u4e4b\u95f4\u7684\u504f\u5dee\u4ec5\u4e3a1.94%\u3002", "conclusion": "\u5c55\u793a\u4e86\u5c06\u57fa\u4e8eAI\u7684\u65b9\u6cd5\u7eb3\u5165\u7a7a\u95f4\u7ed3\u6784\u8bbe\u8ba1\u7684\u6f5c\u529b\u3002"}}
{"id": "2511.18931", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.18931", "abs": "https://arxiv.org/abs/2511.18931", "authors": ["Sahil Kale"], "title": "Look It Up: Analysing Internal Web Search Capabilities of Modern LLMs", "comment": "10 pages, 8 figures", "summary": "Modern large language models integrate web search to provide real-time answers, yet it remains unclear whether they are efficiently calibrated to use search when it is actually needed. We introduce a benchmark evaluating both the necessity and effectiveness of web access across commercial models with no access to internal states or parameters. The dataset includes a static split of 783 temporally anchored questions answerable from pre-cutoff knowledge, aimed at testing whether models invoke search based on low internal confidence, and a dynamic split of 288 post-cutoff queries designed to test whether models recognise when search is required and retrieve updated information. Web access substantially improves static accuracy for GPT-5-mini and Claude Haiku 4.5, though confidence calibration worsens. On dynamic queries, both models frequently invoke search yet remain below 70 percent accuracy due to weak query formulation. Costs per accuracy-improving call remain low, but returns diminish once initial retrieval fails. Selective invocation helps, but models become overconfident and inconsistent after search. Overall, built-in web search meaningfully improves factual accuracy and can be invoked selectively, yet models remain overconfident, skip retrieval when it is essential, and falter once initial search queries underperform. Taken together, internal web search works better as a good low-latency verification layer than a reliable analytical tool, with clear room for improvement.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u65f6\u6709\u6548\u4f7f\u7528\u7f51\u7edc\u641c\u7d22\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u7f51\u7edc\u8bbf\u95ee\u80fd\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\uff0c\u4f46\u6a21\u578b\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u3001\u68c0\u7d22\u65f6\u673a\u4e0d\u5f53\u548c\u67e5\u8be2\u8868\u8ff0\u4e0d\u4f73\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u4e86\u7f51\u7edc\u641c\u7d22\u529f\u80fd\uff0c\u4f46\u5c1a\u4e0d\u6e05\u695a\u5b83\u4eec\u662f\u5426\u80fd\u5728\u771f\u6b63\u9700\u8981\u65f6\u6709\u6548\u4f7f\u7528\u641c\u7d22\u3002\u9700\u8981\u8bc4\u4f30\u6a21\u578b\u57fa\u4e8e\u5185\u90e8\u7f6e\u4fe1\u5ea6\u8c03\u7528\u641c\u7d22\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u8bc6\u522b\u4f55\u65f6\u9700\u8981\u641c\u7d22\u5e76\u68c0\u7d22\u66f4\u65b0\u4fe1\u606f\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b\u9759\u6001\u548c\u52a8\u6001\u95ee\u9898\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002\u9759\u6001\u90e8\u5206\u5305\u542b783\u4e2a\u65f6\u95f4\u951a\u5b9a\u95ee\u9898\uff0c\u6d4b\u8bd5\u6a21\u578b\u57fa\u4e8e\u4f4e\u5185\u90e8\u7f6e\u4fe1\u5ea6\u8c03\u7528\u641c\u7d22\u7684\u80fd\u529b\uff1b\u52a8\u6001\u90e8\u5206\u5305\u542b288\u4e2a\u622a\u6b62\u65e5\u671f\u540e\u67e5\u8be2\uff0c\u6d4b\u8bd5\u6a21\u578b\u8bc6\u522b\u641c\u7d22\u9700\u6c42\u5e76\u68c0\u7d22\u66f4\u65b0\u4fe1\u606f\u7684\u80fd\u529b\u3002", "result": "\u7f51\u7edc\u8bbf\u95ee\u663e\u8457\u63d0\u9ad8\u4e86GPT-5-mini\u548cClaude Haiku 4.5\u7684\u9759\u6001\u51c6\u786e\u6027\uff0c\u4f46\u7f6e\u4fe1\u5ea6\u6821\u51c6\u53d8\u5dee\u3002\u5728\u52a8\u6001\u67e5\u8be2\u4e2d\uff0c\u6a21\u578b\u9891\u7e41\u8c03\u7528\u641c\u7d22\u4f46\u51c6\u786e\u7387\u4ecd\u4f4e\u4e8e70%\uff0c\u4e3b\u8981\u7531\u4e8e\u67e5\u8be2\u8868\u8ff0\u4e0d\u4f73\u3002\u9009\u62e9\u6027\u8c03\u7528\u6709\u5e2e\u52a9\uff0c\u4f46\u641c\u7d22\u540e\u6a21\u578b\u53d8\u5f97\u8fc7\u5ea6\u81ea\u4fe1\u548c\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u5185\u7f6e\u7f51\u7edc\u641c\u7d22\u80fd\u663e\u8457\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\u4e14\u53ef\u9009\u62e9\u6027\u8c03\u7528\uff0c\u4f46\u6a21\u578b\u4ecd\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u3001\u5728\u5fc5\u9700\u65f6\u8df3\u8fc7\u68c0\u7d22\u4ee5\u53ca\u521d\u59cb\u641c\u7d22\u67e5\u8be2\u8868\u73b0\u4e0d\u4f73\u65f6\u8868\u73b0\u4e0b\u964d\u7684\u95ee\u9898\u3002\u7f51\u7edc\u641c\u7d22\u66f4\u9002\u5408\u4f5c\u4e3a\u4f4e\u5ef6\u8fdf\u9a8c\u8bc1\u5c42\u800c\u975e\u53ef\u9760\u5206\u6790\u5de5\u5177\uff0c\u4ecd\u6709\u660e\u663e\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2511.19433", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2511.19433", "abs": "https://arxiv.org/abs/2511.19433", "authors": ["Dong Jing", "Gang Wang", "Jiaqi Liu", "Weiliang Tang", "Zelong Sun", "Yunchao Yao", "Zhenyu Wei", "Yunhui Liu", "Zhiwu Lu", "Mingyu Ding"], "title": "Mixture of Horizons in Action Chunking", "comment": "15 pages, 14 figures", "summary": "Vision-language-action (VLA) models have shown remarkable capabilities in robotic manipulation, but their performance is sensitive to the $\\textbf{action chunk length}$ used during training, termed $\\textbf{horizon}$. Our empirical study reveals an inherent trade-off: longer horizons provide stronger global foresight but degrade fine-grained accuracy, while shorter ones sharpen local control yet struggle on long-term tasks, implying fixed choice of single horizons being suboptimal. To mitigate the trade-off, we propose a $\\textbf{mixture of horizons (MoH)}$ strategy. MoH rearranges the action chunk into several segments with different horizons, processes them in parallel with a shared action transformer, and fuses outputs with a light linear gate. It has three appealing benefits. 1) MoH exploits long-term foresight and short-term precision jointly within a single model, improving both performance and generalizability to complex tasks. 2) MoH is plug-and-play for full-attention action modules with minimal training or inference overhead. 3) MoH enables dynamic inference with adaptive horizons, which selects stable actions through cross-horizon consensus, achieving 2.5$\\times$ higher throughput than baselines while preserving superior performance. Extensive experiments over flow-based policies $\u03c0_0$, $\u03c0_{0.5}$, and one-step regression policy $\u03c0_{\\text{reg}}$ demonstrate that MoH yields consistent and significant gains on both simulations and real-world tasks. Notably, under mixed-task setting, $\u03c0_{0.5}$ with MoH reaches a new state-of-the-art with 99$\\%$ average success rate on LIBERO after only $30k$ training iterations. Project page: https://github.com/Timsty1/MixtureOfHorizons", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u89c6\u91ce(MoH)\u7b56\u7565\u6765\u89e3\u51b3VLA\u6a21\u578b\u4e2d\u884c\u52a8\u5757\u957f\u5ea6\u9009\u62e9\u7684\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u5e76\u884c\u5904\u7406\u4e0d\u540c\u89c6\u91ce\u957f\u5ea6\u7684\u884c\u52a8\u6bb5\u5e76\u878d\u5408\u8f93\u51fa\uff0c\u540c\u65f6\u83b7\u5f97\u957f\u671f\u524d\u77bb\u6027\u548c\u77ed\u671f\u7cbe\u5ea6", "motivation": "VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u6027\u80fd\u5bf9\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u884c\u52a8\u5757\u957f\u5ea6\uff08\u89c6\u91ce\uff09\u654f\u611f\u3002\u7814\u7a76\u53d1\u73b0\u5b58\u5728\u56fa\u6709\u6743\u8861\uff1a\u8f83\u957f\u89c6\u91ce\u63d0\u4f9b\u66f4\u5f3a\u7684\u5168\u5c40\u524d\u77bb\u6027\u4f46\u964d\u4f4e\u7ec6\u7c92\u5ea6\u7cbe\u5ea6\uff0c\u8f83\u77ed\u89c6\u91ce\u5219\u589e\u5f3a\u5c40\u90e8\u63a7\u5236\u4f46\u96be\u4ee5\u5904\u7406\u957f\u671f\u4efb\u52a1", "method": "MoH\u5c06\u884c\u52a8\u5757\u91cd\u65b0\u6392\u5217\u4e3a\u51e0\u4e2a\u5177\u6709\u4e0d\u540c\u89c6\u91ce\u7684\u6bb5\uff0c\u7528\u5171\u4eab\u7684\u884c\u52a8\u53d8\u6362\u5668\u5e76\u884c\u5904\u7406\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7ebf\u6027\u95e8\u878d\u5408\u8f93\u51fa\u3002\u652f\u6301\u52a8\u6001\u63a8\u7406\u548c\u81ea\u9002\u5e94\u89c6\u91ce\u9009\u62e9", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u03c00.5+MoH\u5728\u4ec530k\u8bad\u7ec3\u8fed\u4ee3\u540e\u8fbe\u523099%\u7684\u5e73\u5747\u6210\u529f\u7387\uff0c\u521b\u4e0b\u65b0\u7eaa\u5f55\u3002\u52a8\u6001\u63a8\u7406\u5b9e\u73b02.5\u500d\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4f18\u8d8a\u6027\u80fd", "conclusion": "MoH\u7b56\u7565\u6709\u6548\u7f13\u89e3\u4e86\u89c6\u91ce\u9009\u62e9\u6743\u8861\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5747\u5e26\u6765\u4e00\u81f4\u4e14\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5177\u6709\u5373\u63d2\u5373\u7528\u3001\u8bad\u7ec3\u63a8\u7406\u5f00\u9500\u5c0f\u7b49\u4f18\u70b9"}}
{"id": "2511.18934", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.18934", "abs": "https://arxiv.org/abs/2511.18934", "authors": ["Yuchen Ji", "Bo Xu", "Jie Shi", "Jiaqing Liang", "Deqing Yang", "Yu Mao", "Hai Chen", "Yanghua Xiao"], "title": "Skeletons Matter: Dynamic Data Augmentation for Text-to-Query", "comment": "Accepted at EMNLP 2025", "summary": "The task of translating natural language questions into query languages has long been a central focus in semantic parsing. Recent advancements in Large Language Models (LLMs) have significantly accelerated progress in this field. However, existing studies typically focus on a single query language, resulting in methods with limited generalizability across different languages. In this paper, we formally define the Text-to-Query task paradigm, unifying semantic parsing tasks across various query languages. We identify query skeletons as a shared optimization target of Text-to-Query tasks, and propose a general dynamic data augmentation framework that explicitly diagnoses model-specific weaknesses in handling these skeletons to synthesize targeted training data. Experiments on four Text-to-Query benchmarks demonstrate that our method achieves state-of-the-art performance using only a small amount of synthesized data, highlighting the efficiency and generality of our approach and laying a solid foundation for unified research on Text-to-Query tasks. We release our code at https://github.com/jjjycaptain/Skeletron.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Text-to-Query\u4efb\u52a1\u8303\u5f0f\uff0c\u901a\u8fc7\u8bc6\u522b\u67e5\u8be2\u9aa8\u67b6\u4f5c\u4e3a\u5171\u4eab\u4f18\u5316\u76ee\u6807\uff0c\u5e76\u5f00\u53d1\u52a8\u6001\u6570\u636e\u589e\u5f3a\u6846\u67b6\u6765\u8bca\u65ad\u6a21\u578b\u5f31\u70b9\u5e76\u5408\u6210\u9488\u5bf9\u6027\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u67e5\u8be2\u8bed\u8a00\uff0c\u5bfc\u81f4\u65b9\u6cd5\u5728\u4e0d\u540c\u8bed\u8a00\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u7edf\u4e00\u8bed\u4e49\u89e3\u6790\u4efb\u52a1\uff0c\u63d0\u9ad8\u8de8\u67e5\u8be2\u8bed\u8a00\u7684\u901a\u7528\u6027\u3002", "method": "\u5b9a\u4e49Text-to-Query\u4efb\u52a1\u8303\u5f0f\uff0c\u8bc6\u522b\u67e5\u8be2\u9aa8\u67b6\u4f5c\u4e3a\u5171\u4eab\u4f18\u5316\u76ee\u6807\uff0c\u63d0\u51fa\u52a8\u6001\u6570\u636e\u589e\u5f3a\u6846\u67b6\u6765\u8bca\u65ad\u6a21\u578b\u7279\u5b9a\u5f31\u70b9\u5e76\u5408\u6210\u9488\u5bf9\u6027\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u56db\u4e2aText-to-Query\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u4f7f\u7528\u5c11\u91cf\u5408\u6210\u6570\u636e\u5c31\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6548\u7387\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aText-to-Query\u4efb\u52a1\u7684\u7edf\u4e00\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u901a\u8fc7\u67e5\u8be2\u9aa8\u67b6\u548c\u52a8\u6001\u6570\u636e\u589e\u5f3a\u5b9e\u73b0\u4e86\u8de8\u67e5\u8be2\u8bed\u8a00\u7684\u9ad8\u6548\u8bed\u4e49\u89e3\u6790\u3002"}}
{"id": "2511.18937", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.18937", "abs": "https://arxiv.org/abs/2511.18937", "authors": ["Francois Vandenhende", "Anna Georgiou", "Michalis Georgiou", "Theodoros Psaras", "Ellie Karekla", "Elena Hadjicosta"], "title": "Knowledge-based Graphical Method for Safety Signal Detection in Clinical Trials", "comment": "13 pages, 3 tables, 5 figures", "summary": "We present a graphical, knowledge-based method for reviewing treatment-emergent adverse events (AEs) in clinical trials. The approach enhances MedDRA by adding a hidden medical knowledge layer (Safeterm) that captures semantic relationships between terms in a 2-D map. Using this layer, AE Preferred Terms can be regrouped automatically into similarity clusters, and their association to the trial disease may be quantified. The Safeterm map is available online and connected to aggregated AE incidence tables from ClinicalTrials.gov. For signal detection, we compute treatment-specific disproportionality metrics using shrinkage incidence ratios. Cluster-level EBGM values are then derived through precision-weighted aggregation. Two visual outputs support interpretation: a semantic map showing AE incidence and an expectedness-versus-disproportionality plot for rapid signal detection. Applied to three legacy trials, the automated method clearly recovers all expected safety signals. Overall, augmenting MedDRA with a medical knowledge layer improves clarity, efficiency, and accuracy in AE interpretation for clinical trials.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u5f62\u5316\u77e5\u8bc6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3aMedDRA\u6dfb\u52a0\u9690\u85cf\u533b\u5b66\u77e5\u8bc6\u5c42(Safeterm)\u6765\u81ea\u52a8\u805a\u7c7b\u4e0d\u826f\u4e8b\u4ef6\u672f\u8bed\uff0c\u5e76\u8ba1\u7b97\u5176\u4e0e\u8bd5\u9a8c\u75be\u75c5\u7684\u5173\u8054\u5ea6\uff0c\u4ece\u800c\u63d0\u9ad8\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u4e0d\u826f\u4e8b\u4ef6\u5ba1\u67e5\u7684\u6e05\u6670\u5ea6\u3001\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfMedDRA\u672f\u8bed\u7cfb\u7edf\u7f3a\u4e4f\u8bed\u4e49\u5173\u7cfb\u4fe1\u606f\uff0c\u5bfc\u81f4\u4e0d\u826f\u4e8b\u4ef6\u5ba1\u67e5\u6548\u7387\u4f4e\u4e0b\u4e14\u5bb9\u6613\u9057\u6f0f\u91cd\u8981\u4fe1\u53f7\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u8bc6\u522b\u76f8\u4f3c\u4e0d\u826f\u4e8b\u4ef6\u5e76\u91cf\u5316\u5176\u4e0e\u8bd5\u9a8c\u75be\u75c5\u5173\u8054\u7684\u65b9\u6cd5\u3002", "method": "\u5728MedDRA\u57fa\u7840\u4e0a\u6dfb\u52a0Safeterm\u533b\u5b66\u77e5\u8bc6\u5c42\uff0c\u6784\u5efa2D\u8bed\u4e49\u5730\u56fe\uff1b\u4f7f\u7528\u6536\u7f29\u53d1\u751f\u7387\u6bd4\u8ba1\u7b97\u6cbb\u7597\u7279\u5f02\u6027\u4e0d\u6210\u6bd4\u4f8b\u6307\u6807\uff1b\u901a\u8fc7\u7cbe\u5ea6\u52a0\u6743\u805a\u5408\u63a8\u5bfc\u7c07\u7ea7EBGM\u503c\uff1b\u63d0\u4f9b\u8bed\u4e49\u5730\u56fe\u548c\u671f\u671b\u5ea6-\u4e0d\u6210\u6bd4\u4f8b\u5ea6\u56fe\u4e24\u79cd\u53ef\u89c6\u5316\u8f93\u51fa\u3002", "result": "\u5e94\u7528\u4e8e\u4e09\u4e2a\u5386\u53f2\u8bd5\u9a8c\uff0c\u81ea\u52a8\u5316\u65b9\u6cd5\u80fd\u591f\u6e05\u6670\u6062\u590d\u6240\u6709\u9884\u671f\u7684\u5b89\u5168\u4fe1\u53f7\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u4e3aMedDRA\u6dfb\u52a0\u533b\u5b66\u77e5\u8bc6\u5c42\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e34\u5e8a\u8bd5\u9a8c\u4e2d\u4e0d\u826f\u4e8b\u4ef6\u89e3\u91ca\u7684\u6e05\u6670\u5ea6\u3001\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2511.19063", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19063", "abs": "https://arxiv.org/abs/2511.19063", "authors": ["Hayami Takahashi", "Kensuke Takahashi"], "title": "Logic of Montage", "comment": null, "summary": "In expressing emotions, as an expression form separate from natural language, we propose an alternative form that complements natural language, acting as a proxy or window for emotional states. First, we set up an expression form \"Effect of Contradictory Structure.\" \"Effect of Contradictory Structure\" is not static but dynamic. Effect in \"Effect of Contradictory Structure\" is unpleasant or pleasant, and the orientation to avoid that unpleasantness is considered pseudo-expression of will. Second, \"Effect of Contradictory Structure\" can be overlapped with each other. This overlapping operation is called \"montage.\" A broader \"Structure\" that includes related \"Effect of Contradictory Structure\" and \"Effect of Structure\" are set up. Montage produces \"Effect of Structure\". In montage, it is necessary to set something like \"strength,\" so we adopted Deleuze and Deleuze/Guattari's word \"intensity\" and set it as an element of our model. We set up a general theoretical framework - Word Import Between Systems (Models) and justified the import of \"intensity\" through Austin's use of the word \"force.\" \"Effect of Structure\" process is demonstrated using the example of proceeding to the next level of education.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77db\u76fe\u7ed3\u6784\u6548\u5e94\u7684\u60c5\u611f\u8868\u8fbe\u6a21\u578b\uff0c\u4f5c\u4e3a\u81ea\u7136\u8bed\u8a00\u7684\u8865\u5145\u5f62\u5f0f\uff0c\u901a\u8fc7\u8499\u592a\u5947\u64cd\u4f5c\u4ea7\u751f\u7ed3\u6784\u6548\u5e94\u3002", "motivation": "\u4e3a\u60c5\u611f\u8868\u8fbe\u63d0\u4f9b\u4e00\u79cd\u72ec\u7acb\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u66ff\u4ee3\u5f62\u5f0f\uff0c\u4f5c\u4e3a\u60c5\u611f\u72b6\u6001\u7684\u4ee3\u7406\u6216\u7a97\u53e3\u3002", "method": "\u5efa\u7acb\u77db\u76fe\u7ed3\u6784\u6548\u5e94\u6a21\u578b\uff0c\u901a\u8fc7\u8499\u592a\u5947\u64cd\u4f5c\u91cd\u53e0\u591a\u4e2a\u77db\u76fe\u7ed3\u6784\u6548\u5e94\uff0c\u5f15\u5165\u5f3a\u5ea6\u6982\u5ff5\u4f5c\u4e3a\u6a21\u578b\u8981\u7d20\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u7406\u8bba\u6846\u67b6\u2014\u2014\u7cfb\u7edf\u95f4\u8bcd\u8bed\u5bfc\u5165\uff0c\u5e76\u901a\u8fc7\u6559\u80b2\u5347\u7ea7\u7684\u4f8b\u5b50\u6f14\u793a\u4e86\u7ed3\u6784\u6548\u5e94\u7684\u4ea7\u751f\u8fc7\u7a0b\u3002", "conclusion": "\u77db\u76fe\u7ed3\u6784\u6548\u5e94\u6a21\u578b\u80fd\u591f\u6709\u6548\u8868\u8fbe\u60c5\u611f\u72b6\u6001\uff0c\u8499\u592a\u5947\u64cd\u4f5c\u548c\u5f3a\u5ea6\u6982\u5ff5\u7684\u5f15\u5165\u589e\u5f3a\u4e86\u60c5\u611f\u8868\u8fbe\u7684\u52a8\u6001\u6027\u548c\u5c42\u6b21\u6027\u3002"}}
{"id": "2511.19078", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19078", "abs": "https://arxiv.org/abs/2511.19078", "authors": ["Yutong Li", "Yitian Zhou", "Xudong Wang", "GuoChen", "Caiyan Qin"], "title": "GraphMind: Theorem Selection and Conclusion Generation Framework with Dynamic GNN for LLM Reasoning", "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, including multi-step reasoning such as mathematical proving. However, existing approaches often lack an explicit and dynamic mechanism to structurally represent and evolve intermediate reasoning states, which limits their ability to perform context-aware theorem selection and iterative conclusion generation. To address these challenges, we propose GraphMind, a novel dynamic graph-based framework that integrates the graph neural network (GNN) with LLMs to iteratively select theorems and generate intermediate conclusions for multi-step reasoning. Our method models the reasoning process as a heterogeneous evolving graph, where nodes represent conditions, theorems, and conclusions, while edges capture logical dependencies between nodes. By encoding the current reasoning state with GNN and leveraging semantic matching for theorem selection, our framework enables context-aware, interpretable, and structured reasoning in a closed-loop manner. Experiments on various question-answering (QA) datasets demonstrate that our proposed GraphMind method achieves consistent performance improvements and significantly outperforms existing baselines in multi-step reasoning, validating the effectiveness and generalizability of our approach.", "AI": {"tldr": "GraphMind\u662f\u4e00\u4e2a\u57fa\u4e8e\u52a8\u6001\u56fe\u7684\u6846\u67b6\uff0c\u5c06\u56fe\u795e\u7ecf\u7f51\u7edc\u4e0eLLMs\u7ed3\u5408\uff0c\u7528\u4e8e\u591a\u6b65\u63a8\u7406\u4e2d\u7684\u5b9a\u7406\u9009\u62e9\u548c\u4e2d\u95f4\u7ed3\u8bba\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u660e\u786e\u52a8\u6001\u673a\u5236\u6765\u8868\u793a\u548c\u6f14\u5316\u4e2d\u95f4\u63a8\u7406\u72b6\u6001\uff0c\u9650\u5236\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5b9a\u7406\u9009\u62e9\u548c\u8fed\u4ee3\u7ed3\u8bba\u751f\u6210\u80fd\u529b\u3002", "method": "\u5c06\u63a8\u7406\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u5f02\u6784\u6f14\u5316\u56fe\uff0c\u8282\u70b9\u8868\u793a\u6761\u4ef6\u3001\u5b9a\u7406\u548c\u7ed3\u8bba\uff0c\u8fb9\u6355\u83b7\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f7f\u7528GNN\u7f16\u7801\u5f53\u524d\u63a8\u7406\u72b6\u6001\u5e76\u901a\u8fc7\u8bed\u4e49\u5339\u914d\u8fdb\u884c\u5b9a\u7406\u9009\u62e9\u3002", "result": "\u5728\u591a\u4e2aQA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGraphMind\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5728\u591a\u6b65\u63a8\u7406\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GraphMind\u6846\u67b6\u4e3a\u591a\u6b65\u63a8\u7406\u63d0\u4f9b\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u53ef\u89e3\u91ca\u548c\u7ed3\u6784\u5316\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u6027\u3002"}}
{"id": "2511.19083", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19083", "abs": "https://arxiv.org/abs/2511.19083", "authors": ["Wenxuan Mu", "Jinzhong Ning", "Di Zhao", "Yijia Zhang"], "title": "A Multi-Agent LLM Framework for Multi-Domain Low-Resource In-Context NER via Knowledge Retrieval, Disambiguation and Reflective Analysis", "comment": "This paper has been accepted by AAAI 2026 (Main Technical Track)", "summary": "In-context learning (ICL) with large language models (LLMs) has emerged as a promising paradigm for named entity recognition (NER) in low-resource scenarios. However, existing ICL-based NER methods suffer from three key limitations: (1) reliance on dynamic retrieval of annotated examples, which is problematic when annotated data is scarce; (2) limited generalization to unseen domains due to the LLM's insufficient internal domain knowledge; and (3) failure to incorporate external knowledge or resolve entity ambiguities. To address these challenges, we propose KDR-Agent, a novel multi-agent framework for multi-domain low-resource in-context NER that integrates Knowledge retrieval, Disambiguation, and Reflective analysis. KDR-Agent leverages natural-language type definitions and a static set of entity-level contrastive demonstrations to reduce dependency on large annotated corpora. A central planner coordinates specialized agents to (i) retrieve factual knowledge from Wikipedia for domain-specific mentions, (ii) resolve ambiguous entities via contextualized reasoning, and (iii) reflect on and correct model predictions through structured self-assessment. Experiments across ten datasets from five domains demonstrate that KDR-Agent significantly outperforms existing zero-shot and few-shot ICL baselines across multiple LLM backbones. The code and data can be found at https://github.com/MWXGOD/KDR-Agent.", "AI": {"tldr": "KDR-Agent\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u9886\u57df\u4f4e\u8d44\u6e90\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u68c0\u7d22\u3001\u6d88\u6b67\u548c\u53cd\u601d\u5206\u6790\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u65b9\u6cd5\u7684\u4e09\u4e2a\u5173\u952e\u5c40\u9650\uff1a\u4f9d\u8d56\u52a8\u6001\u68c0\u7d22\u6807\u6ce8\u6570\u636e\u3001\u5bf9\u672a\u89c1\u9886\u57df\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3001\u65e0\u6cd5\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u6216\u89e3\u51b3\u5b9e\u4f53\u6b67\u4e49\u3002", "method": "\u63d0\u51faKDR-Agent\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u7c7b\u578b\u5b9a\u4e49\u548c\u9759\u6001\u5b9e\u4f53\u7ea7\u5bf9\u6bd4\u6f14\u793a\uff0c\u901a\u8fc7\u4e2d\u592e\u89c4\u5212\u5668\u534f\u8c03\u4e13\u95e8\u667a\u80fd\u4f53\u8fdb\u884c\u77e5\u8bc6\u68c0\u7d22\u3001\u6d88\u6b67\u63a8\u7406\u548c\u7ed3\u6784\u5316\u81ea\u8bc4\u4f30\u3002", "result": "\u57285\u4e2a\u9886\u57df\u768410\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cKDR-Agent\u5728\u591a\u4e2aLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "KDR-Agent\u901a\u8fc7\u6574\u5408\u77e5\u8bc6\u68c0\u7d22\u3001\u6d88\u6b67\u548c\u53cd\u601d\u5206\u6790\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u8de8\u9886\u57df\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2511.19097", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19097", "abs": "https://arxiv.org/abs/2511.19097", "authors": ["Ziyuan Gao", "Di Liang", "Xianjie Wu", "Philippe Morel", "Minlong Peng"], "title": "DeCoRL: Decoupling Reasoning Chains via Parallel Sub-Step Generation and Cascaded Reinforcement for Interpretable and Scalable RLHF", "comment": "Accepted by AAAI 2026", "summary": "Existing reinforcement learning methods for Chain-of-Thought reasoning suffer from two critical limitations. First, they operate as monolithic black boxes that provide undifferentiated reward signals, obscuring individual step contributions and hindering error diagnosis. Second, sequential decoding has O(n) time complexity. This makes real-time deployment impractical for complex reasoning tasks. We present DeCoRL (Decoupled Reasoning Chains via Coordinated Reinforcement Learning), a novel framework that transforms reasoning from sequential processing into collaborative modular orchestration. DeCoRL trains lightweight specialized models to generate reasoning sub-steps concurrently, eliminating sequential bottlenecks through parallel processing. To enable precise error attribution, the framework designs modular reward functions that score each sub-step independently. Cascaded DRPO optimization then coordinates these rewards while preserving inter-step dependencies. Comprehensive evaluation demonstrates state-of-the-art results across RM-Bench, RMB, and RewardBench, outperforming existing methods including large-scale models. DeCoRL delivers 3.8 times faster inference while maintaining superior solution quality and offers a 22.7\\% improvement in interpretability through explicit reward attribution. These advancements, combined with a 72.4\\% reduction in energy consumption and a 68\\% increase in throughput, make real-time deployment of complex reasoning systems a reality.", "AI": {"tldr": "DeCoRL\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u63a8\u7406\u8fc7\u7a0b\u4ece\u987a\u5e8f\u5904\u7406\u8f6c\u53d8\u4e3a\u534f\u4f5c\u5f0f\u6a21\u5757\u5316\u7f16\u6392\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5956\u52b1\u4fe1\u53f7\u4e0d\u660e\u786e\u548c\u65f6\u95f4\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u601d\u7ef4\u94fe\u63a8\u7406\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1) \u4f5c\u4e3a\u9ed1\u76d2\u63d0\u4f9b\u65e0\u5dee\u522b\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u96be\u4ee5\u8bc6\u522b\u5355\u4e2a\u6b65\u9aa4\u7684\u8d21\u732e\u548c\u8bca\u65ad\u9519\u8bef\uff1b2) \u987a\u5e8f\u89e3\u7801\u5177\u6709O(n)\u65f6\u95f4\u590d\u6742\u5ea6\uff0c\u4f7f\u5f97\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u5b9e\u65f6\u90e8\u7f72\u4e0d\u5207\u5b9e\u9645\u3002", "method": "DeCoRL\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u4e13\u7528\u6a21\u578b\u6765\u5e76\u884c\u751f\u6210\u63a8\u7406\u5b50\u6b65\u9aa4\uff0c\u901a\u8fc7\u5e76\u884c\u5904\u7406\u6d88\u9664\u987a\u5e8f\u74f6\u9888\u3002\u6846\u67b6\u8bbe\u8ba1\u6a21\u5757\u5316\u5956\u52b1\u51fd\u6570\u6765\u72ec\u7acb\u8bc4\u5206\u6bcf\u4e2a\u5b50\u6b65\u9aa4\uff0c\u5e76\u4f7f\u7528\u7ea7\u8054DRPO\u4f18\u5316\u6765\u534f\u8c03\u8fd9\u4e9b\u5956\u52b1\u540c\u65f6\u4fdd\u6301\u6b65\u9aa4\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5728RM-Bench\u3001RMB\u548cRewardBench\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u663e\u793a\uff0cDeCoRL\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4f18\u4e8e\u5305\u62ec\u5927\u89c4\u6a21\u6a21\u578b\u5728\u5185\u7684\u73b0\u6709\u65b9\u6cd5\u3002\u63a8\u7406\u901f\u5ea6\u63d0\u9ad83.8\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u5353\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff0c\u901a\u8fc7\u663e\u5f0f\u5956\u52b1\u5f52\u56e0\u53ef\u89e3\u91ca\u6027\u63d0\u9ad822.7%\u3002", "conclusion": "DeCoRL\u901a\u8fc73.8\u500d\u63a8\u7406\u52a0\u901f\u300122.7%\u53ef\u89e3\u91ca\u6027\u63d0\u5347\u300172.4%\u80fd\u8017\u964d\u4f4e\u548c68%\u541e\u5410\u91cf\u589e\u52a0\uff0c\u4f7f\u590d\u6742\u63a8\u7406\u7cfb\u7edf\u7684\u5b9e\u65f6\u90e8\u7f72\u6210\u4e3a\u73b0\u5b9e\u3002"}}
{"id": "2511.19118", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19118", "abs": "https://arxiv.org/abs/2511.19118", "authors": ["Juan-Jos\u00e9 Guzm\u00e1n-Landa", "Jes\u00fas V\u00e1zquez-Osorio", "Juan-Manuel Torres-Moreno", "Ligia Quintana Torres", "Miguel Figueroa-Saavedra", "Martha-Lorena Avenda\u00f1o-Garrido", "Graham Ranger", "Patricia Vel\u00e1zquez-Morales", "Gerardo Eugenio Sierra Mart\u00ednez"], "title": "A symbolic Perl algorithm for the unification of Nahuatl word spellings", "comment": "MICAI 2025, LNAI 16221, pp. 141-154, 2026. 10 pages, 4 Figures, 8 Tables", "summary": "In this paper, we describe a symbolic model for the automatic orthographic unification of Nawatl text documents. Our model is based on algorithms that we have previously used to analyze sentences in Nawatl, and on the corpus called $\u03c0$-yalli, consisting of texts in several Nawatl orthographies. Our automatic unification algorithm implements linguistic rules in symbolic regular expressions. We also present a manual evaluation protocol that we have proposed and implemented to assess the quality of the unified sentences generated by our algorithm, by testing in a sentence semantic task. We have obtained encouraging results from the evaluators for most of the desired features of our artificially unified sentences", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b26\u53f7\u6b63\u5219\u8868\u8fbe\u5f0f\u7684\u7eb3\u74e6\u7279\u5c14\u6587\u672c\u81ea\u52a8\u6b63\u5b57\u6cd5\u7edf\u4e00\u6a21\u578b\uff0c\u4f7f\u7528\u03c0-yalli\u8bed\u6599\u5e93\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u534f\u8bae\u9a8c\u8bc1\u7edf\u4e00\u53e5\u5b50\u7684\u8bed\u4e49\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u7eb3\u74e6\u7279\u5c14\u6587\u672c\u5728\u4e0d\u540c\u6b63\u5b57\u6cd5\u7cfb\u7edf\u4e0b\u7684\u7edf\u4e00\u95ee\u9898\uff0c\u4fbf\u4e8e\u6587\u672c\u5904\u7406\u548c\u5206\u6790\u3002", "method": "\u57fa\u4e8e\u5148\u524d\u5206\u6790\u7eb3\u74e6\u7279\u5c14\u53e5\u5b50\u7684\u7b97\u6cd5\uff0c\u4f7f\u7528\u7b26\u53f7\u6b63\u5219\u8868\u8fbe\u5f0f\u5b9e\u73b0\u8bed\u8a00\u89c4\u5219\uff0c\u6784\u5efa\u81ea\u52a8\u7edf\u4e00\u7b97\u6cd5\u3002", "result": "\u5728\u53e5\u5b50\u8bed\u4e49\u4efb\u52a1\u4e2d\u6d4b\u8bd5\uff0c\u83b7\u5f97\u4e86\u8bc4\u4f30\u8005\u5bf9\u4eba\u5de5\u7edf\u4e00\u53e5\u5b50\u5927\u591a\u6570\u671f\u671b\u7279\u5f81\u7684\u79ef\u6781\u53cd\u9988\u3002", "conclusion": "\u8be5\u81ea\u52a8\u7edf\u4e00\u7b97\u6cd5\u5728\u7eb3\u74e6\u7279\u5c14\u6587\u672c\u6b63\u5b57\u6cd5\u7edf\u4e00\u65b9\u9762\u53d6\u5f97\u4e86\u4ee4\u4eba\u9f13\u821e\u7684\u7ed3\u679c\u3002"}}
{"id": "2511.19120", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19120", "abs": "https://arxiv.org/abs/2511.19120", "authors": ["Phong Le", "Mees Lindeman", "Raquel G. Alhama"], "title": "On the Optimality of Discrete Object Naming: a Kinship Case Study", "comment": null, "summary": "The structure of naming systems in natural languages hinges on a trade-off between high informativeness and low complexity. Prior work capitalizes on information theory to formalize these notions; however, these studies generally rely on two simplifications: (i) optimal listeners, and (ii) universal communicative need across languages. Here, we address these limitations by introducing an information-theoretic framework for discrete object naming systems, and we use it to prove that an optimal trade-off is achievable if and only if the listener's decoder is equivalent to the Bayesian decoder of the speaker. Adopting a referential game setup from emergent communication, and focusing on the semantic domain of kinship, we show that our notion of optimality is not only theoretically achievable but also emerges empirically in learned communication systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4fe1\u606f\u8bba\u6846\u67b6\u6765\u5206\u6790\u81ea\u7136\u8bed\u8a00\u547d\u540d\u7cfb\u7edf\uff0c\u8bc1\u660e\u4e86\u5f53\u542c\u8005\u7684\u89e3\u7801\u5668\u7b49\u540c\u4e8e\u8bf4\u8bdd\u8005\u7684\u8d1d\u53f6\u65af\u89e3\u7801\u5668\u65f6\uff0c\u624d\u80fd\u5b9e\u73b0\u4fe1\u606f\u4e30\u5bcc\u6027\u548c\u590d\u6742\u6027\u7684\u6700\u4f18\u6743\u8861\u3002", "motivation": "\u89e3\u51b3\u5148\u524d\u7814\u7a76\u7684\u4e24\u4e2a\u7b80\u5316\u5047\u8bbe\uff1a(i) \u6700\u4f18\u542c\u8005\u5047\u8bbe\u548c(ii) \u8de8\u8bed\u8a00\u901a\u7528\u4ea4\u9645\u9700\u6c42\u5047\u8bbe\uff0c\u4ee5\u66f4\u771f\u5b9e\u5730\u5efa\u6a21\u547d\u540d\u7cfb\u7edf\u3002", "method": "\u5f15\u5165\u79bb\u6563\u5bf9\u8c61\u547d\u540d\u7cfb\u7edf\u7684\u4fe1\u606f\u8bba\u6846\u67b6\uff0c\u91c7\u7528\u6d8c\u73b0\u901a\u4fe1\u4e2d\u7684\u6307\u79f0\u6e38\u620f\u8bbe\u7f6e\uff0c\u805a\u7126\u4eb2\u5c5e\u5173\u7cfb\u8bed\u4e49\u9886\u57df\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u6700\u4f18\u6743\u8861\u662f\u53ef\u5b9e\u73b0\u7684\uff0c\u4e14\u5728\u5b66\u4e60\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7ecf\u9a8c\u6027\u5730\u6d8c\u73b0\u51fa\u6765\u3002", "conclusion": "\u5f53\u542c\u8005\u89e3\u7801\u5668\u4e0e\u8bf4\u8bdd\u8005\u8d1d\u53f6\u65af\u89e3\u7801\u5668\u7b49\u4ef7\u65f6\uff0c\u547d\u540d\u7cfb\u7edf\u80fd\u591f\u5728\u4fe1\u606f\u4e30\u5bcc\u6027\u548c\u590d\u6742\u6027\u4e4b\u95f4\u8fbe\u5230\u6700\u4f18\u5e73\u8861\uff0c\u8fd9\u4e00\u7406\u8bba\u7ed3\u679c\u5728\u5b9e\u9645\u5b66\u4e60\u7cfb\u7edf\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2511.19122", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19122", "abs": "https://arxiv.org/abs/2511.19122", "authors": ["Yaping Chai", "Haoran Xie", "Joe S. Qin"], "title": "Emotion-Enhanced Multi-Task Learning with LLMs for Aspect Category Sentiment Analysis", "comment": "8 pages, 4 figures", "summary": "Aspect category sentiment analysis (ACSA) has achieved remarkable progress with large language models (LLMs), yet existing approaches primarily emphasize sentiment polarity while overlooking the underlying emotional dimensions that shape sentiment expressions. This limitation hinders the model's ability to capture fine-grained affective signals toward specific aspect categories. To address this limitation, we introduce a novel emotion-enhanced multi-task ACSA framework that jointly learns sentiment polarity and category-specific emotions grounded in Ekman's six basic emotions. Leveraging the generative capabilities of LLMs, our approach enables the model to produce emotional descriptions for each aspect category, thereby enriching sentiment representations with affective expressions. Furthermore, to ensure the accuracy and consistency of the generated emotions, we introduce an emotion refinement mechanism based on the Valence-Arousal-Dominance (VAD) dimensional framework. Specifically, emotions predicted by the LLM are projected onto a VAD space, and those inconsistent with their corresponding VAD coordinates are re-annotated using a structured LLM-based refinement strategy. Experimental results demonstrate that our approach significantly outperforms strong baselines on all benchmark datasets. This underlines the effectiveness of integrating affective dimensions into ACSA.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u60c5\u611f\u589e\u5f3a\u7684\u591a\u4efb\u52a1ACSA\u6846\u67b6\uff0c\u8054\u5408\u5b66\u4e60\u60c5\u611f\u6781\u6027\u548c\u57fa\u4e8eEkman\u516d\u79cd\u57fa\u672c\u60c5\u7eea\u7684\u5206\u7c7b\u7279\u5b9a\u60c5\u7eea\uff0c\u901a\u8fc7VAD\u7ef4\u5ea6\u6846\u67b6\u7684\u60c5\u7eea\u7cbe\u70bc\u673a\u5236\u786e\u4fdd\u751f\u6210\u60c5\u7eea\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709ACSA\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u60c5\u611f\u6781\u6027\uff0c\u5ffd\u89c6\u4e86\u5851\u9020\u60c5\u611f\u8868\u8fbe\u7684\u57fa\u7840\u60c5\u7eea\u7ef4\u5ea6\uff0c\u8fd9\u9650\u5236\u4e86\u6a21\u578b\u6355\u6349\u9488\u5bf9\u7279\u5b9a\u65b9\u9762\u7c7b\u522b\u7684\u7ec6\u7c92\u5ea6\u60c5\u611f\u4fe1\u53f7\u7684\u80fd\u529b\u3002", "method": "\u5229\u7528LLMs\u7684\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u6bcf\u4e2a\u65b9\u9762\u7c7b\u522b\u751f\u6210\u60c5\u611f\u63cf\u8ff0\uff1b\u5f15\u5165\u57fa\u4e8eVAD\u7ef4\u5ea6\u6846\u67b6\u7684\u60c5\u7eea\u7cbe\u70bc\u673a\u5236\uff0c\u5c06LLM\u9884\u6d4b\u7684\u60c5\u7eea\u6295\u5f71\u5230VAD\u7a7a\u95f4\uff0c\u5bf9\u4e0d\u4e00\u81f4\u7684\u60c5\u7eea\u8fdb\u884c\u91cd\u65b0\u6807\u6ce8\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u90fd\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u5c06\u60c5\u611f\u7ef4\u5ea6\u6574\u5408\u5230ACSA\u4e2d\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u4e30\u5bcc\u60c5\u611f\u8868\u793a\u5e76\u63d0\u9ad8\u6027\u80fd\u3002"}}
{"id": "2511.19131", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19131", "abs": "https://arxiv.org/abs/2511.19131", "authors": ["Zijian Wang", "Yanxiang Ma", "Chang Xu"], "title": "Eliciting Chain-of-Thought in Base LLMs via Gradient-Based Representation Optimization", "comment": "AAAI2026", "summary": "Chain-of-Thought (CoT) reasoning is a critical capability for large language models (LLMs), enabling them to tackle com- plex multi-step tasks. While base LLMs, pre-trained on general text corpora, often struggle with reasoning due to a lack of specialized training, recent studies reveal their latent reason- ing potential tied to hidden states. However, existing hidden state manipulation methods, such as linear activation steering, suffer from limitations due to their rigid and unconstrained nature, often leading to distribution shifts and degraded text quality. In this work, we propose a novel approach for elic- iting CoT reasoning from base LLMs through hidden state manipulation grounded in probabilistic conditional generation. By reformulating the challenge as an optimization problem with a balanced likelihood and prior regularization framework, our method guides hidden states toward reasoning-oriented trajectories while preserving linguistic coherence. Extensive evaluations across mathematical, commonsense, and logical reasoning benchmarks demonstrate that our approach con- sistently outperforms existing steering methods, offering a theoretically principled and effective solution for enhancing reasoning capabilities in base LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u6761\u4ef6\u751f\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u9690\u85cf\u72b6\u6001\u6765\u6fc0\u53d1\u57fa\u7840\u5927\u8bed\u8a00\u6a21\u578b\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u80fd\u529b\uff0c\u5728\u6570\u5b66\u3001\u5e38\u8bc6\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u57fa\u7840\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9884\u8bad\u7ec3\u65f6\u7f3a\u4e4f\u4e13\u95e8\u7684\u63a8\u7406\u8bad\u7ec3\uff0c\u5bfc\u81f4\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002\u73b0\u6709\u9690\u85cf\u72b6\u6001\u64cd\u7eb5\u65b9\u6cd5\u5b58\u5728\u521a\u6027\u7ea6\u675f\u95ee\u9898\uff0c\u5bb9\u6613\u5bfc\u81f4\u5206\u5e03\u504f\u79fb\u548c\u6587\u672c\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u5c06\u6311\u6218\u91cd\u65b0\u8868\u8ff0\u4e3a\u5e26\u5e73\u8861\u4f3c\u7136\u548c\u5148\u9a8c\u6b63\u5219\u5316\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5728\u6982\u7387\u6761\u4ef6\u751f\u6210\u6846\u67b6\u4e0b\u5f15\u5bfc\u9690\u85cf\u72b6\u6001\u671d\u5411\u63a8\u7406\u8f68\u8ff9\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u8a00\u8fde\u8d2f\u6027\u3002", "result": "\u5728\u6570\u5b66\u3001\u5e38\u8bc6\u548c\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u7684\u64cd\u7eb5\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u589e\u5f3a\u57fa\u7840\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u4e0a\u6709\u539f\u5219\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.19166", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19166", "abs": "https://arxiv.org/abs/2511.19166", "authors": ["Samantha Dies", "Courtney Maynard", "Germans Savcisens", "Tina Eliassi-Rad"], "title": "Representational Stability of Truth in Large Language Models", "comment": "25 pages, 24 figures", "summary": "Large language models (LLMs) are widely used for factual tasks such as \"What treats asthma?\" or \"What is the capital of Latvia?\". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to $40\\%$ flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes ($\\leq 8.2\\%$). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u8868\u793a\u7a33\u5b9a\u6027\u6982\u5ff5\uff0c\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u3001\u865a\u5047\u548c\u4e2d\u6027\u9648\u8ff0\u4e4b\u95f4\u7684\u5185\u90e8\u6982\u7387\u8868\u793a\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u8868\u793a\u7a33\u5b9a\u6027\u66f4\u591a\u6e90\u4e8e\u8ba4\u77e5\u719f\u6089\u5ea6\u800c\u975e\u8bed\u8a00\u5f62\u5f0f\u3002", "motivation": "\u7814\u7a76LLM\u5982\u4f55\u5728\u5176\u5185\u90e8\u6982\u7387\u8868\u793a\u4e2d\u7a33\u5b9a\u5730\u533a\u5206\u771f\u5b9e\u3001\u865a\u5047\u548c\u4e2d\u6027\u5185\u5bb9\uff0c\u4ee5\u53ca\u8fd9\u79cd\u8868\u793a\u5bf9\u771f\u7406\u64cd\u4f5c\u5b9a\u4e49\u6270\u52a8\u7684\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u5728\u7ebf\u6027\u63a2\u9488\u4e0a\u8bad\u7ec3LLM\u6fc0\u6d3b\u6765\u5206\u79bb\u771f\u5b9e\u4e0e\u975e\u771f\u5b9e\u9648\u8ff0\uff0c\u5e76\u5728\u53d7\u63a7\u6807\u7b7e\u53d8\u5316\u4e0b\u6d4b\u91cf\u5b66\u4e60\u51b3\u7b56\u8fb9\u754c\u7684\u53d8\u5316\uff0c\u4f7f\u752816\u4e2a\u5f00\u6e90\u6a21\u578b\u548c3\u4e2a\u4e8b\u5b9e\u9886\u57df\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u964c\u751f\u4e2d\u6027\u9648\u8ff0\uff08\u5173\u4e8e\u8bad\u7ec3\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\u7684\u5b9e\u4f53\u7684\u65ad\u8a00\uff09\u5bfc\u81f4\u6700\u5927\u7684\u8fb9\u754c\u504f\u79fb\uff0c\u5728\u8106\u5f31\u9886\u57df\uff08\u5982\u8bcd\u6c47\u5b9a\u4e49\uff09\u4ea7\u751f\u9ad8\u8fbe40%\u7684\u771f\u5b9e\u5224\u65ad\u7ffb\u8f6c\uff1b\u800c\u719f\u6089\u865a\u6784\u9648\u8ff0\u4fdd\u6301\u66f4\u4e00\u81f4\u7684\u805a\u7c7b\uff0c\u53d8\u5316\u8f83\u5c0f\uff08\u22648.2%\uff09\u3002", "conclusion": "\u8868\u793a\u7a33\u5b9a\u6027\u66f4\u591a\u6e90\u4e8e\u8ba4\u77e5\u719f\u6089\u5ea6\u800c\u975e\u8bed\u8a00\u5f62\u5f0f\uff0c\u8be5\u65b9\u6cd5\u4e3a\u5ba1\u8ba1\u548c\u8bad\u7ec3LLM\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\uff0c\u4ee5\u5728\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\u4e0b\u4fdd\u6301\u8fde\u8d2f\u7684\u771f\u7406\u5206\u914d\u3002"}}
{"id": "2511.19232", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.19232", "abs": "https://arxiv.org/abs/2511.19232", "authors": ["Christos-Nikolaos Zacharopoulos", "Revekka Kyriakoglou"], "title": "In Machina N400: Pinpointing Where a Causal Language Model Detects Semantic Violations", "comment": "Accepted at AICS2025", "summary": "How and where does a transformer notice that a sentence has gone semantically off the rails? To explore this question, we evaluated the causal language model (phi-2) using a carefully curated corpus, with sentences that concluded plausibly or implausibly. Our analysis focused on the hidden states sampled at each model layer. To investigate how violations are encoded, we utilized two complementary probes. First, we conducted a per-layer detection using a linear probe. Our findings revealed that a simple linear decoder struggled to distinguish between plausible and implausible endings in the lowest third of the model's layers. However, its accuracy sharply increased in the middle blocks, reaching a peak just before the top layers. Second, we examined the effective dimensionality of the encoded violation. Initially, the violation widens the representational subspace, followed by a collapse after a mid-stack bottleneck. This might indicate an exploratory phase that transitions into rapid consolidation. Taken together, these results contemplate the idea of alignment with classical psycholinguistic findings in human reading, where semantic anomalies are detected only after syntactic resolution, occurring later in the online processing sequence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86transformer\u6a21\u578b\u5982\u4f55\u68c0\u6d4b\u8bed\u4e49\u5f02\u5e38\u53e5\u5b50\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u4e2d\u95f4\u5c42\u624d\u5f00\u59cb\u6709\u6548\u8bc6\u522b\u8bed\u4e49\u4e0d\u5408\u7406\u6027\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u4e2d\u5148\u8bed\u6cd5\u540e\u8bed\u4e49\u7684\u6a21\u5f0f\u76f8\u4f3c\u3002", "motivation": "\u63a2\u7d22transformer\u6a21\u578b\u5728\u4f55\u5904\u4ee5\u53ca\u5982\u4f55\u68c0\u6d4b\u53e5\u5b50\u8bed\u4e49\u5f02\u5e38\uff0c\u4ee5\u7406\u89e3\u6a21\u578b\u7684\u8bed\u8a00\u5904\u7406\u673a\u5236\u5e76\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u8fdb\u884c\u5bf9\u6bd4\u3002", "method": "\u4f7f\u7528phi-2\u56e0\u679c\u8bed\u8a00\u6a21\u578b\uff0c\u5206\u6790\u4e0d\u540c\u5c42\u7ea7\u7684\u9690\u85cf\u72b6\u6001\uff0c\u91c7\u7528\u7ebf\u6027\u63a2\u6d4b\u5668\u548c\u7ef4\u5ea6\u5206\u6790\u4e24\u79cd\u65b9\u6cd5\u6765\u7814\u7a76\u8bed\u4e49\u8fdd\u89c4\u7684\u7f16\u7801\u65b9\u5f0f\u3002", "result": "\u7ebf\u6027\u63a2\u6d4b\u5668\u5728\u6a21\u578b\u5e95\u5c42\u96be\u4ee5\u533a\u5206\u5408\u7406\u4e0e\u4e0d\u5408\u7406\u7ed3\u5c3e\uff0c\u4f46\u5728\u4e2d\u95f4\u5c42\u51c6\u786e\u7387\u6025\u5267\u4e0a\u5347\uff1b\u8bed\u4e49\u8fdd\u89c4\u7f16\u7801\u5148\u6269\u5c55\u8868\u793a\u5b50\u7a7a\u95f4\uff0c\u540e\u7ecf\u8fc7\u74f6\u9888\u5c42\u538b\u7f29\uff0c\u663e\u793a\u4ece\u63a2\u7d22\u5230\u5feb\u901f\u6574\u5408\u7684\u8f6c\u53d8\u3002", "conclusion": "transformer\u68c0\u6d4b\u8bed\u4e49\u5f02\u5e38\u7684\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u5fc3\u7406\u8bed\u8a00\u5b66\u53d1\u73b0\u4e00\u81f4\uff0c\u5373\u8bed\u4e49\u5f02\u5e38\u68c0\u6d4b\u53d1\u751f\u5728\u8bed\u6cd5\u89e3\u6790\u4e4b\u540e\uff0c\u652f\u6301\u6a21\u578b\u4e0e\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u7684\u76f8\u4f3c\u6027\u3002"}}
{"id": "2511.19317", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19317", "abs": "https://arxiv.org/abs/2511.19317", "authors": ["Md. Tanzim Ferdous", "Naeem Ahsan Chowdhury", "Prithwiraj Bhattacharjee"], "title": "MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset", "comment": null, "summary": "This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b54,000\u591a\u7bc7\u5b5f\u52a0\u62c9\u8bed\u6587\u7ae0\u548c\u6458\u8981\u7684\u65b0\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u591a\u4e2a\u9886\u57df\u548c\u5199\u4f5c\u98ce\u683c\uff0c\u4e3a\u5b5f\u52a0\u62c9\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u4e86\u57fa\u51c6\u8d44\u6e90\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u65b0\u95fb\u6587\u7ae0\uff0c\u800c\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5b5f\u52a0\u62c9\u8bed\u6587\u672c\u5177\u6709\u591a\u6837\u6027\uff0c\u9700\u8981\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u5199\u4f5c\u98ce\u683c\u7684\u6458\u8981\u7cfb\u7edf\u6765\u51cf\u8f7b\u4fe1\u606f\u8fc7\u8f7d\u3002", "method": "\u4ece\u591a\u4e2a\u6765\u6e90\u6536\u96c6\u5b5f\u52a0\u62c9\u8bed\u6587\u7ae0\u548c\u6458\u8981\uff0c\u5305\u62ec\u535a\u5ba2\u548c\u62a5\u7eb8\uff0c\u5e76\u4f7f\u7528LSTM\u3001BanglaT5-small\u548cMTS-small\u7b49\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u6570\u636e\u96c6\u5c55\u793a\u4e86\u4f5c\u4e3a\u5b5f\u52a0\u62c9\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u672a\u6765\u7814\u7a76\u57fa\u51c6\u7684\u6f5c\u529b\uff0c\u4e3a\u6784\u5efa\u7a33\u5065\u7684\u6458\u8981\u7cfb\u7edf\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u6269\u5c55\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684NLP\u8d44\u6e90\uff0c\u5e76\u4e3a\u5f00\u53d1\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u6587\u672c\u591a\u6837\u6027\u7684\u6458\u8981\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2511.19333", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.19333", "abs": "https://arxiv.org/abs/2511.19333", "authors": ["Shaltiel Shmidman", "Asher Fredman", "Oleg Sudakov", "Meriem Bendris"], "title": "Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces", "comment": null, "summary": "Test-time scaling, which leverages additional computation during inference to improve model accuracy, has enabled a new class of Large Language Models (LLMs) that are able to reason through complex problems by understanding the goal, turning this goal into a plan, working through intermediate steps, and checking their own work before answering . Frontier large language models with reasoning capabilities, such as DeepSeek-R1 and OpenAI's gpt-oss, follow the same procedure when solving complex problems by generating intermediate reasoning traces before giving the final answer. Today, these models are being increasingly used to generate reasoning traces that serve as high-quality supervised data for post-training of small and medium-sized language models to teach reasoning capabilities without requiring expensive human curation. In this work, we compare the performance of medium-sized LLMs on Math problems after post-training on two kinds of reasoning traces. We compare the impact of reasoning traces generated by DeepSeek-R1 and gpt-oss LLMs in terms of accuracy and inference efficiency.", "AI": {"tldr": "\u6bd4\u8f83DeepSeek-R1\u548cgpt-oss\u4e24\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u5bf9\u4e2d\u7b49\u89c4\u6a21LLM\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u5229\u7528\u524d\u6cbf\u5927\u6a21\u578b\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u4f5c\u4e3a\u9ad8\u8d28\u91cf\u76d1\u7763\u6570\u636e\uff0c\u8bad\u7ec3\u4e2d\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u83b7\u5f97\u63a8\u7406\u80fd\u529b\uff0c\u907f\u514d\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u3002", "method": "\u5bf9\u4e2d\u7b49\u89c4\u6a21LLM\u8fdb\u884c\u540e\u8bad\u7ec3\uff0c\u4f7f\u7528DeepSeek-R1\u548cgpt-oss\u751f\u6210\u7684\u4e24\u79cd\u63a8\u7406\u8f68\u8ff9\u6570\u636e\uff0c\u6bd4\u8f83\u5176\u5728\u6570\u5b66\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u4e24\u79cd\u63a8\u7406\u8f68\u8ff9\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u7684\u5f71\u54cd\u5dee\u5f02\u3002", "conclusion": "\u6bd4\u8f83\u4e0d\u540c\u5927\u6a21\u578b\u751f\u6210\u7684\u63a8\u7406\u8f68\u8ff9\u5bf9\u4e2d\u5c0f\u6a21\u578b\u63a8\u7406\u80fd\u529b\u8bad\u7ec3\u7684\u6548\u679c\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2511.19399", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19399", "abs": "https://arxiv.org/abs/2511.19399", "authors": ["Rulin Shao", "Akari Asai", "Shannon Zejiang Shen", "Hamish Ivison", "Varsha Kishore", "Jingming Zhuo", "Xinran Zhao", "Molly Park", "Samuel G. Finlayson", "David Sontag", "Tyler Murray", "Sewon Min", "Pradeep Dasigi", "Luca Soldaini", "Faeze Brahman", "Wen-tau Yih", "Tongshuang Wu", "Luke Zettlemoyer", "Yoon Kim", "Hannaneh Hajishirzi", "Pang Wei Koh"], "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research", "comment": null, "summary": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86RLER\uff08\u5f3a\u5316\u5b66\u4e60\u4e0e\u6f14\u8fdb\u5f0f\u8bc4\u5206\u6807\u51c6\uff09\u65b9\u6cd5\uff0c\u5f00\u53d1\u4e86Deep Research Tulu\u6a21\u578b\uff0c\u5728\u957f\u683c\u5f0f\u6df1\u5ea6\u7814\u7a76\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff0c\u6027\u80fd\u5ab2\u7f8e\u4e13\u6709\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u6df1\u5ea6\u7814\u7a76\u6a21\u578b\u4e3b\u8981\u5728\u53ef\u9a8c\u8bc1\u7684\u77ed\u683c\u5f0fQA\u4efb\u52a1\u4e0a\u8bad\u7ec3\uff0c\u65e0\u6cd5\u6269\u5c55\u5230\u73b0\u5b9e\u7684\u957f\u683c\u5f0f\u7814\u7a76\u4efb\u52a1\uff0c\u9700\u8981\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528RLER\u65b9\u6cd5\u6784\u5efa\u548c\u7ef4\u62a4\u4e0e\u7b56\u7565\u6a21\u578b\u5171\u540c\u6f14\u8fdb\u7684\u8bc4\u5206\u6807\u51c6\uff0c\u8ba9\u8bc4\u5206\u6807\u51c6\u80fd\u591f\u6574\u5408\u6a21\u578b\u65b0\u63a2\u7d22\u7684\u4fe1\u606f\u5e76\u63d0\u4f9b\u533a\u5206\u6027\u7684\u5728\u7ebf\u53cd\u9988\u3002", "result": "\u5f00\u53d1\u7684DR Tulu-8B\u6a21\u578b\u5728\u79d1\u5b66\u3001\u533b\u7597\u548c\u901a\u7528\u9886\u57df\u7684\u56db\u4e2a\u957f\u683c\u5f0f\u6df1\u5ea6\u7814\u7a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u6a21\u578b\uff0c\u6027\u80fd\u8fbe\u5230\u6216\u8d85\u8fc7\u4e13\u6709\u7cfb\u7edf\u3002", "conclusion": "RLER\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u957f\u683c\u5f0f\u6df1\u5ea6\u7814\u7a76\u7684\u8bad\u7ec3\u6311\u6218\uff0cDR Tulu\u6a21\u578b\u5c55\u793a\u4e86\u5f00\u6e90\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5173\u6570\u636e\u3001\u6a21\u578b\u548c\u4ee3\u7801\u3002"}}
{"id": "2511.19417", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.19417", "abs": "https://arxiv.org/abs/2511.19417", "authors": ["James Y. Huang", "Sheng Zhang", "Qianchu Liu", "Guanghui Qin", "Tinghui Zhu", "Tristan Naumann", "Muhao Chen", "Hoifung Poon"], "title": "Be My Eyes: Extending Large Language Models to New Modalities Through Multi-Agent Collaboration", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in challenging, knowledge-intensive reasoning tasks. However, extending LLMs to perceive and reason over a new modality (e.g., vision), often requires costly development of large-scale vision language models (VLMs) with LLMs as backbones. Smaller VLMs are more efficient and adaptable but often lack the broad knowledge and reasoning capabilities of frontier LLMs. In this work, we propose BeMyEyes, a modular, multi-agent framework for extending LLMs to multimodal reasoning by orchestrating collaboration between efficient, adaptable VLMs as perceivers and powerful LLMs as reasoners through conversations. We then introduce a data synthesis and supervised fine-tuning pipeline to train the perceiver agent to effectively collaborate with the reasoner agent. By combining the complementary strengths of perception and reasoning agents, BeMyEyes avoids the need for training large-scale multimodal models, preserves the generalization and reasoning capabilities of LLMs, and allows flexible extension to new domains and modalities. Experiments show that our framework unlocks the multimodal reasoning capabilities for LLMs, enabling a lightweight and fully open-source solution, i.e. equipping text-only DeepSeek-R1 with Qwen2.5-VL-7B perceiver, to outperform large-scale proprietary VLMs such as GPT-4o on a wide range of knowledge-intensive multimodal tasks. These results demonstrate the effectiveness, modularity, and scalability of our multi-agent approach for building future multimodal reasoning systems.", "AI": {"tldr": "BeMyEyes\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u9ad8\u6548\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u611f\u77e5\u5668\u4e0e\u5f3a\u5927\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u63a8\u7406\u5668\u8fdb\u884c\u5bf9\u8bdd\u534f\u4f5c\uff0c\u6269\u5c55LLMs\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u8bad\u7ec3\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6a21\u578b\u3002", "motivation": "\u6269\u5c55LLMs\u5230\u591a\u6a21\u6001\u63a8\u7406\u901a\u5e38\u9700\u8981\u5f00\u53d1\u6210\u672c\u9ad8\u6602\u7684\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u800c\u8f83\u5c0f\u7684VLMs\u867d\u7136\u9ad8\u6548\u4f46\u7f3a\u4e4f\u524d\u6cbfLLMs\u7684\u5e7f\u6cdb\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u6a21\u5757\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8bdd\u534f\u8c03\u611f\u77e5\u5668\u548c\u63a8\u7406\u5668\u7684\u534f\u4f5c\uff0c\u5e76\u5f15\u5165\u6570\u636e\u5408\u6210\u548c\u76d1\u7763\u5fae\u8c03\u7ba1\u9053\u6765\u8bad\u7ec3\u611f\u77e5\u5668\u4e0e\u63a8\u7406\u5668\u6709\u6548\u534f\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u4e3aLLMs\u89e3\u9501\u4e86\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\uff08DeepSeek-R1 + Qwen2.5-VL-7B\uff09\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86GPT-4o\u7b49\u5927\u89c4\u6a21\u4e13\u6709VLMs\u3002", "conclusion": "BeMyEyes\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u5728\u6784\u5efa\u672a\u6765\u591a\u6a21\u6001\u63a8\u7406\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u3001\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
