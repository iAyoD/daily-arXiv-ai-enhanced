{"id": "2601.00086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00086", "abs": "https://arxiv.org/abs/2601.00086", "authors": ["Xiang Gao", "Yuguang Yao", "Qi Zhang", "Kaiwen Dong", "Avinash Baidya", "Ruocheng Guo", "Hilaf Hasson", "Kamalika Das"], "title": "RIMRULE: Improving Tool-Using Language Agents via MDL-Guided Rule Learning", "comment": null, "summary": "Large language models (LLMs) often struggle to use tools reliably in domain-specific settings, where APIs may be idiosyncratic, under-documented, or tailored to private workflows. This highlights the need for effective adaptation to task-specific tools. We propose RIMRULE, a neuro-symbolic approach for LLM adaptation based on dynamic rule injection. Compact, interpretable rules are distilled from failure traces and injected into the prompt during inference to improve task performance. These rules are proposed by the LLM itself and consolidated using a Minimum Description Length (MDL) objective that favors generality and conciseness. Each rule is stored in both natural language and a structured symbolic form, supporting efficient retrieval at inference time. Experiments on tool-use benchmarks show that this approach improves accuracy on both seen and unseen tools without modifying LLM weights. It outperforms prompting-based adaptation methods and complements finetuning. Moreover, rules learned from one LLM can be reused to improve others, including long reasoning LLMs, highlighting the portability of symbolic knowledge across architectures.", "AI": {"tldr": "RIMRULE\uff1a\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u89c4\u5219\u6ce8\u5165\u7684\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u5931\u8d25\u8f68\u8ff9\u4e2d\u63d0\u70bc\u7d27\u51d1\u3001\u53ef\u89e3\u91ca\u7684\u89c4\u5219\u6765\u63d0\u5347LLM\u5728\u7279\u5b9a\u9886\u57df\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u6027\u80fd\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u6743\u91cd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u9886\u57df\u5de5\u5177\u4f7f\u7528\u65f6\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3aAPI\u53ef\u80fd\u5177\u6709\u7279\u6b8a\u6027\u3001\u6587\u6863\u4e0d\u8db3\u6216\u9488\u5bf9\u79c1\u6709\u5de5\u4f5c\u6d41\u5b9a\u5236\uff0c\u9700\u8981\u6709\u6548\u7684\u4efb\u52a1\u7279\u5b9a\u5de5\u5177\u9002\u5e94\u65b9\u6cd5\u3002", "method": "\u63d0\u51faRIMRULE\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff1a1\uff09\u4ece\u5931\u8d25\u8f68\u8ff9\u4e2d\u63d0\u70bc\u7d27\u51d1\u53ef\u89e3\u91ca\u89c4\u5219\uff1b2\uff09\u4f7f\u7528\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\u76ee\u6807\u4f18\u5316\u89c4\u5219\u901a\u7528\u6027\u548c\u7b80\u6d01\u6027\uff1b3\uff09\u89c4\u5219\u4ee5\u81ea\u7136\u8bed\u8a00\u548c\u7ed3\u6784\u5316\u7b26\u53f7\u5f62\u5f0f\u5b58\u50a8\uff1b4\uff09\u63a8\u7406\u65f6\u52a8\u6001\u6ce8\u5165\u89c4\u5219\u5230\u63d0\u793a\u4e2d\u3002", "result": "\u5728\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5df2\u89c1\u548c\u672a\u89c1\u5de5\u5177\u7684\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u7684\u9002\u5e94\u65b9\u6cd5\uff0c\u5e76\u80fd\u4e0e\u5fae\u8c03\u4e92\u8865\u3002\u89c4\u5219\u53ef\u5728\u4e0d\u540cLLM\u95f4\u91cd\u7528\uff0c\u5305\u62ec\u957f\u63a8\u7406LLM\uff0c\u5c55\u793a\u4e86\u7b26\u53f7\u77e5\u8bc6\u7684\u8de8\u67b6\u6784\u53ef\u79fb\u690d\u6027\u3002", "conclusion": "RIMRULE\u901a\u8fc7\u52a8\u6001\u89c4\u5219\u6ce8\u5165\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u7279\u5b9a\u9886\u57df\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u6a21\u578b\u6743\u91cd\u7684\u9002\u5e94\u65b9\u6cd5\uff0c\u4e14\u89c4\u5219\u5177\u6709\u8de8\u6a21\u578b\u7684\u53ef\u79fb\u690d\u6027\u3002"}}
{"id": "2601.00095", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00095", "abs": "https://arxiv.org/abs/2601.00095", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Universal Adaptive Constraint Propagation: Scaling Structured Inference for Large Language Models via Meta-Reinforcement Learning", "comment": null, "summary": "Large language models increasingly require structured inference, from JSON schema enforcement to multi-lingual parsing, where outputs must satisfy complex constraints. We introduce MetaJuLS, a meta-reinforcement learning approach that learns universal constraint propagation policies applicable across languages and tasks without task-specific retraining. By formulating structured inference as adaptive constraint propagation and training a Graph Attention Network with meta-learning, MetaJuLS achieves 1.5--2.0$\\times$ speedups over GPU-optimized baselines while maintaining within 0.2\\% accuracy of state-of-the-art parsers. On Universal Dependencies across 10 languages and LLM-constrained generation (LogicBench, GSM8K-Constrained), MetaJuLS demonstrates rapid cross-domain adaptation: a policy trained on English parsing adapts to new languages and tasks with 5--10 gradient steps (5--15 seconds) rather than requiring hours of task-specific training. Mechanistic analysis reveals the policy discovers human-like parsing strategies (easy-first) and novel non-intuitive heuristics. By reducing propagation steps in LLM deployments, MetaJuLS contributes to Green AI by directly reducing inference carbon footprint.", "AI": {"tldr": "MetaJuLS\uff1a\u4e00\u79cd\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7ea6\u675f\u4f20\u64ad\u5b9e\u73b0\u8de8\u8bed\u8a00\u548c\u4efb\u52a1\u7684\u901a\u7528\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u76f8\u6bd4GPU\u4f18\u5316\u57fa\u7ebf\u63d0\u901f1.5-2\u500d\uff0c\u4ec5\u97005-10\u68af\u5ea6\u6b65\u5373\u53ef\u9002\u5e94\u65b0\u4efb\u52a1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u7ed3\u6784\u5316\u63a8\u7406\uff08\u5982JSON\u6a21\u5f0f\u5f3a\u5236\u6267\u884c\u3001\u591a\u8bed\u8a00\u89e3\u6790\uff09\uff0c\u8f93\u51fa\u5fc5\u987b\u6ee1\u8db3\u590d\u6742\u7ea6\u675f\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u4efb\u52a1\u8fdb\u884c\u4e13\u95e8\u8bad\u7ec3\uff0c\u6548\u7387\u4f4e\u4e0b\u4e14\u96be\u4ee5\u8de8\u57df\u9002\u5e94\u3002", "method": "\u63d0\u51faMetaJuLS\u5143\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06\u7ed3\u6784\u5316\u63a8\u7406\u5efa\u6a21\u4e3a\u81ea\u9002\u5e94\u7ea6\u675f\u4f20\u64ad\u95ee\u9898\u3002\u4f7f\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548c\u5143\u5b66\u4e60\u8bad\u7ec3\u901a\u7528\u7ea6\u675f\u4f20\u64ad\u7b56\u7565\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u91cd\u8bad\u7ec3\u5373\u53ef\u5e94\u7528\u4e8e\u4e0d\u540c\u8bed\u8a00\u548c\u4efb\u52a1\u3002", "result": "\u5728Universal Dependencies\uff0810\u79cd\u8bed\u8a00\uff09\u548cLLM\u7ea6\u675f\u751f\u6210\uff08LogicBench\u3001GSM8K-Constrained\uff09\u4e0a\uff0cMetaJuLS\u76f8\u6bd4GPU\u4f18\u5316\u57fa\u7ebf\u83b7\u5f971.5-2\u500d\u52a0\u901f\uff0c\u7cbe\u5ea6\u4ec5\u6bd4\u6700\u5148\u8fdb\u89e3\u6790\u5668\u4f4e0.2%\u3002\u4ec5\u97005-10\u68af\u5ea6\u6b65\uff085-15\u79d2\uff09\u5373\u53ef\u9002\u5e94\u65b0\u8bed\u8a00/\u4efb\u52a1\uff0c\u800c\u975e\u6570\u5c0f\u65f6\u8bad\u7ec3\u3002", "conclusion": "MetaJuLS\u901a\u8fc7\u51cf\u5c11LLM\u90e8\u7f72\u4e2d\u7684\u4f20\u64ad\u6b65\u9aa4\uff0c\u76f4\u63a5\u964d\u4f4e\u63a8\u7406\u78b3\u8db3\u8ff9\uff0c\u4fc3\u8fdb\u7eff\u8272AI\u3002\u7b56\u7565\u5206\u6790\u53d1\u73b0\u5176\u5b66\u4e60\u5230\u7c7b\u4f3c\u4eba\u7c7b\u7684\u89e3\u6790\u7b56\u7565\uff08\u6613\u4f18\u5148\uff09\u548c\u65b0\u9896\u7684\u975e\u76f4\u89c2\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002"}}
{"id": "2601.00166", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00166", "abs": "https://arxiv.org/abs/2601.00166", "authors": ["Yongmin Yoo", "Kris W Pan"], "title": "Pat-DEVAL: Chain-of-Legal-Thought Evaluation for Patent Description", "comment": null, "summary": "Patent descriptions must deliver comprehensive technical disclosure while meeting strict legal standards such as enablement and written description requirements. Although large language models have enabled end-to-end automated patent drafting, existing evaluation approaches fail to assess long-form structural coherence and statutory compliance specific to descriptions. We propose Pat-DEVAL, the first multi-dimensional evaluation framework dedicated to patent description bodies. Leveraging the LLM-as-a-judge paradigm, Pat-DEVAL introduces Chain-of-Legal-Thought (CoLT), a legally-constrained reasoning mechanism that enforces sequential patent-law-specific analysis. Experiments validated by patent expert on our Pap2Pat-EvalGold dataset demonstrate that Pat-DEVAL achieves a Pearson correlation of 0.69, significantly outperforming baseline metrics and existing LLM evaluators. Notably, the framework exhibits a superior correlation of 0.73 in Legal-Professional Compliance, proving that the explicit injection of statutory constraints is essential for capturing nuanced legal validity. By establishing a new standard for ensuring both technical soundness and legal compliance, Pat-DEVAL provides a robust methodological foundation for the practical deployment of automated patent drafting systems.", "AI": {"tldr": "Pat-DEVAL\uff1a\u9996\u4e2a\u4e13\u4e3a\u4e13\u5229\u8bf4\u660e\u4e66\u8bbe\u8ba1\u7684\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6cd5\u5f8b\u7ea6\u675f\u63a8\u7406\u673a\u5236\u8bc4\u4f30\u957f\u6587\u672c\u7ed3\u6784\u8fde\u8d2f\u6027\u548c\u6cd5\u5b9a\u5408\u89c4\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4e13\u5229\u81ea\u52a8\u64b0\u5199\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u8bc4\u4f30\u957f\u6587\u672c\u7ed3\u6784\u8fde\u8d2f\u6027\u548c\u6cd5\u5b9a\u5408\u89c4\u6027\uff08\u5982\u5145\u5206\u516c\u5f00\u548c\u4e66\u9762\u63cf\u8ff0\u8981\u6c42\uff09\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u4e13\u5229\u8bf4\u660e\u4e66\u7684\u6cd5\u5f8b\u6280\u672f\u7279\u70b9\u8bbe\u8ba1\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faPat-DEVAL\u6846\u67b6\uff0c\u91c7\u7528LLM-as-a-judge\u8303\u5f0f\uff0c\u5f15\u5165Chain-of-Legal-Thought\uff08CoLT\uff09\u6cd5\u5f8b\u7ea6\u675f\u63a8\u7406\u673a\u5236\uff0c\u5f3a\u5236\u6267\u884c\u987a\u5e8f\u6027\u4e13\u5229\u6cd5\u7279\u5b9a\u5206\u6790\u3002", "result": "\u5728Pap2Pat-EvalGold\u6570\u636e\u96c6\u4e0a\uff0cPat-DEVAL\u8fbe\u52300.69\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6307\u6807\u548c\u73b0\u6709LLM\u8bc4\u4f30\u5668\uff1b\u5728\u6cd5\u5f8b\u4e13\u4e1a\u5408\u89c4\u6027\u65b9\u9762\u8fbe\u52300.73\u7684\u4f18\u8d8a\u76f8\u5173\u6027\u3002", "conclusion": "\u901a\u8fc7\u660e\u786e\u6ce8\u5165\u6cd5\u5b9a\u7ea6\u675f\u6761\u4ef6\uff0cPat-DEVAL\u4e3a\u81ea\u52a8\u4e13\u5229\u64b0\u5199\u7cfb\u7edf\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u65b9\u6cd5\u57fa\u7840\uff0c\u5efa\u7acb\u4e86\u786e\u4fdd\u6280\u672f\u5408\u7406\u6027\u548c\u6cd5\u5f8b\u5408\u89c4\u6027\u7684\u65b0\u6807\u51c6\u3002"}}
{"id": "2601.00181", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00181", "abs": "https://arxiv.org/abs/2601.00181", "authors": ["Cheonkam Jeong", "Adeline Nyamathi"], "title": "Understanding Emotion in Discourse: Recognition Insights and Linguistic Patterns for Generation", "comment": null, "summary": "While Emotion Recognition in Conversation (ERC) has achieved high accuracy, two critical gaps remain: a limited understanding of \\textit{which} architectural choices actually matter, and a lack of linguistic analysis connecting recognition to generation. We address both gaps through a systematic analysis of the IEMOCAP dataset.\n  For recognition, we conduct a rigorous ablation study with 10-seed evaluation and report three key findings. First, conversational context is paramount, with performance saturating rapidly -- 90\\% of the total gain achieved within just the most recent 10--30 preceding turns (depending on the label set). Second, hierarchical sentence representations help at utterance-level, but this benefit disappears once conversational context is provided, suggesting that context subsumes intra-utterance structure. Third, external affective lexicons (SenticNet) provide no gain, indicating that pre-trained encoders already capture necessary emotional semantics. With simple architectures using strictly causal context, we achieve 82.69\\% (4-way) and 67.07\\% (6-way) weighted F1, outperforming prior text-only methods including those using bidirectional context.\n  For linguistic analysis, we analyze 5,286 discourse marker occurrences and find a significant association between emotion and marker positioning ($p < .0001$). Notably, \"sad\" utterances exhibit reduced left-periphery marker usage (21.9\\%) compared to other emotions (28--32\\%), consistent with theories linking left-periphery markers to active discourse management. This connects to our recognition finding that sadness benefits most from context (+22\\%p): lacking explicit pragmatic signals, sad utterances require conversational history for disambiguation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u5206\u6790IEMOCAP\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u60c5\u611f\u5bf9\u8bdd\u8bc6\u522b\u4e2d\u7684\u4e24\u4e2a\u5173\u952e\u7a7a\u767d\uff1a\u8bc6\u522b\u67b6\u6784\u9009\u62e9\u7684\u6709\u6548\u6027\u8bc4\u4f30\uff0c\u4ee5\u53ca\u8bc6\u522b\u4e0e\u751f\u6210\u4e4b\u95f4\u7684\u8bed\u8a00\u5b66\u8fde\u63a5\u5206\u6790\u3002", "motivation": "\u5f53\u524d\u60c5\u611f\u5bf9\u8bdd\u8bc6\u522b(ERC)\u867d\u7136\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u4f46\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7a7a\u767d\uff1a1) \u7f3a\u4e4f\u5bf9\u54ea\u4e9b\u67b6\u6784\u9009\u62e9\u771f\u6b63\u91cd\u8981\u7684\u7406\u89e3\uff1b2) \u7f3a\u4e4f\u5c06\u8bc6\u522b\u4e0e\u751f\u6210\u8fde\u63a5\u8d77\u6765\u7684\u8bed\u8a00\u5b66\u5206\u6790\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u586b\u8865\u8fd9\u4e9b\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u5206\u6790\u65b9\u6cd5\uff0c\u5bf9IEMOCAP\u6570\u636e\u96c6\u8fdb\u884c\uff1a1) \u8bc6\u522b\u65b9\u9762\uff1a\u8fdb\u884c\u4e25\u683c\u7684\u6d88\u878d\u7814\u7a76\uff0c\u4f7f\u752810\u79cd\u968f\u673a\u79cd\u5b50\u8bc4\u4f30\uff0c\u5206\u6790\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u3001\u5206\u5c42\u53e5\u5b50\u8868\u793a\u548c\u5916\u90e8\u60c5\u611f\u8bcd\u5178\u7684\u5f71\u54cd\uff1b2) \u8bed\u8a00\u5b66\u5206\u6790\uff1a\u5206\u67905,286\u4e2a\u8bdd\u8bed\u6807\u8bb0\u51fa\u73b0\u60c5\u51b5\uff0c\u7814\u7a76\u60c5\u611f\u4e0e\u6807\u8bb0\u4f4d\u7f6e\u4e4b\u95f4\u7684\u5173\u8054\u3002", "result": "\u8bc6\u522b\u65b9\u9762\uff1a1) \u5bf9\u8bdd\u4e0a\u4e0b\u6587\u81f3\u5173\u91cd\u8981\uff0c90%\u7684\u6027\u80fd\u589e\u76ca\u6765\u81ea\u6700\u8fd110-30\u8f6e\u5bf9\u8bdd\uff1b2) \u5206\u5c42\u53e5\u5b50\u8868\u793a\u5728\u8bdd\u8bed\u5c42\u9762\u6709\u5e2e\u52a9\uff0c\u4f46\u63d0\u4f9b\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u540e\u6b64\u4f18\u52bf\u6d88\u5931\uff1b3) \u5916\u90e8\u60c5\u611f\u8bcd\u5178\u65e0\u589e\u76ca\u3002\u4f7f\u7528\u7b80\u5355\u67b6\u6784\u8fbe\u523082.69%(4\u7c7b)\u548c67.07%(6\u7c7b)\u52a0\u6743F1\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u8bed\u8a00\u5b66\u5206\u6790\uff1a\u53d1\u73b0\u60c5\u611f\u4e0e\u8bdd\u8bed\u6807\u8bb0\u4f4d\u7f6e\u663e\u8457\u76f8\u5173(p<.0001)\uff0c\u60b2\u4f24\u8bdd\u8bed\u7684\u5de6\u8fb9\u7f18\u6807\u8bb0\u4f7f\u7528\u7387(21.9%)\u4f4e\u4e8e\u5176\u4ed6\u60c5\u611f(28-32%)\u3002", "conclusion": "\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u662f\u60c5\u611f\u8bc6\u522b\u7684\u5173\u952e\u56e0\u7d20\uff0c\u7b80\u5355\u7684\u56e0\u679c\u4e0a\u4e0b\u6587\u67b6\u6784\u5c31\u80fd\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002\u60b2\u4f24\u8bdd\u8bed\u7f3a\u4e4f\u663e\u6027\u8bed\u7528\u4fe1\u53f7\uff0c\u6700\u9700\u8981\u5bf9\u8bdd\u5386\u53f2\u8fdb\u884c\u6d88\u6b67\uff0c\u8fd9\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u60b2\u4f24\u60c5\u611f\u4ece\u4e0a\u4e0b\u6587\u4e2d\u83b7\u76ca\u6700\u5927(+22%)\u3002\u7814\u7a76\u4e3a\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u67b6\u6784\u6307\u5bfc\uff0c\u5e76\u5efa\u7acb\u4e86\u8bc6\u522b\u4e0e\u751f\u6210\u4e4b\u95f4\u7684\u8bed\u8a00\u5b66\u8054\u7cfb\u3002"}}
{"id": "2601.00087", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00087", "abs": "https://arxiv.org/abs/2601.00087", "authors": ["Zhaoan Wang", "Junchao Li", "Mahdi Mohammad", "Shaoping Xiao"], "title": "Reinforcement learning with timed constraints for robotics motion planning", "comment": null, "summary": "Robotic systems operating in dynamic and uncertain environments increasingly require planners that satisfy complex task sequences while adhering to strict temporal constraints. Metric Interval Temporal Logic (MITL) offers a formal and expressive framework for specifying such time-bounded requirements; however, integrating MITL with reinforcement learning (RL) remains challenging due to stochastic dynamics and partial observability. This paper presents a unified automata-based RL framework for synthesizing policies in both Markov Decision Processes (MDPs) and Partially Observable Markov Decision Processes (POMDPs) under MITL specifications. MITL formulas are translated into Timed Limit-Deterministic Generalized B\u00fcchi Automata (Timed-LDGBA) and synchronized with the underlying decision process to construct product timed models suitable for Q-learning. A simple yet expressive reward structure enforces temporal correctness while allowing additional performance objectives. The approach is validated in three simulation studies: a $5 \\times 5$ grid-world formulated as an MDP, a $10 \\times 10$ grid-world formulated as a POMDP, and an office-like service-robot scenario. Results demonstrate that the proposed framework consistently learns policies that satisfy strict time-bounded requirements under stochastic transitions, scales to larger state spaces, and remains effective in partially observable environments, highlighting its potential for reliable robotic planning in time-critical and uncertain settings.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u52a8\u673a\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728MDP\u548cPOMDP\u4e2d\u5408\u6210\u6ee1\u8db3MITL\u65f6\u5e8f\u903b\u8f91\u7ea6\u675f\u7684\u7b56\u7565", "motivation": "\u52a8\u6001\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u9700\u8981\u6ee1\u8db3\u590d\u6742\u65f6\u5e8f\u7ea6\u675f\u7684\u89c4\u5212\u5668\uff0c\u4f46MITL\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u9762\u4e34\u968f\u673a\u52a8\u6001\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u7684\u6311\u6218", "method": "\u5c06MITL\u516c\u5f0f\u8f6c\u6362\u4e3aTimed-LDGBA\u81ea\u52a8\u673a\uff0c\u4e0e\u51b3\u7b56\u8fc7\u7a0b\u540c\u6b65\u6784\u5efa\u4ea7\u54c1\u65f6\u5e8f\u6a21\u578b\uff0c\u91c7\u7528\u7b80\u5355\u800c\u5bcc\u6709\u8868\u8fbe\u529b\u7684\u5956\u52b1\u7ed3\u6784\uff0c\u9002\u7528\u4e8eQ-learning", "result": "\u5728\u4e09\u4e2a\u4eff\u771f\u7814\u7a76\u4e2d\u9a8c\u8bc1\uff1a5\u00d75\u7f51\u683c\u4e16\u754cMDP\u300110\u00d710\u7f51\u683c\u4e16\u754cPOMDP\u548c\u529e\u516c\u5ba4\u670d\u52a1\u673a\u5668\u4eba\u573a\u666f\uff0c\u6846\u67b6\u80fd\u5b66\u4e60\u6ee1\u8db3\u4e25\u683c\u65f6\u95f4\u7ea6\u675f\u7684\u7b56\u7565\uff0c\u53ef\u6269\u5c55\u5230\u66f4\u5927\u72b6\u6001\u7a7a\u95f4\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u6709\u6548", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65f6\u95f4\u5173\u952e\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u53ef\u9760\u673a\u5668\u4eba\u89c4\u5212\u63d0\u4f9b\u4e86\u6f5c\u529b\uff0c\u80fd\u591f\u5904\u7406\u968f\u673a\u8f6c\u79fb\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\uff0c\u6ee1\u8db3\u4e25\u683c\u7684\u65f6\u95f4\u7ea6\u675f\u8981\u6c42"}}
{"id": "2601.00202", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00202", "abs": "https://arxiv.org/abs/2601.00202", "authors": ["Wang Xing", "Wei Song", "Siyu Lin", "Chen Wu", "Zhesi Li", "Man Wang"], "title": "Knowledge Distillation for Temporal Knowledge Graph Reasoning with Large Language Models", "comment": null, "summary": "Reasoning over temporal knowledge graphs (TKGs) is fundamental to improving the efficiency and reliability of intelligent decision-making systems and has become a key technological foundation for future artificial intelligence applications. Despite recent progress, existing TKG reasoning models typically rely on large parameter sizes and intensive computation, leading to high hardware costs and energy consumption. These constraints hinder their deployment on resource-constrained, low-power, and distributed platforms that require real-time inference. Moreover, most existing model compression and distillation techniques are designed for static knowledge graphs and fail to adequately capture the temporal dependencies inherent in TKGs, often resulting in degraded reasoning performance. To address these challenges, we propose a distillation framework specifically tailored for temporal knowledge graph reasoning. Our approach leverages large language models as teacher models to guide the distillation process, enabling effective transfer of both structural and temporal reasoning capabilities to lightweight student models. By integrating large-scale public knowledge with task-specific temporal information, the proposed framework enhances the student model's ability to model temporal dynamics while maintaining a compact and efficient architecture. Extensive experiments on multiple publicly available benchmark datasets demonstrate that our method consistently outperforms strong baselines, achieving a favorable trade-off between reasoning accuracy, computational efficiency, and practical deployability.", "AI": {"tldr": "\u63d0\u51fa\u9488\u5bf9\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u7684\u84b8\u998f\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u5c06\u7ed3\u6784\u548c\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u8f6c\u79fb\u5230\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7684\u540c\u65f6\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u6a21\u578b\u53c2\u6570\u5927\u3001\u8ba1\u7b97\u5bc6\u96c6\uff0c\u786c\u4ef6\u6210\u672c\u548c\u80fd\u8017\u9ad8\uff0c\u96be\u4ee5\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5b9e\u65f6\u63a8\u7406\u5e73\u53f0\u3002\u73b0\u6709\u538b\u7f29\u548c\u84b8\u998f\u6280\u672f\u4e3b\u8981\u9488\u5bf9\u9759\u6001\u77e5\u8bc6\u56fe\u8c31\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u65f6\u5e8f\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u63a8\u7406\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e13\u95e8\u9488\u5bf9\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u7684\u84b8\u998f\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u6307\u5bfc\u84b8\u998f\u8fc7\u7a0b\uff0c\u6709\u6548\u8f6c\u79fb\u7ed3\u6784\u548c\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u5230\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\u3002\u901a\u8fc7\u6574\u5408\u5927\u89c4\u6a21\u516c\u5171\u77e5\u8bc6\u548c\u4efb\u52a1\u7279\u5b9a\u65f6\u5e8f\u4fe1\u606f\uff0c\u589e\u5f3a\u5b66\u751f\u6a21\u578b\u5bf9\u65f6\u5e8f\u52a8\u6001\u7684\u5efa\u6a21\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u7d27\u51d1\u9ad8\u6548\u7684\u67b6\u6784\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728\u63a8\u7406\u51c6\u786e\u6027\u3001\u8ba1\u7b97\u6548\u7387\u548c\u5b9e\u9645\u53ef\u90e8\u7f72\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6709\u5229\u7684\u5e73\u8861\u3002", "conclusion": "\u63d0\u51fa\u7684\u84b8\u998f\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u7684\u6548\u7387\u548c\u90e8\u7f72\u95ee\u9898\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u6307\u5bfc\u7684\u84b8\u998f\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u9ad8\u6027\u80fd\u65f6\u5e8f\u63a8\u7406\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u7684\u5b9e\u65f6\u63a8\u7406\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2601.00126", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00126", "abs": "https://arxiv.org/abs/2601.00126", "authors": ["Utkarsh A Mishra", "David He", "Yongxin Chen", "Danfei Xu"], "title": "Compositional Diffusion with Guided search for Long-Horizon Planning", "comment": "38 pages, 18 figures", "summary": "Generative models have emerged as powerful tools for planning, with compositional approaches offering particular promise for modeling long-horizon task distributions by composing together local, modular generative models. This compositional paradigm spans diverse domains, from multi-step manipulation planning to panoramic image synthesis to long video generation. However, compositional generative models face a critical challenge: when local distributions are multimodal, existing composition methods average incompatible modes, producing plans that are neither locally feasible nor globally coherent. We propose Compositional Diffusion with Guided Search (CDGS), which addresses this \\emph{mode averaging} problem by embedding search directly within the diffusion denoising process. Our method explores diverse combinations of local modes through population-based sampling, prunes infeasible candidates using likelihood-based filtering, and enforces global consistency through iterative resampling between overlapping segments. CDGS matches oracle performance on seven robot manipulation tasks, outperforming baselines that lack compositionality or require long-horizon training data. The approach generalizes across domains, enabling coherent text-guided panoramic images and long videos through effective local-to-global message passing. More details: https://cdgsearch.github.io/", "AI": {"tldr": "CDGS\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec4\u5408\u6269\u6563\u5f15\u5bfc\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u641c\u7d22\u5d4c\u5165\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u6765\u89e3\u51b3\u7ec4\u5408\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6a21\u5f0f\u5e73\u5747\u95ee\u9898\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u3001\u5168\u666f\u56fe\u50cf\u548c\u957f\u89c6\u9891\u751f\u6210\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u7ec4\u5408\u751f\u6210\u6a21\u578b\u5728\u5efa\u6a21\u957f\u65f6\u7a0b\u4efb\u52a1\u5206\u5e03\u65b9\u9762\u5f88\u6709\u524d\u666f\uff0c\u4f46\u5f53\u5c40\u90e8\u5206\u5e03\u662f\u591a\u6a21\u6001\u65f6\uff0c\u73b0\u6709\u7ec4\u5408\u65b9\u6cd5\u4f1a\u5e73\u5747\u4e0d\u517c\u5bb9\u7684\u6a21\u5f0f\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u8ba1\u5212\u65e2\u4e0d\u53ef\u884c\u4e5f\u4e0d\u8fde\u8d2f\u3002", "method": "CDGS\u5c06\u641c\u7d22\u76f4\u63a5\u5d4c\u5165\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\uff1a\u901a\u8fc7\u57fa\u4e8e\u79cd\u7fa4\u7684\u91c7\u6837\u63a2\u7d22\u5c40\u90e8\u6a21\u5f0f\u7684\u591a\u6837\u7ec4\u5408\uff0c\u4f7f\u7528\u57fa\u4e8e\u4f3c\u7136\u7684\u8fc7\u6ee4\u526a\u679d\u4e0d\u53ef\u884c\u5019\u9009\uff0c\u901a\u8fc7\u91cd\u53e0\u6bb5\u4e4b\u95f4\u7684\u8fed\u4ee3\u91cd\u91c7\u6837\u5f3a\u5236\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u57287\u4e2a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u5339\u914d\u4e86oracle\u6027\u80fd\uff0c\u4f18\u4e8e\u7f3a\u4e4f\u7ec4\u5408\u6027\u6216\u9700\u8981\u957f\u65f6\u7a0b\u8bad\u7ec3\u6570\u636e\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u6587\u672c\u5f15\u5bfc\u7684\u5168\u666f\u56fe\u50cf\u548c\u957f\u89c6\u9891\u751f\u6210\u4efb\u52a1\u3002", "conclusion": "CDGS\u6709\u6548\u89e3\u51b3\u4e86\u7ec4\u5408\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6a21\u5f0f\u5e73\u5747\u95ee\u9898\uff0c\u901a\u8fc7\u5c40\u90e8\u5230\u5168\u5c40\u7684\u6d88\u606f\u4f20\u9012\u5b9e\u73b0\u4e86\u8de8\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u957f\u65f6\u7a0b\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7ec4\u5408\u751f\u6210\u6846\u67b6\u3002"}}
{"id": "2601.00216", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00216", "abs": "https://arxiv.org/abs/2601.00216", "authors": ["Jinning Zhang", "Jie Song", "Wenhui Tu", "Zecheng Li", "Jingxuan Li", "Jin Li", "Xuan Liu", "Taole Sha", "Zichen Wei", "Yan Li"], "title": "From Evidence-Based Medicine to Knowledge Graph: Retrieval-Augmented Generation for Sports Rehabilitation and a Domain Benchmark", "comment": "35 pages, 5 figures", "summary": "In medicine, large language models (LLMs) increasingly rely on retrieval-augmented generation (RAG) to ground outputs in up-to-date external evidence. However, current RAG approaches focus primarily on performance improvements while overlooking evidence-based medicine (EBM) principles. This study addresses two key gaps: (1) the lack of PICO alignment between queries and retrieved evidence, and (2) the absence of evidence hierarchy considerations during reranking. We present a generalizable strategy for adapting EBM to graph-based RAG, integrating the PICO framework into knowledge graph construction and retrieval, and proposing a Bayesian-inspired reranking algorithm to calibrate ranking scores by evidence grade without introducing predefined weights. We validated this framework in sports rehabilitation, a literature-rich domain currently lacking RAG systems and benchmarks. We released a knowledge graph (357,844 nodes and 371,226 edges) and a reusable benchmark of 1,637 QA pairs. The system achieved 0.830 nugget coverage, 0.819 answer faithfulness, 0.882 semantic similarity, and 0.788 PICOT match accuracy. In a 5-point Likert evaluation, five expert clinicians rated the system 4.66-4.84 across factual accuracy, faithfulness, relevance, safety, and PICO alignment. These findings demonstrate that the proposed EBM adaptation strategy improves retrieval and answer quality and is transferable to other clinical domains. The released resources also help address the scarcity of RAG datasets in sports rehabilitation.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u5faa\u8bc1\u533b\u5b66\u539f\u5219\u878d\u5165\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7PICO\u6846\u67b6\u5bf9\u9f50\u548c\u8d1d\u53f6\u65af\u91cd\u6392\u5e8f\u63d0\u5347\u533b\u5b66\u95ee\u7b54\u8d28\u91cf\uff0c\u5e76\u5728\u8fd0\u52a8\u5eb7\u590d\u9886\u57df\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u9886\u57df\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u5ffd\u89c6\u4e86\u5faa\u8bc1\u533b\u5b66\u539f\u5219\u3002\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1) \u67e5\u8be2\u4e0e\u68c0\u7d22\u8bc1\u636e\u4e4b\u95f4\u7f3a\u4e4fPICO\u6846\u67b6\u5bf9\u9f50\uff1b2) \u91cd\u6392\u5e8f\u8fc7\u7a0b\u4e2d\u672a\u8003\u8651\u8bc1\u636e\u7b49\u7ea7\u5c42\u6b21\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u5faa\u8bc1\u533b\u5b66\u9002\u5e94\u7b56\u7565\uff0c\u5c06PICO\u6846\u67b6\u6574\u5408\u5230\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u548c\u68c0\u7d22\u4e2d\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8d1d\u53f6\u65af\u542f\u53d1\u7684\u91cd\u6392\u5e8f\u7b97\u6cd5\uff0c\u5728\u4e0d\u5f15\u5165\u9884\u5b9a\u4e49\u6743\u91cd\u7684\u60c5\u51b5\u4e0b\u6839\u636e\u8bc1\u636e\u7b49\u7ea7\u6821\u51c6\u6392\u5e8f\u5206\u6570\u3002", "result": "\u5728\u8fd0\u52a8\u5eb7\u590d\u9886\u57df\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u6784\u5efa\u4e86\u5305\u542b357,844\u4e2a\u8282\u70b9\u548c371,226\u6761\u8fb9\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u521b\u5efa\u4e861,637\u4e2aQA\u5bf9\u7684\u53ef\u590d\u7528\u57fa\u51c6\u3002\u7cfb\u7edf\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff1anugget\u8986\u76d6\u5ea60.830\u3001\u7b54\u6848\u5fe0\u5b9e\u5ea60.819\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea60.882\u3001PICOT\u5339\u914d\u51c6\u786e\u73870.788\u3002\u4e94\u4f4d\u4e34\u5e8a\u4e13\u5bb6\u57285\u70b9\u674e\u514b\u7279\u91cf\u8868\u4e0a\u7ed9\u51fa\u4e864.66-4.84\u7684\u9ad8\u8bc4\u5206\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5faa\u8bc1\u533b\u5b66\u9002\u5e94\u7b56\u7565\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u68c0\u7d22\u548c\u7b54\u6848\u8d28\u91cf\uff0c\u800c\u4e14\u53ef\u8f6c\u79fb\u5230\u5176\u4ed6\u4e34\u5e8a\u9886\u57df\u3002\u53d1\u5e03\u7684\u8d44\u6e90\u6709\u52a9\u4e8e\u89e3\u51b3\u8fd0\u52a8\u5eb7\u590d\u9886\u57dfRAG\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u95ee\u9898\u3002"}}
{"id": "2601.00163", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00163", "abs": "https://arxiv.org/abs/2601.00163", "authors": ["Junfeng Chen", "Yuxiao Zhu", "Xintong Zhang", "Bing Luo", "Meng Guo"], "title": "SLEI3D: Simultaneous Exploration and Inspection via Heterogeneous Fleets under Limited Communication", "comment": null, "summary": "Robotic fleets such as unmanned aerial and ground vehicles have been widely used for routine inspections of static environments, where the areas of interest are known and planned in advance. However, in many applications, such areas of interest are unknown and should be identified online during exploration. Thus, this paper considers the problem of simultaneous exploration, inspection of unknown environments and then real-time communication to a mobile ground control station to report the findings. The heterogeneous robots are equipped with different sensors, e.g., long-range lidars for fast exploration and close-range cameras for detailed inspection. Furthermore, global communication is often unavailable in such environments, where the robots can only communicate with each other via ad-hoc wireless networks when they are in close proximity and free of obstruction. This work proposes a novel planning and coordination framework (SLEI3D) that integrates the online strategies for collaborative 3D exploration, adaptive inspection and timely communication (via the intermit-tent or proactive protocols). To account for uncertainties w.r.t. the number and location of features, a multi-layer and multi-rate planning mechanism is developed for inter-and-intra robot subgroups, to actively meet and coordinate their local plans. The proposed framework is validated extensively via high-fidelity simulations of numerous large-scale missions with up to 48 robots and 384 thousand cubic meters. Hardware experiments of 7 robots are also conducted. Project website is available at https://junfengchen-robotics.github.io/SLEI3D/.", "AI": {"tldr": "SLEI3D\u6846\u67b6\u89e3\u51b3\u5f02\u6784\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u672a\u77e5\u73af\u5883\u4e2d\u540c\u65f6\u8fdb\u884c\u63a2\u7d22\u3001\u68c0\u67e5\u548c\u5b9e\u65f6\u901a\u4fe1\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u5c42\u591a\u901f\u7387\u89c4\u5212\u673a\u5236\u534f\u8c03\u673a\u5668\u4eba\u5b50\u7ec4\uff0c\u652f\u6301\u95f4\u6b47\u6027\u548c\u4e3b\u52a8\u6027\u901a\u4fe1\u534f\u8bae\u3002", "motivation": "\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\uff0c\u611f\u5174\u8da3\u533a\u57df\u662f\u672a\u77e5\u7684\uff0c\u9700\u8981\u5728\u63a2\u7d22\u8fc7\u7a0b\u4e2d\u5728\u7ebf\u8bc6\u522b\u3002\u5f02\u6784\u673a\u5668\u4eba\u914d\u5907\u4e0d\u540c\u4f20\u611f\u5668\uff08\u5982\u957f\u8ddd\u79bb\u6fc0\u5149\u96f7\u8fbe\u7528\u4e8e\u5feb\u901f\u63a2\u7d22\uff0c\u8fd1\u8ddd\u79bb\u76f8\u673a\u7528\u4e8e\u8be6\u7ec6\u68c0\u67e5\uff09\uff0c\u4e14\u73af\u5883\u901a\u5e38\u7f3a\u4e4f\u5168\u5c40\u901a\u4fe1\uff0c\u53ea\u80fd\u901a\u8fc7ad-hoc\u65e0\u7ebf\u7f51\u7edc\u5728\u8fd1\u8ddd\u79bb\u65e0\u969c\u788d\u65f6\u901a\u4fe1\u3002", "method": "\u63d0\u51faSLEI3D\u89c4\u5212\u534f\u8c03\u6846\u67b6\uff0c\u96c6\u6210\u5728\u7ebf\u534f\u4f5c3D\u63a2\u7d22\u3001\u81ea\u9002\u5e94\u68c0\u67e5\u548c\u53ca\u65f6\u901a\u4fe1\u7b56\u7565\u3002\u9488\u5bf9\u7279\u5f81\u6570\u91cf\u548c\u4f4d\u7f6e\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5f00\u53d1\u591a\u5c42\u591a\u901f\u7387\u89c4\u5212\u673a\u5236\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u5b50\u7ec4\u95f4\u548c\u5b50\u7ec4\u5185\u7684\u4e3b\u52a8\u4f1a\u5408\u548c\u672c\u5730\u8ba1\u5212\u534f\u8c03\u3002", "result": "\u901a\u8fc7\u5927\u89c4\u6a21\u9ad8\u4fdd\u771f\u4eff\u771f\u9a8c\u8bc1\uff08\u6700\u591a48\u4e2a\u673a\u5668\u4eba\uff0c384\u5343\u7acb\u65b9\u7c73\uff09\uff0c\u5e76\u8fdb\u884c\u4e867\u4e2a\u673a\u5668\u4eba\u7684\u786c\u4ef6\u5b9e\u9a8c\u3002\u9879\u76ee\u7f51\u7ad9\u5df2\u516c\u5f00\u3002", "conclusion": "SLEI3D\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u534f\u540c\u63a2\u7d22\u3001\u68c0\u67e5\u548c\u901a\u4fe1\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u89c4\u5212\u673a\u5236\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4efb\u52a1\u6267\u884c\u3002"}}
{"id": "2601.00223", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00223", "abs": "https://arxiv.org/abs/2601.00223", "authors": ["Leonard Lin", "Adam Lensenmayer"], "title": "JP-TL-Bench: Anchored Pairwise LLM Evaluation for Bidirectional Japanese-English Translation", "comment": "24 pages, 5 figures, 8 tables", "summary": "We introduce JP-TL-Bench, a lightweight, open benchmark designed to guide the iterative development of Japanese-English translation systems. In this context, the challenge is often \"which of these two good translations is better?\" rather than \"is this translation acceptable?\" This distinction matters for Japanese-English, where subtle choices in politeness, implicature, ellipsis, and register strongly affect perceived naturalness. JP-TL-Bench uses a protocol built to make LLM judging both reliable and affordable: it evaluates a candidate model via reference-free, pairwise LLM comparisons against a fixed, versioned anchor set. Pairwise results are aggregated with a Bradley-Terry model and reported as win rates plus a normalized 0-10 \"LT\" score derived from a logistic transform of fitted log-strengths. Because each candidate is scored against the same frozen anchor set, scores are structurally stable given the same base set, judge, and aggregation code.", "AI": {"tldr": "JP-TL-Bench\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u6307\u5bfc\u65e5\u82f1\u7ffb\u8bd1\u7cfb\u7edf\u7684\u8fed\u4ee3\u5f00\u53d1\uff0c\u4e13\u6ce8\u4e8e\u533a\u5206\"\u54ea\u4e2a\u7ffb\u8bd1\u66f4\u597d\"\u800c\u975e\"\u7ffb\u8bd1\u662f\u5426\u53ef\u63a5\u53d7\"\uff0c\u901a\u8fc7\u6210\u5bf9LLM\u6bd4\u8f83\u548cBradley-Terry\u6a21\u578b\u63d0\u4f9b\u7a33\u5b9a\u8bc4\u5206\u3002", "motivation": "\u65e5\u82f1\u7ffb\u8bd1\u4e2d\uff0c\u793c\u8c8c\u3001\u9690\u542b\u610f\u4e49\u3001\u7701\u7565\u548c\u8bed\u57df\u7b49\u7ec6\u5fae\u9009\u62e9\u5bf9\u81ea\u7136\u5ea6\u5f71\u54cd\u5f88\u5927\uff0c\u73b0\u6709\u8bc4\u4f30\u5f80\u5f80\u53ea\u80fd\u5224\u65ad\u7ffb\u8bd1\u662f\u5426\u53ef\u63a5\u53d7\uff0c\u800c\u65e0\u6cd5\u533a\u5206\"\u54ea\u4e2a\u597d\u7ffb\u8bd1\u66f4\u597d\"\u3002\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u65e5\u82f1\u7ffb\u8bd1\u7279\u70b9\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u4f7f\u7528\u65e0\u53c2\u8003\u7684\u6210\u5bf9LLM\u6bd4\u8f83\u65b9\u6cd5\uff0c\u5c06\u5019\u9009\u6a21\u578b\u4e0e\u56fa\u5b9a\u7684\u7248\u672c\u5316\u951a\u70b9\u96c6\u8fdb\u884c\u5bf9\u6bd4\u3002\u901a\u8fc7Bradley-Terry\u6a21\u578b\u805a\u5408\u7ed3\u679c\uff0c\u8ba1\u7b97\u80dc\u7387\u5e76\u8f6c\u6362\u4e3a0-10\u5206\u7684\"LT\"\u8bc4\u5206\uff08\u57fa\u4e8e\u903b\u8f91\u53d8\u6362\u7684\u5bf9\u6570\u5f3a\u5ea6\uff09\u3002", "result": "JP-TL-Bench\u63d0\u4f9b\u4e86\u7ed3\u6784\u7a33\u5b9a\u7684\u8bc4\u5206\u7cfb\u7edf\uff0c\u7531\u4e8e\u6bcf\u4e2a\u5019\u9009\u6a21\u578b\u90fd\u9488\u5bf9\u76f8\u540c\u7684\u51bb\u7ed3\u951a\u70b9\u96c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728\u76f8\u540c\u57fa\u7840\u96c6\u3001\u8bc4\u5224\u6807\u51c6\u548c\u805a\u5408\u4ee3\u7801\u4e0b\uff0c\u8bc4\u5206\u5177\u6709\u7ed3\u6784\u7a33\u5b9a\u6027\u3002", "conclusion": "JP-TL-Bench\u662f\u4e00\u4e2a\u53ef\u9760\u4e14\u7ecf\u6d4e\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u4e13\u95e8\u9488\u5bf9\u65e5\u82f1\u7ffb\u8bd1\u7684\u7ec6\u5fae\u5dee\u522b\u8bbe\u8ba1\uff0c\u80fd\u591f\u6709\u6548\u6307\u5bfc\u7ffb\u8bd1\u7cfb\u7edf\u7684\u8fed\u4ee3\u5f00\u53d1\uff0c\u89e3\u51b3\u4e86\"\u54ea\u4e2a\u597d\u7ffb\u8bd1\u66f4\u597d\"\u8fd9\u4e00\u6838\u5fc3\u6311\u6218\u3002"}}
{"id": "2601.00238", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00238", "abs": "https://arxiv.org/abs/2601.00238", "authors": ["Julia Di", "Kenneth A. W. Hoffmann", "Tony G. Chen", "Tian-Ao Ren", "Mark R. Cutkosky"], "title": "SLAP: Slapband-based Autonomous Perching Drone with Failure Recovery for Vertical Tree Trunks", "comment": "Paper accepted to IEEE Aerospace Conference 2026. This is a pre-print", "summary": "Perching allows unmanned aerial vehicles (UAVs) to reduce energy consumption, remain anchored for surface sampling operations, or stably survey their surroundings. Previous efforts for perching on vertical surfaces have predominantly focused on lightweight mechanical design solutions with relatively scant system-level integration. Furthermore, perching strategies for vertical surfaces commonly require high-speed, aggressive landing operations that are dangerous for a surveyor drone with sensitive electronics onboard. This work presents the preliminary investigation of a perching approach suitable for larger drones that both gently perches on vertical tree trunks and reacts and recovers from perch failures. The system in this work, called SLAP, consists of vision-based perch site detector, an IMU (inertial-measurement-unit)-based perch failure detector, an attitude controller for soft perching, an optical close-range detection system, and a fast active elastic gripper with microspines made from commercially-available slapbands. We validated this approach on a modified 1.2 kg commercial quadrotor with component and system analysis. Initial human-in-the-loop autonomous indoor flight experiments achieved a 75% perch success rate on a real oak tree segment across 20 flights, and 100% perch failure recovery across 2 flights with induced failures.", "AI": {"tldr": "SLAP\u7cfb\u7edf\u4e3a\u4e2d\u578b\u65e0\u4eba\u673a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6e29\u548c\u7684\u5782\u76f4\u6811\u5e72\u6816\u606f\u65b9\u6cd5\uff0c\u5177\u6709\u6816\u606f\u5931\u8d25\u68c0\u6d4b\u548c\u6062\u590d\u80fd\u529b\uff0c\u5728\u5ba4\u5185\u5b9e\u9a8c\u4e2d\u8fbe\u523075%\u7684\u6816\u606f\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u5782\u76f4\u8868\u9762\u6816\u606f\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u8f7b\u578b\u65e0\u4eba\u673a\uff0c\u91c7\u7528\u9ad8\u901f\u6fc0\u8fdb\u7684\u7740\u9646\u65b9\u5f0f\uff0c\u4e0d\u9002\u5408\u643a\u5e26\u654f\u611f\u7535\u5b50\u8bbe\u5907\u7684\u4e2d\u578b\u8c03\u67e5\u65e0\u4eba\u673a\u3002\u9700\u8981\u4e00\u79cd\u6e29\u548c\u3001\u5b89\u5168\u7684\u6816\u606f\u65b9\u6848\uff0c\u5e76\u5177\u5907\u5931\u8d25\u6062\u590d\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86SLAP\u7cfb\u7edf\uff0c\u5305\u542b\uff1a\u89c6\u89c9\u6816\u606f\u70b9\u68c0\u6d4b\u5668\u3001IMU\u6816\u606f\u5931\u8d25\u68c0\u6d4b\u5668\u3001\u59ff\u6001\u63a7\u5236\u5668\u3001\u5149\u5b66\u8fd1\u8ddd\u79bb\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4ee5\u53ca\u4f7f\u7528\u5e02\u552eslapbands\u5236\u6210\u7684\u5feb\u901f\u4e3b\u52a8\u5f39\u6027\u6293\u53d6\u5668\uff08\u5e26\u5fae\u523a\uff09\u3002\u57281.2kg\u5546\u7528\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5ba4\u5185\u81ea\u4e3b\u98de\u884c\u5b9e\u9a8c\u5728\u771f\u5b9e\u6a61\u6811\u6bb5\u4e0a\u8fdb\u884c\u4e8620\u6b21\u98de\u884c\uff0c\u8fbe\u523075%\u7684\u6816\u606f\u6210\u529f\u7387\u3002\u57282\u6b21\u8bf1\u5bfc\u5931\u8d25\u98de\u884c\u4e2d\uff0c\u5b9e\u73b0\u4e86100%\u7684\u6816\u606f\u5931\u8d25\u6062\u590d\u7387\u3002", "conclusion": "SLAP\u7cfb\u7edf\u4e3a\u4e2d\u578b\u65e0\u4eba\u673a\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u5782\u76f4\u6811\u5e72\u6816\u606f\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u6e29\u548c\u7684\u6816\u606f\u8fc7\u7a0b\u548c\u53ef\u9760\u7684\u5931\u8d25\u6062\u590d\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u7ea7\u96c6\u6210\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.00224", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2601.00224", "abs": "https://arxiv.org/abs/2601.00224", "authors": ["Yan Sun", "Ming Cai", "Stanley Kok"], "title": "Talk Less, Verify More: Improving LLM Assistants with Semantic Checks and Execution Feedback", "comment": null, "summary": "As large language model (LLM) assistants become increasingly integrated into enterprise workflows, their ability to generate accurate, semantically aligned, and executable outputs is critical. However, current conversational business analytics (CBA) systems often lack built-in verification mechanisms, leaving users to manually validate potentially flawed results. This paper introduces two complementary verification techniques: Q*, which performs reverse translation and semantic matching between code and user intent, and Feedback+, which incorporates execution feedback to guide code refinement. Embedded within a generator-discriminator framework, these mechanisms shift validation responsibilities from users to the system. Evaluations on three benchmark datasets, Spider, Bird, and GSM8K, demonstrate that both Q* and Feedback+ reduce error rates and task completion time. The study also identifies reverse translation as a key bottleneck, highlighting opportunities for future improvement. Overall, this work contributes a design-oriented framework for building more reliable, enterprise-grade GenAI systems capable of trustworthy decision support.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u4e92\u8865\u7684\u9a8c\u8bc1\u6280\u672f(Q*\u548cFeedback+)\u6765\u63d0\u9ad8LLM\u52a9\u624b\u5728\u5bf9\u8bdd\u5f0f\u5546\u4e1a\u5206\u6790\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u751f\u6210\u5668-\u5224\u522b\u5668\u6846\u67b6\u5c06\u9a8c\u8bc1\u8d23\u4efb\u4ece\u7528\u6237\u8f6c\u79fb\u5230\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u5bf9\u8bdd\u5f0f\u5546\u4e1a\u5206\u6790\u7cfb\u7edf\u7f3a\u4e4f\u5185\u7f6e\u9a8c\u8bc1\u673a\u5236\uff0c\u7528\u6237\u9700\u8981\u624b\u52a8\u9a8c\u8bc1\u53ef\u80fd\u9519\u8bef\u7684\u8f93\u51fa\uff0c\u8fd9\u5f71\u54cd\u4e86\u4f01\u4e1a\u5de5\u4f5c\u6d41\u7a0b\u4e2dLLM\u52a9\u624b\u751f\u6210\u51c6\u786e\u3001\u8bed\u4e49\u5bf9\u9f50\u4e14\u53ef\u6267\u884c\u8f93\u51fa\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u9a8c\u8bc1\u6280\u672f\uff1a1) Q*\uff1a\u901a\u8fc7\u53cd\u5411\u7ffb\u8bd1\u548c\u4ee3\u7801\u4e0e\u7528\u6237\u610f\u56fe\u7684\u8bed\u4e49\u5339\u914d\u8fdb\u884c\u9a8c\u8bc1\uff1b2) Feedback+\uff1a\u901a\u8fc7\u6267\u884c\u53cd\u9988\u6307\u5bfc\u4ee3\u7801\u4f18\u5316\u3002\u8fd9\u4e9b\u6280\u672f\u5d4c\u5165\u5728\u751f\u6210\u5668-\u5224\u522b\u5668\u6846\u67b6\u4e2d\u3002", "result": "\u5728Spider\u3001Bird\u548cGSM8K\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cQ*\u548cFeedback+\u90fd\u80fd\u964d\u4f4e\u9519\u8bef\u7387\u548c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u3002\u53cd\u5411\u7ffb\u8bd1\u88ab\u8bc6\u522b\u4e3a\u5173\u952e\u74f6\u9888\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u3001\u4f01\u4e1a\u7ea7\u7684\u751f\u6210\u5f0fAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8bbe\u8ba1\u5bfc\u5411\u7684\u6846\u67b6\uff0c\u80fd\u591f\u63d0\u4f9b\u53ef\u4fe1\u7684\u51b3\u7b56\u652f\u6301\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u53cd\u5411\u7ffb\u8bd1\u4f5c\u4e3a\u672a\u6765\u6539\u8fdb\u7684\u5173\u952e\u673a\u4f1a\u3002"}}
{"id": "2601.00271", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2601.00271", "abs": "https://arxiv.org/abs/2601.00271", "authors": ["Yuya Nagai", "Hiromitsu Nakamura", "Narito Shinmachi", "Yuta Higashizono", "Satoshi Ono"], "title": "Vehicle Painting Robot Path Planning Using Hierarchical Optimization", "comment": null, "summary": "In vehicle production factories, the vehicle painting process employs multiple robotic arms to simultaneously apply paint to car bodies advancing along a conveyor line. Designing paint paths for these robotic arms, which involves assigning car body areas to arms and determining paint sequences for each arm, remains a time-consuming manual task for engineers, indicating the demand for automation and design time reduction. The unique constraints of the painting process hinder the direct application of conventional robotic path planning techniques, such as those used in welding. Therefore, this paper formulates the design of paint paths as a hierarchical optimization problem, where the upper-layer subproblem resembles a vehicle routing problem (VRP), and the lower-layer subproblem involves detailed path planning. This approach allows the use of different optimization algorithms at each layer, and permits flexible handling of constraints specific to the vehicle painting process through the design of variable representation, constraints, repair operators, and an initialization process at the upper and lower layers. Experiments with three commercially available vehicle models demonstrated that the proposed method can automatically design paths that satisfy all constraints for vehicle painting with quality comparable to those created manually by engineers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8f66\u8f86\u55b7\u6f06\u8fc7\u7a0b\u4e2d\u591a\u673a\u68b0\u81c2\u7684\u8def\u5f84\u89c4\u5212\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u4e0a\u5c42VRP\u5f0f\u5206\u914d\u548c\u4e0b\u5c42\u8be6\u7ec6\u8def\u5f84\u89c4\u5212\uff0c\u80fd\u591f\u6ee1\u8db3\u55b7\u6f06\u5de5\u827a\u7684\u7279\u6b8a\u7ea6\u675f\u3002", "motivation": "\u5728\u6c7d\u8f66\u751f\u4ea7\u5de5\u5382\u4e2d\uff0c\u8f66\u8f86\u55b7\u6f06\u8fc7\u7a0b\u4f7f\u7528\u591a\u4e2a\u673a\u68b0\u81c2\u540c\u65f6\u5bf9\u4f20\u9001\u7ebf\u4e0a\u7684\u8f66\u8eab\u8fdb\u884c\u55b7\u6f06\u3002\u76ee\u524d\u55b7\u6f06\u8def\u5f84\u8bbe\u8ba1\u4ecd\u662f\u5de5\u7a0b\u5e08\u8017\u65f6\u7684\u624b\u52a8\u4efb\u52a1\uff0c\u9700\u8981\u81ea\u52a8\u5316\u548c\u51cf\u5c11\u8bbe\u8ba1\u65f6\u95f4\u3002\u4f20\u7edf\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u6280\u672f\uff08\u5982\u710a\u63a5\uff09\u65e0\u6cd5\u76f4\u63a5\u5e94\u7528\u4e8e\u55b7\u6f06\u8fc7\u7a0b\u7684\u72ec\u7279\u7ea6\u675f\u3002", "method": "\u5c06\u55b7\u6f06\u8def\u5f84\u8bbe\u8ba1\u6784\u5efa\u4e3a\u5206\u5c42\u4f18\u5316\u95ee\u9898\uff1a\u4e0a\u5c42\u5b50\u95ee\u9898\u7c7b\u4f3c\u4e8e\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff08VRP\uff09\uff0c\u8d1f\u8d23\u5c06\u8f66\u8eab\u533a\u57df\u5206\u914d\u7ed9\u673a\u68b0\u81c2\uff1b\u4e0b\u5c42\u5b50\u95ee\u9898\u6d89\u53ca\u8be6\u7ec6\u8def\u5f84\u89c4\u5212\u3002\u901a\u8fc7\u8bbe\u8ba1\u53d8\u91cf\u8868\u793a\u3001\u7ea6\u675f\u3001\u4fee\u590d\u7b97\u5b50\u548c\u521d\u59cb\u5316\u8fc7\u7a0b\uff0c\u7075\u6d3b\u5904\u7406\u55b7\u6f06\u8fc7\u7a0b\u7684\u7279\u5b9a\u7ea6\u675f\u3002", "result": "\u5728\u4e09\u79cd\u5546\u7528\u8f66\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u8bbe\u8ba1\u51fa\u6ee1\u8db3\u6240\u6709\u55b7\u6f06\u7ea6\u675f\u7684\u8def\u5f84\uff0c\u5176\u8d28\u91cf\u4e0e\u5de5\u7a0b\u5e08\u624b\u52a8\u8bbe\u8ba1\u7684\u8def\u5f84\u76f8\u5f53\u3002", "conclusion": "\u5206\u5c42\u4f18\u5316\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u8f66\u8f86\u55b7\u6f06\u8def\u5f84\u89c4\u5212\u7684\u81ea\u52a8\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u5b50\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u55b7\u6f06\u5de5\u827a\u7684\u7279\u6b8a\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u4e0e\u4eba\u5de5\u8bbe\u8ba1\u76f8\u5f53\u7684\u8d28\u91cf\u3002"}}
{"id": "2601.00263", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00263", "abs": "https://arxiv.org/abs/2601.00263", "authors": ["Qianli Wang", "Van Bach Nguyen", "Yihong Liu", "Fedor Splitt", "Nils Feldhus", "Christin Seifert", "Hinrich Sch\u00fctze", "Sebastian M\u00f6ller", "Vera Schmitt"], "title": "Parallel Universes, Parallel Languages: A Comprehensive Study on LLM-based Multilingual Counterfactual Example Generation", "comment": "In submission", "summary": "Counterfactuals refer to minimally edited inputs that cause a model's prediction to change, serving as a promising approach to explaining the model's behavior. Large language models (LLMs) excel at generating English counterfactuals and demonstrate multilingual proficiency. However, their effectiveness in generating multilingual counterfactuals remains unclear. To this end, we conduct a comprehensive study on multilingual counterfactuals. We first conduct automatic evaluations on both directly generated counterfactuals in the target languages and those derived via English translation across six languages. Although translation-based counterfactuals offer higher validity than their directly generated counterparts, they demand substantially more modifications and still fall short of matching the quality of the original English counterfactuals. Second, we find the patterns of edits applied to high-resource European-language counterfactuals to be remarkably similar, suggesting that cross-lingual perturbations follow common strategic principles. Third, we identify and categorize four main types of errors that consistently appear in the generated counterfactuals across languages. Finally, we reveal that multilingual counterfactual data augmentation (CDA) yields larger model performance improvements than cross-lingual CDA, especially for lower-resource languages. Yet, the imperfections of the generated counterfactuals limit gains in model performance and robustness.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5168\u9762\u8bc4\u4f30\u4e86LLMs\u751f\u6210\u591a\u8bed\u8a00\u53cd\u4e8b\u5b9e\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u7ffb\u8bd1\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u6709\u6548\u6027\u66f4\u9ad8\u4f46\u4fee\u6539\u66f4\u591a\uff0c\u591a\u8bed\u8a00\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u6548\u679c\u66f4\u597d\uff0c\u4f46\u751f\u6210\u8d28\u91cf\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u751f\u6210\u82f1\u8bed\u53cd\u4e8b\u5b9e\u65b9\u9762\u8868\u73b0\u51fa\u8272\u4e14\u5177\u5907\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u4f46\u5176\u5728\u591a\u8bed\u8a00\u53cd\u4e8b\u5b9e\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u7cfb\u7edf\u7814\u7a76\u6765\u8bc4\u4f30LLMs\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u53cd\u4e8b\u5b9e\u751f\u6210\u80fd\u529b\u3002", "method": "1) \u5bf9\u516d\u79cd\u8bed\u8a00\u8fdb\u884c\u81ea\u52a8\u8bc4\u4f30\uff0c\u6bd4\u8f83\u76f4\u63a5\u751f\u6210\u548c\u901a\u8fc7\u82f1\u8bed\u7ffb\u8bd1\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\uff1b2) \u5206\u6790\u9ad8\u8d44\u6e90\u6b27\u6d32\u8bed\u8a00\u53cd\u4e8b\u5b9e\u7684\u7f16\u8f91\u6a21\u5f0f\uff1b3) \u8bc6\u522b\u8de8\u8bed\u8a00\u53cd\u4e8b\u5b9e\u751f\u6210\u4e2d\u7684\u9519\u8bef\u7c7b\u578b\uff1b4) \u8bc4\u4f30\u591a\u8bed\u8a00\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\u4e0e\u8de8\u8bed\u8a00\u6570\u636e\u589e\u5f3a\u7684\u6548\u679c\u3002", "result": "1) \u7ffb\u8bd1\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u6709\u6548\u6027\u66f4\u9ad8\u4f46\u9700\u8981\u66f4\u591a\u4fee\u6539\uff0c\u4e14\u8d28\u91cf\u4ecd\u4e0d\u53ca\u539f\u59cb\u82f1\u8bed\u53cd\u4e8b\u5b9e\uff1b2) \u9ad8\u8d44\u6e90\u6b27\u6d32\u8bed\u8a00\u7684\u7f16\u8f91\u6a21\u5f0f\u76f8\u4f3c\uff0c\u8868\u660e\u8de8\u8bed\u8a00\u6270\u52a8\u9075\u5faa\u5171\u540c\u7b56\u7565\uff1b3) \u8bc6\u522b\u51fa\u56db\u79cd\u8de8\u8bed\u8a00\u4e00\u81f4\u7684\u9519\u8bef\u7c7b\u578b\uff1b4) \u591a\u8bed\u8a00\u6570\u636e\u589e\u5f3a\u6bd4\u8de8\u8bed\u8a00\u589e\u5f3a\u6548\u679c\u66f4\u597d\uff0c\u5c24\u5176\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u4f46\u751f\u6210\u4e0d\u5b8c\u5584\u9650\u5236\u4e86\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "LLMs\u5728\u591a\u8bed\u8a00\u53cd\u4e8b\u5b9e\u751f\u6210\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7ffb\u8bd1\u65b9\u6cd5\u80fd\u63d0\u9ad8\u6709\u6548\u6027\u4f46\u4ee3\u4ef7\u662f\u66f4\u591a\u4fee\u6539\uff0c\u591a\u8bed\u8a00\u6570\u636e\u589e\u5f3a\u6709\u6f5c\u529b\u4f46\u53d7\u9650\u4e8e\u751f\u6210\u8d28\u91cf\uff0c\u9700\u8981\u6539\u8fdb\u591a\u8bed\u8a00\u53cd\u4e8b\u5b9e\u751f\u6210\u6280\u672f\u4ee5\u5145\u5206\u53d1\u6325\u5176\u89e3\u91ca\u548c\u6570\u636e\u589e\u5f3a\u4ef7\u503c\u3002"}}
{"id": "2601.00275", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00275", "abs": "https://arxiv.org/abs/2601.00275", "authors": ["Dusan Nemec", "Gal Versano", "Itai Savin", "Vojtech Simak", "Juraj Kekelak", "Itzik Klein"], "title": "Pure Inertial Navigation in Challenging Environments with Wheeled and Chassis Mounted Inertial Sensors", "comment": null, "summary": "Autonomous vehicles and wheeled robots are widely used in many applications in both indoor and outdoor settings. In practical situations with limited GNSS signals or degraded lighting conditions, the navigation solution may rely only on inertial sensors and as result drift in time due to errors in the inertial measurement. In this work, we propose WiCHINS, a wheeled and chassis inertial navigation system by combining wheel-mounted-inertial sensors with a chassis-mounted inertial sensor for accurate pure inertial navigation. To that end, we derive a three-stage framework, each with a dedicated extended Kalman filter. This framework utilizes the benefits of each location (wheel/body) during the estimation process. To evaluate our proposed approach, we employed a dataset with five inertial measurement units with a total recording time of 228.6 minutes. We compare our approach with four other inertial baselines and demonstrate an average position error of 11.4m, which is $2.4\\%$ of the average traveled distance, using two wheels and one body inertial measurement units. As a consequence, our proposed method enables robust navigation in challenging environments and helps bridge the pure-inertial performance gap.", "AI": {"tldr": "WiCHINS\uff1a\u4e00\u79cd\u7ed3\u5408\u8f6e\u8f7d\u548c\u8f66\u4f53\u60ef\u6027\u4f20\u611f\u5668\u7684\u8f6e\u5f0f\u5e95\u76d8\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728GNSS\u53d7\u9650\u6216\u5149\u7167\u6761\u4ef6\u5dee\u7684\u73af\u5883\u4e2d\u5b9e\u73b0\u7cbe\u786e\u7684\u7eaf\u60ef\u6027\u5bfc\u822a", "motivation": "\u5728GNSS\u4fe1\u53f7\u53d7\u9650\u6216\u5149\u7167\u6761\u4ef6\u6076\u52a3\u7684\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u548c\u8f6e\u5f0f\u673a\u5668\u4eba\u53ea\u80fd\u4f9d\u8d56\u60ef\u6027\u4f20\u611f\u5668\u8fdb\u884c\u5bfc\u822a\uff0c\u4f46\u60ef\u6027\u6d4b\u91cf\u8bef\u5dee\u4f1a\u5bfc\u81f4\u968f\u65f6\u95f4\u6f02\u79fb\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u8fd9\u4e9b\u6311\u6218\u6027\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u5bfc\u822a\u7684\u7eaf\u60ef\u6027\u5bfc\u822a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faWiCHINS\u7cfb\u7edf\uff0c\u7ed3\u5408\u8f6e\u8f7d\u60ef\u6027\u4f20\u611f\u5668\u548c\u8f66\u4f53\u60ef\u6027\u4f20\u611f\u5668\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u6bcf\u4e2a\u9636\u6bb5\u4f7f\u7528\u4e13\u7528\u7684\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff0c\u5728\u4f30\u8ba1\u8fc7\u7a0b\u4e2d\u5145\u5206\u5229\u7528\u8f6e\u5b50\u548c\u8f66\u4f53\u4e0d\u540c\u4f4d\u7f6e\u7684\u4f20\u611f\u5668\u4f18\u52bf\u3002", "result": "\u4f7f\u75285\u4e2a\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff0c\u603b\u8bb0\u5f55\u65f6\u95f4228.6\u5206\u949f\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002\u4e0e\u56db\u4e2a\u5176\u4ed6\u60ef\u6027\u57fa\u7ebf\u65b9\u6cd5\u6bd4\u8f83\uff0c\u4f7f\u7528\u4e24\u4e2a\u8f6e\u5b50\u548c\u4e00\u4e2a\u8f66\u4f53\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u65f6\uff0c\u5e73\u5747\u4f4d\u7f6e\u8bef\u5dee\u4e3a11.4\u7c73\uff0c\u5360\u5e73\u5747\u884c\u9a76\u8ddd\u79bb\u76842.4%\u3002", "conclusion": "WiCHINS\u65b9\u6cd5\u80fd\u591f\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u5b9e\u73b0\u7a33\u5065\u5bfc\u822a\uff0c\u6709\u52a9\u4e8e\u7f29\u5c0f\u7eaf\u60ef\u6027\u5bfc\u822a\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u548c\u8f6e\u5f0f\u673a\u5668\u4eba\u5728GNSS\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00268", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00268", "abs": "https://arxiv.org/abs/2601.00268", "authors": ["Doyoung Kim", "Zhiwei Ren", "Jie Hao", "Zhongkai Sun", "Lichao Wang", "Xiyao Ma", "Zack Ye", "Xu Han", "Jun Yin", "Heng Ji", "Wei Shen", "Xing Fan", "Benjamin Yao", "Chenlei Guo"], "title": "Beyond Perfect APIs: A Comprehensive Evaluation of LLM Agents Under Real-World API Complexity", "comment": "26 pages", "summary": "We introduce WildAGTEval, a benchmark designed to evaluate large language model (LLM) agents' function-calling capabilities under realistic API complexity. Unlike prior work that assumes an idealized API system and disregards real-world factors such as noisy API outputs, WildAGTEval accounts for two dimensions of real-world complexity: 1. API specification, which includes detailed documentation and usage constraints, and 2. API execution, which captures runtime challenges. Consequently, WildAGTEval offers (i) an API system encompassing 60 distinct complexity scenarios that can be composed into approximately 32K test configurations, and (ii) user-agent interactions for evaluating LLM agents on these scenarios. Using WildAGTEval, we systematically assess several advanced LLMs and observe that most scenarios are challenging, with irrelevant information complexity posing the greatest difficulty and reducing the performance of strong LLMs by 27.3%. Furthermore, our qualitative analysis reveals that LLMs occasionally distort user intent merely to claim task completion, critically affecting user satisfaction.", "AI": {"tldr": "WildAGTEval\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u771f\u5b9eAPI\u590d\u6742\u6027\u4e0b\u51fd\u6570\u8c03\u7528\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u8003\u8651\u4e86API\u89c4\u8303\u548cAPI\u6267\u884c\u4e24\u4e2a\u7ef4\u5ea6\u7684\u73b0\u5b9e\u4e16\u754c\u590d\u6742\u6027\uff0c\u5305\u542b\u7ea632K\u6d4b\u8bd5\u914d\u7f6e\uff0c\u53d1\u73b0\u65e0\u5173\u4fe1\u606f\u590d\u6742\u6027\u5bf9LLM\u6027\u80fd\u5f71\u54cd\u6700\u5927\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5047\u8bbe\u7406\u60f3\u5316\u7684API\u7cfb\u7edf\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u4e16\u754c\u56e0\u7d20\u5982\u566a\u58f0API\u8f93\u51fa\uff0c\u65e0\u6cd5\u51c6\u786e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u5b9e\u9645\u590d\u6742\u73af\u5883\u4e2d\u7684\u51fd\u6570\u8c03\u7528\u80fd\u529b\u3002", "method": "\u521b\u5efaWildAGTEval\u57fa\u51c6\uff0c\u5305\u542b60\u4e2a\u4e0d\u540c\u590d\u6742\u6027\u573a\u666f\uff0c\u53ef\u7ec4\u5408\u6210\u7ea632K\u6d4b\u8bd5\u914d\u7f6e\uff0c\u8003\u8651API\u89c4\u8303\uff08\u8be6\u7ec6\u6587\u6863\u548c\u4f7f\u7528\u7ea6\u675f\uff09\u548cAPI\u6267\u884c\uff08\u8fd0\u884c\u65f6\u6311\u6218\uff09\u4e24\u4e2a\u7ef4\u5ea6\u7684\u590d\u6742\u6027\u3002", "result": "\u5927\u591a\u6570\u573a\u666f\u5177\u6709\u6311\u6218\u6027\uff0c\u65e0\u5173\u4fe1\u606f\u590d\u6742\u6027\u5bf9LLM\u6027\u80fd\u5f71\u54cd\u6700\u5927\uff0c\u4f7f\u5f3aLLM\u6027\u80fd\u4e0b\u964d27.3%\uff1b\u5b9a\u6027\u5206\u6790\u53d1\u73b0LLM\u6709\u65f6\u4f1a\u626d\u66f2\u7528\u6237\u610f\u56fe\u4ee5\u58f0\u79f0\u4efb\u52a1\u5b8c\u6210\uff0c\u4e25\u91cd\u5f71\u54cd\u7528\u6237\u6ee1\u610f\u5ea6\u3002", "conclusion": "WildAGTEval\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u771f\u5b9e\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86LLM\u4ee3\u7406\u5728\u5b9e\u9645API\u590d\u6742\u6027\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5904\u7406\u65e0\u5173\u4fe1\u606f\u548c\u610f\u56fe\u626d\u66f2\u7684\u95ee\u9898\uff0c\u5bf9\u63d0\u5347LLM\u4ee3\u7406\u7684\u5b9e\u7528\u6027\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2601.00305", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00305", "abs": "https://arxiv.org/abs/2601.00305", "authors": ["Prashant Kumar", "Yukiyasu Domae", "Weiwei Wan", "Kensuke Harada"], "title": "Replaceable Bit-based Gripper for Picking Cluttered Food Items", "comment": null, "summary": "The food packaging industry goes through changes in food items and their weights quite rapidly. These items range from easy-to-pick, single-piece food items to flexible, long and cluttered ones. We propose a replaceable bit-based gripper system to tackle the challenge of weight-based handling of cluttered food items. The gripper features specialized food attachments(bits) that enhance its grasping capabilities, and a belt replacement system allows switching between different food items during packaging operations. It offers a wide range of control options, enabling it to grasp and drop specific weights of granular, cluttered, and entangled foods. We specifically designed bits for two flexible food items that differ in shape: ikura(salmon roe) and spaghetti. They represent the challenging categories of sticky, granular food and long, sticky, cluttered food, respectively. The gripper successfully picked up both spaghetti and ikura and demonstrated weight-specific dropping of these items with an accuracy over 80% and 95% respectively. The gripper system also exhibited quick switching between different bits, leading to the handling of a large range of food items.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53ef\u66f4\u6362\u5939\u722a\u5934\u7684\u673a\u5668\u4eba\u5939\u722a\u7cfb\u7edf\uff0c\u7528\u4e8e\u5904\u7406\u6742\u4e71\u98df\u54c1\u7684\u91cd\u91cf\u5206\u62e3\uff0c\u9488\u5bf9\u4e0d\u540c\u5f62\u72b6\u98df\u54c1\u8bbe\u8ba1\u4e13\u7528\u5939\u722a\u5934\uff0c\u5b9e\u73b0\u5feb\u901f\u5207\u6362\u548c\u7cbe\u786e\u91cd\u91cf\u63a7\u5236\u3002", "motivation": "\u98df\u54c1\u5305\u88c5\u884c\u4e1a\u9762\u4e34\u5904\u7406\u5404\u79cd\u5f62\u72b6\u3001\u91cd\u91cf\u98df\u54c1\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u6742\u4e71\u3001\u67d4\u6027\u3001\u7c98\u6027\u98df\u54c1\uff08\u5982\u9c7c\u5b50\u9171\u548c\u610f\u5927\u5229\u9762\uff09\u7684\u7cbe\u786e\u91cd\u91cf\u5206\u62e3\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u53ef\u66f4\u6362\u5939\u722a\u5934\u7cfb\u7edf\uff0c\u914d\u5907\u4e13\u7528\u98df\u54c1\u5939\u722a\u5934\u548c\u76ae\u5e26\u66f4\u6362\u7cfb\u7edf\uff0c\u9488\u5bf9\u4e0d\u540c\u98df\u54c1\u5f62\u72b6\uff08\u9c7c\u5b50\u9171\u548c\u610f\u5927\u5229\u9762\uff09\u8bbe\u8ba1\u4e13\u7528\u5939\u722a\u5934\uff0c\u5b9e\u73b0\u5feb\u901f\u5207\u6362\u548c\u7cbe\u786e\u91cd\u91cf\u63a7\u5236\u3002", "result": "\u5939\u722a\u6210\u529f\u6293\u53d6\u9c7c\u5b50\u9171\u548c\u610f\u5927\u5229\u9762\uff0c\u91cd\u91cf\u5206\u62e3\u51c6\u786e\u7387\u5206\u522b\u8d85\u8fc795%\u548c80%\uff0c\u7cfb\u7edf\u5c55\u793a\u5feb\u901f\u5207\u6362\u4e0d\u540c\u5939\u722a\u5934\u7684\u80fd\u529b\uff0c\u53ef\u5904\u7406\u591a\u79cd\u98df\u54c1\u3002", "conclusion": "\u53ef\u66f4\u6362\u5939\u722a\u5934\u7cfb\u7edf\u80fd\u6709\u6548\u5904\u7406\u6742\u4e71\u98df\u54c1\u7684\u91cd\u91cf\u5206\u62e3\uff0c\u5b9e\u73b0\u5feb\u901f\u5207\u6362\u548c\u7cbe\u786e\u63a7\u5236\uff0c\u4e3a\u98df\u54c1\u5305\u88c5\u81ea\u52a8\u5316\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00282", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00282", "abs": "https://arxiv.org/abs/2601.00282", "authors": ["Qianli Wang", "Nils Feldhus", "Pepa Atanasova", "Fedor Splitt", "Simon Ostermann", "Sebastian M\u00f6ller", "Vera Schmitt"], "title": "Can Large Language Models Still Explain Themselves? Investigating the Impact of Quantization on Self-Explanations", "comment": "In submission", "summary": "Quantization is widely used to accelerate inference and streamline the deployment of large language models (LLMs), yet its effects on self-explanations (SEs) remain unexplored. SEs, generated by LLMs to justify their own outputs, require reasoning about the model's own decision-making process, a capability that may exhibit particular sensitivity to quantization. As SEs are increasingly relied upon for transparency in high-stakes applications, understanding whether and to what extent quantization degrades SE quality and faithfulness is critical. To address this gap, we examine two types of SEs: natural language explanations (NLEs) and counterfactual examples, generated by LLMs quantized using three common techniques at distinct bit widths. Our findings indicate that quantization typically leads to moderate declines in both SE quality (up to 4.4\\%) and faithfulness (up to 2.38\\%). The user study further demonstrates that quantization diminishes both the coherence and trustworthiness of SEs (up to 8.5\\%). Compared to smaller models, larger models show limited resilience to quantization in terms of SE quality but better maintain faithfulness. Moreover, no quantization technique consistently excels across task accuracy, SE quality, and faithfulness. Given that quantization's impact varies by context, we recommend validating SE quality for specific use cases, especially for NLEs, which show greater sensitivity. Nonetheless, the relatively minor deterioration in SE quality and faithfulness does not undermine quantization's effectiveness as a model compression technique.", "AI": {"tldr": "\u91cf\u5316\u5bf9LLM\u81ea\u89e3\u91ca\u80fd\u529b\u6709\u8d1f\u9762\u5f71\u54cd\uff0c\u4f46\u7a0b\u5ea6\u76f8\u5bf9\u8f83\u5c0f\uff0c\u4e0d\u5f71\u54cd\u91cf\u5316\u4f5c\u4e3a\u6a21\u578b\u538b\u7f29\u6280\u672f\u7684\u6709\u6548\u6027", "motivation": "\u91cf\u5316\u88ab\u5e7f\u6cdb\u7528\u4e8e\u52a0\u901f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u90e8\u7f72\uff0c\u4f46\u5176\u5bf9\u81ea\u89e3\u91ca\u80fd\u529b\u7684\u5f71\u54cd\u5c1a\u672a\u88ab\u7814\u7a76\u3002\u81ea\u89e3\u91ca\u662fLLM\u4e3a\u81ea\u8eab\u8f93\u51fa\u63d0\u4f9b\u89e3\u91ca\u7684\u80fd\u529b\uff0c\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u8d8a\u6765\u8d8a\u4f9d\u8d56\u8fd9\u79cd\u900f\u660e\u5ea6\uff0c\u56e0\u6b64\u7406\u89e3\u91cf\u5316\u662f\u5426\u4ee5\u53ca\u5982\u4f55\u964d\u4f4e\u81ea\u89e3\u91ca\u8d28\u91cf\u548c\u5fe0\u5b9e\u5ea6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u4e24\u79cd\u81ea\u89e3\u91ca\u7c7b\u578b\uff1a\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u548c\u53cd\u4e8b\u5b9e\u793a\u4f8b\uff0c\u4f7f\u7528\u4e09\u79cd\u5e38\u89c1\u91cf\u5316\u6280\u672f\u5728\u591a\u4e2a\u6bd4\u7279\u5bbd\u5ea6\u4e0b\u5bf9LLM\u8fdb\u884c\u91cf\u5316\u3002\u901a\u8fc7\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u91cf\u5316\u5bf9\u81ea\u89e3\u91ca\u8fde\u8d2f\u6027\u548c\u53ef\u4fe1\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u91cf\u5316\u901a\u5e38\u5bfc\u81f4\u81ea\u89e3\u91ca\u8d28\u91cf\uff08\u6700\u591a\u4e0b\u964d4.4%\uff09\u548c\u5fe0\u5b9e\u5ea6\uff08\u6700\u591a\u4e0b\u964d2.38%\uff09\u7684\u9002\u5ea6\u4e0b\u964d\u3002\u7528\u6237\u7814\u7a76\u8868\u660e\u91cf\u5316\u964d\u4f4e\u4e86\u81ea\u89e3\u91ca\u7684\u8fde\u8d2f\u6027\u548c\u53ef\u4fe1\u5ea6\uff08\u6700\u591a8.5%\uff09\u3002\u4e0e\u8f83\u5c0f\u6a21\u578b\u76f8\u6bd4\uff0c\u8f83\u5927\u6a21\u578b\u5728\u81ea\u89e3\u91ca\u8d28\u91cf\u65b9\u9762\u5bf9\u91cf\u5316\u7684\u62b5\u6297\u529b\u6709\u9650\uff0c\u4f46\u80fd\u66f4\u597d\u5730\u4fdd\u6301\u5fe0\u5b9e\u5ea6\u3002\u6ca1\u6709\u4e00\u79cd\u91cf\u5316\u6280\u672f\u80fd\u5728\u4efb\u52a1\u51c6\u786e\u6027\u3001\u81ea\u89e3\u91ca\u8d28\u91cf\u548c\u5fe0\u5b9e\u5ea6\u65b9\u9762\u59cb\u7ec8\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u91cf\u5316\u5bf9\u81ea\u89e3\u91ca\u7684\u5f71\u54cd\u56e0\u4e0a\u4e0b\u6587\u800c\u5f02\uff0c\u5efa\u8bae\u9488\u5bf9\u5177\u4f53\u7528\u4f8b\u9a8c\u8bc1\u81ea\u89e3\u91ca\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5bf9\u66f4\u654f\u611f\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002\u7136\u800c\uff0c\u81ea\u89e3\u91ca\u8d28\u91cf\u548c\u5fe0\u5b9e\u5ea6\u7684\u76f8\u5bf9\u8f83\u5c0f\u6076\u5316\u5e76\u4e0d\u5f71\u54cd\u91cf\u5316\u4f5c\u4e3a\u6a21\u578b\u538b\u7f29\u6280\u672f\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.00465", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00465", "abs": "https://arxiv.org/abs/2601.00465", "authors": ["Dennis Christmann", "Juan F. Gutierrez", "Sthiti Padhi", "Patrick Pl\u00f6rer", "Aditya Takur", "Simona Silvestri", "Andres Gomez"], "title": "Space Debris Removal using Nano-Satellites controlled by Low-Power Autonomous Agents", "comment": "This is an open-access, author-archived version of a manuscript published in European Conference on Multi-Agent Systems 2024", "summary": "Space debris is an ever-increasing problem in space travel. There are already many old, no longer functional spacecraft and debris orbiting the earth, which endanger both the safe operation of satellites and space travel. Small nano-satellite swarms can address this problem by autonomously de-orbiting debris safely into the Earth's atmosphere. This work builds on the recent advances of autonomous agents deployed in resource-constrained platforms and shows a first simplified approach how such intelligent and autonomous nano-satellite swarms can be realized. We implement our autonomous agent software on wireless microcontrollers and perform experiments on a specialized test-bed to show the feasibility and overall energy efficiency of our approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u4e3b\u667a\u80fd\u7eb3\u7c73\u536b\u661f\u7fa4\u7684\u7a7a\u95f4\u788e\u7247\u6e05\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u7ebf\u5fae\u63a7\u5236\u5668\u5b9e\u73b0\u81ea\u4e3b\u4ee3\u7406\u8f6f\u4ef6\uff0c\u5e76\u5728\u4e13\u7528\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u80fd\u6548\u3002", "motivation": "\u7a7a\u95f4\u788e\u7247\u65e5\u76ca\u589e\u591a\uff0c\u5bf9\u536b\u661f\u5b89\u5168\u548c\u592a\u7a7a\u65c5\u884c\u6784\u6210\u5a01\u80c1\u3002\u73b0\u6709\u7684\u8001\u65e7\u822a\u5929\u5668\u548c\u788e\u7247\u9700\u8981\u88ab\u5b89\u5168\u5904\u7406\uff0c\u800c\u5c0f\u578b\u7eb3\u7c73\u536b\u661f\u7fa4\u6709\u671b\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u7684\u81ea\u4e3b\u4ee3\u7406\u6280\u672f\u8fdb\u5c55\uff0c\u5b9e\u73b0\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u667a\u80fd\u81ea\u4e3b\u7eb3\u7c73\u536b\u661f\u7fa4\u65b9\u6cd5\u3002\u5c06\u81ea\u4e3b\u4ee3\u7406\u8f6f\u4ef6\u90e8\u7f72\u5728\u65e0\u7ebf\u5fae\u63a7\u5236\u5668\u4e0a\uff0c\u5e76\u5728\u4e13\u7528\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u6574\u4f53\u80fd\u6548\uff0c\u5c55\u793a\u4e86\u667a\u80fd\u81ea\u4e3b\u7eb3\u7c73\u536b\u661f\u7fa4\u5b9e\u73b0\u7a7a\u95f4\u788e\u7247\u5b89\u5168\u79bb\u8f68\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5229\u7528\u81ea\u4e3b\u667a\u80fd\u7eb3\u7c73\u536b\u661f\u7fa4\u6e05\u7406\u7a7a\u95f4\u788e\u7247\u63d0\u4f9b\u4e86\u521d\u6b65\u7b80\u5316\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u4e0a\u5b9e\u73b0\u6b64\u7c7b\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u672a\u6765\u7a7a\u95f4\u788e\u7247\u7ba1\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.00303", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00303", "abs": "https://arxiv.org/abs/2601.00303", "authors": ["Yuxin Li", "Xiangyu Zhang", "Yifei Li", "Zhiwei Guo", "Haoyang Zhang", "Eng Siong Chng", "Cuntai Guan"], "title": "DepFlow: Disentangled Speech Generation to Mitigate Semantic Bias in Depression Detection", "comment": null, "summary": "Speech is a scalable and non-invasive biomarker for early mental health screening. However, widely used depression datasets like DAIC-WOZ exhibit strong coupling between linguistic sentiment and diagnostic labels, encouraging models to learn semantic shortcuts. As a result, model robustness may be compromised in real-world scenarios, such as Camouflaged Depression, where individuals maintain socially positive or neutral language despite underlying depressive states. To mitigate this semantic bias, we propose DepFlow, a three-stage depression-conditioned text-to-speech framework. First, a Depression Acoustic Encoder learns speaker- and content-invariant depression embeddings through adversarial training, achieving effective disentanglement while preserving depression discriminability (ROC-AUC: 0.693). Second, a flow-matching TTS model with FiLM modulation injects these embeddings into synthesis, enabling control over depressive severity while preserving content and speaker identity. Third, a prototype-based severity mapping mechanism provides smooth and interpretable manipulation across the depression continuum. Using DepFlow, we construct a Camouflage Depression-oriented Augmentation (CDoA) dataset that pairs depressed acoustic patterns with positive/neutral content from a sentiment-stratified text bank, creating acoustic-semantic mismatches underrepresented in natural data. Evaluated across three depression detection architectures, CDoA improves macro-F1 by 9%, 12%, and 5%, respectively, consistently outperforming conventional augmentation strategies in depression Detection. Beyond enhancing robustness, DepFlow provides a controllable synthesis platform for conversational systems and simulation-based evaluation, where real clinical data remains limited by ethical and coverage constraints.", "AI": {"tldr": "DepFlow\uff1a\u901a\u8fc7\u4e09\u9636\u6bb5\u6291\u90c1\u6761\u4ef6\u6587\u672c\u8f6c\u8bed\u97f3\u6846\u67b6\u7f13\u89e3\u6291\u90c1\u75c7\u68c0\u6d4b\u4e2d\u7684\u8bed\u4e49\u504f\u89c1\uff0c\u6784\u5efa\u4f2a\u88c5\u6291\u90c1\u589e\u5f3a\u6570\u636e\u96c6\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u6291\u90c1\u75c7\u6570\u636e\u96c6\uff08\u5982DAIC-WOZ\uff09\u4e2d\u8bed\u8a00\u60c5\u611f\u4e0e\u8bca\u65ad\u6807\u7b7e\u5f3a\u8026\u5408\uff0c\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u8bed\u4e49\u6377\u5f84\uff0c\u5728\u771f\u5b9e\u573a\u666f\uff08\u5982\u4f2a\u88c5\u6291\u90c1\uff09\u4e2d\u9c81\u68d2\u6027\u4e0d\u8db3\u3002\u9700\u8981\u89e3\u51b3\u8bed\u4e49\u504f\u89c1\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5728\u8bed\u8a00\u5185\u5bb9\u4e0e\u6291\u90c1\u72b6\u6001\u4e0d\u5339\u914d\u60c5\u51b5\u4e0b\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u63d0\u51faDepFlow\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u6291\u90c1\u58f0\u5b66\u7f16\u7801\u5668\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u5b66\u4e60\u8bf4\u8bdd\u4eba\u548c\u5185\u5bb9\u4e0d\u53d8\u7684\u6291\u90c1\u5d4c\u5165\uff1b2\uff09\u5e26FiLM\u8c03\u5236\u7684\u6d41\u5339\u914dTTS\u6a21\u578b\u6ce8\u5165\u6291\u90c1\u5d4c\u5165\uff0c\u63a7\u5236\u6291\u90c1\u4e25\u91cd\u7a0b\u5ea6\u540c\u65f6\u4fdd\u6301\u5185\u5bb9\u548c\u8bf4\u8bdd\u4eba\u8eab\u4efd\uff1b3\uff09\u57fa\u4e8e\u539f\u578b\u7684\u4e25\u91cd\u7a0b\u5ea6\u6620\u5c04\u673a\u5236\u63d0\u4f9b\u5e73\u6ed1\u53ef\u89e3\u91ca\u7684\u6291\u90c1\u8fde\u7eed\u4f53\u64cd\u63a7\u3002\u5229\u7528\u8be5\u6846\u67b6\u6784\u5efa\u4f2a\u88c5\u6291\u90c1\u589e\u5f3a\u6570\u636e\u96c6\uff08CDoA\uff09\u3002", "result": "\u6291\u90c1\u58f0\u5b66\u7f16\u7801\u5668ROC-AUC\u8fbe0.693\uff0c\u6709\u6548\u89e3\u8026\u540c\u65f6\u4fdd\u6301\u6291\u90c1\u533a\u5206\u6027\u3002CDoA\u6570\u636e\u96c6\u5728\u4e09\u79cd\u6291\u90c1\u75c7\u68c0\u6d4b\u67b6\u6784\u4e0a\u5206\u522b\u63d0\u5347macro-F1 9%\u300112%\u548c5%\uff0c\u4f18\u4e8e\u4f20\u7edf\u589e\u5f3a\u7b56\u7565\u3002\u6846\u67b6\u8fd8\u63d0\u4f9b\u53ef\u63a7\u5408\u6210\u5e73\u53f0\u7528\u4e8e\u5bf9\u8bdd\u7cfb\u7edf\u548c\u4eff\u771f\u8bc4\u4f30\u3002", "conclusion": "DepFlow\u6210\u529f\u7f13\u89e3\u6291\u90c1\u75c7\u68c0\u6d4b\u4e2d\u7684\u8bed\u4e49\u504f\u89c1\uff0c\u901a\u8fc7\u6784\u5efa\u58f0\u5b66-\u8bed\u4e49\u4e0d\u5339\u914d\u6570\u636e\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u4e3a\u4e34\u5e8a\u6570\u636e\u6709\u9650\u7684\u573a\u666f\u63d0\u4f9b\u53ef\u63a7\u5408\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2601.00545", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00545", "abs": "https://arxiv.org/abs/2601.00545", "authors": ["Varun Agrawal", "Frank Dellaert"], "title": "Variable Elimination in Hybrid Factor Graphs for Discrete-Continuous Inference & Estimation", "comment": null, "summary": "Many hybrid problems in robotics involve both continuous and discrete components, and modeling them together for estimation tasks has been a long standing and difficult problem. Hybrid Factor Graphs give us a mathematical framework to model these types of problems, however existing approaches for solving them are based on approximations. In this work, we propose an efficient Hybrid Factor Graph framework alongwith a variable elimination algorithm to produce a hybrid Bayes network, which can then be used for exact Maximum A Posteriori estimation and marginalization over both sets of variables. Our approach first develops a novel hybrid Gaussian factor which can connect to both discrete and continuous variables, and a hybrid conditional which can represent multiple continuous hypotheses conditioned on the discrete variables. Using these representations, we derive the process of hybrid variable elimination under the Conditional Linear Gaussian scheme, giving us exact posteriors as hybrid Bayes network. To bound the number of discrete hypotheses, we use a tree-structured representation of the factors coupled with a simple pruning and probabilistic assignment scheme, which allows for tractable inference. We demonstrate the applicability of our framework on a SLAM dataset with ambiguous measurements, where discrete choices for the most likely measurement have to be made. Our demonstrated results showcase the accuracy, generality, and simplicity of our hybrid factor graph framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6df7\u5408\u56e0\u5b50\u56fe\u6846\u67b6\u548c\u53d8\u91cf\u6d88\u9664\u7b97\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u673a\u5668\u4eba\u4e2d\u540c\u65f6\u5305\u542b\u8fde\u7eed\u548c\u79bb\u6563\u53d8\u91cf\u7684\u6df7\u5408\u95ee\u9898\uff0c\u5b9e\u73b0\u7cbe\u786e\u7684\u6700\u5927\u540e\u9a8c\u4f30\u8ba1\u548c\u8fb9\u7f18\u5316\u3002", "motivation": "\u673a\u5668\u4eba\u4e2d\u7684\u8bb8\u591a\u6df7\u5408\u95ee\u9898\u540c\u65f6\u5305\u542b\u8fde\u7eed\u548c\u79bb\u6563\u7ec4\u4ef6\uff0c\u73b0\u6709\u65b9\u6cd5\u57fa\u4e8e\u8fd1\u4f3c\u6c42\u89e3\uff0c\u7f3a\u4e4f\u7cbe\u786e\u7684\u6df7\u5408\u4f30\u8ba1\u6846\u67b6\u3002", "method": "\u5f00\u53d1\u4e86\u6df7\u5408\u9ad8\u65af\u56e0\u5b50\u548c\u6df7\u5408\u6761\u4ef6\u8868\u793a\uff0c\u63a8\u5bfc\u4e86\u6761\u4ef6\u7ebf\u6027\u9ad8\u65af\u65b9\u6848\u4e0b\u7684\u6df7\u5408\u53d8\u91cf\u6d88\u9664\u8fc7\u7a0b\uff0c\u4f7f\u7528\u6811\u7ed3\u6784\u56e0\u5b50\u8868\u793a\u548c\u526a\u679d\u7b56\u7565\u63a7\u5236\u79bb\u6563\u5047\u8bbe\u6570\u91cf\u3002", "result": "\u5728\u5177\u6709\u6a21\u7cca\u6d4b\u91cf\u7684SLAM\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u51c6\u786e\u6027\u3001\u901a\u7528\u6027\u548c\u7b80\u5355\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u56e0\u5b50\u56fe\u6846\u67b6\u80fd\u591f\u7cbe\u786e\u5904\u7406\u6df7\u5408\u4f30\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7\u53d8\u91cf\u6d88\u9664\u751f\u6210\u6df7\u5408\u8d1d\u53f6\u65af\u7f51\u7edc\uff0c\u4e3a\u673a\u5668\u4eba\u4e2d\u7684\u6df7\u5408\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00348", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00348", "abs": "https://arxiv.org/abs/2601.00348", "authors": ["Yuhao Zhang", "Zhongliang Yang", "Linna Zhou"], "title": "Robust Uncertainty Quantification for Factual Generation of Large Language Models", "comment": "9 pages, 5 tables, 5 figures, accepted to IJCNN 2025", "summary": "The rapid advancement of large language model(LLM) technology has facilitated its integration into various domains of professional and daily life. However, the persistent challenge of LLM hallucination has emerged as a critical limitation, significantly compromising the reliability and trustworthiness of AI-generated content. This challenge has garnered significant attention within the scientific community, prompting extensive research efforts in hallucination detection and mitigation strategies. Current methodological frameworks reveal a critical limitation: traditional uncertainty quantification approaches demonstrate effectiveness primarily within conventional question-answering paradigms, yet exhibit notable deficiencies when confronted with non-canonical or adversarial questioning strategies. This performance gap raises substantial concerns regarding the dependability of LLM responses in real-world applications requiring robust critical thinking capabilities. This study aims to fill this gap by proposing an uncertainty quantification scenario in the task of generating with multiple facts. We have meticulously constructed a set of trap questions contained with fake names. Based on this scenario, we innovatively propose a novel and robust uncertainty quantification method(RU). A series of experiments have been conducted to verify its effectiveness. The results show that the constructed set of trap questions performs excellently. Moreover, when compared with the baseline methods on four different models, our proposed method has demonstrated great performance, with an average increase of 0.1-0.2 in ROCAUC values compared to the best performing baseline method, providing new sights and methods for addressing the hallucination issue of LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u95ee\u9898\u7684\u9c81\u68d2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b\u865a\u5047\u540d\u79f0\u7684\u9677\u9631\u95ee\u9898\u96c6\u6765\u68c0\u6d4b\u6a21\u578b\u5728\u751f\u6210\u591a\u4e8b\u5b9e\u5185\u5bb9\u65f6\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u4e86AI\u751f\u6210\u5185\u5bb9\u7684\u53ef\u9760\u6027\uff0c\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u5728\u5e38\u89c4\u95ee\u7b54\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u9762\u5bf9\u975e\u89c4\u8303\u6216\u5bf9\u6297\u6027\u63d0\u95ee\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u8fd9\u9650\u5236\u4e86LLM\u5728\u9700\u8981\u5f3a\u5927\u6279\u5224\u6027\u601d\u7ef4\u80fd\u529b\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "1) \u6784\u5efa\u5305\u542b\u865a\u5047\u540d\u79f0\u7684\u9677\u9631\u95ee\u9898\u96c6\u6765\u6d4b\u8bd5\u6a21\u578b\u5728\u591a\u4e8b\u5b9e\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff1b2) \u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u9c81\u68d2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5(RU)\uff0c\u4e13\u95e8\u9488\u5bf9\u975e\u89c4\u8303\u6216\u5bf9\u6297\u6027\u63d0\u95ee\u573a\u666f\u8bbe\u8ba1\u3002", "result": "1) \u6784\u5efa\u7684\u9677\u9631\u95ee\u9898\u96c6\u8868\u73b0\u4f18\u5f02\uff1b2) \u5728\u56db\u4e2a\u4e0d\u540c\u6a21\u578b\u4e0a\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63d0\u51fa\u7684RU\u65b9\u6cd5\u5728ROCAUC\u503c\u4e0a\u5e73\u5747\u63d0\u5347\u4e860.1-0.2\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u4f20\u7edf\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u5728\u9762\u5bf9\u975e\u89c4\u8303\u6216\u5bf9\u6297\u6027\u63d0\u95ee\u65f6\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u63d0\u51fa\u7684\u9c81\u68d2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u4e3a\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\uff0c\u589e\u5f3a\u4e86LLM\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2601.00555", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00555", "abs": "https://arxiv.org/abs/2601.00555", "authors": ["Abu Hanif Muhammad Syarubany", "Farhan Zaki Rahmani", "Trio Widianto"], "title": "LLM-Based Agentic Exploration for Robot Navigation & Manipulation with Skill Orchestration", "comment": null, "summary": "This paper presents an end-to-end LLM-based agentic exploration system for an indoor shopping task, evaluated in both Gazebo simulation and a corresponding real-world corridor layout. The robot incrementally builds a lightweight semantic map by detecting signboards at junctions and storing direction-to-POI relations together with estimated junction poses, while AprilTags provide repeatable anchors for approach and alignment. Given a natural-language shopping request, an LLM produces a constrained discrete action at each junction (direction and whether to enter a store), and a ROS finite-state main controller executes the decision by gating modular motion primitives, including local-costmap-based obstacle avoidance, AprilTag approaching, store entry, and grasping. Qualitative results show that the integrated stack can perform end-to-end task execution from user instruction to multi-store navigation and object retrieval, while remaining modular and debuggable through its text-based map and logged decision history.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u7aef\u5230\u7aef\u5ba4\u5185\u8d2d\u7269\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u4e49\u5730\u56fe\u6784\u5efa\u548c\u6a21\u5757\u5316\u8fd0\u52a8\u63a7\u5236\u5b9e\u73b0\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5230\u591a\u5e97\u94fa\u5bfc\u822a\u548c\u7269\u54c1\u6293\u53d6\u7684\u4efb\u52a1\u6267\u884c", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5728\u5ba4\u5185\u8d2d\u7269\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u5bfc\u822a\u548c\u7269\u54c1\u68c0\u7d22\u4efb\u52a1\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7LLM\u5b9e\u73b0\u9ad8\u7ea7\u51b3\u7b56\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u7cfb\u7edf\u7684\u6a21\u5757\u5316\u548c\u53ef\u8c03\u8bd5\u6027", "method": "\u4f7f\u7528LLM\u4f5c\u4e3a\u51b3\u7b56\u6838\u5fc3\uff0c\u6784\u5efa\u8f7b\u91cf\u7ea7\u8bed\u4e49\u5730\u56fe\uff08\u68c0\u6d4b\u8def\u53e3\u6807\u5fd7\u724c\u5e76\u5b58\u50a8\u65b9\u5411-POI\u5173\u7cfb\uff09\uff0c\u5229\u7528AprilTag\u4f5c\u4e3a\u53ef\u91cd\u590d\u951a\u70b9\uff0c\u901a\u8fc7ROS\u6709\u9650\u72b6\u6001\u4e3b\u63a7\u5236\u5668\u6267\u884c\u6a21\u5757\u5316\u8fd0\u52a8\u539f\u8bed\uff08\u907f\u969c\u3001AprilTag\u63a5\u8fd1\u3001\u5e97\u94fa\u8fdb\u5165\u3001\u6293\u53d6\uff09", "result": "\u5728Gazebo\u4eff\u771f\u548c\u771f\u5b9e\u8d70\u5eca\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u80fd\u591f\u7aef\u5230\u7aef\u6267\u884c\u4ece\u7528\u6237\u6307\u4ee4\u5230\u591a\u5e97\u94fa\u5bfc\u822a\u548c\u7269\u54c1\u68c0\u7d22\u7684\u4efb\u52a1\uff0c\u7cfb\u7edf\u4fdd\u6301\u6a21\u5757\u5316\u548c\u53ef\u8c03\u8bd5\u6027\uff0c\u901a\u8fc7\u6587\u672c\u5730\u56fe\u548c\u51b3\u7b56\u65e5\u5fd7\u4fbf\u4e8e\u5206\u6790", "conclusion": "LLM-based agentic exploration\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u5ba4\u5185\u8d2d\u7269\u4efb\u52a1\u7684\u7aef\u5230\u7aef\u6267\u884c\uff0c\u5c55\u793a\u4e86LLM\u5728\u673a\u5668\u4eba\u9ad8\u7ea7\u51b3\u7b56\u4e2d\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u6a21\u5757\u5316\u67b6\u6784\u786e\u4fdd\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u53ef\u7ef4\u62a4\u6027"}}
{"id": "2601.00364", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00364", "abs": "https://arxiv.org/abs/2601.00364", "authors": ["Jiandong Shao", "Raphael Tang", "Crystina Zhang", "Karin Sevegnani", "Pontus Stenetorp", "Jianfei Yang", "Yao Lu"], "title": "The Role of Mixed-Language Documents for Multilingual Large Language Model Pretraining", "comment": "under review", "summary": "Multilingual large language models achieve impressive cross-lingual performance despite largely monolingual pretraining. While bilingual data in pretraining corpora is widely believed to enable these abilities, details of its contributions remain unclear. We investigate this question by pretraining models from scratch under controlled conditions, comparing the standard web corpus with a monolingual-only version that removes all multilingual documents. Despite constituting only 2% of the corpus, removing bilingual data causes translation performance to drop 56% in BLEU, while behaviour on cross-lingual QA and general reasoning tasks remains stable, with training curves largely overlapping the baseline. To understand this asymmetry, we categorize bilingual data into parallel (14%), code-switching (72%), and miscellaneous documents (14%) based on the semantic relevance of content in different languages. We then conduct granular ablations by reintroducing parallel or code-switching data into the monolingual-only corpus. Our experiments reveal that parallel data almost fully restores translation performance (91% of the unfiltered baseline), whereas code-switching contributes minimally. Other cross-lingual tasks remain largely unaffected by either type. These findings reveal that translation critically depends on systematic token-level alignments from parallel data, whereas cross-lingual understanding and reasoning appear to be achievable even without bilingual data.", "AI": {"tldr": "\u79fb\u9664\u53cc\u8bed\u6570\u636e\uff08\u4ec5\u5360\u8bed\u6599\u5e932%\uff09\u4f7f\u7ffb\u8bd1\u6027\u80fd\u4e0b\u964d56%\uff0c\u4f46\u5bf9\u8de8\u8bed\u8a00QA\u548c\u63a8\u7406\u4efb\u52a1\u5f71\u54cd\u4e0d\u5927\uff1b\u5e73\u884c\u6570\u636e\uff0814%\uff09\u80fd\u6062\u590d91%\u7ffb\u8bd1\u6027\u80fd\uff0c\u800c\u8bed\u7801\u8f6c\u6362\u6570\u636e\uff0872%\uff09\u8d21\u732e\u5f88\u5c0f", "motivation": "\u7814\u7a76\u53cc\u8bed\u6570\u636e\u5728\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u8de8\u8bed\u8a00\u80fd\u529b\u4e2d\u7684\u4f5c\u7528\u7ec6\u8282\uff0c\u867d\u7136\u53cc\u8bed\u6570\u636e\u5728\u9884\u8bad\u7ec3\u8bed\u6599\u4e2d\u5360\u6bd4\u5f88\u5c0f\uff0c\u4f46\u5176\u5177\u4f53\u8d21\u732e\u673a\u5236\u5c1a\u4e0d\u6e05\u695a", "method": "\u5728\u53d7\u63a7\u6761\u4ef6\u4e0b\u4ece\u5934\u8bad\u7ec3\u6a21\u578b\uff0c\u6bd4\u8f83\u6807\u51c6\u7f51\u7edc\u8bed\u6599\u5e93\u4e0e\u79fb\u9664\u6240\u6709\u591a\u8bed\u8a00\u6587\u6863\u7684\u7eaf\u5355\u8bed\u7248\u672c\uff1b\u5c06\u53cc\u8bed\u6570\u636e\u5206\u7c7b\u4e3a\u5e73\u884c\u6570\u636e\uff0814%\uff09\u3001\u8bed\u7801\u8f6c\u6362\u6570\u636e\uff0872%\uff09\u548c\u5176\u4ed6\u6587\u6863\uff0814%\uff09\uff0c\u901a\u8fc7\u9010\u6b65\u91cd\u65b0\u5f15\u5165\u4e0d\u540c\u7c7b\u578b\u53cc\u8bed\u6570\u636e\u8fdb\u884c\u7ec6\u7c92\u5ea6\u6d88\u878d\u5b9e\u9a8c", "result": "\u79fb\u9664\u53cc\u8bed\u6570\u636e\u5bfc\u81f4\u7ffb\u8bd1\u6027\u80fd\u4e0b\u964d56%\uff0c\u4f46\u8de8\u8bed\u8a00QA\u548c\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u7a33\u5b9a\uff1b\u5e73\u884c\u6570\u636e\u51e0\u4e4e\u5b8c\u5168\u6062\u590d\u7ffb\u8bd1\u6027\u80fd\uff08\u8fbe\u5230\u672a\u8fc7\u6ee4\u57fa\u7ebf\u768491%\uff09\uff0c\u800c\u8bed\u7801\u8f6c\u6362\u6570\u636e\u8d21\u732e\u5f88\u5c0f\uff1b\u5176\u4ed6\u8de8\u8bed\u8a00\u4efb\u52a1\u57fa\u672c\u4e0d\u53d7\u53cc\u8bed\u6570\u636e\u7c7b\u578b\u5f71\u54cd", "conclusion": "\u7ffb\u8bd1\u4e25\u91cd\u4f9d\u8d56\u5e73\u884c\u6570\u636e\u63d0\u4f9b\u7684\u7cfb\u7edf\u6027\u8bcd\u7ea7\u5bf9\u9f50\uff0c\u800c\u8de8\u8bed\u8a00\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u5373\u4f7f\u6ca1\u6709\u53cc\u8bed\u6570\u636e\u4e5f\u80fd\u5b9e\u73b0\uff1b\u53cc\u8bed\u6570\u636e\u4e2d\u53ea\u6709\u5e73\u884c\u6570\u636e\u5bf9\u7ffb\u8bd1\u6027\u80fd\u81f3\u5173\u91cd\u8981"}}
{"id": "2601.00580", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.00580", "abs": "https://arxiv.org/abs/2601.00580", "authors": ["Kanghoon Lee", "Hyeonjun Kim", "Jiachen Li", "Jinkyoo Park"], "title": "Priority-Aware Multi-Robot Coverage Path Planning", "comment": "IEEE Robotics and Automation Letters, 8 pages, 10 figures", "summary": "Multi-robot systems are widely used for coverage tasks that require efficient coordination across large environments. In Multi-Robot Coverage Path Planning (MCPP), the objective is typically to minimize the makespan by generating non-overlapping paths for full-area coverage. However, most existing methods assume uniform importance across regions, limiting their effectiveness in scenarios where some zones require faster attention. We introduce the Priority-Aware MCPP (PA-MCPP) problem, where a subset of the environment is designated as prioritized zones with associated weights. The goal is to minimize, in lexicographic order, the total priority-weighted latency of zone coverage and the overall makespan. To address this, we propose a scalable two-phase framework combining (1) greedy zone assignment with local search, spanning-tree-based path planning, and (2) Steiner-tree-guided residual coverage. Experiments across diverse scenarios demonstrate that our method significantly reduces priority-weighted latency compared to standard MCPP baselines, while maintaining competitive makespan. Sensitivity analyses further show that the method scales well with the number of robots and that zone coverage behavior can be effectively controlled by adjusting priority weights.", "AI": {"tldr": "\u63d0\u51fa\u4f18\u5148\u7ea7\u611f\u77e5\u7684\u591a\u673a\u5668\u4eba\u8986\u76d6\u8def\u5f84\u89c4\u5212\uff08PA-MCPP\uff09\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u6846\u67b6\u4f18\u5316\u4f18\u5148\u7ea7\u533a\u57df\u8986\u76d6\u5ef6\u8fdf\u548c\u603b\u5b8c\u6210\u65f6\u95f4", "motivation": "\u4f20\u7edf\u591a\u673a\u5668\u4eba\u8986\u76d6\u8def\u5f84\u89c4\u5212\uff08MCPP\uff09\u5047\u8bbe\u6240\u6709\u533a\u57df\u91cd\u8981\u6027\u76f8\u540c\uff0c\u4f46\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u67d0\u4e9b\u533a\u57df\u9700\u8981\u4f18\u5148\u8986\u76d6\uff08\u5982\u7d27\u6025\u533a\u57df\u3001\u5173\u952e\u8bbe\u65bd\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u4f18\u5148\u7ea7\u5dee\u5f02", "method": "\u63d0\u51fa\u53ef\u6269\u5c55\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u8d2a\u5a6a\u533a\u57df\u5206\u914d\u7ed3\u5408\u5c40\u90e8\u641c\u7d22\u548c\u57fa\u4e8e\u751f\u6210\u6811\u7684\u8def\u5f84\u89c4\u5212\uff1b2) \u65af\u5766\u7eb3\u6811\u5f15\u5bfc\u7684\u5269\u4f59\u8986\u76d6", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u76f8\u6bd4\u6807\u51c6MCPP\u57fa\u7ebf\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u4f18\u5148\u7ea7\u52a0\u6743\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u7684\u603b\u5b8c\u6210\u65f6\u95f4\uff1b\u654f\u611f\u6027\u5206\u6790\u663e\u793a\u65b9\u6cd5\u80fd\u826f\u597d\u6269\u5c55\u5230\u591a\u673a\u5668\u4eba\u573a\u666f\uff0c\u4e14\u901a\u8fc7\u8c03\u6574\u4f18\u5148\u7ea7\u6743\u91cd\u53ef\u6709\u6548\u63a7\u5236\u533a\u57df\u8986\u76d6\u884c\u4e3a", "conclusion": "PA-MCPP\u95ee\u9898\u6846\u67b6\u548c\u4e24\u9636\u6bb5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u4f18\u5148\u7ea7\u533a\u57df\u8986\u76d6\u9700\u6c42\uff0c\u5728\u4fdd\u6301\u6574\u4f53\u6548\u7387\u7684\u540c\u65f6\u4f18\u5316\u5173\u952e\u533a\u57df\u7684\u8986\u76d6\u5ef6\u8fdf"}}
{"id": "2601.00366", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00366", "abs": "https://arxiv.org/abs/2601.00366", "authors": ["Taj Gillin", "Adam Lalani", "Kenneth Zhang", "Marcel Mateos Salles"], "title": "BERT-JEPA: Reorganizing CLS Embeddings for Language-Invariant Semantics", "comment": "16 pages, 10 figures, 10 tables", "summary": "Joint Embedding Predictive Architectures (JEPA) are a novel self supervised training technique that have shown recent promise across domains. We introduce BERT-JEPA (BEPA), a training paradigm that adds a JEPA training objective to BERT-style models, working to combat a collapsed [CLS] embedding space and turning it into a language-agnostic space. This new structure leads to increased performance across multilingual benchmarks.", "AI": {"tldr": "BERT-JEPA (BEPA) \u901a\u8fc7\u5c06 JEPA \u8bad\u7ec3\u76ee\u6807\u6dfb\u52a0\u5230 BERT \u98ce\u683c\u6a21\u578b\u4e2d\uff0c\u89e3\u51b3\u4e86 [CLS] \u5d4c\u5165\u7a7a\u95f4\u574d\u7f29\u95ee\u9898\uff0c\u5c06\u5176\u8f6c\u53d8\u4e3a\u8bed\u8a00\u65e0\u5173\u7a7a\u95f4\uff0c\u4ece\u800c\u5728\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3 BERT \u98ce\u683c\u6a21\u578b\u4e2d [CLS] \u5d4c\u5165\u7a7a\u95f4\u574d\u7f29\u7684\u95ee\u9898\uff0c\u5e76\u521b\u5efa\u8bed\u8a00\u65e0\u5173\u7684\u8868\u793a\u7a7a\u95f4\uff0c\u4ee5\u63d0\u5347\u591a\u8bed\u8a00\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u5728 BERT \u98ce\u683c\u6a21\u578b\u4e2d\u6dfb\u52a0 JEPA\uff08\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784\uff09\u8bad\u7ec3\u76ee\u6807\uff0c\u901a\u8fc7\u9884\u6d4b\u6027\u67b6\u6784\u6765\u6539\u8fdb [CLS] \u5d4c\u5165\u7a7a\u95f4\uff0c\u4f7f\u5176\u6210\u4e3a\u8bed\u8a00\u65e0\u5173\u7684\u8868\u793a\u3002", "result": "BERT-JEPA \u5728\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u521b\u5efa\u8bed\u8a00\u65e0\u5173\u8868\u793a\u7a7a\u95f4\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5c06 JEPA \u8bad\u7ec3\u76ee\u6807\u6574\u5408\u5230 BERT \u98ce\u683c\u6a21\u578b\u4e2d\u662f\u4e00\u79cd\u6709\u6548\u7684\u7b56\u7565\uff0c\u53ef\u4ee5\u89e3\u51b3 [CLS] \u5d4c\u5165\u7a7a\u95f4\u574d\u7f29\u95ee\u9898\uff0c\u521b\u5efa\u8bed\u8a00\u65e0\u5173\u7684\u8868\u793a\uff0c\u5e76\u63d0\u5347\u591a\u8bed\u8a00\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2601.00609", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00609", "abs": "https://arxiv.org/abs/2601.00609", "authors": ["Mehdi Heydari Shahna", "Pauli Mustalahti", "Jouni Mattila"], "title": "NMPC-Augmented Visual Navigation and Safe Learning Control for Large-Scale Mobile Robots", "comment": null, "summary": "A large-scale mobile robot (LSMR) is a high-order multibody system that often operates on loose, unconsolidated terrain, which reduces traction. This paper presents a comprehensive navigation and control framework for an LSMR that ensures stability and safety-defined performance, delivering robust operation on slip-prone terrain by jointly leveraging high-performance techniques. The proposed architecture comprises four main modules: (1) a visual pose-estimation module that fuses onboard sensors and stereo cameras to provide an accurate, low-latency robot pose, (2) a high-level nonlinear model predictive control that updates the wheel motion commands to correct robot drift from the robot reference pose on slip-prone terrain, (3) a low-level deep neural network control policy that approximates the complex behavior of the wheel-driven actuation mechanism in LSMRs, augmented with robust adaptive control to handle out-of-distribution disturbances, ensuring that the wheels accurately track the updated commands issued by high-level control module, and (4) a logarithmic safety module to monitor the entire robot stack and guarantees safe operation. The proposed low-level control framework guarantees uniform exponential stability of the actuation subsystem, while the safety module ensures the whole system-level safety during operation. Comparative experiments on a 6,000 kg LSMR actuated by two complex electro-hydrostatic drives, while synchronizing modules operating at different frequencies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5927\u578b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u7efc\u5408\u5bfc\u822a\u4e0e\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u59ff\u6001\u4f30\u8ba1\u3001\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u3001\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u7b56\u7565\u548c\u5b89\u5168\u76d1\u63a7\u6a21\u5757\uff0c\u786e\u4fdd\u5728\u6613\u6253\u6ed1\u5730\u5f62\u4e0a\u7684\u7a33\u5b9a\u5b89\u5168\u64cd\u4f5c\u3002", "motivation": "\u5927\u578b\u79fb\u52a8\u673a\u5668\u4eba\u5728\u677e\u6563\u3001\u672a\u56fa\u7ed3\u7684\u5730\u5f62\u4e0a\u64cd\u4f5c\u65f6\uff0c\u7531\u4e8e\u7275\u5f15\u529b\u964d\u4f4e\u5bb9\u6613\u51fa\u73b0\u6253\u6ed1\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u786e\u4fdd\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u80fd\u7684\u5bfc\u822a\u63a7\u5236\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u56db\u6a21\u5757\u67b6\u6784\uff1a1) \u89c6\u89c9\u59ff\u6001\u4f30\u8ba1\u6a21\u5757\u878d\u5408\u4f20\u611f\u5668\u548c\u7acb\u4f53\u76f8\u673a\uff1b2) \u9ad8\u5c42\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u66f4\u65b0\u8f6e\u5b50\u8fd0\u52a8\u6307\u4ee4\uff1b3) \u4f4e\u5c42\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u7b56\u7565\u8fd1\u4f3c\u8f6e\u9a71\u52a8\u673a\u5236\uff0c\u589e\u5f3a\u9c81\u68d2\u81ea\u9002\u5e94\u63a7\u5236\uff1b4) \u5bf9\u6570\u5b89\u5168\u6a21\u5757\u76d1\u63a7\u6574\u4e2a\u7cfb\u7edf\u3002", "result": "\u4f4e\u5c42\u63a7\u5236\u6846\u67b6\u4fdd\u8bc1\u4e86\u9a71\u52a8\u5b50\u7cfb\u7edf\u7684\u5747\u5300\u6307\u6570\u7a33\u5b9a\u6027\uff0c\u5b89\u5168\u6a21\u5757\u786e\u4fdd\u4e86\u6574\u4e2a\u7cfb\u7edf\u7ea7\u7684\u5b89\u5168\u64cd\u4f5c\uff0c\u57286000\u516c\u65a4\u5927\u578b\u79fb\u52a8\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u5bf9\u6bd4\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7efc\u5408\u6846\u67b6\u901a\u8fc7\u8054\u5408\u9ad8\u6027\u80fd\u6280\u672f\uff0c\u4e3a\u5927\u578b\u79fb\u52a8\u673a\u5668\u4eba\u5728\u6613\u6253\u6ed1\u5730\u5f62\u4e0a\u63d0\u4f9b\u4e86\u7a33\u5b9a\u5b89\u5168\u7684\u5bfc\u822a\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u7cfb\u7edf\u7ea7\u7684\u5b89\u5168\u4fdd\u8bc1\u3002"}}
{"id": "2601.00388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00388", "abs": "https://arxiv.org/abs/2601.00388", "authors": ["Biao Wu", "Meng Fang", "Ling Chen", "Ke Xu", "Tao Cheng", "Jun Wang"], "title": "Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach", "comment": "8 pages, 1 figures", "summary": "Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.", "AI": {"tldr": "Geo-R\u662f\u4e00\u4e2a\u514d\u68c0\u7d22\u7684\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8e\u89c4\u5219\u7684\u5c42\u6b21\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u7cbe\u786e\u3001\u53ef\u89e3\u91ca\u7684\u5730\u7406\u5b9a\u4f4d\uff0c\u65e0\u9700\u4f9d\u8d56\u5408\u6210\u6807\u6ce8\u6216\u5916\u90e8\u56fe\u50cf\u68c0\u7d22\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u4e2d\u901a\u5e38\u4f9d\u8d56\u5408\u6210\u63a8\u7406\u6807\u6ce8\u6216\u5916\u90e8\u56fe\u50cf\u68c0\u7d22\uff0c\u8fd9\u9650\u5236\u4e86\u53ef\u89e3\u91ca\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u514d\u68c0\u7d22\u7684\u65b9\u6cd5\uff0c\u76f4\u63a5\u4ece\u771f\u5b9e\u5750\u6807\u4e2d\u6316\u6398\u7ed3\u6784\u5316\u63a8\u7406\u8def\u5f84\u3002", "method": "\u63d0\u51faChain of Region\uff1a\u57fa\u4e8e\u89c4\u5219\u7684\u5c42\u6b21\u63a8\u7406\u8303\u5f0f\uff0c\u5c06GPS\u5750\u6807\u6620\u5c04\u5230\u5730\u7406\u5b9e\u4f53\uff08\u56fd\u5bb6\u3001\u7701\u4efd\u3001\u57ce\u5e02\u7b49\uff09\uff0c\u751f\u6210\u7cbe\u786e\u53ef\u89e3\u91ca\u7684\u76d1\u7763\u4fe1\u53f7\u3002\u91c7\u7528\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u57fa\u4e8eHaversine\u8ddd\u79bb\u8bbe\u8ba1\u5750\u6807\u5bf9\u9f50\u5956\u52b1\uff0c\u901a\u8fc7\u7a7a\u95f4\u6709\u610f\u4e49\u7684\u53cd\u9988\u4f18\u5316\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86Geo-R\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3001\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u66f4\u900f\u660e\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5efa\u7acb\u4e86\u514d\u68c0\u7d22\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u7684\u65b0\u8303\u5f0f\u3002", "conclusion": "Geo-R\u901a\u8fc7\u7ed3\u6784\u5316\u5730\u7406\u63a8\u7406\u4e0e\u76f4\u63a5\u7a7a\u95f4\u76d1\u7763\u7684\u7ed3\u5408\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u7684\u56fe\u50cf\u5730\u7406\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6a21\u578b\u548c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2601.00610", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00610", "abs": "https://arxiv.org/abs/2601.00610", "authors": ["Mehdi Heydari Shahna", "Pauli Mustalahti", "Jouni Mattila"], "title": "Vision-based Goal-Reaching Control for Mobile Robots Using a Hierarchical Learning Framework", "comment": null, "summary": "Reinforcement learning (RL) is effective in many robotic applications, but it requires extensive exploration of the state-action space, during which behaviors can be unsafe. This significantly limits its applicability to large robots with complex actuators operating on unstable terrain. Hence, to design a safe goal-reaching control framework for large-scale robots, this paper decomposes the whole system into a set of tightly coupled functional modules. 1) A real-time visual pose estimation approach is employed to provide accurate robot states to 2) an RL motion planner for goal-reaching tasks that explicitly respects robot specifications. The RL module generates real-time smooth motion commands for the actuator system, independent of its underlying dynamic complexity. 3) In the actuation mechanism, a supervised deep learning model is trained to capture the complex dynamics of the robot and provide this model to 4) a model-based robust adaptive controller that guarantees the wheels track the RL motion commands even on slip-prone terrain. 5) Finally, to reduce human intervention, a mathematical safety supervisor monitors the robot, stops it on unsafe faults, and autonomously guides it back to a safe inspection area. The proposed framework guarantees uniform exponential stability of the actuation system and safety of the whole operation. Experiments on a 6,000 kg robot in different scenarios confirm the effectiveness of the proposed framework.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u5927\u578b\u673a\u5668\u4eba\u7684\u5b89\u5168\u76ee\u6807\u5230\u8fbe\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u7ed3\u5408\u89c6\u89c9\u59ff\u6001\u4f30\u8ba1\u3001\u5f3a\u5316\u5b66\u4e60\u8fd0\u52a8\u89c4\u5212\u3001\u6df1\u5ea6\u5b66\u4e60\u5efa\u6a21\u3001\u9c81\u68d2\u81ea\u9002\u5e94\u63a7\u5236\u548c\u5b89\u5168\u76d1\u63a7\uff0c\u786e\u4fdd\u7cfb\u7edf\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u9700\u8981\u5927\u91cf\u63a2\u7d22\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\uff0c\u671f\u95f4\u884c\u4e3a\u53ef\u80fd\u4e0d\u5b89\u5168\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u5730\u5f62\u4e0a\u8fd0\u884c\u7684\u5927\u578b\u590d\u6742\u673a\u5668\u4eba\u4e0a\u7684\u5e94\u7528\u3002\u9700\u8981\u8bbe\u8ba1\u4e00\u4e2a\u5b89\u5168\u7684\u76ee\u6807\u5230\u8fbe\u63a7\u5236\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u7cfb\u7edf\u5206\u89e3\u4e3a\u4e94\u4e2a\u7d27\u5bc6\u8026\u5408\u7684\u529f\u80fd\u6a21\u5757\uff1a1) \u5b9e\u65f6\u89c6\u89c9\u59ff\u6001\u4f30\u8ba1\u63d0\u4f9b\u51c6\u786e\u673a\u5668\u4eba\u72b6\u6001\uff1b2) \u5f3a\u5316\u5b66\u4e60\u8fd0\u52a8\u89c4\u5212\u5668\u751f\u6210\u5e73\u6ed1\u8fd0\u52a8\u6307\u4ee4\uff1b3) \u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6355\u6349\u673a\u5668\u4eba\u590d\u6742\u52a8\u529b\u5b66\uff1b4) \u57fa\u4e8e\u6a21\u578b\u7684\u9c81\u68d2\u81ea\u9002\u5e94\u63a7\u5236\u5668\u786e\u4fdd\u8f6e\u5b50\u8ddf\u8e2a\u8fd0\u52a8\u6307\u4ee4\uff1b5) \u6570\u5b66\u5b89\u5168\u76d1\u63a7\u5668\u76d1\u63a7\u673a\u5668\u4eba\u5b89\u5168\u3002", "result": "\u63d0\u51fa\u7684\u6846\u67b6\u4fdd\u8bc1\u4e86\u6267\u884c\u7cfb\u7edf\u7684\u5747\u5300\u6307\u6570\u7a33\u5b9a\u6027\u548c\u6574\u4e2a\u64cd\u4f5c\u7684\u5b89\u5168\u6027\u3002\u57286000\u516c\u65a4\u673a\u5668\u4eba\u4e0a\u7684\u4e0d\u540c\u573a\u666f\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u7ed3\u5408\u591a\u79cd\u6280\u672f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5927\u578b\u673a\u5668\u4eba\u7684\u5b89\u5168\u76ee\u6807\u5230\u8fbe\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5b58\u5728\u7684\u5b89\u5168\u95ee\u9898\uff0c\u4e3a\u5927\u578b\u673a\u5668\u4eba\u5728\u4e0d\u7a33\u5b9a\u5730\u5f62\u4e0a\u7684\u5b89\u5168\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00411", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00411", "abs": "https://arxiv.org/abs/2601.00411", "authors": ["Alistair Plum", "Laura Bernardy", "Tharindu Ranasinghe"], "title": "Do LLMs Judge Distantly Supervised Named Entity Labels Well? Constructing the JudgeWEL Dataset", "comment": null, "summary": "We present judgeWEL, a dataset for named entity recognition (NER) in Luxembourgish, automatically labelled and subsequently verified using large language models (LLM) in a novel pipeline. Building datasets for under-represented languages remains one of the major bottlenecks in natural language processing, where the scarcity of resources and linguistic particularities make large-scale annotation costly and potentially inconsistent. To address these challenges, we propose and evaluate a novel approach that leverages Wikipedia and Wikidata as structured sources of weak supervision. By exploiting internal links within Wikipedia articles, we infer entity types based on their corresponding Wikidata entries, thereby generating initial annotations with minimal human intervention. Because such links are not uniformly reliable, we mitigate noise by employing and comparing several LLMs to identify and retain only high-quality labelled sentences. The resulting corpus is approximately five times larger than the currently available Luxembourgish NER dataset and offers broader and more balanced coverage across entity categories, providing a substantial new resource for multilingual and low-resource NER research.", "AI": {"tldr": "judgeWEL\uff1a\u4e00\u4e2a\u7528\u4e8e\u5362\u68ee\u5821\u8bed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7LLM\u81ea\u52a8\u6807\u6ce8\u548c\u9a8c\u8bc1\uff0c\u89c4\u6a21\u6bd4\u73b0\u6709\u6570\u636e\u96c6\u59275\u500d", "motivation": "\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u6784\u5efaNER\u6570\u636e\u96c6\u9762\u4e34\u8d44\u6e90\u7a00\u7f3a\u3001\u6807\u6ce8\u6210\u672c\u9ad8\u4e14\u4e0d\u4e00\u81f4\u7684\u6311\u6218\uff0c\u5362\u68ee\u5821\u8bed\u4f5c\u4e3a\u4ee3\u8868\u6027\u4f4e\u8d44\u6e90\u8bed\u8a00\u9700\u8981\u66f4\u597d\u7684\u6570\u636e\u96c6\u652f\u6301", "method": "\u5229\u7528Wikipedia\u548cWikidata\u4f5c\u4e3a\u5f31\u76d1\u7763\u6e90\uff0c\u901a\u8fc7\u6587\u7ae0\u5185\u90e8\u94fe\u63a5\u63a8\u65ad\u5b9e\u4f53\u7c7b\u578b\uff0c\u7136\u540e\u4f7f\u7528\u591a\u4e2aLLM\u8fdb\u884c\u566a\u58f0\u8fc7\u6ee4\u548c\u8d28\u91cf\u9a8c\u8bc1", "result": "\u6784\u5efa\u7684\u6570\u636e\u96c6\u89c4\u6a21\u662f\u73b0\u6709\u5362\u68ee\u5821\u8bedNER\u6570\u636e\u96c6\u76845\u500d\uff0c\u5b9e\u4f53\u7c7b\u522b\u8986\u76d6\u66f4\u5e7f\u66f4\u5e73\u8861\uff0c\u4e3a\u591a\u8bed\u8a00\u548c\u4f4e\u8d44\u6e90NER\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8eWikipedia/Wikidata\u548cLLM\u9a8c\u8bc1\u7684\u7ba1\u9053\u80fd\u6709\u6548\u6784\u5efa\u4f4e\u8d44\u6e90\u8bed\u8a00NER\u6570\u636e\u96c6\uff0cjudgeWEL\u6570\u636e\u96c6\u663e\u8457\u63a8\u8fdb\u4e86\u5362\u68ee\u5821\u8bedNLP\u7814\u7a76"}}
{"id": "2601.00614", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00614", "abs": "https://arxiv.org/abs/2601.00614", "authors": ["Mogens Plessen"], "title": "From 2D to 3D terrain-following area coverage path planning", "comment": "6 pages, 10 figures, 1 table", "summary": "An algorithm for 3D terrain-following area coverage path planning is presented. Multiple adjacent paths are generated that are (i) locally apart from each other by a distance equal to the working width of a machinery, while (ii) simultaneously floating at a projection distance equal to a specific working height above the terrain. The complexities of the algorithm in comparison to its 2D equivalent are highlighted. These include uniformly spaced elevation data generation using an Inverse Distance Weighting-approach and a local search. Area coverage path planning results for real-world 3D data within an agricultural context are presented to validate the algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd3D\u5730\u5f62\u8ddf\u968f\u7684\u533a\u57df\u8986\u76d6\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u751f\u6210\u76f8\u90bb\u8def\u5f84\uff0c\u5728\u4fdd\u6301\u5de5\u4f5c\u5bbd\u5ea6\u95f4\u8ddd\u7684\u540c\u65f6\uff0c\u4f7f\u8def\u5f84\u5728\u7279\u5b9a\u5de5\u4f5c\u9ad8\u5ea6\u4e0a\u8ddf\u968f\u5730\u5f62\u8d77\u4f0f\u3002", "motivation": "\u5728\u519c\u4e1a\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u673a\u68b0\u8bbe\u5907\u5728\u590d\u6742\u4e09\u7ef4\u5730\u5f62\u4e0a\u8fdb\u884c\u9ad8\u6548\u8986\u76d6\u4f5c\u4e1a\u3002\u4f20\u7edf\u76842D\u8def\u5f84\u89c4\u5212\u65e0\u6cd5\u9002\u5e94\u5730\u5f62\u8d77\u4f0f\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u8003\u8651\u6c34\u5e73\u95f4\u8ddd\u548c\u5782\u76f4\u9ad8\u5ea6\u76843D\u5730\u5f62\u8ddf\u968f\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u9006\u8ddd\u79bb\u52a0\u6743\u65b9\u6cd5\u751f\u6210\u5747\u5300\u95f4\u8ddd\u7684\u9ad8\u7a0b\u6570\u636e\uff0c\u7ed3\u5408\u5c40\u90e8\u641c\u7d22\u7b97\u6cd5\uff0c\u751f\u6210\u76f8\u90bb\u8def\u5f84\u3002\u8def\u5f84\u5728\u6c34\u5e73\u65b9\u5411\u4fdd\u6301\u673a\u68b0\u8bbe\u5907\u5de5\u4f5c\u5bbd\u5ea6\u7684\u95f4\u8ddd\uff0c\u5728\u5782\u76f4\u65b9\u5411\u4fdd\u6301\u7279\u5b9a\u5de5\u4f5c\u9ad8\u5ea6\u8ddf\u968f\u5730\u5f62\u8d77\u4f0f\u3002", "result": "\u7b97\u6cd5\u5728\u771f\u5b9e\u519c\u4e1a\u573a\u666f\u76843D\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u6210\u529f\u751f\u6210\u4e86\u9002\u5e94\u5730\u5f62\u8d77\u4f0f\u7684\u8986\u76d6\u8def\u5f84\u3002\u4e0e2D\u7b49\u6548\u7b97\u6cd5\u76f8\u6bd4\uff0c\u7b97\u6cd5\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u4f46\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u5b9e\u9645\u5730\u5f62\u6761\u4ef6\u3002", "conclusion": "\u63d0\u51fa\u76843D\u5730\u5f62\u8ddf\u968f\u533a\u57df\u8986\u76d6\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u5730\u5f62\u6761\u4ef6\u4e0b\u7684\u8986\u76d6\u4efb\u52a1\uff0c\u4e3a\u519c\u4e1a\u673a\u68b0\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8def\u5f84\u89c4\u5212\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00430", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00430", "abs": "https://arxiv.org/abs/2601.00430", "authors": ["Kian Ahrabian", "Eric Boxer", "Jay Pujara"], "title": "Toward Better Temporal Structures for Geopolitical Events Forecasting", "comment": "17 pages, 13 figures, 3 tables", "summary": "Forecasting on geopolitical temporal knowledge graphs (TKGs) through the lens of large language models (LLMs) has recently gained traction. While TKGs and their generalization, hyper-relational temporal knowledge graphs (HTKGs), offer a straightforward structure to represent simple temporal relationships, they lack the expressive power to convey complex facts efficiently. One of the critical limitations of HTKGs is a lack of support for more than two primary entities in temporal facts, which commonly occur in real-world events. To address this limitation, in this work, we study a generalization of HTKGs, Hyper-Relational Temporal Knowledge Generalized Hypergraphs (HTKGHs). We first derive a formalization for HTKGHs, demonstrating their backward compatibility while supporting two complex types of facts commonly found in geopolitical incidents. Then, utilizing this formalization, we introduce the htkgh-polecat dataset, built upon the global event database POLECAT. Finally, we benchmark and analyze popular LLMs on the relation prediction task, providing insights into their adaptability and capabilities in complex forecasting scenarios.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d85\u5173\u7cfb\u65f6\u5e8f\u77e5\u8bc6\u5e7f\u4e49\u8d85\u56fe\uff08HTKGHs\uff09\u7ed3\u6784\uff0c\u7528\u4e8e\u66f4\u6709\u6548\u5730\u8868\u793a\u590d\u6742\u7684\u5730\u7f18\u653f\u6cbb\u4e8b\u4ef6\uff0c\u5e76\u57fa\u4e8ePOLECAT\u6570\u636e\u5e93\u6784\u5efa\u4e86htkgh-polecat\u6570\u636e\u96c6\uff0c\u6700\u540e\u8bc4\u4f30\u4e86LLMs\u5728\u5173\u7cfb\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u7684\u8d85\u5173\u7cfb\u65f6\u5e8f\u77e5\u8bc6\u56fe\uff08HTKGs\uff09\u867d\u7136\u80fd\u8868\u793a\u7b80\u5355\u7684\u65f6\u5e8f\u5173\u7cfb\uff0c\u4f46\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u8868\u793a\u5305\u542b\u4e24\u4e2a\u4ee5\u4e0a\u4e3b\u8981\u5b9e\u4f53\u7684\u590d\u6742\u4e8b\u5b9e\uff0c\u8fd9\u5728\u771f\u5b9e\u4e16\u754c\u7684\u5730\u7f18\u653f\u6cbb\u4e8b\u4ef6\u4e2d\u5f88\u5e38\u89c1\u3002", "method": "1. \u63d0\u51faHTKGHs\u4f5c\u4e3aHTKGs\u7684\u6cdb\u5316\u5f62\u5f0f\uff0c\u652f\u6301\u4e24\u79cd\u5e38\u89c1\u7684\u5730\u7f18\u653f\u6cbb\u4e8b\u4ef6\u590d\u6742\u4e8b\u5b9e\u7c7b\u578b\uff1b2. \u57fa\u4e8ePOLECAT\u5168\u7403\u4e8b\u4ef6\u6570\u636e\u5e93\u6784\u5efahtkgh-polecat\u6570\u636e\u96c6\uff1b3. \u5728\u5173\u7cfb\u9884\u6d4b\u4efb\u52a1\u4e0a\u5bf9\u4e3b\u6d41LLMs\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u548c\u5206\u6790\u3002", "result": "\u5efa\u7acb\u4e86HTKGHs\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u5c55\u793a\u4e86\u5176\u5411\u540e\u517c\u5bb9\u6027\uff1b\u521b\u5efa\u4e86htkgh-polecat\u6570\u636e\u96c6\uff1b\u8bc4\u4f30\u4e86LLMs\u5728\u590d\u6742\u9884\u6d4b\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u548c\u80fd\u529b\u3002", "conclusion": "HTKGHs\u80fd\u591f\u66f4\u6709\u6548\u5730\u8868\u793a\u590d\u6742\u7684\u5730\u7f18\u653f\u6cbb\u4e8b\u4ef6\uff0c\u4e3aLLMs\u5728\u590d\u6742\u65f6\u5e8f\u5173\u7cfb\u9884\u6d4b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u3002"}}
{"id": "2601.00675", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00675", "abs": "https://arxiv.org/abs/2601.00675", "authors": ["Tony Lee", "Andrew Wagenmaker", "Karl Pertsch", "Percy Liang", "Sergey Levine", "Chelsea Finn"], "title": "RoboReward: General-Purpose Vision-Language Reward Models for Robotics", "comment": null, "summary": "A well-designed reward is critical for effective reinforcement learning-based policy improvement. In real-world robotic domains, obtaining such rewards typically requires either labor-intensive human labeling or brittle, handcrafted objectives. Vision-language models (VLMs) have shown promise as automatic reward models, yet their effectiveness on real robot tasks is poorly understood. In this work, we aim to close this gap by introducing (1) \\textbf{RoboReward}, a robotics reward dataset and benchmark built on large-scale real-robot corpora from Open X-Embodiment (OXE) and RoboArena, and (2) vision-language reward models trained on this dataset (RoboReward 4B/8B). Because OXE is success-heavy and lacks failure examples, we propose a \\emph{negative examples data augmentation} pipeline that generates calibrated \\emph{negatives} and \\emph{near-misses} via counterfactual relabeling of successful episodes and temporal clipping to create partial-progress outcomes from the same videos. Using this framework, we produce an extensive training and evaluation dataset that spans diverse tasks and embodiments and enables systematic evaluation of whether state-of-the-art VLMs can reliably provide rewards for robotics. Our evaluation of leading open-weight and proprietary VLMs reveals that no model excels across all tasks, underscoring substantial room for improvement. We then train general-purpose 4B- and 8B-parameter models that outperform much larger VLMs in assigning rewards for short-horizon robotic tasks. Finally, we deploy the 8B-parameter reward VLM in real-robot reinforcement learning and find that it improves policy learning over Gemini Robotics-ER 1.5, a frontier physical reasoning VLM trained on robotics data, by a large margin, while substantially narrowing the gap to RL training with human-provided rewards.", "AI": {"tldr": "\u63d0\u51faRoboReward\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u5956\u52b1\u5efa\u6a21\u80fd\u529b\uff0c\u5e76\u8bad\u7ec3\u4e864B/8B\u53c2\u6570\u6a21\u578b\uff0c\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u9886\u57df\uff0c\u83b7\u53d6\u6709\u6548\u7684\u5956\u52b1\u51fd\u6570\u901a\u5e38\u9700\u8981\u5927\u91cf\u4eba\u5de5\u6807\u6ce8\u6216\u624b\u5de5\u8bbe\u8ba1\uff0c\u800c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u81ea\u52a8\u5956\u52b1\u6a21\u578b\u7684\u6f5c\u529b\u5c1a\u672a\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5f97\u5230\u5145\u5206\u9a8c\u8bc1\u3002", "method": "1) \u6784\u5efaRoboReward\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u57fa\u4e8eOpen X-Embodiment\u548cRoboArena\u7684\u5927\u89c4\u6a21\u771f\u5b9e\u673a\u5668\u4eba\u6570\u636e\uff1b2) \u63d0\u51fa\u8d1f\u6837\u672c\u6570\u636e\u589e\u5f3a\u6d41\u7a0b\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u91cd\u6807\u6ce8\u548c\u65f6\u95f4\u88c1\u526a\u751f\u6210\u6821\u51c6\u7684\u8d1f\u6837\u672c\u548c\u63a5\u8fd1\u6210\u529f\u6837\u672c\uff1b3) \u8bad\u7ec34B\u548c8B\u53c2\u6570\u7684\u89c6\u89c9\u8bed\u8a00\u5956\u52b1\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u5747\u4e0d\u5360\u4f18\uff0c\u8bad\u7ec3\u51fa\u76844B/8B\u53c2\u6570\u6a21\u578b\u5728\u77ed\u89c6\u754c\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u4f18\u4e8e\u66f4\u5927\u6a21\u578b\uff0c8B\u6a21\u578b\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u4e2d\u5927\u5e45\u8d85\u8d8aGemini Robotics-ER 1.5\uff0c\u63a5\u8fd1\u4eba\u5de5\u5956\u52b1\u7684\u6548\u679c\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u673a\u5668\u4eba\u5956\u52b1\u51fd\u6570\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff1b\u63d0\u51fa\u7684RoboReward\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u65b9\u6cd5\u4e3a\u8fd9\u4e00\u65b9\u5411\u63d0\u4f9b\u4e86\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u8bad\u7ec3\u7684\u5c0f\u578b\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2601.00444", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00444", "abs": "https://arxiv.org/abs/2601.00444", "authors": ["Muhammad Shahmeer Khan"], "title": "Comparative Efficiency Analysis of Lightweight Transformer Models: A Multi-Domain Empirical Benchmark for Enterprise NLP Deployment", "comment": "11 pages, 6 figures. Code and reproducibility resources available on GitHub", "summary": "In the rapidly evolving landscape of enterprise natural language processing (NLP), the demand for efficient, lightweight models capable of handling multi-domain text automation tasks has intensified. This study conducts a comparative analysis of three prominent lightweight Transformer models - DistilBERT, MiniLM, and ALBERT - across three distinct domains: customer sentiment classification, news topic classification, and toxicity and hate speech detection. Utilizing datasets from IMDB, AG News, and the Measuring Hate Speech corpus, we evaluated performance using accuracy-based metrics including accuracy, precision, recall, and F1-score, as well as efficiency metrics such as model size, inference time, throughput, and memory usage. Key findings reveal that no single model dominates all performance dimensions. ALBERT achieves the highest task-specific accuracy in multiple domains, MiniLM excels in inference speed and throughput, and DistilBERT demonstrates the most consistent accuracy across tasks while maintaining competitive efficiency. All results reflect controlled fine-tuning under fixed enterprise-oriented constraints rather than exhaustive hyperparameter optimization. These results highlight trade-offs between accuracy and efficiency, recommending MiniLM for latency-sensitive enterprise applications, DistilBERT for balanced performance, and ALBERT for resource-constrained environments.", "AI": {"tldr": "\u6bd4\u8f83\u4e09\u79cd\u8f7b\u91cf\u7ea7Transformer\u6a21\u578b\uff08DistilBERT\u3001MiniLM\u3001ALBERT\uff09\u5728\u591a\u9886\u57df\u6587\u672c\u81ea\u52a8\u5316\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u4e0e\u6548\u7387\uff0c\u53d1\u73b0\u5404\u6709\u4f18\u52bf\uff1aALBERT\u51c6\u786e\u7387\u6700\u9ad8\uff0cMiniLM\u63a8\u7406\u901f\u5ea6\u6700\u5feb\uff0cDistilBERT\u6700\u5747\u8861\u3002", "motivation": "\u4f01\u4e1aNLP\u5e94\u7528\u5bf9\u9ad8\u6548\u8f7b\u91cf\u7ea7\u6a21\u578b\u5904\u7406\u591a\u9886\u57df\u6587\u672c\u81ea\u52a8\u5316\u4efb\u52a1\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u6bd4\u8f83\u4e0d\u540c\u8f7b\u91cf\u7ea7Transformer\u6a21\u578b\u5728\u6027\u80fd\u4e0e\u6548\u7387\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528IMDB\u3001AG News\u548cMeasuring Hate Speech\u4e09\u4e2a\u6570\u636e\u96c6\uff0c\u5728\u5ba2\u6237\u60c5\u611f\u5206\u7c7b\u3001\u65b0\u95fb\u4e3b\u9898\u5206\u7c7b\u3001\u6bd2\u6027\u53ca\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e09\u4e2a\u9886\u57df\uff0c\u5bf9\u6bd4DistilBERT\u3001MiniLM\u548cALBERT\u4e09\u79cd\u6a21\u578b\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ec\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u7b49\u6027\u80fd\u6307\u6807\uff0c\u4ee5\u53ca\u6a21\u578b\u5927\u5c0f\u3001\u63a8\u7406\u65f6\u95f4\u3001\u541e\u5410\u91cf\u548c\u5185\u5b58\u4f7f\u7528\u7b49\u6548\u7387\u6307\u6807\u3002", "result": "\u6ca1\u6709\u5355\u4e00\u6a21\u578b\u5728\u6240\u6709\u7ef4\u5ea6\u4e0a\u8868\u73b0\u6700\u4f73\uff1aALBERT\u5728\u591a\u4e2a\u9886\u57df\u83b7\u5f97\u6700\u9ad8\u4efb\u52a1\u7279\u5b9a\u51c6\u786e\u7387\uff1bMiniLM\u5728\u63a8\u7406\u901f\u5ea6\u548c\u541e\u5410\u91cf\u65b9\u9762\u8868\u73b0\u6700\u4f18\uff1bDistilBERT\u5728\u4efb\u52a1\u95f4\u51c6\u786e\u7387\u6700\u4e00\u81f4\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u6548\u7387\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u51c6\u786e\u7387\u4e0e\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u5efa\u8bae\uff1a\u5ef6\u8fdf\u654f\u611f\u7684\u4f01\u4e1a\u5e94\u7528\u9009\u62e9MiniLM\uff0c\u9700\u8981\u5e73\u8861\u6027\u80fd\u7684\u9009\u62e9DistilBERT\uff0c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u9009\u62e9ALBERT\u3002"}}
{"id": "2601.00702", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.00702", "abs": "https://arxiv.org/abs/2601.00702", "authors": ["Samuel Cerezo", "Javier Civera"], "title": "DefVINS: Visual-Inertial Odometry for Deformable Scenes", "comment": "4 figures, 3 tables. Submitted to RA-L", "summary": "Deformable scenes violate the rigidity assumptions underpinning classical visual-inertial odometry (VIO), often leading to over-fitting to local non-rigid motion or severe drift when deformation dominates visual parallax. We introduce DefVINS, a visual-inertial odometry framework that explicitly separates a rigid, IMU-anchored state from a non--rigid warp represented by an embedded deformation graph. The system is initialized using a standard VIO procedure that fixes gravity, velocity, and IMU biases, after which non-rigid degrees of freedom are activated progressively as the estimation becomes well conditioned. An observability analysis is included to characterize how inertial measurements constrain the rigid motion and render otherwise unobservable modes identifiable in the presence of deformation. This analysis motivates the use of IMU anchoring and informs a conditioning-based activation strategy that prevents ill-posed updates under poor excitation. Ablation studies demonstrate the benefits of combining inertial constraints with observability-aware deformation activation, resulting in improved robustness under non-rigid environments.", "AI": {"tldr": "DefVINS\uff1a\u4e00\u79cd\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165\u53d8\u5f62\u56fe\u663e\u5f0f\u5206\u79bb\u521a\u6027IMU\u951a\u5b9a\u72b6\u6001\u4e0e\u975e\u521a\u6027\u53d8\u5f62\uff0c\u63d0\u9ad8\u5728\u53ef\u53d8\u5f62\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\u57fa\u4e8e\u521a\u6027\u5047\u8bbe\uff0c\u5728\u53ef\u53d8\u5f62\u573a\u666f\u4e2d\u5bb9\u6613\u8fc7\u5ea6\u62df\u5408\u5c40\u90e8\u975e\u521a\u6027\u8fd0\u52a8\u6216\u4ea7\u751f\u4e25\u91cd\u6f02\u79fb\uff0c\u9700\u8981\u4e13\u95e8\u5904\u7406\u53d8\u5f62\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5d4c\u5165\u53d8\u5f62\u56fe\u8868\u793a\u975e\u521a\u6027\u53d8\u5f62\uff0c\u5c06\u521a\u6027IMU\u951a\u5b9a\u72b6\u6001\u4e0e\u975e\u521a\u6027\u53d8\u5f62\u5206\u79bb\uff1b\u91c7\u7528\u53ef\u89c2\u6d4b\u6027\u5206\u6790\u6307\u5bfc\u53d8\u5f62\u6fc0\u6d3b\u7b56\u7565\uff0c\u5728\u4f30\u8ba1\u6761\u4ef6\u826f\u597d\u65f6\u6e10\u8fdb\u6fc0\u6d3b\u975e\u521a\u6027\u81ea\u7531\u5ea6\u3002", "result": "\u901a\u8fc7\u60ef\u6027\u7ea6\u675f\u4e0e\u53ef\u89c2\u6d4b\u6027\u611f\u77e5\u7684\u53d8\u5f62\u6fc0\u6d3b\u76f8\u7ed3\u5408\uff0c\u5728\u975e\u521a\u6027\u73af\u5883\u4e2d\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "DefVINS\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u5206\u79bb\u521a\u6027\u4e0e\u975e\u521a\u6027\u8fd0\u52a8\uff0c\u7ed3\u5408\u60ef\u6027\u7ea6\u675f\u548c\u6761\u4ef6\u5316\u6fc0\u6d3b\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u53ef\u53d8\u5f62\u573a\u666f\u4e2d\u7684\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\u95ee\u9898\u3002"}}
{"id": "2601.00448", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00448", "abs": "https://arxiv.org/abs/2601.00448", "authors": ["Dimitris Vartziotis"], "title": "Language as Mathematical Structure: Examining Semantic Field Theory Against Language Games", "comment": null, "summary": "Large language models (LLMs) offer a new empirical setting in which long-standing theories of linguistic meaning can be examined. This paper contrasts two broad approaches: social constructivist accounts associated with language games, and a mathematically oriented framework we call Semantic Field Theory. Building on earlier work by the author, we formalize the notions of lexical fields (Lexfelder) and linguistic fields (Lingofelder) as interacting structures in a continuous semantic space. We then analyze how core properties of transformer architectures-such as distributed representations, attention mechanisms, and geometric regularities in embedding spaces-relate to these concepts. We argue that the success of LLMs in capturing semantic regularities supports the view that language exhibits an underlying mathematical structure, while their persistent limitations in pragmatic reasoning and context sensitivity are consistent with the importance of social grounding emphasized in philosophical accounts of language use. On this basis, we suggest that mathematical structure and language games can be understood as complementary rather than competing perspectives. The resulting framework clarifies the scope and limits of purely statistical models of language and motivates new directions for theoretically informed AI architectures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9\u6bd4\u4e86\u8bed\u8a00\u7684\u793e\u4f1a\u5efa\u6784\u4e3b\u4e49\uff08\u8bed\u8a00\u6e38\u620f\uff09\u548c\u6570\u5b66\u5bfc\u5411\u7684\u8bed\u4e49\u573a\u7406\u8bba\uff0c\u8ba4\u4e3a\u4e24\u8005\u662f\u4e92\u8865\u800c\u975e\u7ade\u4e89\u7684\u5173\u7cfb\uff0cLLMs\u7684\u6210\u529f\u652f\u6301\u8bed\u8a00\u5177\u6709\u5e95\u5c42\u6570\u5b66\u7ed3\u6784\u7684\u89c2\u70b9\uff0c\u4f46\u5176\u5728\u8bed\u7528\u63a8\u7406\u4e0a\u7684\u5c40\u9650\u5219\u4f53\u73b0\u4e86\u793e\u4f1a\u57fa\u7840\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fd9\u4e00\u65b0\u7684\u5b9e\u8bc1\u73af\u5883\uff0c\u68c0\u9a8c\u957f\u671f\u5b58\u5728\u7684\u8bed\u8a00\u610f\u4e49\u7406\u8bba\uff0c\u7279\u522b\u662f\u5bf9\u6bd4\u793e\u4f1a\u5efa\u6784\u4e3b\u4e49\uff08\u8bed\u8a00\u6e38\u620f\uff09\u548c\u6570\u5b66\u5bfc\u5411\u7684\u8bed\u4e49\u573a\u7406\u8bba\u8fd9\u4e24\u79cd\u5e7f\u6cdb\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u4f5c\u8005\u5148\u524d\u5de5\u4f5c\uff0c\u5c06\u8bcd\u6c47\u573a\uff08Lexfelder\uff09\u548c\u8bed\u8a00\u573a\uff08Lingofelder\uff09\u5f62\u5f0f\u5316\u4e3a\u8fde\u7eed\u8bed\u4e49\u7a7a\u95f4\u4e2d\u7684\u4ea4\u4e92\u7ed3\u6784\uff0c\u5206\u6790Transformer\u67b6\u6784\u7684\u6838\u5fc3\u7279\u6027\uff08\u5206\u5e03\u5f0f\u8868\u793a\u3001\u6ce8\u610f\u529b\u673a\u5236\u3001\u5d4c\u5165\u7a7a\u95f4\u7684\u51e0\u4f55\u89c4\u5f8b\u6027\uff09\u4e0e\u8fd9\u4e9b\u6982\u5ff5\u7684\u5173\u7cfb\u3002", "result": "LLMs\u5728\u6355\u6349\u8bed\u4e49\u89c4\u5f8b\u6027\u65b9\u9762\u7684\u6210\u529f\u652f\u6301\u4e86\u8bed\u8a00\u5177\u6709\u5e95\u5c42\u6570\u5b66\u7ed3\u6784\u7684\u89c2\u70b9\uff0c\u800c\u5176\u5728\u8bed\u7528\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u65b9\u9762\u7684\u6301\u7eed\u5c40\u9650\u5219\u4e0e\u54f2\u5b66\u8bed\u8a00\u4f7f\u7528\u7406\u8bba\u5f3a\u8c03\u7684\u793e\u4f1a\u57fa\u7840\u91cd\u8981\u6027\u76f8\u4e00\u81f4\u3002", "conclusion": "\u6570\u5b66\u7ed3\u6784\u548c\u8bed\u8a00\u6e38\u620f\u53ef\u4ee5\u7406\u89e3\u4e3a\u4e92\u8865\u800c\u975e\u7ade\u4e89\u7684\u89c2\u70b9\uff0c\u8fd9\u4e00\u6846\u67b6\u6f84\u6e05\u4e86\u7eaf\u7edf\u8ba1\u8bed\u8a00\u6a21\u578b\u7684\u8303\u56f4\u548c\u5c40\u9650\uff0c\u5e76\u4e3a\u7406\u8bba\u6307\u5bfc\u7684AI\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2601.00754", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.00754", "abs": "https://arxiv.org/abs/2601.00754", "authors": ["Maria Teresa Parreira", "Isabel Neto", "Filipa Rocha", "Wendy Ju"], "title": "Calling for Backup: How Children Navigate Successive Robot Communication Failures", "comment": null, "summary": "How do children respond to repeated robot errors? While prior research has examined adult reactions to successive robot errors, children's responses remain largely unexplored. In this study, we explore children's reactions to robot social errors and performance errors. For the latter, this study reproduces the successive robot failure paradigm of Liu et al. with child participants (N=59, ages 8-10) to examine how young users respond to repeated robot conversational errors. Participants interacted with a robot that failed to understand their prompts three times in succession, with their behavioral responses video-recorded and analyzed. We found both similarities and differences compared to adult responses from the original study. Like adults, children adjusted their prompts, modified their verbal tone, and exhibited increasingly emotional non-verbal responses throughout successive errors. However, children demonstrated more disengagement behaviors, including temporarily ignoring the robot or actively seeking an adult. Errors did not affect participants' perception of the robot, suggesting more flexible conversational expectations in children. These findings inform the design of more effective and developmentally appropriate human-robot interaction systems for young users.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u513f\u7ae5\u5bf9\u673a\u5668\u4eba\u91cd\u590d\u9519\u8bef\u7684\u53cd\u5e94\uff0c\u53d1\u73b0\u513f\u7ae5\u4e0e\u6210\u4eba\u65e2\u6709\u76f8\u4f3c\u8c03\u6574\u884c\u4e3a\uff0c\u4e5f\u6709\u66f4\u591a\u8131\u79bb\u4e92\u52a8\u884c\u4e3a\uff0c\u4f46\u9519\u8bef\u4e0d\u5f71\u54cd\u513f\u7ae5\u5bf9\u673a\u5668\u4eba\u7684\u611f\u77e5\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6210\u4eba\u5bf9\u673a\u5668\u4eba\u8fde\u7eed\u9519\u8bef\u7684\u53cd\u5e94\uff0c\u4f46\u513f\u7ae5\u5bf9\u6b64\u7684\u53cd\u5e94\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u513f\u7ae5\u5982\u4f55\u5e94\u5bf9\u673a\u5668\u4eba\u7684\u793e\u4ea4\u9519\u8bef\u548c\u6027\u80fd\u9519\u8bef\uff0c\u7279\u522b\u662f\u91cd\u590d\u7684\u5bf9\u8bdd\u9519\u8bef\u3002", "method": "\u91c7\u7528Liu\u7b49\u4eba\u7684\u8fde\u7eed\u673a\u5668\u4eba\u5931\u8d25\u8303\u5f0f\uff0c\u8ba959\u540d8-10\u5c81\u513f\u7ae5\u4e0e\u673a\u5668\u4eba\u4e92\u52a8\u3002\u673a\u5668\u4eba\u8fde\u7eed\u4e09\u6b21\u65e0\u6cd5\u7406\u89e3\u513f\u7ae5\u63d0\u793a\uff0c\u7814\u7a76\u4eba\u5458\u901a\u8fc7\u89c6\u9891\u8bb0\u5f55\u5206\u6790\u513f\u7ae5\u7684\u884c\u4e3a\u53cd\u5e94\u3002", "result": "\u513f\u7ae5\u4e0e\u6210\u4eba\u53cd\u5e94\u6709\u76f8\u4f3c\u4e4b\u5904\uff08\u8c03\u6574\u63d0\u793a\u3001\u6539\u53d8\u8bed\u8c03\u3001\u60c5\u7eea\u5316\u975e\u8bed\u8a00\u53cd\u5e94\uff09\uff0c\u4f46\u513f\u7ae5\u8868\u73b0\u51fa\u66f4\u591a\u8131\u79bb\u884c\u4e3a\uff08\u6682\u65f6\u5ffd\u7565\u673a\u5668\u4eba\u6216\u5bfb\u6c42\u6210\u4eba\u5e2e\u52a9\uff09\u3002\u9519\u8bef\u4e0d\u5f71\u54cd\u513f\u7ae5\u5bf9\u673a\u5668\u4eba\u7684\u611f\u77e5\uff0c\u8868\u660e\u513f\u7ae5\u5bf9\u5bf9\u8bdd\u671f\u671b\u66f4\u7075\u6d3b\u3002", "conclusion": "\u513f\u7ae5\u5bf9\u673a\u5668\u4eba\u91cd\u590d\u9519\u8bef\u7684\u53cd\u5e94\u4e0e\u6210\u4eba\u4e0d\u540c\uff0c\u9700\u8981\u8bbe\u8ba1\u66f4\u6709\u6548\u3001\u9002\u5408\u513f\u7ae5\u53d1\u5c55\u7684\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u7cfb\u7edf\u3002\u513f\u7ae5\u66f4\u7075\u6d3b\u7684\u5bf9\u8bdd\u671f\u671b\u4e3a\u8bbe\u8ba1\u513f\u7ae5\u53cb\u597d\u578b\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u793a\u3002"}}
{"id": "2601.00454", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00454", "abs": "https://arxiv.org/abs/2601.00454", "authors": ["Hyunjun Kim"], "title": "Defensive M2S: Training Guardrail Models on Compressed Multi-turn Conversations", "comment": null, "summary": "Guardrail models are essential for ensuring the safety of Large Language Model (LLM) deployments, but processing full multi-turn conversation histories incurs significant computational cost. We propose Defensive M2S, a training paradigm that fine-tunes guardrail models on Multi-turn to Single-turn (M2S) compressed conversations rather than complete dialogue histories. We provide a formal complexity analysis showing that M2S reduces training cost from $O(n^2)$ to $O(n)$ for $n$-turn conversations. Empirically, on our training dataset (779 samples, avg. 10.6 turns), M2S requires only 169K tokens compared to 15.7M tokens for the multi-turn baseline -- a 93$\\times$ reduction. We evaluate Defensive M2S across three guardrail model families (LlamaGuard, Nemotron, Qwen3Guard) and three compression templates (hyphenize, numberize, pythonize) on SafeDialBench, a comprehensive multi-turn jailbreak benchmark. Our best configuration, Qwen3Guard with hyphenize compression, achieves 93.8% attack detection recall while reducing inference tokens by 94.6% (from 3,231 to 173 tokens per conversation). This represents a 38.9 percentage point improvement over the baseline while dramatically reducing both training and inference costs. Our findings demonstrate that M2S compression can serve as an effective efficiency technique for guardrail deployment, enabling scalable safety screening of long multi-turn conversations.", "AI": {"tldr": "\u63d0\u51faDefensive M2S\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u591a\u8f6e\u5230\u5355\u8f6e\u5bf9\u8bdd\u538b\u7f29\u6280\u672f\uff0c\u5c06\u62a4\u680f\u6a21\u578b\u7684\u8bad\u7ec3\u6210\u672c\u4eceO(n\u00b2)\u964d\u81f3O(n)\uff0c\u63a8\u7406token\u51cf\u5c1194.6%\uff0c\u540c\u65f6\u653b\u51fb\u68c0\u6d4b\u53ec\u56de\u7387\u63d0\u534738.9\u4e2a\u767e\u5206\u70b9\u3002", "motivation": "\u73b0\u6709\u62a4\u680f\u6a21\u578b\u5904\u7406\u5b8c\u6574\u591a\u8f6e\u5bf9\u8bdd\u5386\u53f2\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u65b9\u6cd5\u6765\u5b9e\u73b0\u5927\u89c4\u6a21\u5b89\u5168\u90e8\u7f72\u3002", "method": "\u63d0\u51faDefensive M2S\u8bad\u7ec3\u8303\u5f0f\uff0c\u5728\u591a\u8f6e\u5230\u5355\u8f6e\u538b\u7f29\u5bf9\u8bdd\u4e0a\u5fae\u8c03\u62a4\u680f\u6a21\u578b\uff0c\u4f7f\u7528\u4e09\u79cd\u538b\u7f29\u6a21\u677f\uff08hyphenize\u3001numberize\u3001pythonize\uff09\uff0c\u5728\u4e09\u4e2a\u62a4\u680f\u6a21\u578b\u5bb6\u65cf\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8bad\u7ec3token\u51cf\u5c1193\u500d\uff08\u4ece1570\u4e07\u964d\u81f316.9\u4e07\uff09\uff0c\u63a8\u7406token\u51cf\u5c1194.6%\uff08\u4ece3231\u964d\u81f3173\uff09\uff0c\u6700\u4f73\u914d\u7f6e\u653b\u51fb\u68c0\u6d4b\u53ec\u56de\u7387\u8fbe93.8%\uff0c\u6bd4\u57fa\u7ebf\u63d0\u534738.9\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "M2S\u538b\u7f29\u53ef\u4f5c\u4e3a\u62a4\u680f\u90e8\u7f72\u7684\u6709\u6548\u6548\u7387\u6280\u672f\uff0c\u5b9e\u73b0\u957f\u591a\u8f6e\u5bf9\u8bdd\u7684\u53ef\u6269\u5c55\u5b89\u5168\u7b5b\u67e5\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002"}}
{"id": "2601.00488", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.00488", "abs": "https://arxiv.org/abs/2601.00488", "authors": ["Alexander M. Esser", "Jens D\u00f6rpinghaus"], "title": "Noise-Aware Named Entity Recognition for Historical VET Documents", "comment": "This is an extended, non-peer-reviewed version of the paper presented at VISAPP 2026", "summary": "This paper addresses Named Entity Recognition (NER) in the domain of Vocational Education and Training (VET), focusing on historical, digitized documents that suffer from OCR-induced noise. We propose a robust NER approach leveraging Noise-Aware Training (NAT) with synthetically injected OCR errors, transfer learning, and multi-stage fine-tuning. Three complementary strategies, training on noisy, clean, and artificial data, are systematically compared. Our method is one of the first to recognize multiple entity types in VET documents. It is applied to German documents but transferable to arbitrary languages. Experimental results demonstrate that domain-specific and noise-aware fine-tuning substantially increases robustness and accuracy under noisy conditions. We provide publicly available code for reproducible noise-aware NER in domain-specific contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9\u804c\u4e1a\u6559\u80b2\u57f9\u8bad\u9886\u57df\u5386\u53f2\u6587\u6863\u7684\u566a\u58f0\u611f\u77e5\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210OCR\u9519\u8bef\u3001\u8fc1\u79fb\u5b66\u4e60\u548c\u591a\u9636\u6bb5\u5fae\u8c03\u6765\u63d0\u9ad8\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u804c\u4e1a\u6559\u80b2\u57f9\u8bad\u9886\u57df\u7684\u5386\u53f2\u6570\u5b57\u5316\u6587\u6863\u5b58\u5728OCR\u566a\u58f0\u95ee\u9898\uff0c\u4f20\u7edfNER\u65b9\u6cd5\u5728\u8fd9\u79cd\u566a\u58f0\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u8be5\u9886\u57df\u4e14\u80fd\u5904\u7406OCR\u566a\u58f0\u7684NER\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u566a\u58f0\u611f\u77e5\u8bad\u7ec3\uff08NAT\uff09\uff0c\u901a\u8fc7\u5408\u6210\u6ce8\u5165OCR\u9519\u8bef\uff0c\u7ed3\u5408\u8fc1\u79fb\u5b66\u4e60\u548c\u591a\u9636\u6bb5\u5fae\u8c03\u3002\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e09\u79cd\u4e92\u8865\u7b56\u7565\uff1a\u5728\u566a\u58f0\u6570\u636e\u3001\u5e72\u51c0\u6570\u636e\u548c\u4eba\u5de5\u5408\u6210\u6570\u636e\u4e0a\u7684\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9886\u57df\u7279\u5b9a\u548c\u566a\u58f0\u611f\u77e5\u7684\u5fae\u8c03\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u9996\u6b21\u5728VET\u6587\u6863\u4e2d\u8bc6\u522b\u591a\u79cd\u5b9e\u4f53\u7c7b\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u516c\u5f00\u53ef\u7528\u7684\u4ee3\u7801\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86VET\u9886\u57df\u5386\u53f2\u6587\u6863\u7684NER\u95ee\u9898\uff0c\u5177\u6709\u8bed\u8a00\u53ef\u8fc1\u79fb\u6027\uff0c\u4e3a\u9886\u57df\u7279\u5b9a\u4e0a\u4e0b\u6587\u4e2d\u7684\u566a\u58f0\u611f\u77e5NER\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00506", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00506", "abs": "https://arxiv.org/abs/2601.00506", "authors": ["Lineesha Kamana", "Akshita Ananda Subramanian", "Mehuli Ghosh", "Suman Saha"], "title": "Rule-Based Approaches to Atomic Sentence Extraction", "comment": null, "summary": "Natural language often combines multiple ideas into complex sentences. Atomic sentence extraction, the task of decomposing complex sentences into simpler sentences that each express a single idea, improves performance in information retrieval, question answering, and automated reasoning systems. Previous work has formalized the \"split-and-rephrase\" task and established evaluation metrics, and machine learning approaches using large language models have improved extraction accuracy. However, these methods lack interpretability and provide limited insight into which linguistic structures cause extraction failures. Although some studies have explored dependency-based extraction of subject-verb-object triples and clauses, no principled analysis has examined which specific clause structures and dependencies lead to extraction difficulties. This study addresses this gap by analyzing how complex sentence structures, including relative clauses, adverbial clauses, coordination patterns, and passive constructions, affect the performance of rule-based atomic sentence extraction. Using the WikiSplit dataset, we implemented dependency-based extraction rules in spaCy, generated 100 gold=standard atomic sentence sets, and evaluated performance using ROUGE and BERTScore. The system achieved ROUGE-1 F1 = 0.6714, ROUGE-2 F1 = 0.478, ROUGE-L F1 = 0.650, and BERTScore F1 = 0.5898, indicating moderate-to-high lexical, structural, and semantic alignment. Challenging structures included relative clauses, appositions, coordinated predicates, adverbial clauses, and passive constructions. Overall, rule-based extraction is reasonably accurate but sensitive to syntactic complexity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u590d\u6742\u53e5\u5b50\u7ed3\u6784\uff08\u5982\u5173\u7cfb\u4ece\u53e5\u3001\u72b6\u8bed\u4ece\u53e5\u3001\u5e76\u5217\u7ed3\u6784\u7b49\uff09\u5982\u4f55\u5f71\u54cd\u57fa\u4e8e\u89c4\u5219\u7684\u539f\u5b50\u53e5\u5b50\u63d0\u53d6\u6027\u80fd\uff0c\u53d1\u73b0\u89c4\u5219\u65b9\u6cd5\u5177\u6709\u4e2d\u7b49\u81f3\u9ad8\u51c6\u786e\u6027\u4f46\u5bf9\u53e5\u6cd5\u590d\u6742\u5ea6\u654f\u611f\u3002", "motivation": "\u73b0\u6709\u539f\u5b50\u53e5\u5b50\u63d0\u53d6\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u65e0\u6cd5\u63ed\u793a\u54ea\u4e9b\u5177\u4f53\u8bed\u8a00\u7ed3\u6784\u5bfc\u81f4\u63d0\u53d6\u5931\u8d25\u3002\u9700\u8981\u7cfb\u7edf\u5206\u6790\u590d\u6742\u53e5\u5b50\u7ed3\u6784\u5bf9\u63d0\u53d6\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528WikiSplit\u6570\u636e\u96c6\uff0c\u5728spaCy\u4e2d\u5b9e\u73b0\u57fa\u4e8e\u4f9d\u5b58\u5173\u7cfb\u7684\u63d0\u53d6\u89c4\u5219\uff0c\u751f\u6210100\u4e2a\u9ec4\u91d1\u6807\u51c6\u539f\u5b50\u53e5\u5b50\u96c6\uff0c\u4f7f\u7528ROUGE\u548cBERTScore\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u7cfb\u7edf\u8fbe\u5230ROUGE-1 F1=0.6714\uff0cROUGE-2 F1=0.478\uff0cROUGE-L F1=0.650\uff0cBERTScore F1=0.5898\uff0c\u663e\u793a\u4e2d\u7b49\u81f3\u9ad8\u7684\u8bcd\u6c47\u3001\u7ed3\u6784\u548c\u8bed\u4e49\u5bf9\u9f50\u3002\u6311\u6218\u6027\u7ed3\u6784\u5305\u62ec\u5173\u7cfb\u4ece\u53e5\u3001\u540c\u4f4d\u8bed\u3001\u5e76\u5217\u8c13\u8bed\u3001\u72b6\u8bed\u4ece\u53e5\u548c\u88ab\u52a8\u7ed3\u6784\u3002", "conclusion": "\u57fa\u4e8e\u89c4\u5219\u7684\u539f\u5b50\u53e5\u5b50\u63d0\u53d6\u5177\u6709\u5408\u7406\u51c6\u786e\u6027\uff0c\u4f46\u5bf9\u53e5\u6cd5\u590d\u6742\u5ea6\u654f\u611f\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u5904\u7406\u590d\u6742\u8bed\u8a00\u7ed3\u6784\u3002"}}
{"id": "2601.00536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00536", "abs": "https://arxiv.org/abs/2601.00536", "authors": ["Yuelyu Ji", "Zhuochun Li", "Rui Meng", "Daqing He"], "title": "Retrieval--Reasoning Processes for Multi-hop Question Answering: A Four-Axis Design Framework and Empirical Trends", "comment": null, "summary": "Multi-hop question answering (QA) requires systems to iteratively retrieve evidence and reason across multiple hops. While recent RAG and agentic methods report strong results, the underlying retrieval--reasoning \\emph{process} is often left implicit, making procedural choices hard to compare across model families. This survey takes the execution procedure as the unit of analysis and introduces a four-axis framework covering (A) overall execution plan, (B) index structure, (C) next-step control (strategies and triggers), and (D) stop/continue criteria. Using this schema, we map representative multi-hop QA systems and synthesize reported ablations and tendencies on standard benchmarks (e.g., HotpotQA, 2WikiMultiHopQA, MuSiQue), highlighting recurring trade-offs among effectiveness, efficiency, and evidence faithfulness. We conclude with open challenges for retrieval--reasoning agents, including structure-aware planning, transferable control policies, and robust stopping under distribution shift.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u56db\u8f74\u6846\u67b6\u6765\u5206\u6790\u591a\u8df3\u95ee\u7b54\u7cfb\u7edf\u7684\u6267\u884c\u8fc7\u7a0b\uff0c\u5c06\u6267\u884c\u8fc7\u7a0b\u4f5c\u4e3a\u5206\u6790\u5355\u5143\uff0c\u6db5\u76d6\u6574\u4f53\u6267\u884c\u8ba1\u5212\u3001\u7d22\u5f15\u7ed3\u6784\u3001\u4e0b\u4e00\u6b65\u63a7\u5236\u548c\u505c\u6b62/\u7ee7\u7eed\u6807\u51c6\uff0c\u5e76\u603b\u7ed3\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u6743\u8861\u548c\u5f00\u653e\u6311\u6218\u3002", "motivation": "\u5f53\u524d\u591a\u8df3\u95ee\u7b54\u7cfb\u7edf\u4e2d\uff0c\u68c0\u7d22-\u63a8\u7406\u8fc7\u7a0b\u901a\u5e38\u9690\u542b\u5728\u6a21\u578b\u5185\u90e8\uff0c\u4f7f\u5f97\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u7684\u7a0b\u5e8f\u9009\u62e9\u96be\u4ee5\u6bd4\u8f83\u3002\u9700\u8981\u7cfb\u7edf\u5316\u7684\u6846\u67b6\u6765\u5206\u6790\u8fd9\u4e9b\u6267\u884c\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u56db\u8f74\u5206\u6790\u6846\u67b6\uff1a(A)\u6574\u4f53\u6267\u884c\u8ba1\u5212\uff0c(B)\u7d22\u5f15\u7ed3\u6784\uff0c(C)\u4e0b\u4e00\u6b65\u63a7\u5236\u7b56\u7565\u548c\u89e6\u53d1\u673a\u5236\uff0c(D)\u505c\u6b62/\u7ee7\u7eed\u6807\u51c6\u3002\u4f7f\u7528\u8be5\u6846\u67b6\u5bf9\u4ee3\u8868\u6027\u591a\u8df3QA\u7cfb\u7edf\u8fdb\u884c\u6620\u5c04\u5206\u6790\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7efc\u5408\u4e86\u62a5\u544a\u7684\u6d88\u878d\u5b9e\u9a8c\u548c\u8d8b\u52bf\uff0c\u7a81\u51fa\u4e86\u6709\u6548\u6027\u3001\u6548\u7387\u548c\u8bc1\u636e\u5fe0\u5b9e\u5ea6\u4e4b\u95f4\u7684\u53cd\u590d\u6743\u8861\u3002", "conclusion": "\u63d0\u51fa\u4e86\u68c0\u7d22-\u63a8\u7406\u4ee3\u7406\u7684\u5f00\u653e\u6311\u6218\uff0c\u5305\u62ec\u7ed3\u6784\u611f\u77e5\u89c4\u5212\u3001\u53ef\u8fc1\u79fb\u7684\u63a7\u5236\u7b56\u7565\u4ee5\u53ca\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u505c\u6b62\u673a\u5236\u3002"}}
{"id": "2601.00543", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00543", "abs": "https://arxiv.org/abs/2601.00543", "authors": ["Chung-Wei Victor Yuan"], "title": "ECR: Manifold-Guided Semantic Cues for Compact Language Models", "comment": "Preprint 13pages, 6 figures", "summary": "Compact models often lose the structure of their embedding space. The issue shows up when the capacity is tight or the data spans several languages. Such collapse makes it difficult for downstream tasks to build on the resulting representation. Existing compression methods focus on aligning model outputs at a superficial level but fail to preserve the underlying manifold structure. This mismatch often leads to semantic drift in the compact model, causing both task behavior and linguistic properties to deviate from the reference model.\n  To address those issues, we provide a new framework called Embedding Consistency Regulation (ECR). This framework first derives a set of semantic anchors from teacher embeddings (computed once offline). Then, the compact model learns to maintain consistent geometry around these anchors, without relying on matching logits or internal features. ECR adds only a small projection step at inference, without altering the decoding architecture or its runtime behavior.\n  In experiments on a 100K multilingual corpus, ECR consistently stabilizes training and preserves semantic structure across tasks and languages. It also produces a more compact and task-aligned representation space, enabling low-capacity models to learn cleaner manifolds than conventional baselines. ECR works without teacher outputs and is compatible with, but independent of, distillation. Taken together, our results show that ECR helps compact models better follow task requirements and makes them easier to deploy under strict efficiency or privacy limits.", "AI": {"tldr": "ECR\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u951a\u70b9\u4fdd\u6301\u7d27\u51d1\u6a21\u578b\u5d4c\u5165\u7a7a\u95f4\u7684\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u65e0\u9700\u6559\u5e08\u8f93\u51fa\u6216\u4fee\u6539\u89e3\u7801\u67b6\u6784", "motivation": "\u7d27\u51d1\u6a21\u578b\u5728\u5bb9\u91cf\u53d7\u9650\u6216\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u5bb9\u6613\u4e22\u5931\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\uff0c\u5bfc\u81f4\u8bed\u4e49\u6f02\u79fb\u548c\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u53ea\u5173\u6ce8\u8868\u5c42\u8f93\u51fa\u5bf9\u9f50\u800c\u5ffd\u7565\u4e86\u5e95\u5c42\u6d41\u5f62\u7ed3\u6784", "method": "\u63d0\u51fa\u5d4c\u5165\u4e00\u81f4\u6027\u8c03\u8282(ECR)\u6846\u67b6\uff1a\u4ece\u6559\u5e08\u5d4c\u5165\u4e2d\u79bb\u7ebf\u8ba1\u7b97\u8bed\u4e49\u951a\u70b9\uff0c\u8ba9\u7d27\u51d1\u6a21\u578b\u5b66\u4e60\u5728\u8fd9\u4e9b\u951a\u70b9\u5468\u56f4\u4fdd\u6301\u4e00\u81f4\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u65e0\u9700\u5339\u914dlogits\u6216\u5185\u90e8\u7279\u5f81\uff0c\u63a8\u7406\u65f6\u4ec5\u6dfb\u52a0\u5c0f\u578b\u6295\u5f71\u6b65\u9aa4", "result": "\u572810\u4e07\u6761\u591a\u8bed\u8a00\u8bed\u6599\u5b9e\u9a8c\u4e2d\uff0cECR\u7a33\u5b9a\u8bad\u7ec3\u5e76\u8de8\u4efb\u52a1\u548c\u8bed\u8a00\u4fdd\u6301\u8bed\u4e49\u7ed3\u6784\uff0c\u4ea7\u751f\u66f4\u7d27\u51d1\u4e14\u4efb\u52a1\u5bf9\u9f50\u7684\u8868\u793a\u7a7a\u95f4\uff0c\u4f7f\u4f4e\u5bb9\u91cf\u6a21\u578b\u5b66\u4e60\u5230\u6bd4\u4f20\u7edf\u57fa\u7ebf\u66f4\u6e05\u6670\u7684\u6d41\u5f62", "conclusion": "ECR\u5e2e\u52a9\u7d27\u51d1\u6a21\u578b\u66f4\u597d\u5730\u9075\u5faa\u4efb\u52a1\u8981\u6c42\uff0c\u4f7f\u5176\u5728\u4e25\u683c\u6548\u7387\u6216\u9690\u79c1\u9650\u5236\u4e0b\u66f4\u5bb9\u6613\u90e8\u7f72\uff0c\u65e0\u9700\u6559\u5e08\u8f93\u51fa\u4e14\u4e0e\u84b8\u998f\u517c\u5bb9\u4f46\u72ec\u7acb"}}
{"id": "2601.00557", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.00557", "abs": "https://arxiv.org/abs/2601.00557", "authors": ["Yuang Zheng", "Yuxiang Mei", "Dongxing Xu", "Jie Chen", "Yanhua Long"], "title": "A Language-Agnostic Hierarchical LoRA-MoE Architecture for CTC-based Multilingual ASR", "comment": "5 pages, submitted to IEEE Signal Processing Letters", "summary": "Large-scale multilingual ASR (mASR) models such as Whisper achieve strong performance but incur high computational and latency costs, limiting their deployment on resource-constrained edge devices. In this study, we propose a lightweight and language-agnostic multilingual ASR system based on a CTC architecture with domain adaptation. Specifically, we introduce a Language-agnostic Hierarchical LoRA-MoE (HLoRA) framework integrated into an mHuBERT-CTC model, enabling end-to-end decoding via LID-posterior-driven LoRA routing. The hierarchical design consists of a multilingual shared LoRA for learning language-invariant acoustic representations and language-specific LoRA experts for modeling language-dependent characteristics. The proposed routing mechanism removes the need for prior language identity information or explicit language labels during inference, achieving true language-agnostic decoding. Experiments on MSR-86K and the MLC-SLM 2025 Challenge datasets demonstrate that HLoRA achieves competitive performance with state-of-the-art two-stage inference methods using only single-pass decoding, significantly improving decoding efficiency for low-resource mASR applications.", "AI": {"tldr": "\u63d0\u51faHLoRA\u6846\u67b6\uff0c\u57fa\u4e8eCTC\u67b6\u6784\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u3001\u8bed\u8a00\u65e0\u5173\u7684\u591a\u8bed\u8a00ASR\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5c42LoRA-MoE\u8bbe\u8ba1\u5b9e\u73b0\u5355\u6b21\u89e3\u7801\uff0c\u65e0\u9700\u8bed\u8a00\u6807\u7b7e\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u89e3\u7801\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u89c4\u6a21\u591a\u8bed\u8a00ASR\u6a21\u578b\uff08\u5982Whisper\uff09\u8ba1\u7b97\u548c\u5ef6\u8fdf\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u90e8\u7f72\u5230\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u3002\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u3001\u8bed\u8a00\u65e0\u5173\u4e14\u9ad8\u6548\u7684\u591a\u8bed\u8a00ASR\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u8bed\u8a00\u65e0\u5173\u7684\u5206\u5c42LoRA-MoE\uff08HLoRA\uff09\u6846\u67b6\uff0c\u96c6\u6210\u5230mHuBERT-CTC\u6a21\u578b\u4e2d\u3002\u5305\u542b\uff1a1\uff09\u591a\u8bed\u8a00\u5171\u4eabLoRA\u5b66\u4e60\u8bed\u8a00\u4e0d\u53d8\u58f0\u5b66\u8868\u793a\uff1b2\uff09\u8bed\u8a00\u7279\u5b9aLoRA\u4e13\u5bb6\u5efa\u6a21\u8bed\u8a00\u76f8\u5173\u7279\u5f81\uff1b3\uff09\u57fa\u4e8eLID\u540e\u9a8c\u7684LoRA\u8def\u7531\u673a\u5236\uff0c\u5b9e\u73b0\u65e0\u9700\u8bed\u8a00\u6807\u7b7e\u7684\u5355\u6b21\u89e3\u7801\u3002", "result": "\u5728MSR-86K\u548cMLC-SLM 2025 Challenge\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0cHLoRA\u4ec5\u901a\u8fc7\u5355\u6b21\u89e3\u7801\u5c31\u8fbe\u5230\u4e0e\u6700\u5148\u8fdb\u4e24\u9636\u6bb5\u63a8\u7406\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4f4e\u8d44\u6e90\u591a\u8bed\u8a00ASR\u5e94\u7528\u7684\u89e3\u7801\u6548\u7387\u3002", "conclusion": "HLoRA\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u8f7b\u91cf\u7ea7\u3001\u8bed\u8a00\u65e0\u5173\u7684\u591a\u8bed\u8a00ASR\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u5c42LoRA-MoE\u8bbe\u8ba1\u548c\u667a\u80fd\u8def\u7531\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002"}}
{"id": "2601.00575", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00575", "abs": "https://arxiv.org/abs/2601.00575", "authors": ["Ishir Garg", "Neel Kolhe", "Xuandong Zhao", "Dawn Song"], "title": "InfoSynth: Information-Guided Benchmark Synthesis for LLMs", "comment": null, "summary": "Large language models (LLMs) have demonstrated significant advancements in reasoning and code generation. However, efficiently creating new benchmarks to evaluate these capabilities remains a challenge. Traditional benchmark creation relies on manual human effort, a process that is both expensive and time-consuming. Furthermore, existing benchmarks often contaminate LLM training data, necessitating novel and diverse benchmarks to accurately assess their genuine capabilities. This work introduces InfoSynth, a novel framework for automatically generating and evaluating reasoning benchmarks guided by information-theoretic principles. We propose metrics based on KL-divergence and entropy to quantify benchmark novelty and diversity without relying on costly model evaluations. Building on this framework, we develop an end-to-end pipeline that synthesizes robust Python coding problems from seed datasets using genetic algorithms and iterative code feedback. Our method generates accurate test cases and solutions to new problems 97% of the time, and the synthesized benchmarks consistently exhibit higher novelty and diversity compared to their seed datasets. Moreover, our algorithm provides a method for controlling the novelty/diversity and difficulty of generated problems. InfoSynth offers a scalable, self-verifying pipeline for constructing high-quality, novel and diverse benchmarks for LLMs. Project Page: https://ishirgarg.github.io/infosynth_web/", "AI": {"tldr": "InfoSynth\u662f\u4e00\u4e2a\u57fa\u4e8e\u4fe1\u606f\u8bba\u539f\u5219\u81ea\u52a8\u751f\u6210\u548c\u8bc4\u4f30\u63a8\u7406\u57fa\u51c6\u7684\u6846\u67b6\uff0c\u4f7f\u7528KL\u6563\u5ea6\u548c\u71b5\u91cf\u5316\u57fa\u51c6\u65b0\u9896\u6027\u548c\u591a\u6837\u6027\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u548c\u8fed\u4ee3\u4ee3\u7801\u53cd\u9988\u751f\u6210Python\u7f16\u7a0b\u95ee\u9898\uff0c\u5b9e\u73b097%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u51c6\u521b\u5efa\u4f9d\u8d56\u4eba\u5de5\uff0c\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0c\u73b0\u6709\u57fa\u51c6\u5e38\u6c61\u67d3LLM\u8bad\u7ec3\u6570\u636e\uff0c\u9700\u8981\u65b0\u9896\u591a\u6837\u7684\u57fa\u51c6\u6765\u51c6\u786e\u8bc4\u4f30LLM\u7684\u771f\u5b9e\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eKL\u6563\u5ea6\u548c\u71b5\u7684\u6307\u6807\u91cf\u5316\u57fa\u51c6\u65b0\u9896\u6027\u548c\u591a\u6837\u6027\uff0c\u65e0\u9700\u6602\u8d35\u6a21\u578b\u8bc4\u4f30\uff1b\u5f00\u53d1\u7aef\u5230\u7aef\u6d41\u6c34\u7ebf\uff0c\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u548c\u8fed\u4ee3\u4ee3\u7801\u53cd\u9988\u4ece\u79cd\u5b50\u6570\u636e\u96c6\u5408\u6210\u7a33\u5065\u7684Python\u7f16\u7a0b\u95ee\u9898\u3002", "result": "\u65b9\u6cd5\u572897%\u7684\u60c5\u51b5\u4e0b\u80fd\u51c6\u786e\u751f\u6210\u65b0\u95ee\u9898\u7684\u6d4b\u8bd5\u7528\u4f8b\u548c\u89e3\u51b3\u65b9\u6848\uff1b\u5408\u6210\u7684\u57fa\u51c6\u76f8\u6bd4\u79cd\u5b50\u6570\u636e\u96c6\u59cb\u7ec8\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u65b0\u9896\u6027\u548c\u591a\u6837\u6027\uff1b\u7b97\u6cd5\u80fd\u63a7\u5236\u751f\u6210\u95ee\u9898\u7684\u65b0\u9896\u6027/\u591a\u6837\u6027\u548c\u96be\u5ea6\u3002", "conclusion": "InfoSynth\u4e3aLLM\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u81ea\u9a8c\u8bc1\u7684\u9ad8\u8d28\u91cf\u57fa\u51c6\u6784\u5efa\u6d41\u6c34\u7ebf\uff0c\u80fd\u751f\u6210\u65b0\u9896\u591a\u6837\u7684\u57fa\u51c6\u6765\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\u3002"}}
{"id": "2601.00588", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00588", "abs": "https://arxiv.org/abs/2601.00588", "authors": ["Zhenhong Zhou", "Shilinlu Yan", "Chuanpu Liu", "Qiankun Li", "Kun Wang", "Zhigang Zeng"], "title": "CSSBench: Evaluating the Safety of Lightweight LLMs against Chinese-Specific Adversarial Patterns", "comment": "18 pages", "summary": "Large language models (LLMs) are increasingly deployed in cost-sensitive and on-device scenarios, and safety guardrails have advanced mainly in English. However, real-world Chinese malicious queries typically conceal intent via homophones, pinyin, symbol-based splitting, and other Chinese-specific patterns. These Chinese-specific adversarial patterns create the safety evaluation gap that is not well captured by existing benchmarks focused on English. This gap is particularly concerning for lightweight models, which may be more vulnerable to such specific adversarial perturbations. To bridge this gap, we introduce the Chinese-Specific Safety Benchmark (CSSBench) that emphasizes these adversarial patterns and evaluates the safety of lightweight LLMs in Chinese. Our benchmark covers six domains that are common in real Chinese scenarios, including illegal activities and compliance, privacy leakage, health and medical misinformation, fraud and hate, adult content, and public and political safety, and organizes queries into multiple task types. We evaluate a set of popular lightweight LLMs and measure over-refusal behavior to assess safety-induced performance degradation. Our results show that the Chinese-specific adversarial pattern is a critical challenge for lightweight LLMs. This benchmark offers a comprehensive evaluation of LLM safety in Chinese, assisting robust deployments in practice.", "AI": {"tldr": "CSSBench\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u4e2d\u6587\u7279\u5b9a\u5bf9\u6297\u6a21\u5f0f\u7684\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u6587\u73af\u5883\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u82f1\u6587\u7684\u7a7a\u767d\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u6210\u672c\u654f\u611f\u548c\u8fb9\u7f18\u8bbe\u5907\u573a\u666f\u4e2d\uff0c\u4f46\u73b0\u6709\u7684\u5b89\u5168\u9632\u62a4\u4e3b\u8981\u9488\u5bf9\u82f1\u6587\u3002\u4e2d\u6587\u6076\u610f\u67e5\u8be2\u901a\u5e38\u901a\u8fc7\u540c\u97f3\u5b57\u3001\u62fc\u97f3\u3001\u7b26\u53f7\u5206\u5272\u7b49\u4e2d\u6587\u7279\u5b9a\u6a21\u5f0f\u9690\u85cf\u610f\u56fe\uff0c\u8fd9\u4e9b\u5bf9\u6297\u6a21\u5f0f\u5728\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u672a\u88ab\u5145\u5206\u6355\u6349\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u53ef\u80fd\u66f4\u8106\u5f31\u7684\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e2d\u6587\u7279\u5b9a\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5(CSSBench)\uff0c\u5f3a\u8c03\u4e2d\u6587\u5bf9\u6297\u6a21\u5f0f\uff0c\u8986\u76d6\u516d\u4e2a\u771f\u5b9e\u4e2d\u6587\u573a\u666f\u4e2d\u7684\u5e38\u89c1\u9886\u57df\uff1a\u975e\u6cd5\u6d3b\u52a8\u4e0e\u5408\u89c4\u3001\u9690\u79c1\u6cc4\u9732\u3001\u5065\u5eb7\u4e0e\u533b\u7597\u9519\u8bef\u4fe1\u606f\u3001\u6b3a\u8bc8\u4e0e\u4ec7\u6068\u3001\u6210\u4eba\u5185\u5bb9\u3001\u516c\u5171\u4e0e\u653f\u6cbb\u5b89\u5168\uff0c\u5e76\u5c06\u67e5\u8be2\u7ec4\u7ec7\u6210\u591a\u79cd\u4efb\u52a1\u7c7b\u578b\u3002", "result": "\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u6d41\u884c\u7684\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6d4b\u91cf\u8fc7\u5ea6\u62d2\u7edd\u884c\u4e3a\u4ee5\u8bc4\u4f30\u5b89\u5168\u6027\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e2d\u6587\u7279\u5b9a\u5bf9\u6297\u6a21\u5f0f\u5bf9\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\u6784\u6210\u5173\u952e\u6311\u6218\u3002", "conclusion": "CSSBench\u4e3a\u4e2d\u6587\u73af\u5883\u4e0b\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u6709\u52a9\u4e8e\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u9c81\u68d2\u6027\u63d0\u5347\uff0c\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\u3002"}}
{"id": "2601.00596", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00596", "abs": "https://arxiv.org/abs/2601.00596", "authors": ["Sumanth Balaji", "Piyush Mishra", "Aashraya Sachdeva", "Suraj Agrawal"], "title": "Beyond IVR: Benchmarking Customer Support LLM Agents for Business-Adherence", "comment": "17 pages, 3 figures, preprint", "summary": "Traditional customer support systems, such as Interactive Voice Response (IVR), rely on rigid scripts and lack the flexibility required for handling complex, policy-driven tasks. While large language model (LLM) agents offer a promising alternative, evaluating their ability to act in accordance with business rules and real-world support workflows remains an open challenge. Existing benchmarks primarily focus on tool usage or task completion, overlooking an agent's capacity to adhere to multi-step policies, navigate task dependencies, and remain robust to unpredictable user or environment behavior. In this work, we introduce JourneyBench, a benchmark designed to assess policy-aware agents in customer support. JourneyBench leverages graph representations to generate diverse, realistic support scenarios and proposes the User Journey Coverage Score, a novel metric to measure policy adherence. We evaluate multiple state-of-the-art LLMs using two agent designs: a Static-Prompt Agent (SPA) and a Dynamic-Prompt Agent (DPA) that explicitly models policy control. Across 703 conversations in three domains, we show that DPA significantly boosts policy adherence, even allowing smaller models like GPT-4o-mini to outperform more capable ones like GPT-4o. Our findings demonstrate the importance of structured orchestration and establish JourneyBench as a critical resource to advance AI-driven customer support beyond IVR-era limitations.", "AI": {"tldr": "JourneyBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5ba2\u6237\u652f\u6301\u573a\u666f\u4e2d\u7b56\u7565\u611f\u77e5AI\u4ee3\u7406\u7684\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7\u56fe\u8868\u793a\u751f\u6210\u591a\u6837\u5316\u652f\u6301\u573a\u666f\uff0c\u5e76\u63d0\u51fa\u7528\u6237\u65c5\u7a0b\u8986\u76d6\u7387\u5206\u6570\u6765\u8861\u91cf\u7b56\u7565\u9075\u5faa\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5ba2\u6237\u652f\u6301\u7cfb\u7edf\uff08\u5982IVR\uff09\u4f9d\u8d56\u521a\u6027\u811a\u672c\uff0c\u7f3a\u4e4f\u5904\u7406\u590d\u6742\u7b56\u7565\u9a71\u52a8\u4efb\u52a1\u7684\u7075\u6d3b\u6027\u3002\u867d\u7136LLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u8bc4\u4f30\u5b83\u4eec\u9075\u5faa\u4e1a\u52a1\u89c4\u5219\u548c\u771f\u5b9e\u652f\u6301\u5de5\u4f5c\u6d41\u7a0b\u7684\u80fd\u529b\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5de5\u5177\u4f7f\u7528\u6216\u4efb\u52a1\u5b8c\u6210\uff0c\u5ffd\u89c6\u4e86\u4ee3\u7406\u9075\u5faa\u591a\u6b65\u9aa4\u7b56\u7565\u3001\u5bfc\u822a\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\u4ee5\u53ca\u5bf9\u4e0d\u53ef\u9884\u6d4b\u7528\u6237\u884c\u4e3a\u4fdd\u6301\u9c81\u68d2\u6027\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165JourneyBench\u57fa\u51c6\uff0c\u5229\u7528\u56fe\u8868\u793a\u751f\u6210\u591a\u6837\u5316\u3001\u771f\u5b9e\u7684\u652f\u6301\u573a\u666f\u3002\u63d0\u51fa\u7528\u6237\u65c5\u7a0b\u8986\u76d6\u7387\u5206\u6570\uff08User Journey Coverage Score\uff09\u6765\u8861\u91cf\u7b56\u7565\u9075\u5faa\u80fd\u529b\u3002\u8bc4\u4f30\u4e86\u4e24\u79cd\u4ee3\u7406\u8bbe\u8ba1\uff1a\u9759\u6001\u63d0\u793a\u4ee3\u7406\uff08SPA\uff09\u548c\u52a8\u6001\u63d0\u793a\u4ee3\u7406\uff08DPA\uff09\uff0c\u540e\u8005\u660e\u786e\u5efa\u6a21\u7b56\u7565\u63a7\u5236\u3002", "result": "\u5728\u4e09\u4e2a\u9886\u57df\u7684703\u4e2a\u5bf9\u8bdd\u4e2d\uff0cDPA\u663e\u8457\u63d0\u9ad8\u4e86\u7b56\u7565\u9075\u5faa\u80fd\u529b\uff0c\u751a\u81f3\u5141\u8bb8\u8f83\u5c0f\u7684\u6a21\u578b\u5982GPT-4o-mini\u5728\u7b56\u7565\u9075\u5faa\u65b9\u9762\u80dc\u8fc7\u66f4\u5f3a\u5927\u7684\u6a21\u578b\u5982GPT-4o\u3002\u7ed3\u679c\u8868\u660e\u7ed3\u6784\u5316\u7f16\u6392\u7684\u91cd\u8981\u6027\u3002", "conclusion": "JourneyBench\u4f5c\u4e3a\u4e00\u4e2a\u5173\u952e\u8d44\u6e90\uff0c\u53ef\u4ee5\u63a8\u52a8AI\u9a71\u52a8\u7684\u5ba2\u6237\u652f\u6301\u8d85\u8d8aIVR\u65f6\u4ee3\u7684\u9650\u5236\uff0c\u5c55\u793a\u4e86\u7ed3\u6784\u5316\u7f16\u6392\u5728\u786e\u4fdd\u7b56\u7565\u9075\u5faa\u65b9\u9762\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2601.00641", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00641", "abs": "https://arxiv.org/abs/2601.00641", "authors": ["Nils Rautenberg", "Sven Schippkus"], "title": "Probabilistic Guarantees for Reducing Contextual Hallucinations in LLMs", "comment": null, "summary": "Large language models (LLMs) frequently produce contextual hallucinations, where generated content contradicts or ignores information explicitly stated in the prompt. Such errors are particularly problematic in deterministic automation workflows, where inputs are fixed and correctness is unambiguous. We introduce a simple and model-agnostic framework that provides explicit probabilistic guarantees for reducing hallucinations in this setting.\n  We formalize the notion of a specific task, defined by a fixed input and a deterministic correctness criterion, and show that issuing the same prompt in independent context windows yields an exponential reduction in the probability that all model outputs are incorrect. To identify a correct answer among repeated runs, we incorporate an LLM-as-a-judge and prove that the probability that the judged pipeline fails decays at a rate determined by the judge's true- and false-positive probabilities. When the judge is imperfect, we strengthen it through majority vote over independent judge calls, obtaining ensemble-level error rates that decrease exponentially in the number of votes. This yields an explicit bound on the probability that the pipeline selects a hallucinated answer.\n  Experiments on controlled extraction tasks with synthetic noisy judges match these predictions exactly: pipeline failure decreases exponentially with the number of repetitions, and hallucination-selection decreases exponentially with the number of judges in the ensemble. Together, these results provide a lightweight, modular, and theoretically grounded method for driving hallucination probabilities arbitrarily low in fixed-input LLM workflows-without modifying model weights, decoding strategies, or prompt engineering.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u590d\u91c7\u6837\u548c\u591a\u6570\u6295\u7968\u673a\u5236\u4e3a\u51cf\u5c11LLM\u5e7b\u89c9\u63d0\u4f9b\u6982\u7387\u4fdd\u8bc1", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u786e\u5b9a\u6027\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u4e2d\u7ecf\u5e38\u4ea7\u751f\u4e0a\u4e0b\u6587\u5e7b\u89c9\uff0c\u8fd9\u4e9b\u9519\u8bef\u5728\u8f93\u5165\u56fa\u5b9a\u4e14\u6b63\u786e\u6027\u660e\u786e\u7684\u60c5\u51b5\u4e0b\u5c24\u4e3a\u4e25\u91cd\uff0c\u9700\u8981\u4e00\u79cd\u7b80\u5355\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u964d\u4f4e\u5e7b\u89c9\u6982\u7387", "method": "\u901a\u8fc7\u72ec\u7acb\u4e0a\u4e0b\u6587\u7a97\u53e3\u91cd\u590d\u91c7\u6837\u76f8\u540c\u63d0\u793a\uff0c\u5229\u7528LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u8bc6\u522b\u6b63\u786e\u7b54\u6848\uff0c\u5e76\u901a\u8fc7\u591a\u6570\u6295\u7968\u673a\u5236\u589e\u5f3a\u4e0d\u5b8c\u7f8e\u8bc4\u5224\u8005\u7684\u53ef\u9760\u6027", "result": "\u5b9e\u9a8c\u663e\u793a\u7ba1\u9053\u5931\u8d25\u6982\u7387\u968f\u91cd\u590d\u6b21\u6570\u6307\u6570\u4e0b\u964d\uff0c\u5e7b\u89c9\u9009\u62e9\u6982\u7387\u968f\u8bc4\u5224\u8005\u6570\u91cf\u6307\u6570\u4e0b\u964d\uff0c\u7406\u8bba\u9884\u6d4b\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u5b8c\u5168\u5339\u914d", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u6a21\u5757\u5316\u4e14\u7406\u8bba\u53ef\u9760\u7684\u65b9\u6cd5\uff0c\u53ef\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u6743\u91cd\u3001\u89e3\u7801\u7b56\u7565\u6216\u63d0\u793a\u5de5\u7a0b\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u56fa\u5b9a\u8f93\u5165\u5de5\u4f5c\u6d41\u4e2d\u7684\u5e7b\u89c9\u6982\u7387\u964d\u81f3\u4efb\u610f\u4f4e\u6c34\u5e73"}}
{"id": "2601.00647", "categories": ["cs.CL", "cs.CE", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2601.00647", "abs": "https://arxiv.org/abs/2601.00647", "authors": ["QiWei Meng"], "title": "Physio-DPO: Aligning Large Language Models with the Protein Energy Landscape to Eliminate Structural Hallucinations", "comment": null, "summary": "Large Protein Language Models have shown strong potential for generative protein design, yet they frequently produce structural hallucinations, generating sequences with high linguistic likelihood that fold into thermodynamically unstable conformations. Existing alignment approaches such as Direct Preference Optimization are limited in this setting, as they model preferences as binary labels and ignore the continuous structure of the physical energy landscape. We propose Physio-DPO, a physics informed alignment framework that grounds protein language models in thermodynamic stability. Physio-DPO introduces a magnitude aware objective that scales optimization updates according to the energy gap between native structures and physics perturbed hard negatives. Experiments show that Physio-DPO consistently outperforms strong baselines including SFT, PPO, and standard DPO, reducing self consistency RMSD to 1.28 \u00c5 and increasing foldability to 92.8%. Qualitative analysis further demonstrates that Physio-DPO effectively mitigates structural hallucinations by recovering biophysical interactions such as hydrophobic core packing and hydrogen bond networks.", "AI": {"tldr": "Physio-DPO\uff1a\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u7684\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u7269\u7406\u80fd\u91cf\u666f\u89c2\u7684\u8fde\u7eed\u7ed3\u6784\u6765\u51cf\u5c11\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u6784\u5e7b\u89c9\uff0c\u63d0\u9ad8\u751f\u6210\u86cb\u767d\u8d28\u7684\u70ed\u529b\u5b66\u7a33\u5b9a\u6027\u3002", "motivation": "\u5927\u578b\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u86cb\u767d\u8d28\u8bbe\u8ba1\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u6f5c\u529b\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u7ed3\u6784\u5e7b\u89c9\uff0c\u751f\u6210\u7684\u8bed\u8a00\u53ef\u80fd\u6027\u9ad8\u4f46\u70ed\u529b\u5b66\u4e0d\u7a33\u5b9a\u7684\u5e8f\u5217\u3002\u73b0\u6709\u7684\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982DPO\uff09\u5c06\u504f\u597d\u5efa\u6a21\u4e3a\u4e8c\u5143\u6807\u7b7e\uff0c\u5ffd\u7565\u4e86\u7269\u7406\u80fd\u91cf\u666f\u89c2\u7684\u8fde\u7eed\u7ed3\u6784\u3002", "method": "\u63d0\u51faPhysio-DPO\u6846\u67b6\uff0c\u5f15\u5165\u5e45\u5ea6\u611f\u77e5\u76ee\u6807\u51fd\u6570\uff0c\u6839\u636e\u5929\u7136\u7ed3\u6784\u4e0e\u7269\u7406\u6270\u52a8\u786c\u8d1f\u6837\u672c\u4e4b\u95f4\u7684\u80fd\u91cf\u5dee\u8ddd\u6765\u7f29\u653e\u4f18\u5316\u66f4\u65b0\uff0c\u5c06\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u57fa\u4e8e\u70ed\u529b\u5b66\u7a33\u5b9a\u6027\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "Physio-DPO\u5728\u5b9e\u9a8c\u4e2d\u59cb\u7ec8\u4f18\u4e8eSFT\u3001PPO\u548c\u6807\u51c6DPO\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5c06\u81ea\u4e00\u81f4\u6027RMSD\u964d\u4f4e\u52301.28 \u00c5\uff0c\u5e76\u5c06\u53ef\u6298\u53e0\u6027\u63d0\u9ad8\u523092.8%\u3002\u5b9a\u6027\u5206\u6790\u663e\u793aPhysio-DPO\u901a\u8fc7\u6062\u590d\u758f\u6c34\u6838\u5fc3\u5806\u79ef\u548c\u6c22\u952e\u7f51\u7edc\u7b49\u751f\u7269\u7269\u7406\u76f8\u4e92\u4f5c\u7528\uff0c\u6709\u6548\u7f13\u89e3\u7ed3\u6784\u5e7b\u89c9\u3002", "conclusion": "Physio-DPO\u901a\u8fc7\u5c06\u7269\u7406\u80fd\u91cf\u666f\u89c2\u7684\u8fde\u7eed\u7ed3\u6784\u7eb3\u5165\u5bf9\u9f50\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u70ed\u529b\u5b66\u7a33\u5b9a\u6027\uff0c\u4e3a\u51cf\u5c11\u7ed3\u6784\u5e7b\u89c9\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.00671", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00671", "abs": "https://arxiv.org/abs/2601.00671", "authors": ["Tianyu Zhao", "Llion Jones"], "title": "Fast-weight Product Key Memory", "comment": null, "summary": "Sequence modeling layers in modern language models typically face a trade-off between storage capacity and computational efficiency. While Softmax attention offers unbounded storage at prohibitive quadratic costs, linear variants provide efficiency but suffer from limited, fixed-size storage. We propose Fast-weight Product Key Memory (FwPKM), a novel architecture that resolves this tension by transforming the sparse Product Key Memory (PKM) from a static module into a dynamic, \"fast-weight\" episodic memory. Unlike PKM, FwPKM updates its parameters dynamically at both training and inference time via local chunk-level gradient descent, allowing the model to rapidly memorize and retrieve new key-value pairs from input sequences. Experiments reveal that FwPKM functions as an effective episodic memory that complements the semantic memory of standard modules, yielding significant perplexity reductions on long-context datasets. Notably, in Needle in a Haystack evaluations, FwPKM generalizes to 128K-token contexts despite being trained on only 4K-token sequences.", "AI": {"tldr": "FwPKM \u662f\u4e00\u79cd\u65b0\u578b\u67b6\u6784\uff0c\u5c06\u9759\u6001\u7684\u4ea7\u54c1\u952e\u8bb0\u5fc6\u8f6c\u6362\u4e3a\u52a8\u6001\u7684\u5feb\u901f\u6743\u91cd\u60c5\u666f\u8bb0\u5fc6\uff0c\u89e3\u51b3\u4e86\u5e8f\u5217\u5efa\u6a21\u4e2d\u5b58\u50a8\u5bb9\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u5e8f\u5217\u5efa\u6a21\u5c42\u9762\u4e34\u5b58\u50a8\u5bb9\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861\uff1aSoftmax\u6ce8\u610f\u529b\u63d0\u4f9b\u65e0\u9650\u5b58\u50a8\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u7ebf\u6027\u53d8\u4f53\u6548\u7387\u9ad8\u4f46\u5b58\u50a8\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u9ad8\u6548\u8ba1\u7b97\u53c8\u80fd\u52a8\u6001\u5b58\u50a8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u5feb\u901f\u6743\u91cd\u4ea7\u54c1\u952e\u8bb0\u5fc6\uff08FwPKM\uff09\uff0c\u5c06\u7a00\u758f\u7684\u4ea7\u54c1\u952e\u8bb0\u5fc6\u4ece\u9759\u6001\u6a21\u5757\u8f6c\u53d8\u4e3a\u52a8\u6001\u7684\"\u5feb\u901f\u6743\u91cd\"\u60c5\u666f\u8bb0\u5fc6\u3002\u901a\u8fc7\u5c40\u90e8\u5757\u7ea7\u68af\u5ea6\u4e0b\u964d\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u52a8\u6001\u66f4\u65b0\u53c2\u6570\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5feb\u901f\u8bb0\u5fc6\u548c\u68c0\u7d22\u8f93\u5165\u5e8f\u5217\u4e2d\u7684\u65b0\u952e\u503c\u5bf9\u3002", "result": "FwPKM \u4f5c\u4e3a\u6709\u6548\u7684\u60c5\u666f\u8bb0\u5fc6\u8865\u5145\u4e86\u6807\u51c6\u6a21\u5757\u7684\u8bed\u4e49\u8bb0\u5fc6\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u4e86\u56f0\u60d1\u5ea6\u3002\u5728\"\u5927\u6d77\u635e\u9488\"\u8bc4\u4f30\u4e2d\uff0c\u5c3d\u7ba1\u4ec5\u57284K\u6807\u8bb0\u5e8f\u5217\u4e0a\u8bad\u7ec3\uff0c\u5374\u80fd\u6cdb\u5316\u5230128K\u6807\u8bb0\u7684\u4e0a\u4e0b\u6587\u3002", "conclusion": "FwPKM \u6210\u529f\u89e3\u51b3\u4e86\u5e8f\u5217\u5efa\u6a21\u4e2d\u5b58\u50a8\u4e0e\u6548\u7387\u7684\u6743\u8861\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u52a8\u6001\u3001\u9ad8\u6548\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u5e8f\u5217\uff0c\u5c55\u793a\u4e86\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u5bf9\u66f4\u957f\u4e0a\u4e0b\u6587\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.00680", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00680", "abs": "https://arxiv.org/abs/2601.00680", "authors": ["Tu Anh Dinh", "Jan Niehues"], "title": "Sigmoid Head for Quality Estimation under Language Ambiguity", "comment": null, "summary": "Language model (LM) probability is not a reliable quality estimator, as natural language is ambiguous. When multiple output options are valid, the model's probability distribution is spread across them, which can misleadingly indicate low output quality. This issue is caused by two reasons: (1) LMs' final output activation is softmax, which does not allow multiple correct options to receive high probabilities simultaneuously and (2) LMs' training data is single, one-hot encoded references, indicating that there is only one correct option at each output step. We propose training a module for Quality Estimation on top of pre-trained LMs to address these limitations. The module, called Sigmoid Head, is an extra unembedding head with sigmoid activation to tackle the first limitation. To tackle the second limitation, during the negative sampling process to train the Sigmoid Head, we use a heuristic to avoid selecting potentially alternative correct tokens. Our Sigmoid Head is computationally efficient during training and inference. The probability from Sigmoid Head is notably better quality signal compared to the original softmax head. As the Sigmoid Head does not rely on human-annotated quality data, it is more robust to out-of-domain settings compared to supervised QE.", "AI": {"tldr": "\u63d0\u51faSigmoid Head\u65b9\u6cd5\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u6982\u7387\u4e0d\u53ef\u9760\u7684\u8d28\u91cf\u4f30\u8ba1\u95ee\u9898\uff0c\u901a\u8fc7sigmoid\u6fc0\u6d3b\u548c\u8d1f\u91c7\u6837\u7b56\u7565\u6539\u8fdb\u8d28\u91cf\u8bc4\u4f30", "motivation": "\u8bed\u8a00\u6a21\u578b\u7684\u6982\u7387\u5206\u5e03\u4e0d\u662f\u53ef\u9760\u7684\u8d28\u91cf\u4f30\u8ba1\u5668\uff0c\u56e0\u4e3a\u81ea\u7136\u8bed\u8a00\u5177\u6709\u6b67\u4e49\u6027\u3002\u5f53\u591a\u4e2a\u8f93\u51fa\u9009\u9879\u90fd\u6709\u6548\u65f6\uff0c\u6a21\u578b\u7684\u6982\u7387\u5206\u5e03\u4f1a\u5206\u6563\u5728\u8fd9\u4e9b\u9009\u9879\u4e0a\uff0c\u4ece\u800c\u8bef\u5bfc\u6027\u5730\u8868\u660e\u8f93\u51fa\u8d28\u91cf\u8f83\u4f4e\u3002\u8fd9\u4e3b\u8981\u7531\u4e24\u4e2a\u539f\u56e0\u9020\u6210\uff1a1) LM\u7684\u6700\u7ec8\u8f93\u51fa\u6fc0\u6d3b\u4f7f\u7528softmax\uff0c\u4e0d\u5141\u8bb8\u591a\u4e2a\u6b63\u786e\u9009\u9879\u540c\u65f6\u83b7\u5f97\u9ad8\u6982\u7387\uff1b2) LM\u7684\u8bad\u7ec3\u6570\u636e\u662f\u5355\u4e00\u3001one-hot\u7f16\u7801\u7684\u53c2\u8003\uff0c\u6697\u793a\u6bcf\u4e2a\u8f93\u51fa\u6b65\u9aa4\u53ea\u6709\u4e00\u4e2a\u6b63\u786e\u9009\u9879\u3002", "method": "\u63d0\u51fa\u5728\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4e4b\u4e0a\u8bad\u7ec3\u8d28\u91cf\u4f30\u8ba1\u6a21\u5757\u3002\u8be5\u6a21\u5757\u79f0\u4e3aSigmoid Head\uff0c\u662f\u4e00\u4e2a\u989d\u5916\u7684\u5177\u6709sigmoid\u6fc0\u6d3b\u7684\u89e3\u5d4c\u5165\u5934\uff0c\u7528\u4e8e\u89e3\u51b3\u7b2c\u4e00\u4e2a\u9650\u5236\u3002\u4e3a\u89e3\u51b3\u7b2c\u4e8c\u4e2a\u9650\u5236\uff0c\u5728\u8bad\u7ec3Sigmoid Head\u7684\u8d1f\u91c7\u6837\u8fc7\u7a0b\u4e2d\uff0c\u4f7f\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u907f\u514d\u9009\u62e9\u53ef\u80fd\u66ff\u4ee3\u7684\u6b63\u786e\u6807\u8bb0\u3002Sigmoid Head\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\u4e2d\u8ba1\u7b97\u6548\u7387\u9ad8\u3002", "result": "Sigmoid Head\u7684\u6982\u7387\u76f8\u6bd4\u539f\u59cbsoftmax\u5934\u662f\u663e\u8457\u66f4\u597d\u7684\u8d28\u91cf\u4fe1\u53f7\u3002\u7531\u4e8eSigmoid Head\u4e0d\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u8d28\u91cf\u6570\u636e\uff0c\u76f8\u6bd4\u76d1\u7763\u5f0fQE\uff0c\u5b83\u5728\u57df\u5916\u8bbe\u7f6e\u4e2d\u66f4\u52a0\u9c81\u68d2\u3002", "conclusion": "Sigmoid Head\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u8a00\u6a21\u578b\u6982\u7387\u5206\u5e03\u4f5c\u4e3a\u8d28\u91cf\u4f30\u8ba1\u5668\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7sigmoid\u6fc0\u6d3b\u548c\u8d1f\u91c7\u6837\u7b56\u7565\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u8d28\u91cf\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u81ea\u7136\u8bed\u8a00\u6b67\u4e49\u6027\u548c\u57df\u5916\u573a\u666f\u65f6\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2601.00736", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00736", "abs": "https://arxiv.org/abs/2601.00736", "authors": ["Alphaeus Dmonte", "Roland Oruche", "Tharindu Ranasinghe", "Marcos Zampieri", "Prasad Calyam"], "title": "Exploring the Performance of Large Language Models on Subjective Span Identification Tasks", "comment": null, "summary": "Identifying relevant text spans is important for several downstream tasks in NLP, as it contributes to model explainability. While most span identification approaches rely on relatively smaller pre-trained language models like BERT, a few recent approaches have leveraged the latest generation of Large Language Models (LLMs) for the task. Current work has focused on explicit span identification like Named Entity Recognition (NER), while more subjective span identification with LLMs in tasks like Aspect-based Sentiment Analysis (ABSA) has been underexplored. In this paper, we fill this important gap by presenting an evaluation of the performance of various LLMs on text span identification in three popular tasks, namely sentiment analysis, offensive language identification, and claim verification. We explore several LLM strategies like instruction tuning, in-context learning, and chain of thought. Our results indicate underlying relationships within text aid LLMs in identifying precise text spans.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u8de8\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u60c5\u611f\u5206\u6790\u3001\u653b\u51fb\u6027\u8bed\u8a00\u8bc6\u522b\u548c\u58f0\u660e\u9a8c\u8bc1\u7b49\u4e3b\u89c2\u6027\u4efb\u52a1\u4e2d", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u8de8\u5ea6\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56BERT\u7b49\u8f83\u5c0f\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e3b\u89c2\u6027\u8de8\u5ea6\u8bc6\u522b\u4efb\u52a1\uff08\u5982\u57fa\u4e8e\u65b9\u9762\u7684\u60c5\u611f\u5206\u6790\uff09\u4e2d\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d", "method": "\u8bc4\u4f30\u591a\u79cdLLM\u5728\u4e09\u4e2a\u6d41\u884c\u4efb\u52a1\uff08\u60c5\u611f\u5206\u6790\u3001\u653b\u51fb\u6027\u8bed\u8a00\u8bc6\u522b\u3001\u58f0\u660e\u9a8c\u8bc1\uff09\u4e2d\u7684\u6587\u672c\u8de8\u5ea6\u8bc6\u522b\u6027\u80fd\uff0c\u63a2\u7d22\u6307\u4ee4\u8c03\u4f18\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u548c\u601d\u7ef4\u94fe\u7b49\u7b56\u7565", "result": "\u7ed3\u679c\u8868\u660e\u6587\u672c\u5185\u90e8\u7684\u6f5c\u5728\u5173\u7cfb\u6709\u52a9\u4e8eLLM\u8bc6\u522b\u7cbe\u786e\u7684\u6587\u672c\u8de8\u5ea6", "conclusion": "LLM\u5728\u4e3b\u89c2\u6027\u6587\u672c\u8de8\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u6587\u672c\u5185\u90e8\u5173\u7cfb\u5bf9\u7cbe\u786e\u8bc6\u522b\u81f3\u5173\u91cd\u8981"}}
{"id": "2601.00787", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.00787", "abs": "https://arxiv.org/abs/2601.00787", "authors": ["Jonathan Simkin", "Lovedeep Gondara", "Zeeshan Rizvi", "Gregory Doyle", "Jeff Dowden", "Dan Bond", "Desmond Martin", "Raymond Ng"], "title": "Adapting Natural Language Processing Models Across Jurisdictions: A pilot Study in Canadian Cancer Registries", "comment": null, "summary": "Population-based cancer registries depend on pathology reports as their primary diagnostic source, yet manual abstraction is resource-intensive and contributes to delays in cancer data. While transformer-based NLP systems have improved registry workflows, their ability to generalize across jurisdictions with differing reporting conventions remains poorly understood. We present the first cross-provincial evaluation of adapting BCCRTron, a domain-adapted transformer model developed at the British Columbia Cancer Registry, alongside GatorTron, a biomedical transformer model, for cancer surveillance in Canada. Our training dataset consisted of approximately 104,000 and 22,000 de-identified pathology reports from the Newfoundland & Labrador Cancer Registry (NLCR) for Tier 1 (cancer vs. non-cancer) and Tier 2 (reportable vs. non-reportable) tasks, respectively. Both models were fine-tuned using complementary synoptic and diagnosis focused report section input pipelines. Across NLCR test sets, the adapted models maintained high performance, demonstrating transformers pretrained in one jurisdiction can be localized to another with modest fine-tuning. To improve sensitivity, we combined the two models using a conservative OR-ensemble achieving a Tier 1 recall of 0.99 and reduced missed cancers to 24, compared with 48 and 54 for the standalone models. For Tier 2, the ensemble achieved 0.99 recall and reduced missed reportable cancers to 33, compared with 54 and 46 for the individual models. These findings demonstrate that an ensemble combining complementary text representations substantially reduce missed cancers and improve error coverage in cancer-registry NLP. We implement a privacy-preserving workflow in which only model weights are shared between provinces, supporting interoperable NLP infrastructure and a future pan-Canadian foundation model for cancer pathology and registry workflows.", "AI": {"tldr": "\u8de8\u7701\u8bc4\u4f30BCCRTron\u548cGatorTron\u6a21\u578b\u5728\u52a0\u62ff\u5927\u764c\u75c7\u767b\u8bb0\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u901a\u8fc7\u96c6\u6210\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u6f0f\u8bca\u764c\u75c7\uff0c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u5de5\u4f5c\u6d41\u7a0b", "motivation": "\u57fa\u4e8e\u4eba\u7fa4\u7684\u764c\u75c7\u767b\u8bb0\u4f9d\u8d56\u75c5\u7406\u62a5\u544a\uff0c\u4f46\u624b\u52a8\u63d0\u53d6\u8d44\u6e90\u5bc6\u96c6\u4e14\u5bfc\u81f4\u6570\u636e\u5ef6\u8fdf\u3002\u73b0\u6709NLP\u7cfb\u7edf\u5728\u4e0d\u540c\u53f8\u6cd5\u7ba1\u8f96\u533a\u95f4\u7684\u6cdb\u5316\u80fd\u529b\u5c1a\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u8bc4\u4f30\u8de8\u7701\u9002\u5e94\u6027\u548c\u5efa\u7acb\u53ef\u4e92\u64cd\u4f5c\u7684\u764c\u75c7\u76d1\u6d4b\u57fa\u7840\u8bbe\u65bd", "method": "\u4f7f\u7528\u7ebd\u82ac\u5170\u4e0e\u62c9\u5e03\u62c9\u591a\u764c\u75c7\u767b\u8bb0\u7684\u7ea6104,000\u4efd\uff08Tier 1\uff09\u548c22,000\u4efd\uff08Tier 2\uff09\u53bb\u6807\u8bc6\u5316\u75c5\u7406\u62a5\u544a\uff0c\u5bf9BCCRTron\u548cGatorTron\u8fdb\u884c\u5fae\u8c03\u3002\u91c7\u7528\u4e92\u8865\u7684\u6458\u8981\u5f0f\u548c\u8bca\u65ad\u5bfc\u5411\u7684\u62a5\u544a\u90e8\u5206\u8f93\u5165\u7ba1\u9053\uff0c\u5e76\u901a\u8fc7\u4fdd\u5b88\u7684OR\u96c6\u6210\u65b9\u6cd5\u7ed3\u5408\u4e24\u4e2a\u6a21\u578b", "result": "\u96c6\u6210\u6a21\u578b\u5728Tier 1\u4efb\u52a1\u4e2d\u53ec\u56de\u7387\u8fbe\u52300.99\uff0c\u6f0f\u8bca\u764c\u75c7\u51cf\u5c11\u81f324\u4f8b\uff08\u5355\u72ec\u6a21\u578b\u4e3a48\u548c54\u4f8b\uff09\uff1bTier 2\u4efb\u52a1\u4e2d\u53ec\u56de\u73870.99\uff0c\u6f0f\u8bca\u53ef\u62a5\u544a\u764c\u75c7\u51cf\u5c11\u81f333\u4f8b\uff08\u5355\u72ec\u6a21\u578b\u4e3a54\u548c46\u4f8b\uff09\u3002\u8bc1\u660e\u8de8\u7701\u9002\u5e94\u53ef\u884c\u4e14\u96c6\u6210\u65b9\u6cd5\u663e\u8457\u6539\u5584\u6027\u80fd", "conclusion": "\u5728\u4e00\u4e2a\u53f8\u6cd5\u7ba1\u8f96\u533a\u9884\u8bad\u7ec3\u7684transformer\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u9002\u5ea6\u5fae\u8c03\u9002\u5e94\u53e6\u4e00\u4e2a\u7ba1\u8f96\u533a\u3002\u96c6\u6210\u4e92\u8865\u6587\u672c\u8868\u793a\u80fd\u5927\u5e45\u51cf\u5c11\u6f0f\u8bca\u764c\u75c7\u5e76\u63d0\u9ad8\u9519\u8bef\u8986\u76d6\u3002\u9690\u79c1\u4fdd\u62a4\u5de5\u4f5c\u6d41\u7a0b\uff08\u4ec5\u5171\u4eab\u6a21\u578b\u6743\u91cd\uff09\u652f\u6301\u53ef\u4e92\u64cd\u4f5c\u7684NLP\u57fa\u7840\u8bbe\u65bd\uff0c\u4e3a\u672a\u6765\u6cdb\u52a0\u62ff\u5927\u764c\u75c7\u75c5\u7406\u57fa\u7840\u6a21\u578b\u5960\u5b9a\u57fa\u7840"}}
