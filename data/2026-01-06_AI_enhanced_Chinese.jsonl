{"id": "2601.00969", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.00969", "abs": "https://arxiv.org/abs/2601.00969", "authors": ["Ali Salamatian", "Ke", "Ren", "Kieran Pattison", "Cyrus Neary"], "title": "Value Vision-Language-Action Planning & Search", "comment": "10 pages, 3 figures", "summary": "Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic manipulation, yet they remain fundamentally limited by their reliance on behavior cloning, leading to brittleness under distribution shift. While augmenting pretrained models with test-time search algorithms like Monte Carlo Tree Search (MCTS) can mitigate these failures, existing formulations rely solely on the VLA prior for guidance, lacking a grounded estimate of expected future return. Consequently, when the prior is inaccurate, the planner can only correct action selection via the exploration term, which requires extensive simulation to become effective. To address this limitation, we introduce Value Vision-Language-Action Planning and Search (V-VLAPS), a framework that augments MCTS with a lightweight, learnable value function. By training a simple multilayer perceptron (MLP) on the latent representations of a fixed VLA backbone (Octo), we provide the search with an explicit success signal that biases action selection toward high-value regions. We evaluate V-VLAPS on the LIBERO robotic manipulation suite, demonstrating that our value-guided search improves success rates by over 5 percentage points while reducing the average number of MCTS simulations by 5-15 percent compared to baselines that rely only on the VLA prior.", "AI": {"tldr": "V-VLAPS\u6846\u67b6\u901a\u8fc7\u4e3aVLA\u6a21\u578b\u6dfb\u52a0\u8f7b\u91cf\u7ea7\u53ef\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\uff0c\u7ed3\u5408MCTS\u641c\u7d22\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u641c\u7d22\u6548\u7387", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u4f9d\u8d56\u884c\u4e3a\u514b\u9686\uff0c\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u8868\u73b0\u8106\u5f31\u3002\u867d\u7136\u53ef\u4ee5\u901a\u8fc7MCTS\u7b49\u6d4b\u8bd5\u65f6\u641c\u7d22\u7b97\u6cd5\u7f13\u89e3\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56VLA\u5148\u9a8c\u6307\u5bfc\uff0c\u7f3a\u4e4f\u5bf9\u672a\u6765\u56de\u62a5\u7684\u53ef\u9760\u4f30\u8ba1\uff0c\u5bfc\u81f4\u641c\u7d22\u6548\u7387\u4f4e\u4e0b", "method": "\u63d0\u51faV-VLAPS\u6846\u67b6\uff0c\u5728MCTS\u641c\u7d22\u4e2d\u96c6\u6210\u8f7b\u91cf\u7ea7\u53ef\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\u3002\u5728\u56fa\u5b9aVLA\u9aa8\u5e72\u7f51\u7edc\uff08Octo\uff09\u7684\u6f5c\u5728\u8868\u793a\u4e0a\u8bad\u7ec3\u7b80\u5355\u7684\u591a\u5c42\u611f\u77e5\u673a\uff0c\u4e3a\u641c\u7d22\u63d0\u4f9b\u660e\u786e\u7684\u6210\u529f\u4fe1\u53f7\uff0c\u5f15\u5bfc\u52a8\u4f5c\u9009\u62e9\u5411\u9ad8\u4ef7\u503c\u533a\u57df", "result": "\u5728LIBERO\u673a\u5668\u4eba\u64cd\u4f5c\u5957\u4ef6\u4e0a\u8bc4\u4f30\uff0c\u4ef7\u503c\u5f15\u5bfc\u7684\u641c\u7d22\u5c06\u6210\u529f\u7387\u63d0\u5347\u8d85\u8fc75\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u5c06\u5e73\u5747MCTS\u6a21\u62df\u6b21\u6570\u51cf\u5c115-15%\uff0c\u76f8\u6bd4\u4ec5\u4f9d\u8d56VLA\u5148\u9a8c\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "V-VLAPS\u901a\u8fc7\u7ed3\u5408VLA\u5148\u9a8c\u548c\u53ef\u5b66\u4e60\u4ef7\u503c\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u641c\u7d22\u6548\u7387\uff0c\u4e3aVLA\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89c4\u5212\u6846\u67b6"}}
{"id": "2601.00978", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.00978", "abs": "https://arxiv.org/abs/2601.00978", "authors": ["Yanyi Chen", "Min Deng"], "title": "From Perception to Symbolic Task Planning: Vision-Language Guided Human-Robot Collaborative Structured Assembly", "comment": null, "summary": "Human-robot collaboration (HRC) in structured assembly requires reliable state estimation and adaptive task planning under noisy perception and human interventions. To address these challenges, we introduce a design-grounded human-aware planning framework for human-robot collaborative structured assembly. The framework comprises two coupled modules. Module I, Perception-to-Symbolic State (PSS), employs vision-language models (VLMs) based agents to align RGB-D observations with design specifications and domain knowledge, synthesizing verifiable symbolic assembly states. It outputs validated installed and uninstalled component sets for online state tracking. Module II, Human-Aware Planning and Replanning (HPR), performs task-level multi-robot assignment and updates the plan only when the observed state deviates from the expected execution outcome. It applies a minimal-change replanning rule to selectively revise task assignments and preserve plan stability even under human interventions. We validate the framework on a 27-component timber-frame assembly. The PSS module achieves 97% state synthesis accuracy, and the HPR module maintains feasible task progression across diverse HRC scenarios. Results indicate that integrating VLM-based perception with knowledge-driven planning improves robustness of state estimation and task planning under dynamic conditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bbe\u8ba1\u77e5\u8bc6\u7684\u4eba\u7c7b\u611f\u77e5\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u4eba\u673a\u534f\u4f5c\u7ed3\u6784\u5316\u88c5\u914d\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u72b6\u6001\u611f\u77e5\u548c\u6700\u5c0f\u53d8\u66f4\u91cd\u89c4\u5212", "motivation": "\u7ed3\u6784\u5316\u4eba\u673a\u534f\u4f5c\u88c5\u914d\u9700\u8981\u53ef\u9760\u7684\u72b6\u6001\u4f30\u8ba1\u548c\u81ea\u9002\u5e94\u4efb\u52a1\u89c4\u5212\uff0c\u4ee5\u5e94\u5bf9\u566a\u58f0\u611f\u77e5\u548c\u4eba\u4e3a\u5e72\u9884\u7684\u6311\u6218", "method": "\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u8026\u5408\u6a21\u5757\uff1aPSS\u6a21\u5757\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5c06RGB-D\u89c2\u6d4b\u4e0e\u8bbe\u8ba1\u89c4\u8303\u5bf9\u9f50\uff0c\u5408\u6210\u53ef\u9a8c\u8bc1\u7684\u7b26\u53f7\u88c5\u914d\u72b6\u6001\uff1bHPR\u6a21\u5757\u6267\u884c\u4efb\u52a1\u7ea7\u591a\u673a\u5668\u4eba\u5206\u914d\uff0c\u4ec5\u5728\u89c2\u6d4b\u72b6\u6001\u504f\u79bb\u9884\u671f\u65f6\u8fdb\u884c\u6700\u5c0f\u53d8\u66f4\u91cd\u89c4\u5212", "result": "\u572827\u7ec4\u4ef6\u6728\u6846\u67b6\u88c5\u914d\u4e0a\u9a8c\u8bc1\uff0cPSS\u6a21\u5757\u8fbe\u523097%\u72b6\u6001\u5408\u6210\u51c6\u786e\u7387\uff0cHPR\u6a21\u5757\u5728\u5404\u79cd\u4eba\u673a\u534f\u4f5c\u573a\u666f\u4e2d\u4fdd\u6301\u53ef\u884c\u4efb\u52a1\u8fdb\u5c55", "conclusion": "\u5c06\u57fa\u4e8eVLM\u7684\u611f\u77e5\u4e0e\u77e5\u8bc6\u9a71\u52a8\u89c4\u5212\u76f8\u7ed3\u5408\uff0c\u63d0\u9ad8\u4e86\u52a8\u6001\u6761\u4ef6\u4e0b\u72b6\u6001\u4f30\u8ba1\u548c\u4efb\u52a1\u89c4\u5212\u7684\u9c81\u68d2\u6027"}}
{"id": "2601.00981", "categories": ["cs.RO", "cs.CV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.00981", "abs": "https://arxiv.org/abs/2601.00981", "authors": ["Wenhui Chu", "Khang Tran", "Nikolaos V. Tsekos"], "title": "Simulations of MRI Guided and Powered Ferric Applicators for Tetherless Delivery of Therapeutic Interventions", "comment": "9 pages, 8 figures, published in ICBBB 2022", "summary": "Magnetic Resonance Imaging (MRI) is a well-established modality for pre-operative planning and is also explored for intra-operative guidance of procedures such as intravascular interventions. Among the experimental robot-assisted technologies, the magnetic field gradients of the MRI scanner are used to power and maneuver ferromagnetic applicators for accessing sites in the patient's body via the vascular network. In this work, we propose a computational platform for preoperative planning and modeling of MRI-powered applicators inside blood vessels. This platform was implemented as a two-way data and command pipeline that links the MRI scanner, the computational core, and the operator. The platform first processes multi-slice MR data to extract the vascular bed and then fits a virtual corridor inside the vessel. This corridor serves as a virtual fixture (VF), a forbidden region for the applicators to avoid vessel perforation or collision. The geometric features of the vessel centerline, the VF, and MRI safety compliance (dB/dt, max available gradient) are then used to generate magnetic field gradient waveforms. Different blood flow profiles can be user-selected, and those parameters are used for modeling the applicator's maneuvering. The modeling module further generates cues about whether the selected vascular path can be safely maneuvered. Given future experimental studies that require a real-time operation, the platform was implemented on the Qt framework (C/C++) with software modules performing specific tasks running on dedicated threads: PID controller, generation of VF, generation of MR gradient waveforms.", "AI": {"tldr": "\u5f00\u53d1\u7528\u4e8eMRI\u5f15\u5bfc\u8840\u7ba1\u4ecb\u5165\u624b\u672f\u7684\u8ba1\u7b97\u5e73\u53f0\uff0c\u5b9e\u73b0\u672f\u524d\u89c4\u5212\u548c\u5efa\u6a21\uff0c\u901a\u8fc7\u865a\u62df\u5939\u5177\u786e\u4fdd\u5668\u68b0\u5b89\u5168\u64cd\u4f5c", "motivation": "MRI\u5f15\u5bfc\u7684\u673a\u5668\u4eba\u8f85\u52a9\u8840\u7ba1\u4ecb\u5165\u624b\u672f\u9700\u8981\u5b89\u5168\u6709\u6548\u7684\u672f\u524d\u89c4\u5212\u548c\u5b9e\u65f6\u64cd\u4f5c\u5e73\u53f0\uff0c\u4ee5\u907f\u514d\u8840\u7ba1\u7a7f\u5b54\u548c\u78b0\u649e\u98ce\u9669", "method": "\u5f00\u53d1\u53cc\u5411\u6570\u636e\u7ba1\u9053\u5e73\u53f0\uff0c\u5904\u7406MRI\u6570\u636e\u63d0\u53d6\u8840\u7ba1\u7ed3\u6784\uff0c\u521b\u5efa\u865a\u62df\u5939\u5177\u4f5c\u4e3a\u5b89\u5168\u8d70\u5eca\uff0c\u751f\u6210\u78c1\u573a\u68af\u5ea6\u6ce2\u5f62\uff0c\u5efa\u6a21\u5668\u68b0\u64cd\u63a7", "result": "\u5b9e\u73b0\u4e86\u57fa\u4e8eQt\u6846\u67b6\u7684\u5b9e\u65f6\u64cd\u4f5c\u5e73\u53f0\uff0c\u5305\u542bPID\u63a7\u5236\u5668\u3001\u865a\u62df\u5939\u5177\u751f\u6210\u548cMRI\u68af\u5ea6\u6ce2\u5f62\u751f\u6210\u7b49\u6a21\u5757\u5316\u8f6f\u4ef6\u7ec4\u4ef6", "conclusion": "\u8be5\u5e73\u53f0\u4e3aMRI\u5f15\u5bfc\u7684\u8840\u7ba1\u4ecb\u5165\u624b\u672f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u672f\u524d\u89c4\u5212\u548c\u5efa\u6a21\u5de5\u5177\uff0c\u4e3a\u672a\u6765\u5b9e\u65f6\u5b9e\u9a8c\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2601.01067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01067", "abs": "https://arxiv.org/abs/2601.01067", "authors": ["Wenzheng Zhang", "Yoshitaka Hara", "Sousuke Nakamura"], "title": "Topological Mapping and Navigation using a Monocular Camera based on AnyLoc", "comment": "Published in Proc. IEEE CASE 2025. 7 pages, 11 figures", "summary": "This paper proposes a method for topological mapping and navigation using a monocular camera. Based on AnyLoc, keyframes are converted into descriptors to construct topological relationships, enabling loop detection and map building. Unlike metric maps, topological maps simplify path planning and navigation by representing environments with key nodes instead of precise coordinates. Actions for visual navigation are determined by comparing segmented images with the image associated with target nodes. The system relies solely on a monocular camera, ensuring fast map building and navigation using key nodes. Experiments show effective loop detection and navigation in real and simulation environments without pre-training. Compared to a ResNet-based method, this approach improves success rates by 60.2% on average while reducing time and space costs, offering a lightweight solution for robot and human navigation in various scenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5355\u76ee\u76f8\u673a\u7684\u62d3\u6251\u5efa\u56fe\u4e0e\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5173\u952e\u5e27\u63cf\u8ff0\u7b26\u6784\u5efa\u62d3\u6251\u5173\u7cfb\uff0c\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u73af\u5883\u611f\u77e5\u4e0e\u8def\u5f84\u89c4\u5212", "motivation": "\u4f20\u7edf\u5ea6\u91cf\u5730\u56fe\u9700\u8981\u7cbe\u786e\u5750\u6807\u4fe1\u606f\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u4e14\u5bf9\u73af\u5883\u53d8\u5316\u654f\u611f\u3002\u62d3\u6251\u5730\u56fe\u901a\u8fc7\u5173\u952e\u8282\u70b9\u8868\u793a\u73af\u5883\uff0c\u7b80\u5316\u4e86\u8def\u5f84\u89c4\u5212\uff0c\u66f4\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\u9700\u6c42\u3002", "method": "\u57fa\u4e8eAnyLoc\u6846\u67b6\uff0c\u5c06\u5355\u76ee\u76f8\u673a\u91c7\u96c6\u7684\u5173\u952e\u5e27\u8f6c\u6362\u4e3a\u63cf\u8ff0\u7b26\uff0c\u6784\u5efa\u62d3\u6251\u5173\u7cfb\u56fe\u3002\u901a\u8fc7\u56fe\u50cf\u5206\u5272\u4e0e\u76ee\u6807\u8282\u70b9\u56fe\u50cf\u6bd4\u8f83\u786e\u5b9a\u5bfc\u822a\u52a8\u4f5c\uff0c\u5b9e\u73b0\u65e0\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u5bfc\u822a\u3002", "result": "\u5728\u771f\u5b9e\u4e0e\u4eff\u771f\u73af\u5883\u4e2d\u5747\u80fd\u6709\u6548\u68c0\u6d4b\u56de\u73af\u5e76\u5b8c\u6210\u5bfc\u822a\u4efb\u52a1\u3002\u76f8\u6bd4\u57fa\u4e8eResNet\u7684\u65b9\u6cd5\uff0c\u5e73\u5747\u6210\u529f\u7387\u63d0\u534760.2%\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u65f6\u95f4\u548c\u7a7a\u95f4\u6210\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u9ad8\u6548\u7684\u62d3\u6251\u5efa\u56fe\u4e0e\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\uff0c\u4ec5\u9700\u5355\u76ee\u76f8\u673a\u5373\u53ef\u5feb\u901f\u6784\u5efa\u5730\u56fe\u5e76\u5b9e\u73b0\u5bfc\u822a\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u53ca\u4eba\u7c7b\u5bfc\u822a\u573a\u666f\u3002"}}
{"id": "2601.00797", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.00797", "abs": "https://arxiv.org/abs/2601.00797", "authors": ["Hugues Draelants"], "title": "The Qualitative Laboratory: Theory Prototyping and Hypothesis Generation with Large Language Models", "comment": "26 pages, 3 tables. Manuscript submitted for peer-reviewed journal publication", "summary": "A central challenge in social science is to generate rich qualitative hypotheses about how diverse social groups might interpret new information. This article introduces and illustrates a novel methodological approach for this purpose: sociological persona simulation using Large Language Models (LLMs), which we frame as a \"qualitative laboratory\". We argue that for this specific task, persona simulation offers a distinct advantage over established methods. By generating naturalistic discourse, it overcomes the lack of discursive depth common in vignette surveys, and by operationalizing complex worldviews through natural language, it bypasses the formalization bottleneck of rule-based agent-based models (ABMs). To demonstrate this potential, we present a protocol where personas derived from a sociological theory of climate reception react to policy messages. The simulation produced nuanced and counter-intuitive hypotheses - such as a conservative persona's rejection of a national security frame - that challenge theoretical assumptions. We conclude that this method, used as part of a \"simulation then validation\" workflow, represents a superior tool for generating deeply textured hypotheses for subsequent empirical testing.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u793e\u4f1a\u5b66\u89d2\u8272\u6a21\u62df\u4f5c\u4e3a\"\u5b9a\u6027\u5b9e\u9a8c\u5ba4\"\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u5173\u4e8e\u4e0d\u540c\u793e\u4f1a\u7fa4\u4f53\u5982\u4f55\u89e3\u8bfb\u65b0\u4fe1\u606f\u7684\u4e30\u5bcc\u5b9a\u6027\u5047\u8bbe\u3002", "motivation": "\u793e\u4f1a\u79d1\u5b66\u9762\u4e34\u7684\u6838\u5fc3\u6311\u6218\u662f\u5982\u4f55\u751f\u6210\u5173\u4e8e\u4e0d\u540c\u793e\u4f1a\u7fa4\u4f53\u5982\u4f55\u89e3\u8bfb\u65b0\u4fe1\u606f\u7684\u4e30\u5bcc\u5b9a\u6027\u5047\u8bbe\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u5c0f\u63d2\u56fe\u8c03\u67e5\u7f3a\u4e4f\u8bdd\u8bed\u6df1\u5ea6\uff0c\u800c\u57fa\u4e8e\u89c4\u5219\u7684ABM\u5b58\u5728\u5f62\u5f0f\u5316\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u793e\u4f1a\u5b66\u89d2\u8272\u6a21\u62df\u65b9\u6cd5\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\"\u5b9a\u6027\u5b9e\u9a8c\u5ba4\"\u3002\u901a\u8fc7\u4ece\u6c14\u5019\u63a5\u53d7\u793e\u4f1a\u5b66\u7406\u8bba\u4e2d\u63d0\u53d6\u89d2\u8272\uff0c\u8ba9\u8fd9\u4e9b\u89d2\u8272\u5bf9\u653f\u7b56\u4fe1\u606f\u505a\u51fa\u53cd\u5e94\uff0c\u751f\u6210\u81ea\u7136\u4e3b\u4e49\u7684\u8bdd\u8bed\u3002", "result": "\u6a21\u62df\u4ea7\u751f\u4e86\u7ec6\u81f4\u4e14\u53cd\u76f4\u89c9\u7684\u5047\u8bbe\uff0c\u4f8b\u5982\u4fdd\u5b88\u89d2\u8272\u62d2\u7edd\u56fd\u5bb6\u5b89\u5168\u6846\u67b6\uff0c\u8fd9\u6311\u6218\u4e86\u7406\u8bba\u5047\u8bbe\u3002\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u6df1\u5ea6\u5047\u8bbe\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8fd9\u79cd\u65b9\u6cd5\u4f5c\u4e3a\"\u6a21\u62df\u540e\u9a8c\u8bc1\"\u5de5\u4f5c\u6d41\u7a0b\u7684\u4e00\u90e8\u5206\uff0c\u4ee3\u8868\u4e86\u751f\u6210\u6df1\u5ea6\u5047\u8bbe\u4ee5\u4f9b\u540e\u7eed\u5b9e\u8bc1\u68c0\u9a8c\u7684\u4f18\u8d8a\u5de5\u5177\uff0c\u80fd\u591f\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.01106", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01106", "abs": "https://arxiv.org/abs/2601.01106", "authors": ["Michele Grimaldi", "Yosaku Maeda", "Hitoshi Kakami", "Ignacio Carlucho", "Yvan Petillot", "Tomoya Inoue"], "title": "Towards reliable subsea object recovery: a simulation study of an auv with a suction-actuated end effector", "comment": null, "summary": "Autonomous object recovery in the hadal zone is challenging due to extreme hydrostatic pressure, limited visibility and currents, and the need for precise manipulation at full ocean depth. Field experimentation in such environments is costly, high-risk, and constrained by limited vehicle availability, making early validation of autonomous behaviors difficult. This paper presents a simulation-based study of a complete autonomous subsea object recovery mission using a Hadal Small Vehicle (HSV) equipped with a three-degree-of-freedom robotic arm and a suction-actuated end effector. The Stonefish simulator is used to model realistic vehicle dynamics, hydrodynamic disturbances, sensing, and interaction with a target object under hadal-like conditions. The control framework combines a world-frame PID controller for vehicle navigation and stabilization with an inverse-kinematics-based manipulator controller augmented by acceleration feed-forward, enabling coordinated vehicle - manipulator operation. In simulation, the HSV autonomously descends from the sea surface to 6,000 m, performs structured seafloor coverage, detects a target object, and executes a suction-based recovery. The results demonstrate that high-fidelity simulation provides an effective and low-risk means of evaluating autonomous deep-sea intervention behaviors prior to field deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u9a8c\u8bc1\u6df1\u6d77\u81ea\u4e3b\u7269\u4f53\u56de\u6536\u4efb\u52a1\uff0c\u901a\u8fc7\u6a21\u62df\u5668\u5728\u865a\u62df\u73af\u5883\u4e2d\u6d4b\u8bd5\u5c0f\u578b\u6df1\u6d77\u8f66\u8f86\u57286000\u7c73\u6df1\u5ea6\u6267\u884c\u81ea\u4e3b\u63a2\u6d4b\u548c\u56de\u6536\u4f5c\u4e1a\u3002", "motivation": "\u6df1\u6d77\uff08\u5c24\u5176\u662f\u8d85\u6df1\u6e0a\u5e26\uff09\u73af\u5883\u6781\u7aef\uff0c\u538b\u529b\u5de8\u5927\u3001\u80fd\u89c1\u5ea6\u4f4e\u3001\u5b58\u5728\u6d77\u6d41\uff0c\u81ea\u4e3b\u7269\u4f53\u56de\u6536\u6781\u5177\u6311\u6218\u3002\u5b9e\u5730\u5b9e\u9a8c\u6210\u672c\u9ad8\u3001\u98ce\u9669\u5927\u3001\u8f66\u8f86\u53ef\u7528\u6027\u6709\u9650\uff0c\u96be\u4ee5\u5728\u90e8\u7f72\u524d\u9a8c\u8bc1\u81ea\u4e3b\u884c\u4e3a\u3002", "method": "\u4f7f\u7528Stonefish\u6a21\u62df\u5668\u6a21\u62df\u771f\u5b9e\u8f66\u8f86\u52a8\u529b\u5b66\u3001\u6c34\u52a8\u529b\u6270\u52a8\u3001\u611f\u77e5\u548c\u76ee\u6807\u7269\u4f53\u4ea4\u4e92\u3002\u63a7\u5236\u6846\u67b6\u7ed3\u5408\u4e16\u754c\u5750\u6807\u7cfbPID\u63a7\u5236\u5668\u7528\u4e8e\u8f66\u8f86\u5bfc\u822a\u7a33\u5b9a\uff0c\u4ee5\u53ca\u57fa\u4e8e\u9006\u8fd0\u52a8\u5b66\u7684\u673a\u68b0\u81c2\u63a7\u5236\u5668\uff08\u5e26\u52a0\u901f\u5ea6\u524d\u9988\uff09\uff0c\u5b9e\u73b0\u8f66\u8f86-\u673a\u68b0\u81c2\u534f\u8c03\u64cd\u4f5c\u3002", "result": "\u5728\u6a21\u62df\u4e2d\uff0c\u5c0f\u578b\u6df1\u6d77\u8f66\u8f86\u6210\u529f\u4ece\u6d77\u9762\u81ea\u4e3b\u4e0b\u6f5c\u81f36000\u7c73\u6df1\u5ea6\uff0c\u6267\u884c\u7ed3\u6784\u5316\u6d77\u5e95\u8986\u76d6\u63a2\u6d4b\uff0c\u8bc6\u522b\u76ee\u6807\u7269\u4f53\uff0c\u5e76\u5b8c\u6210\u57fa\u4e8e\u5438\u529b\u7684\u56de\u6536\u64cd\u4f5c\u3002", "conclusion": "\u9ad8\u4fdd\u771f\u6a21\u62df\u4e3a\u6df1\u6d77\u81ea\u4e3b\u5e72\u9884\u884c\u4e3a\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u4f4e\u98ce\u9669\u7684\u8bc4\u4f30\u624b\u6bb5\uff0c\u53ef\u5728\u5b9e\u5730\u90e8\u7f72\u524d\u8fdb\u884c\u9a8c\u8bc1\uff0c\u964d\u4f4e\u6df1\u6d77\u4f5c\u4e1a\u7684\u98ce\u9669\u548c\u6210\u672c\u3002"}}
{"id": "2601.00938", "categories": ["cs.CL", "math.OC"], "pdf": "https://arxiv.org/pdf/2601.00938", "abs": "https://arxiv.org/abs/2601.00938", "authors": ["Faruk Alpay", "Bugra Kilictas"], "title": "Rate-Distortion Analysis of Compressed Query Delegation with Low-Rank Riemannian Updates", "comment": "9 pages", "summary": "Bounded-context agents fail when intermediate reasoning exceeds an effective working-memory budget. We study compressed query delegation (CQD): (i) compress a high-dimensional latent reasoning state into a low-rank tensor query, (ii) delegate the minimal query to an external oracle, and (iii) update the latent state via Riemannian optimization on fixed-rank manifolds. We give a math-first formulation: CQD is a constrained stochastic program with a query-budget functional and an oracle modeled as a noisy operator. We connect CQD to classical rate-distortion and information bottleneck principles, showing that spectral hard-thresholding is optimal for a natural constrained quadratic distortion problem, and we derive convergence guarantees for Riemannian stochastic approximation under bounded oracle noise and smoothness assumptions. Empirically, we report (A) a 2,500-item bounded-context reasoning suite (BBH-derived tasks plus curated paradox instances) comparing CQD against chain-of-thought baselines under fixed compute and context; and (B) a human \"cognitive mirror\" benchmark (N=200) measuring epistemic gain and semantic drift across modern oracles.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u538b\u7f29\u67e5\u8be2\u59d4\u6258(CQD)\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u5f20\u91cf\u67e5\u8be2\u538b\u7f29\u9ad8\u7ef4\u63a8\u7406\u72b6\u6001\uff0c\u59d4\u6258\u7ed9\u5916\u90e8oracle\uff0c\u5e76\u5728\u56fa\u5b9a\u79e9\u6d41\u5f62\u4e0a\u8fdb\u884c\u9ece\u66fc\u4f18\u5316\uff0c\u89e3\u51b3\u6709\u754c\u4e0a\u4e0b\u6587\u4ee3\u7406\u7684\u5de5\u4f5c\u8bb0\u5fc6\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u6709\u754c\u4e0a\u4e0b\u6587\u4ee3\u7406\u5728\u4e2d\u95f4\u63a8\u7406\u8d85\u8fc7\u6709\u6548\u5de5\u4f5c\u8bb0\u5fc6\u9884\u7b97\u65f6\u4f1a\u5931\u8d25\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u538b\u7f29\u9ad8\u7ef4\u63a8\u7406\u72b6\u6001\u5e76\u59d4\u6258\u7ed9\u5916\u90e8oracle\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u538b\u7f29\u67e5\u8be2\u59d4\u6258(CQD)\u6846\u67b6\uff1a1)\u5c06\u9ad8\u7ef4\u6f5c\u5728\u63a8\u7406\u72b6\u6001\u538b\u7f29\u4e3a\u4f4e\u79e9\u5f20\u91cf\u67e5\u8be2\uff1b2)\u5c06\u6700\u5c0f\u5316\u67e5\u8be2\u59d4\u6258\u7ed9\u5916\u90e8oracle\uff1b3)\u901a\u8fc7\u56fa\u5b9a\u79e9\u6d41\u5f62\u4e0a\u7684\u9ece\u66fc\u4f18\u5316\u66f4\u65b0\u6f5c\u5728\u72b6\u6001\u3002\u5efa\u7acb\u6570\u5b66\u6a21\u578b\uff1aCQD\u662f\u5177\u6709\u67e5\u8be2\u9884\u7b97\u51fd\u6570\u548c\u566a\u58f0oracle\u7b97\u5b50\u7684\u7ea6\u675f\u968f\u673a\u89c4\u5212\u95ee\u9898\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a\u5bf9\u4e8e\u81ea\u7136\u7ea6\u675f\u4e8c\u6b21\u5931\u771f\u95ee\u9898\uff0c\u8c31\u786c\u9608\u503c\u662f\u6700\u4f18\u7684\uff1b\u5728oracle\u566a\u58f0\u6709\u754c\u548c\u5e73\u6ed1\u5047\u8bbe\u4e0b\uff0c\u63a8\u5bfc\u51fa\u9ece\u66fc\u968f\u673a\u8fd1\u4f3c\u7684\u6536\u655b\u4fdd\u8bc1\u3002\u5b9e\u8bc1\u7ed3\u679c\uff1aA)\u57282,500\u9879\u6709\u754c\u4e0a\u4e0b\u6587\u63a8\u7406\u5957\u4ef6\u4e0a\uff0cCQD\u5728\u56fa\u5b9a\u8ba1\u7b97\u548c\u4e0a\u4e0b\u6587\u6761\u4ef6\u4e0b\u4f18\u4e8e\u601d\u7ef4\u94fe\u57fa\u7ebf\uff1bB)\u4eba\u7c7b\"\u8ba4\u77e5\u955c\u50cf\"\u57fa\u51c6(N=200)\u6d4b\u91cf\u73b0\u4ee3oracle\u7684\u8ba4\u77e5\u589e\u76ca\u548c\u8bed\u4e49\u6f02\u79fb\u3002", "conclusion": "CQD\u4e3a\u6709\u754c\u4e0a\u4e0b\u6587\u4ee3\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u538b\u7f29\u548c\u59d4\u6258\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u5b66\u5f62\u5f0f\u5316\u8fde\u63a5\u4e86\u7ecf\u5178\u7387\u5931\u771f\u548c\u4fe1\u606f\u74f6\u9888\u539f\u7406\uff0c\u5e76\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u4e3a\u89e3\u51b3\u5de5\u4f5c\u8bb0\u5fc6\u9650\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2601.01139", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.01139", "abs": "https://arxiv.org/abs/2601.01139", "authors": ["Sriram Rajasekar", "Ashwini Ratnoo"], "title": "Latent Space Reinforcement Learning for Multi-Robot Exploration", "comment": null, "summary": "Autonomous mapping of unknown environments is a critical challenge, particularly in scenarios where time is limited. Multi-agent systems can enhance efficiency through collaboration, but the scalability of motion-planning algorithms remains a key limitation. Reinforcement learning has been explored as a solution, but existing approaches are constrained by the limited input size required for effective learning, restricting their applicability to discrete environments. This work addresses that limitation by leveraging autoencoders to perform dimensionality reduction, compressing high-fidelity occupancy maps into latent state vectors while preserving essential spatial information. Additionally, we introduce a novel procedural generation algorithm based on Perlin noise, designed to generate topologically complex training environments that simulate asteroid fields, caves and forests. These environments are used for training the autoencoder and the navigation algorithm using a hierarchical deep reinforcement learning framework for decentralized coordination. We introduce a weighted consensus mechanism that modulates reliance on shared data via a tuneable trust parameter, ensuring robustness to accumulation of errors. Experimental results demonstrate that the proposed system scales effectively with number of agents and generalizes well to unfamiliar, structurally distinct environments and is resilient in communication-constrained settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u964d\u7ef4\u548c\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53\u81ea\u4e3b\u5efa\u56fe\u7cfb\u7edf\uff0c\u80fd\u591f\u5904\u7406\u9ad8\u4fdd\u771f\u5ea6\u5730\u56fe\u5e76\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u672a\u77e5\u73af\u5883\u81ea\u4e3b\u5efa\u56fe\u4e2d\u80fd\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u73b0\u6709\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\u6269\u5c55\u6027\u6709\u9650\u3002\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u53d7\u9650\u4e8e\u8f93\u5165\u5c3a\u5bf8\u8981\u6c42\uff0c\u53ea\u80fd\u5e94\u7528\u4e8e\u79bb\u6563\u73af\u5883\uff0c\u9700\u8981\u89e3\u51b3\u9ad8\u7ef4\u8fde\u7eed\u73af\u5883\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "1) \u4f7f\u7528\u81ea\u52a8\u7f16\u7801\u5668\u8fdb\u884c\u7ef4\u5ea6\u538b\u7f29\uff0c\u5c06\u9ad8\u4fdd\u771f\u5ea6\u5360\u636e\u6805\u683c\u56fe\u538b\u7f29\u4e3a\u6f5c\u5728\u72b6\u6001\u5411\u91cf\uff1b2) \u57fa\u4e8ePerlin\u566a\u58f0\u7684\u7a0b\u5e8f\u751f\u6210\u7b97\u6cd5\u521b\u5efa\u62d3\u6251\u590d\u6742\u8bad\u7ec3\u73af\u5883\uff1b3) \u5206\u5c42\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\uff1b4) \u5f15\u5165\u52a0\u6743\u5171\u8bc6\u673a\u5236\uff0c\u901a\u8fc7\u53ef\u8c03\u4fe1\u4efb\u53c2\u6570\u8c03\u8282\u5bf9\u5171\u4eab\u6570\u636e\u7684\u4f9d\u8d56\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u7cfb\u7edf\u80fd\u6709\u6548\u968f\u667a\u80fd\u4f53\u6570\u91cf\u6269\u5c55\uff0c\u5728\u964c\u751f\u4e14\u7ed3\u6784\u4e0d\u540c\u7684\u73af\u5883\u4e2d\u6cdb\u5316\u826f\u597d\uff0c\u5728\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e0b\u5177\u6709\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8fde\u7eed\u9ad8\u7ef4\u73af\u5883\u4e2d\u7684\u9650\u5236\u3002", "conclusion": "\u901a\u8fc7\u81ea\u52a8\u7f16\u7801\u5668\u964d\u7ef4\u548c\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u591a\u667a\u80fd\u4f53\u5728\u590d\u6742\u8fde\u7eed\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u81ea\u4e3b\u5efa\u56fe\uff0c\u4e3a\u65f6\u95f4\u53d7\u9650\u573a\u666f\u4e0b\u7684\u672a\u77e5\u73af\u5883\u63a2\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01011", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01011", "abs": "https://arxiv.org/abs/2601.01011", "authors": ["Patricio Vera"], "title": "Intention Collapse: Intention-Level Metrics for Reasoning in Language Models", "comment": "21 pages, 4 figures, 3 tables. Code: https://github.com/patriciomvera/intention-collapse-experiments", "summary": "Every act of language generation compresses a rich internal state into a single token sequence. We call this process intention collapse: a many-to-one projection from a high dimensional intention space I into an external language space L. We formalize intention collapse for contemporary language models, define three simple, model agnostic intention metrics (intention entropy Hint, effective dimensionality dimeff, and latent knowledge recoverability Recov), and propose an empirical agenda for studying how inference time computation shapes internal intentions before they are verbalized. We also report a first small scale experiment. Using a 4 bit Mistral 7B model on 200 GSM8K problems, we compare a direct answer baseline, a chain of thought (CoT) regime, and a babble control. CoT raises accuracy from 5.5 percent to 53 percent, sharply reduces pre collapse intention entropy (from 1.42 to 0.37 bits), and shows higher global effective dimensionality than the other regimes despite producing fewer tokens than babble. At the same time, Hint has little item level predictive power, and a linear probe on I achieves AUROC 0.65 in the CoT regime but only about chance in the baseline regime, where it collapses to the majority class. These preliminary results indicate that intention level metrics can distinguish inference regimes and expose latent information that is partly lost during collapse, while also revealing important limitations of our current proxies", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u610f\u56fe\u574d\u7f29\"\u6982\u5ff5\uff0c\u5c06\u8bed\u8a00\u751f\u6210\u89c6\u4e3a\u9ad8\u7ef4\u610f\u56fe\u7a7a\u95f4\u5230\u8bed\u8a00\u5e8f\u5217\u7684\u6620\u5c04\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e09\u4e2a\u610f\u56fe\u5ea6\u91cf\u6307\u6807\u6765\u7814\u7a76\u63a8\u7406\u8ba1\u7b97\u5982\u4f55\u5f71\u54cd\u5185\u90e8\u610f\u56fe\u8868\u8fbe", "motivation": "\u7814\u7a76\u8bed\u8a00\u751f\u6210\u8fc7\u7a0b\u4e2d\u4e30\u5bcc\u7684\u5185\u90e8\u72b6\u6001\u5982\u4f55\u88ab\u538b\u7f29\u4e3a\u5355\u4e00token\u5e8f\u5217\uff0c\u63a2\u7d22\u63a8\u7406\u65f6\u8ba1\u7b97\u5982\u4f55\u5851\u9020\u5185\u90e8\u610f\u56fe\u5728\u8bed\u8a00\u5316\u4e4b\u524d\u7684\u72b6\u6001", "method": "\u5f62\u5f0f\u5316\u610f\u56fe\u574d\u7f29\u6982\u5ff5\uff0c\u5b9a\u4e49\u4e09\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u610f\u56fe\u5ea6\u91cf\u6307\u6807\uff08\u610f\u56fe\u71b5Hint\u3001\u6709\u6548\u7ef4\u5ea6dimeff\u3001\u6f5c\u5728\u77e5\u8bc6\u53ef\u6062\u590d\u6027Recov\uff09\uff0c\u5e76\u901a\u8fc7\u5c0f\u89c4\u6a21\u5b9e\u9a8c\u6bd4\u8f83\u76f4\u63a5\u56de\u7b54\u3001\u601d\u7ef4\u94fe\u548cbabble\u63a7\u5236\u4e09\u79cd\u63a8\u7406\u673a\u5236", "result": "\u601d\u7ef4\u94fe\u5c06\u51c6\u786e\u7387\u4ece5.5%\u63d0\u5347\u81f353%\uff0c\u663e\u8457\u964d\u4f4e\u610f\u56fe\u71b5\uff081.42\u21920.37\u6bd4\u7279\uff09\uff0c\u663e\u793a\u66f4\u9ad8\u7684\u5168\u5c40\u6709\u6548\u7ef4\u5ea6\uff1b\u610f\u56fe\u71b5\u5728\u9879\u76ee\u7ea7\u522b\u9884\u6d4b\u529b\u6709\u9650\uff0c\u601d\u7ef4\u94fe\u673a\u5236\u4e0b\u7ebf\u6027\u63a2\u9488AUROC\u8fbe0.65\uff0c\u800c\u57fa\u7ebf\u673a\u5236\u4ec5\u8fbe\u968f\u673a\u6c34\u5e73", "conclusion": "\u610f\u56fe\u5c42\u9762\u6307\u6807\u80fd\u533a\u5206\u63a8\u7406\u673a\u5236\u5e76\u63ed\u793a\u574d\u7f29\u8fc7\u7a0b\u4e2d\u90e8\u5206\u4e22\u5931\u7684\u6f5c\u5728\u4fe1\u606f\uff0c\u4f46\u5f53\u524d\u4ee3\u7406\u6307\u6807\u4ecd\u6709\u91cd\u8981\u5c40\u9650\u6027\uff0c\u4e3a\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u610f\u56fe\u8868\u8fbe\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6"}}
{"id": "2601.01144", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01144", "abs": "https://arxiv.org/abs/2601.01144", "authors": ["Shu Pan", "Simon Archieri", "Ahmet Cinar", "Jonatan Scharff Willners", "Ignacio Carlucho", "Yvan Petillot"], "title": "VISO: Robust Underwater Visual-Inertial-Sonar SLAM with Photometric Rendering for Dense 3D Reconstruction", "comment": null, "summary": "Visual challenges in underwater environments significantly hinder the accuracy of vision-based localisation and the high-fidelity dense reconstruction. In this paper, we propose VISO, a robust underwater SLAM system that fuses a stereo camera, an inertial measurement unit (IMU), and a 3D sonar to achieve accurate 6-DoF localisation and enable efficient dense 3D reconstruction with high photometric fidelity. We introduce a coarse-to-fine online calibration approach for extrinsic parameters estimation between the 3D sonar and the camera. Additionally, a photometric rendering strategy is proposed for the 3D sonar point cloud to enrich the sonar map with visual information. Extensive experiments in a laboratory tank and an open lake demonstrate that VISO surpasses current state-of-the-art underwater and visual-based SLAM algorithms in terms of localisation robustness and accuracy, while also exhibiting real-time dense 3D reconstruction performance comparable to the offline dense mapping method.", "AI": {"tldr": "VISO\u662f\u4e00\u4e2a\u9c81\u68d2\u7684\u6c34\u4e0bSLAM\u7cfb\u7edf\uff0c\u878d\u5408\u7acb\u4f53\u76f8\u673a\u3001IMU\u548c3D\u58f0\u7eb3\uff0c\u5b9e\u73b0\u7cbe\u786e\u76846\u81ea\u7531\u5ea6\u5b9a\u4f4d\u548c\u9ad8\u6548\u7684\u9ad8\u4fdd\u771f\u5bc6\u96c63D\u91cd\u5efa\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u7684\u89c6\u89c9\u6311\u6218\u4e25\u91cd\u5f71\u54cd\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9ad8\u4fdd\u771f\u5bc6\u96c6\u91cd\u5efa\u7684\u8d28\u91cf\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faVISO\u7cfb\u7edf\uff0c\u878d\u5408\u7acb\u4f53\u76f8\u673a\u3001IMU\u548c3D\u58f0\u7eb3\uff1b\u91c7\u7528\u4ece\u7c97\u5230\u7cbe\u7684\u5728\u7ebf\u6807\u5b9a\u65b9\u6cd5\u4f30\u8ba13D\u58f0\u7eb3\u4e0e\u76f8\u673a\u4e4b\u95f4\u7684\u5916\u53c2\uff1b\u63d0\u51fa3D\u58f0\u7eb3\u70b9\u4e91\u7684\u5149\u5ea6\u6e32\u67d3\u7b56\u7565\uff0c\u4e3a\u58f0\u7eb3\u5730\u56fe\u6dfb\u52a0\u89c6\u89c9\u4fe1\u606f\u3002", "result": "\u5728\u5b9e\u9a8c\u5ba4\u6c34\u7bb1\u548c\u5f00\u653e\u6e56\u6cca\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cVISO\u5728\u5b9a\u4f4d\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684\u6c34\u4e0b\u548c\u57fa\u4e8e\u89c6\u89c9\u7684SLAM\u7b97\u6cd5\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u4e0e\u79bb\u7ebf\u5bc6\u96c6\u5efa\u56fe\u65b9\u6cd5\u76f8\u5f53\u7684\u5b9e\u65f6\u5bc6\u96c63D\u91cd\u5efa\u6027\u80fd\u3002", "conclusion": "VISO\u901a\u8fc7\u591a\u4f20\u611f\u5668\u878d\u5408\u548c\u521b\u65b0\u7684\u6807\u5b9a\u4e0e\u6e32\u67d3\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6c34\u4e0b\u73af\u5883\u7684\u89c6\u89c9\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u5b9a\u4f4d\u548c\u9ad8\u4fdd\u771f\u7684\u5bc6\u96c6\u91cd\u5efa\u3002"}}
{"id": "2601.01015", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2601.01015", "abs": "https://arxiv.org/abs/2601.01015", "authors": ["Shiyuan Liu", "Jianwei Wang", "Xuemin Lin", "Lu Qin", "Wenjie Zhang", "Ying Zhang"], "title": "HyperJoin: LLM-augmented Hypergraph Link Prediction for Joinable Table Discovery", "comment": null, "summary": "As a pivotal task in data lake management, joinable table discovery has attracted widespread interest. While existing language model-based methods achieve remarkable performance by combining offline column representation learning with online ranking, their design insufficiently accounts for the underlying structural interactions: (1) offline, they directly model tables into isolated or pairwise columns, thereby struggling to capture the rich inter-table and intra-table structural information; and (2) online, they rank candidate columns based solely on query-candidate similarity, ignoring the mutual interactions among the candidates, leading to incoherent result sets. To address these limitations, we propose HyperJoin, a large language model (LLM)-augmented Hypergraph framework for Joinable table discovery. Specifically, we first construct a hypergraph to model tables using both the intra-table hyperedges and the LLM-augmented inter-table hyperedges. Consequently, the task of joinable table discovery is formulated as link prediction on this constructed hypergraph. We then design HIN, a Hierarchical Interaction Network that learns expressive column representations through bidirectional message passing over columns and hyperedges. To strengthen coherence and internal consistency in the result columns, we cast online ranking as a coherence-aware top-k column selection problem. We then introduce a reranking module that leverages a maximum spanning tree algorithm to prune noisy connections and maximize coherence. Experiments demonstrate the superiority of HyperJoin, achieving average improvements of 21.4% (Precision@15) and 17.2% (Recall@15) over the best baseline.", "AI": {"tldr": "HyperJoin\uff1a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u7684\u8d85\u56fe\u6846\u67b6\uff0c\u7528\u4e8e\u53ef\u8fde\u63a5\u8868\u53d1\u73b0\uff0c\u901a\u8fc7\u5efa\u6a21\u8868\u5185\u548c\u8868\u95f4\u7ed3\u6784\u5173\u7cfb\uff0c\u5c06\u4efb\u52a1\u8f6c\u5316\u4e3a\u8d85\u56fe\u94fe\u63a5\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u53ef\u8fde\u63a5\u8868\u53d1\u73b0\u4efb\u52a1\u4e2d\uff0c\u79bb\u7ebf\u9636\u6bb5\u5c06\u8868\u5efa\u6a21\u4e3a\u5b64\u7acb\u6216\u6210\u5bf9\u5217\uff0c\u96be\u4ee5\u6355\u6349\u4e30\u5bcc\u7684\u8868\u95f4\u548c\u8868\u5185\u7ed3\u6784\u4fe1\u606f\uff1b\u5728\u7ebf\u9636\u6bb5\u4ec5\u57fa\u4e8e\u67e5\u8be2-\u5019\u9009\u76f8\u4f3c\u5ea6\u6392\u5e8f\uff0c\u5ffd\u7565\u5019\u9009\u5217\u4e4b\u95f4\u7684\u76f8\u4e92\u5f71\u54cd\uff0c\u5bfc\u81f4\u7ed3\u679c\u96c6\u4e0d\u8fde\u8d2f\u3002", "method": "1. \u6784\u5efa\u8d85\u56fe\uff1a\u4f7f\u7528\u8868\u5185\u8d85\u8fb9\u548cLLM\u589e\u5f3a\u7684\u8868\u95f4\u8d85\u8fb9\u5efa\u6a21\u8868\u7ed3\u6784\uff1b2. \u8bbe\u8ba1HIN\uff08\u5206\u5c42\u4ea4\u4e92\u7f51\u7edc\uff09\uff1a\u901a\u8fc7\u5217\u548c\u8d85\u8fb9\u7684\u53cc\u5411\u6d88\u606f\u4f20\u9012\u5b66\u4e60\u8868\u8fbe\u6027\u5217\u8868\u793a\uff1b3. \u5728\u7ebf\u91cd\u6392\u5e8f\uff1a\u5c06\u6392\u540d\u8f6c\u5316\u4e3a\u8fde\u8d2f\u6027\u611f\u77e5\u7684top-k\u5217\u9009\u62e9\u95ee\u9898\uff0c\u4f7f\u7528\u6700\u5927\u751f\u6210\u6811\u7b97\u6cd5\u4fee\u526a\u566a\u58f0\u8fde\u63a5\u5e76\u6700\u5927\u5316\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aHyperJoin\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728Precision@15\u548cRecall@15\u6307\u6807\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u534721.4%\u548c17.2%\u3002", "conclusion": "HyperJoin\u901a\u8fc7\u8d85\u56fe\u5efa\u6a21\u8868\u7ed3\u6784\u5173\u7cfb\uff0c\u7ed3\u5408\u5206\u5c42\u4ea4\u4e92\u7f51\u7edc\u548c\u8fde\u8d2f\u6027\u611f\u77e5\u91cd\u6392\u5e8f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u6784\u4ea4\u4e92\u5efa\u6a21\u548c\u7ed3\u679c\u8fde\u8d2f\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u8fde\u63a5\u8868\u53d1\u73b0\u7684\u6027\u80fd\u3002"}}
{"id": "2601.01155", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01155", "abs": "https://arxiv.org/abs/2601.01155", "authors": ["Zhang Shizhe", "Liang Jingsong", "Zhou Zhitao", "Ye Shuhan", "Wang Yizhuo", "Tan Ming Siang Derek", "Chiun Jimmy", "Cao Yuhong", "Sartoretti Guillaume"], "title": "ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation", "comment": null, "summary": "Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation.", "AI": {"tldr": "ORION\u662f\u4e00\u4e2a\u7528\u4e8e\u90e8\u5206\u5df2\u77e5\u73af\u5883\u4e2d\u591a\u667a\u80fd\u4f53\u5728\u7ebf\u5bfc\u822a\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u5148\u9a8c\u5730\u56fe\u4e0e\u5728\u7ebf\u611f\u77e5\u3001\u5b66\u4e60\u9ad8\u5c42\u5408\u4f5c\u6a21\u5f0f\u3001\u91c7\u7528\u53cc\u9636\u6bb5\u534f\u4f5c\u7b56\u7565\uff0c\u5b9e\u73b0\u5206\u6563\u5f0f\u51b3\u7b56\u548c\u5730\u56fe\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u5bfc\u822a\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u73af\u5883\u5b8c\u5168\u5df2\u77e5\uff0c\u5bf9\u4ed3\u5e93\u6216\u5de5\u5382\u7b49\u90e8\u5206\u5df2\u77e5\u573a\u666f\u652f\u6301\u6709\u9650\u3002\u5728\u8fd9\u4e9b\u573a\u666f\u4e2d\uff0c\u667a\u80fd\u4f53\u9700\u8981\u89c4\u5212\u65e2\u80fd\u4f18\u5316\u81ea\u8eab\u8def\u5f84\u53c8\u80fd\u6536\u96c6\u5171\u4eab\u73af\u5883\u4fe1\u606f\u4ee5\u5e2e\u52a9\u961f\u53cb\u7684\u8f68\u8ff9\u3002", "method": "1) \u8bbe\u8ba1\u5171\u4eab\u56fe\u7f16\u7801\u5668\uff0c\u5c06\u5148\u9a8c\u5730\u56fe\u4e0e\u5728\u7ebf\u611f\u77e5\u878d\u5408\u4e3a\u7edf\u4e00\u8868\u793a\uff1b2) \u91c7\u7528\u9009\u9879-\u8bc4\u8bba\u5bb6\u6846\u67b6\u5b66\u4e60\u9ad8\u5c42\u5408\u4f5c\u6a21\u5f0f\uff0c\u667a\u80fd\u4f53\u53ef\u5728\u4e2a\u4f53\u5bfc\u822a\u548c\u56e2\u961f\u63a2\u7d22\u95f4\u81ea\u9002\u5e94\u5207\u6362\uff1b3) \u5f15\u5165\u53cc\u9636\u6bb5\u534f\u4f5c\u7b56\u7565\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u5728\u5730\u56fe\u4e0d\u786e\u5b9a\u6027\u4e0b\u534f\u52a9\u961f\u53cb\u3002", "result": "\u5728\u8ff7\u5bab\u5f0f\u5730\u56fe\u548c\u5927\u89c4\u6a21\u4ed3\u5e93\u73af\u5883\u4e2d\uff0cORION\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u5b9e\u65f6\u7684\u5206\u6563\u5f0f\u534f\u4f5c\uff0c\u9002\u5e94\u4e0d\u540c\u56e2\u961f\u89c4\u6a21\uff0c\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7ecf\u5178\u65b9\u6cd5\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728\u7269\u7406\u673a\u5668\u4eba\u56e2\u961f\u4e0a\u7684\u9a8c\u8bc1\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6027\u3002", "conclusion": "ORION\u662f\u4e00\u4e2a\u6709\u6548\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u5728\u90e8\u5206\u5df2\u77e5\u73af\u5883\u4e2d\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u7684\u5728\u7ebf\u534f\u4f5c\u5bfc\u822a\uff0c\u901a\u8fc7\u611f\u77e5-\u884c\u52a8\u95ed\u73af\u4e2d\u7684\u4fe1\u606f\u5171\u4eab\u548c\u81ea\u9002\u5e94\u5408\u4f5c\u6a21\u5f0f\u5207\u6362\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5bfc\u822a\u6548\u7387\u548c\u534f\u4f5c\u80fd\u529b\u3002"}}
{"id": "2601.01037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01037", "abs": "https://arxiv.org/abs/2601.01037", "authors": ["Livia Leong Hui Teng"], "title": "Multi-Dimensional Prompt Chaining to Improve Open-Domain Dialogue Generation", "comment": null, "summary": "Small language models (SLMs) offer significant deployment advantages but often struggle to match the dialogue quality of larger models in open-domain settings. In this paper, we propose a multi-dimensional prompt-chaining framework that integrates Naturalness, Coherence, and Engagingness dimensions to enhance human-likeness in open-domain dialogue generation. We apply the framework to two SLMs, TinyLlama and Llama-2-7B, and benchmark their performance against responses generated by substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. We then employ automatic and human evaluation to assess the responses based on diversity, contextual coherence, as well as overall quality. Results show that the full framework improves response diversity by up to 29%, contextual coherence by up to 28%, and engagingness as well as naturalness by up to 29%. Notably, Llama-2-7B achieves performance comparable to substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. Overall, the findings demonstrate that carefully designed prompt-based strategies provide an effective and resource-efficient pathway to improving open-domain dialogue quality in SLMs.", "AI": {"tldr": "\u63d0\u51fa\u591a\u7ef4\u5ea6\u63d0\u793a\u94fe\u6846\u67b6\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u57df\u5bf9\u8bdd\u4e2d\u7684\u4eba\u7c7b\u76f8\u4f3c\u5ea6\uff0c\u901a\u8fc7\u81ea\u7136\u6027\u3001\u8fde\u8d2f\u6027\u548c\u5438\u5f15\u6027\u4e09\u4e2a\u7ef4\u5ea6\u4f18\u5316\uff0c\u4f7f7B\u6a21\u578b\u8fbe\u5230\u4e0e70B\u6a21\u578b\u76f8\u5f53\u7684\u5bf9\u8bdd\u8d28\u91cf\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u5728\u90e8\u7f72\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u5728\u5f00\u653e\u57df\u5bf9\u8bdd\u4e2d\u96be\u4ee5\u8fbe\u5230\u5927\u6a21\u578b\u7684\u5bf9\u8bdd\u8d28\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u5176\u5bf9\u8bdd\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u591a\u7ef4\u5ea6\u63d0\u793a\u94fe\u6846\u67b6\uff0c\u6574\u5408\u81ea\u7136\u6027\u3001\u8fde\u8d2f\u6027\u548c\u5438\u5f15\u6027\u4e09\u4e2a\u7ef4\u5ea6\u6765\u589e\u5f3a\u5bf9\u8bdd\u7684\u4eba\u7c7b\u76f8\u4f3c\u5ea6\u3002\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8eTinyLlama\u548cLlama-2-7B\u6a21\u578b\uff0c\u5e76\u4e0eLlama-2-70B\u548cGPT-3.5 Turbo\u7b49\u5927\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u5b8c\u6574\u6846\u67b6\u5c06\u54cd\u5e94\u591a\u6837\u6027\u63d0\u5347\u9ad8\u8fbe29%\uff0c\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u63d0\u5347\u9ad8\u8fbe28%\uff0c\u5438\u5f15\u6027\u548c\u81ea\u7136\u6027\u63d0\u5347\u9ad8\u8fbe29%\u3002Llama-2-7B\u8fbe\u5230\u4e86\u4e0eLlama-2-70B\u548cGPT-3.5 Turbo\u7b49\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u7b56\u7565\u4e3a\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u57df\u5bf9\u8bdd\u8d28\u91cf\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u9014\u5f84\uff0c\u4f7f\u5c0f\u6a21\u578b\u80fd\u591f\u8fbe\u5230\u5927\u6a21\u578b\u7684\u5bf9\u8bdd\u8d28\u91cf\u6c34\u5e73\u3002"}}
{"id": "2601.01188", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01188", "abs": "https://arxiv.org/abs/2601.01188", "authors": ["Zhiwei Huang", "Yanwei Fu", "Yi Zhou", "Xieyuanli Chen", "Qijun Chen", "Rui Fan"], "title": "DST-Calib: A Dual-Path, Self-Supervised, Target-Free LiDAR-Camera Extrinsic Calibration Network", "comment": null, "summary": "LiDAR-camera extrinsic calibration is essential for multi-modal data fusion in robotic perception systems. However, existing approaches typically rely on handcrafted calibration targets (e.g., checkerboards) or specific, static scene types, limiting their adaptability and deployment in real-world autonomous and robotic applications. This article presents the first self-supervised LiDAR-camera extrinsic calibration network that operates in an online fashion and eliminates the need for specific calibration targets. We first identify a significant generalization degradation problem in prior methods, caused by the conventional single-sided data augmentation strategy. To overcome this limitation, we propose a novel double-sided data augmentation technique that generates multi-perspective camera views using estimated depth maps, thereby enhancing robustness and diversity during training. Built upon this augmentation strategy, we design a dual-path, self-supervised calibration framework that reduces the dependence on high-precision ground truth labels and supports fully adaptive online calibration. Furthermore, to improve cross-modal feature association, we replace the traditional dual-branch feature extraction design with a difference map construction process that explicitly correlates LiDAR and camera features. This not only enhances calibration accuracy but also reduces model complexity. Extensive experiments conducted on five public benchmark datasets, as well as our own recorded dataset, demonstrate that the proposed method significantly outperforms existing approaches in terms of generalizability.", "AI": {"tldr": "\u9996\u4e2a\u81ea\u76d1\u7763\u7684\u5728\u7ebfLiDAR-\u76f8\u673a\u5916\u53c2\u6807\u5b9a\u7f51\u7edc\uff0c\u65e0\u9700\u7279\u5b9a\u6807\u5b9a\u677f\uff0c\u901a\u8fc7\u53cc\u9762\u6570\u636e\u589e\u5f3a\u548c\u5dee\u5f02\u56fe\u6784\u5efa\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709LiDAR-\u76f8\u673a\u5916\u53c2\u6807\u5b9a\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u6807\u5b9a\u677f\u6216\u7279\u5b9a\u9759\u6001\u573a\u666f\uff0c\u9650\u5236\u4e86\u5728\u771f\u5b9e\u81ea\u4e3b\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u9002\u5e94\u6027\u548c\u90e8\u7f72\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u53cc\u9762\u6570\u636e\u589e\u5f3a\u6280\u672f\u751f\u6210\u591a\u89c6\u89d2\u76f8\u673a\u89c6\u56fe\uff0c\u8bbe\u8ba1\u53cc\u8def\u5f84\u81ea\u76d1\u7763\u6807\u5b9a\u6846\u67b6\uff0c\u7528\u5dee\u5f02\u56fe\u6784\u5efa\u66ff\u4ee3\u4f20\u7edf\u53cc\u5206\u652f\u7279\u5f81\u63d0\u53d6\uff0c\u663e\u5f0f\u5173\u8054LiDAR\u548c\u76f8\u673a\u7279\u5f81\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u57fa\u51c6\u6570\u636e\u96c6\u548c\u81ea\u5f55\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u4e86\u65e0\u9700\u7279\u5b9a\u6807\u5b9a\u677f\u7684\u5728\u7ebf\u81ea\u76d1\u7763LiDAR-\u76f8\u673a\u5916\u53c2\u6807\u5b9a\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u589e\u5f3a\u548c\u7279\u5f81\u5173\u8054\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6807\u5b9a\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2601.01046", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01046", "abs": "https://arxiv.org/abs/2601.01046", "authors": ["Yixuan Tang", "Yi Yang"], "title": "KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs", "comment": null, "summary": "While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.", "AI": {"tldr": "KV-Embedding\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u8def\u7531LLM\u6700\u540e\u4e00\u5c42token\u7684KV\u72b6\u6001\u4f5c\u4e3a\u524d\u7f00\uff0c\u6fc0\u6d3b\u51bb\u7ed3LLM\u7684\u6f5c\u5728\u8868\u793a\u80fd\u529b\uff0c\u63d0\u5347\u5e8f\u5217\u7ea7\u4e0a\u4e0b\u6587\u8bbf\u95ee", "motivation": "LLM\u5728\u65e0\u9700\u8bad\u7ec3\u7684\u573a\u666f\u4e2d\u5b58\u5728\u4e24\u4e2a\u7ed3\u6784\u6027\u95ee\u9898\uff1a\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u9650\u5236\u65e9\u671ftoken\u8bbf\u95ee\u540e\u7eed\u4e0a\u4e0b\u6587\uff0c\u4ee5\u53ca\u4e0b\u4e00token\u9884\u6d4b\u76ee\u6807\u4f7f\u8868\u793a\u504f\u5411\u751f\u6210\u800c\u975e\u8bed\u4e49\u538b\u7f29\u3002\u9700\u8981\u6fc0\u6d3b\u51bb\u7ed3LLM\u7684\u6f5c\u5728\u8868\u793a\u80fd\u529b\u3002", "method": "\u63d0\u51faKV-Embedding\u6846\u67b6\uff0c\u5229\u7528\u89c2\u5bdf\u53d1\u73b0\u6bcf\u5c42\u6700\u540e\u4e00\u4e2atoken\u7684\u952e\u503c\uff08KV\uff09\u72b6\u6001\u7f16\u7801\u4e86\u5e8f\u5217\u7684\u538b\u7f29\u89c6\u56fe\u3002\u901a\u8fc7\u5c06\u8fd9\u4e9b\u72b6\u6001\u91cd\u8def\u7531\u4e3a\u9884\u7f6e\u524d\u7f00\uff0c\u4f7f\u6240\u6709token\u80fd\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u8bbf\u95ee\u5e8f\u5217\u7ea7\u4e0a\u4e0b\u6587\u3002\u91c7\u7528\u57fa\u4e8e\u5185\u5728\u7ef4\u5ea6\u7684\u81ea\u52a8\u5c42\u9009\u62e9\u7b56\u7565\u786e\u4fdd\u6a21\u578b\u65e0\u5173\u6027\u3002", "result": "\u5728MTEB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4f7f\u7528Qwen\u3001Mistral\u548cLlama\u9aa8\u5e72\u7f51\u7edc\uff0cKV-Embedding\u6bd4\u73b0\u6709\u65e0\u9700\u8bad\u7ec3\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe10%\uff0c\u5728\u957f\u8fbe4,096\u4e2atoken\u7684\u5e8f\u5217\u4e0a\u4fdd\u6301\u7a33\u5065\u6027\u80fd\u3002", "conclusion": "\u5185\u90e8\u72b6\u6001\u64cd\u4f5c\u4e3a\u8f93\u5165\u4fee\u6539\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u8fd9\u9879\u5de5\u4f5c\u9f13\u52b1\u8fdb\u4e00\u6b65\u63a2\u7d22LLM\u5185\u90e8\u673a\u5236\u7528\u4e8e\u8868\u793a\u5b66\u4e60\u3002"}}
{"id": "2601.01196", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01196", "abs": "https://arxiv.org/abs/2601.01196", "authors": ["Shenqi Lu", "Liangwei Zhang"], "title": "EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners", "comment": null, "summary": "In recent years, the rapid development of Large Language Models (LLMs) has significantly enhanced natural language understanding and human-computer interaction, creating new opportunities in the field of robotics. However, the integration of natural language understanding into robotic control is an important challenge in the rapid development of human-robot interaction and intelligent automation industries. This challenge hinders intuitive human control over complex robotic systems, limiting their educational and practical accessibility. To address this, we present the EduSim-LLM, an educational platform that integrates LLMs with robot simulation and constructs a language-drive control model that translates natural language instructions into executable robot behavior sequences in CoppeliaSim. We design two human-robot interaction models: direct control and autonomous control, conduct systematic simulations based on multiple language models, and evaluate multi-robot collaboration, motion planning, and manipulation capabilities. Experiential results show that LLMs can reliably convert natural language into structured robot actions; after applying prompt-engineering templates instruction-parsing accuracy improves significantly; as task complexity increases, overall accuracy rate exceeds 88.9% in the highest complexity tests.", "AI": {"tldr": "EduSim-LLM\u662f\u4e00\u4e2a\u6559\u80b2\u5e73\u53f0\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u673a\u5668\u4eba\u4eff\u771f\u7ed3\u5408\uff0c\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u63a7\u5236\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u673a\u5668\u4eba\u884c\u4e3a\u5e8f\u5217\uff0c\u5728CoppeliaSim\u4e2d\u5b9e\u73b0\uff0c\u652f\u6301\u76f4\u63a5\u63a7\u5236\u548c\u81ea\u4e3b\u63a7\u5236\u4e24\u79cd\u4ea4\u4e92\u6a21\u5f0f\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5c06\u5176\u4e0e\u673a\u5668\u4eba\u63a7\u5236\u96c6\u6210\u4ecd\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\uff0c\u8fd9\u963b\u788d\u4e86\u4eba\u7c7b\u5bf9\u590d\u6742\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u76f4\u89c2\u63a7\u5236\uff0c\u9650\u5236\u4e86\u5176\u5728\u6559\u80b2\u548c\u5b9e\u8df5\u4e2d\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u5f00\u53d1EduSim-LLM\u6559\u80b2\u5e73\u53f0\uff0c\u96c6\u6210LLMs\u4e0e\u673a\u5668\u4eba\u4eff\u771f\uff0c\u6784\u5efa\u8bed\u8a00\u9a71\u52a8\u63a7\u5236\u6a21\u578b\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7ffb\u8bd1\u4e3aCoppeliaSim\u4e2d\u7684\u53ef\u6267\u884c\u673a\u5668\u4eba\u884c\u4e3a\u5e8f\u5217\u3002\u8bbe\u8ba1\u4e24\u79cd\u4eba\u673a\u4ea4\u4e92\u6a21\u578b\uff1a\u76f4\u63a5\u63a7\u5236\u548c\u81ea\u4e3b\u63a7\u5236\uff0c\u57fa\u4e8e\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u4eff\u771f\uff0c\u8bc4\u4f30\u591a\u673a\u5668\u4eba\u534f\u4f5c\u3001\u8fd0\u52a8\u89c4\u5212\u548c\u64cd\u4f5c\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1aLLMs\u80fd\u591f\u53ef\u9760\u5730\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u673a\u5668\u4eba\u52a8\u4f5c\uff1b\u5e94\u7528\u63d0\u793a\u5de5\u7a0b\u6a21\u677f\u540e\uff0c\u6307\u4ee4\u89e3\u6790\u51c6\u786e\u7387\u663e\u8457\u63d0\u9ad8\uff1b\u968f\u7740\u4efb\u52a1\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u5728\u6700\u9ad8\u590d\u6742\u5ea6\u6d4b\u8bd5\u4e2d\u6574\u4f53\u51c6\u786e\u7387\u8d85\u8fc788.9%\u3002", "conclusion": "EduSim-LLM\u5e73\u53f0\u6210\u529f\u5c55\u793a\u4e86LLMs\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u8bed\u8a00\u9a71\u52a8\u63a7\u5236\u6a21\u578b\u6709\u6548\u63d0\u5347\u4e86\u81ea\u7136\u8bed\u8a00\u5230\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u8f6c\u6362\u51c6\u786e\u7387\uff0c\u4e3a\u6559\u80b2\u673a\u5668\u4eba\u5b66\u548c\u667a\u80fd\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01060", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01060", "abs": "https://arxiv.org/abs/2601.01060", "authors": ["Shuhuan Gu", "Wenbiao Tao", "Xinchen Ma", "Kangkang He", "Ye Guo", "Xiang Li", "Yunshi Lan"], "title": "Unsupervised Text Style Transfer for Controllable Intensity", "comment": null, "summary": "Unsupervised Text Style Transfer (UTST) aims to build a system to transfer the stylistic properties of a given text without parallel text pairs. Compared with text transfer between style polarities, UTST for controllable intensity is more challenging due to the subtle differences in stylistic features across different intensity levels. Faced with the challenges posed by the lack of parallel data and the indistinguishability between adjacent intensity levels, we propose a SFT-then-PPO paradigm to fine-tune an LLM. We first fine-tune the LLM with synthesized parallel data. Then, we further train the LLM with PPO, where the rewards are elaborately designed for distinguishing the stylistic intensity in hierarchical levels. Both the global and local stylistic features are considered to formulate the reward functions. The experiments on two UTST benchmarks showcase that both rewards have their advantages and applying them to LLM fine-tuning can effectively improve the performance of an LLM backbone based on various evaluation metrics. Even for close levels of intensity, we can still observe the noticeable stylistic difference between the generated text.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSFT-then-PPO\u8303\u5f0f\uff0c\u901a\u8fc7\u5408\u6210\u5e73\u884c\u6570\u636e\u5fae\u8c03LLM\uff0c\u518d\u4f7f\u7528\u5206\u5c42\u5956\u52b1\u8bbe\u8ba1\u7684PPO\u8bad\u7ec3\uff0c\u89e3\u51b3\u65e0\u76d1\u7763\u6587\u672c\u98ce\u683c\u8f6c\u79fb\u4e2d\u5f3a\u5ea6\u63a7\u5236\u7684\u6311\u6218\u3002", "motivation": "\u65e0\u76d1\u7763\u6587\u672c\u98ce\u683c\u8f6c\u79fb\uff08UTST\uff09\u5728\u7f3a\u4e4f\u5e73\u884c\u6587\u672c\u5bf9\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u53ef\u63a7\u5f3a\u5ea6\u8f6c\u79fb\u66f4\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u4e0d\u540c\u5f3a\u5ea6\u7ea7\u522b\u95f4\u7684\u98ce\u683c\u7279\u5f81\u5dee\u5f02\u7ec6\u5fae\uff0c\u76f8\u90bb\u5f3a\u5ea6\u7ea7\u522b\u96be\u4ee5\u533a\u5206\u3002", "method": "\u63d0\u51faSFT-then-PPO\u8303\u5f0f\uff1a1\uff09\u5148\u7528\u5408\u6210\u5e73\u884c\u6570\u636e\u5fae\u8c03LLM\uff1b2\uff09\u518d\u7528PPO\u8fdb\u4e00\u6b65\u8bad\u7ec3\uff0c\u8bbe\u8ba1\u5206\u5c42\u5956\u52b1\u51fd\u6570\u540c\u65f6\u8003\u8651\u5168\u5c40\u548c\u5c40\u90e8\u98ce\u683c\u7279\u5f81\u6765\u533a\u5206\u98ce\u683c\u5f3a\u5ea6\u3002", "result": "\u5728\u4e24\u4e2aUTST\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e24\u79cd\u5956\u52b1\u51fd\u6570\u5404\u6709\u4f18\u52bf\uff0c\u5e94\u7528\u4e8eLLM\u5fae\u8c03\u80fd\u6709\u6548\u63d0\u5347\u57fa\u4e8e\u5404\u79cd\u8bc4\u4f30\u6307\u6807\u7684LLM\u9aa8\u5e72\u6027\u80fd\u3002\u5373\u4f7f\u5728\u76f8\u8fd1\u5f3a\u5ea6\u7ea7\u522b\uff0c\u751f\u6210\u6587\u672c\u95f4\u4e5f\u80fd\u89c2\u5bdf\u5230\u660e\u663e\u7684\u98ce\u683c\u5dee\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684SFT-then-PPO\u8303\u5f0f\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u6587\u672c\u98ce\u683c\u8f6c\u79fb\u4e2d\u53ef\u63a7\u5f3a\u5ea6\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u4e0d\u540c\u5f3a\u5ea6\u7ea7\u522b\u7684\u6709\u6548\u533a\u5206\u3002"}}
{"id": "2601.01282", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01282", "abs": "https://arxiv.org/abs/2601.01282", "authors": ["Fang Nan", "Meher Malladi", "Qingqing Li", "Fan Yang", "Joonas Juola", "Tiziano Guadagnino", "Jens Behley", "Cesar Cadena", "Cyrill Stachniss", "Marco Hutter"], "title": "SAHA: Supervised Autonomous HArvester for selective forest thinning", "comment": null, "summary": "Forestry plays a vital role in our society, creating significant ecological, economic, and recreational value. Efficient forest management involves labor-intensive and complex operations. One essential task for maintaining forest health and productivity is selective thinning, which requires skilled operators to remove specific trees to create optimal growing conditions for the remaining ones. In this work, we present a solution based on a small-scale robotic harvester (SAHA) designed for executing this task with supervised autonomy. We build on a 4.5-ton harvester platform and implement key hardware modifications for perception and automatic control. We implement learning- and model-based approaches for precise control of hydraulic actuators, accurate navigation through cluttered environments, robust state estimation, and reliable semantic estimation of terrain traversability. Integrating state-of-the-art techniques in perception, planning, and control, our robotic harvester can autonomously navigate forest environments and reach targeted trees for selective thinning. We present experimental results from extensive field trials over kilometer-long autonomous missions in northern European forests, demonstrating the harvester's ability to operate in real forests. We analyze the performance and provide the lessons learned for advancing robotic forest management.", "AI": {"tldr": "\u5f00\u53d1\u5c0f\u578b\u673a\u5668\u4eba\u91c7\u4f10\u673aSAHA\uff0c\u7528\u4e8e\u68ee\u6797\u9009\u62e9\u6027\u758f\u4f10\u4f5c\u4e1a\uff0c\u901a\u8fc7\u76d1\u7763\u81ea\u4e3b\u65b9\u5f0f\u5b9e\u73b0\u68ee\u6797\u73af\u5883\u81ea\u4e3b\u5bfc\u822a\u548c\u6811\u6728\u5b9a\u4f4d", "motivation": "\u68ee\u6797\u7ba1\u7406\u9700\u8981\u52b3\u52a8\u5bc6\u96c6\u578b\u590d\u6742\u64cd\u4f5c\uff0c\u9009\u62e9\u6027\u758f\u4f10\u5bf9\u7ef4\u6301\u68ee\u6797\u5065\u5eb7\u81f3\u5173\u91cd\u8981\u4f46\u9700\u8981\u719f\u7ec3\u64cd\u4f5c\u5458\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u63d0\u9ad8\u6548\u7387", "method": "\u57fa\u4e8e4.5\u5428\u91c7\u4f10\u673a\u5e73\u53f0\uff0c\u8fdb\u884c\u611f\u77e5\u548c\u81ea\u52a8\u63a7\u5236\u786c\u4ef6\u6539\u9020\uff0c\u91c7\u7528\u5b66\u4e60\u548c\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u63a7\u5236\u6db2\u538b\u6267\u884c\u5668\uff0c\u5b9e\u73b0\u7cbe\u786e\u5bfc\u822a\u3001\u72b6\u6001\u4f30\u8ba1\u548c\u5730\u5f62\u53ef\u901a\u884c\u6027\u8bed\u4e49\u4f30\u8ba1", "result": "\u5728\u5317\u65b9\u6b27\u6d32\u68ee\u6797\u8fdb\u884c\u4e86\u516c\u91cc\u7ea7\u81ea\u4e3b\u4efb\u52a1\u5b9e\u5730\u8bd5\u9a8c\uff0c\u673a\u5668\u4eba\u91c7\u4f10\u673a\u80fd\u591f\u5728\u771f\u5b9e\u68ee\u6797\u73af\u5883\u4e2d\u81ea\u4e3b\u5bfc\u822a\u5e76\u5230\u8fbe\u76ee\u6807\u6811\u6728\u8fdb\u884c\u9009\u62e9\u6027\u758f\u4f10", "conclusion": "\u96c6\u6210\u5148\u8fdb\u7684\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u6280\u672f\uff0c\u673a\u5668\u4eba\u91c7\u4f10\u673a\u80fd\u591f\u81ea\u4e3b\u6267\u884c\u68ee\u6797\u7ba1\u7406\u4efb\u52a1\uff0c\u4e3a\u63a8\u8fdb\u673a\u5668\u4eba\u68ee\u6797\u7ba1\u7406\u63d0\u4f9b\u4e86\u6027\u80fd\u5206\u6790\u548c\u7ecf\u9a8c\u6559\u8bad"}}
{"id": "2601.01091", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01091", "abs": "https://arxiv.org/abs/2601.01091", "authors": ["Haq Nawaz Malik"], "title": "ks-lit-3m: A 3.1 million word kashmiri text dataset for large language model pretraining", "comment": null, "summary": "Large Language Models (LLMs) demonstrate remarkable fluency across high-resource languages yet consistently fail to generate coherent text in Kashmiri, a language spoken by approximately seven million people. This performance disparity stems not from inherent model limitations but from a critical scarcity of high-quality training data. Decades of Kashmiri literature remain inaccessible to modern NLP pipelines due to their encoding in the proprietary InPage desktop publishing format. This paper introduces KS-LIT-3M, a curated corpus of 3.1 million words (16.4 million characters) specifically designed for pretraining language models on Kashmiri. The dataset is structured as a single continuous linear text stream, optimized for causal language model training where models learn to predict subsequent tokens from preceding context. The corpus was constructed through the development of a specialized InPage-to-Unicode converter, followed by rigorous preprocessing including English contamination removal, character normalization, and quality validation. Encompassing 131,607 unique words drawn from diverse genres including literary works, journalistic writing, academic texts, and religious scholarship, KS-LIT-3M addresses a fundamental resource gap for Kashmiri language technology. The dataset is released under the CC-BY-4.0 license to facilitate research in Kashmiri natural language processing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u514b\u4ec0\u7c73\u5c14\u8bed\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u6784\u5efa\u4e86\u5305\u542b310\u4e07\u5355\u8bcd\u7684KS-LIT-3M\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u5f00\u53d1InPage\u5230Unicode\u8f6c\u6362\u5668\u89e3\u51b3\u4e86\u5386\u53f2\u6587\u732e\u8bbf\u95ee\u969c\u788d\uff0c\u4e3a\u514b\u4ec0\u7c73\u5c14\u8bedNLP\u7814\u7a76\u586b\u8865\u4e86\u8d44\u6e90\u7a7a\u767d\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u62e5\u6709\u7ea6700\u4e07\u4f7f\u7528\u8005\u7684\u514b\u4ec0\u7c73\u5c14\u8bed\u4e0a\u5374\u65e0\u6cd5\u751f\u6210\u8fde\u8d2f\u6587\u672c\u3002\u4e3b\u8981\u95ee\u9898\u4e0d\u662f\u6a21\u578b\u672c\u8eab\u9650\u5236\uff0c\u800c\u662f\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u4e25\u91cd\u7f3a\u4e4f\u3002\u6570\u5341\u5e74\u7684\u514b\u4ec0\u7c73\u5c14\u8bed\u6587\u732e\u7531\u4e8e\u4f7f\u7528\u4e13\u6709\u7684InPage\u684c\u9762\u51fa\u7248\u683c\u5f0f\u7f16\u7801\uff0c\u65e0\u6cd5\u88ab\u73b0\u4ee3NLP\u6d41\u7a0b\u8bbf\u95ee\u3002", "method": "1. \u5f00\u53d1\u4e13\u95e8\u7684InPage\u5230Unicode\u8f6c\u6362\u5668\uff0c\u89e3\u51b3\u5386\u53f2\u6587\u732e\u8bbf\u95ee\u95ee\u9898\uff1b2. \u6784\u5efa\u5305\u542b310\u4e07\u5355\u8bcd\uff081640\u4e07\u5b57\u7b26\uff09\u7684KS-LIT-3M\u8bed\u6599\u5e93\uff1b3. \u91c7\u7528\u4e25\u683c\u7684\u9884\u5904\u7406\u6d41\u7a0b\uff1a\u53bb\u9664\u82f1\u8bed\u6c61\u67d3\u3001\u5b57\u7b26\u89c4\u8303\u5316\u3001\u8d28\u91cf\u9a8c\u8bc1\uff1b4. \u8bed\u6599\u5e93\u8bbe\u8ba1\u4e3a\u5355\u4e00\u8fde\u7eed\u7ebf\u6027\u6587\u672c\u6d41\uff0c\u4f18\u5316\u7528\u4e8e\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u5305\u542b131,607\u4e2a\u72ec\u7279\u5355\u8bcd\u7684\u514b\u4ec0\u7c73\u5c14\u8bed\u8bed\u6599\u5e93\uff0c\u6db5\u76d6\u6587\u5b66\u3001\u65b0\u95fb\u3001\u5b66\u672f\u548c\u5b97\u6559\u7b49\u591a\u79cd\u4f53\u88c1\u3002\u8bed\u6599\u5e93\u4ee5CC-BY-4.0\u8bb8\u53ef\u8bc1\u53d1\u5e03\uff0c\u4e13\u95e8\u4e3a\u514b\u4ec0\u7c73\u5c14\u8bed\u8bed\u8a00\u6a21\u578b\u9884\u8bad\u7ec3\u8bbe\u8ba1\uff0c\u586b\u8865\u4e86\u514b\u4ec0\u7c73\u5c14\u8bed\u6280\u672f\u8d44\u6e90\u7684\u6839\u672c\u6027\u7a7a\u767d\u3002", "conclusion": "KS-LIT-3M\u8bed\u6599\u5e93\u89e3\u51b3\u4e86\u514b\u4ec0\u7c73\u5c14\u8bed\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u5173\u952e\u95ee\u9898\uff0c\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u514b\u670d\u4e86\u5386\u53f2\u6587\u732e\u683c\u5f0f\u969c\u788d\uff0c\u4e3a\u514b\u4ec0\u7c73\u5c14\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u6709\u671b\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u5728\u514b\u4ec0\u7c73\u5c14\u8bed\u4e0a\u7684\u8868\u73b0\u3002"}}
{"id": "2601.01438", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01438", "abs": "https://arxiv.org/abs/2601.01438", "authors": ["Russell Buchanan", "Adrian R\u00f6fer", "Jo\u00e3o Moura", "Abhinav Valada", "Sethu Vijayakumar"], "title": "Online Estimation and Manipulation of Articulated Objects", "comment": "This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this article is published in Autonomous Robots, and is available online at [Link will be updated when available]", "summary": "From refrigerators to kitchen drawers, humans interact with articulated objects effortlessly every day while completing household chores. For automating these tasks, service robots must be capable of manipulating arbitrary articulated objects. Recent deep learning methods have been shown to predict valuable priors on the affordance of articulated objects from vision. In contrast, many other works estimate object articulations by observing the articulation motion, but this requires the robot to already be capable of manipulating the object. In this article, we propose a novel approach combining these methods by using a factor graph for online estimation of articulation which fuses learned visual priors and proprioceptive sensing during interaction into an analytical model of articulation based on Screw Theory. With our method, a robotic system makes an initial prediction of articulation from vision before touching the object, and then quickly updates the estimate from kinematic and force sensing during manipulation. We evaluate our method extensively in both simulations and real-world robotic manipulation experiments. We demonstrate several closed-loop estimation and manipulation experiments in which the robot was capable of opening previously unseen drawers. In real hardware experiments, the robot achieved a 75% success rate for autonomous opening of unknown articulated objects.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u5148\u9a8c\u548c\u672c\u4f53\u611f\u77e5\u7684\u56e0\u5b50\u56fe\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7ebf\u4f30\u8ba1\u94f0\u63a5\u7269\u4f53\u7684\u8fd0\u52a8\u5b66\u53c2\u6570\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u5bf9\u672a\u77e5\u94f0\u63a5\u7269\u4f53\u7684\u81ea\u4e3b\u64cd\u4f5c\u3002", "motivation": "\u670d\u52a1\u673a\u5668\u4eba\u9700\u8981\u80fd\u591f\u64cd\u4f5c\u4efb\u610f\u94f0\u63a5\u7269\u4f53\u6765\u5b8c\u6210\u5bb6\u52a1\u4efb\u52a1\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u89c6\u89c9\u9884\u6d4b\u4f46\u7f3a\u4e4f\u5b9e\u9645\u4ea4\u4e92\uff0c\u8981\u4e48\u9700\u8981\u5148\u80fd\u64cd\u4f5c\u7269\u4f53\u624d\u80fd\u4f30\u8ba1\u5176\u8fd0\u52a8\u5b66\u53c2\u6570\uff0c\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528\u56e0\u5b50\u56fe\u5728\u7ebf\u4f30\u8ba1\u94f0\u63a5\u53c2\u6570\uff0c\u878d\u5408\u5b66\u4e60\u5230\u7684\u89c6\u89c9\u5148\u9a8c\u548c\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u7684\u672c\u4f53\u611f\u77e5\uff08\u8fd0\u52a8\u5b66\u548c\u529b\u4f20\u611f\uff09\uff0c\u57fa\u4e8e\u87ba\u65cb\u7406\u8bba\u6784\u5efa\u89e3\u6790\u6a21\u578b\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u591a\u4e2a\u95ed\u73af\u4f30\u8ba1\u548c\u64cd\u4f5c\u5b9e\u9a8c\uff0c\u673a\u5668\u4eba\u80fd\u591f\u6253\u5f00\u672a\u89c1\u8fc7\u7684\u62bd\u5c49\u3002\u771f\u5b9e\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0c\u5bf9\u672a\u77e5\u94f0\u63a5\u7269\u4f53\u7684\u81ea\u4e3b\u6253\u5f00\u6210\u529f\u7387\u8fbe\u5230\u4e8675%\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u89c6\u89c9\u5148\u9a8c\u548c\u672c\u4f53\u611f\u77e5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u672a\u77e5\u94f0\u63a5\u7269\u4f53\u7684\u5728\u7ebf\u4f30\u8ba1\u548c\u81ea\u4e3b\u64cd\u4f5c\uff0c\u4e3a\u670d\u52a1\u673a\u5668\u4eba\u5b8c\u6210\u65e5\u5e38\u5bb6\u52a1\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01112", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01112", "abs": "https://arxiv.org/abs/2601.01112", "authors": ["Zilin Li", "Weiwei Xu", "Xuanbo Lu", "Zheda Liu"], "title": "EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation", "comment": "This paper presents an initial and self-contained study of a lightweight screening pipeline for emotion-aware language modeling, intended as a reproducible baseline and system-level design reference", "summary": "We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion.", "AI": {"tldr": "EmoLoom-2B\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u53ef\u590d\u73b0\u7684\u6d41\u7a0b\uff0c\u53ef\u5c06\u5c0f\u4e8e20\u4ebf\u53c2\u6570\u7684\u5c0f\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u60c5\u611f\u5206\u7c7b\u548cVAD\u9884\u6d4b\u7684\u5feb\u901f\u7b5b\u9009\u5019\u9009\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u534f\u8bae\u3001\u8bed\u4e49\u6b63\u5219\u5316\u548c\u6570\u636e\u589e\u5f3a\u5b9e\u73b0\u9ad8\u6548\u8bc4\u4f30\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u590d\u73b0\u7684\u65b9\u6cd5\u6765\u5c06\u5c0f\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u60c5\u611f\u5206\u6790\u4efb\u52a1\u7684\u5feb\u901f\u7b5b\u9009\u5de5\u5177\uff0c\u907f\u514d\u8bc4\u4f30\u4e2d\u7684\u534f\u8bae\u504f\u5dee\u548c\u4e0d\u53ef\u63a7\u65b9\u5dee\uff0c\u4e3a\u540e\u7eed\u66f4\u91cd\u7684\u8bad\u7ec3\u6216\u591a\u6a21\u6001\u878d\u5408\u63d0\u4f9b\u53ef\u9760\u7684\u521d\u6b65\u7b5b\u9009\u3002", "method": "1) \u7edf\u4e00JSON\u8f93\u5165\u8f93\u51fa\u534f\u8bae\u786e\u4fdd\u8bc4\u4f30\u4e00\u81f4\u6027\uff1b2) \u91c7\u7528KV-off\u89e3\u7801\u51cf\u5c11\u65b9\u5dee\uff1b3) \u5f15\u5165\u4e24\u79cd\u6b63\u4ea4\u8bed\u4e49\u6b63\u5219\u5668\uff1aVAD\u4fdd\u6301\u7ea6\u675f\u548c\u8f7b\u91cf\u5916\u90e8\u8bc4\u4f30\u5206\u7c7b\u5668\uff1b4) \u57fa\u4e8e\u955c\u50cf\u60c5\u611f\u5bf9\u7684Valence Flip\u589e\u5f3a\uff1b5) \u76d1\u7763\u5fae\u8c03\u4e2d\u4f7f\u7528A/B\u6df7\u5408\u91c7\u6837\u548c\u71b5\u611f\u77e5\u6e29\u5ea6\u8c03\u5ea6\u3002", "result": "\u4ee5Qwen-1.8B-Chat\u4e3a\u57fa\u7840\u6a21\u578b\uff0cEmoLoom-2B\u5728GoEmotions\u548cEmpatheticDialogues\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u5e76\u5728DailyDialog\u4e0a\u5c55\u793a\u4e86\u7a33\u5065\u7684\u8de8\u8bed\u6599\u5e93\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "EmoLoom-2B\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9884\u7b97\u53cb\u597d\u3001\u53ef\u5ba1\u8ba1\u3001\u53ef\u91cd\u5165\u7684\u53ef\u9760\u7b5b\u9009\u6d41\u7a0b\uff0c\u53ef\u4f5c\u4e3a\u91cd\u578b\u8bad\u7ec3\u6216\u591a\u6a21\u6001\u878d\u5408\u524d\u7684\u6709\u6548\u521d\u6b65\u7b5b\u9009\u5de5\u5177\uff0c\u786e\u4fdd\u60c5\u611f\u5206\u6790\u4efb\u52a1\u7684\u9ad8\u6548\u8bc4\u4f30\u548c\u6a21\u578b\u9009\u62e9\u3002"}}
{"id": "2601.01561", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01561", "abs": "https://arxiv.org/abs/2601.01561", "authors": ["Yujian Qiu", "Yuqiu Mu", "Wen Yang", "Hao Zhu"], "title": "AIMS: An Adaptive Integration of Multi-Sensor Measurements for Quadrupedal Robot Localization", "comment": null, "summary": "This paper addresses the problem of accurate localization for quadrupedal robots operating in narrow tunnel-like environments. Due to the long and homogeneous characteristics of such scenarios, LiDAR measurements often provide weak geometric constraints, making traditional sensor fusion methods susceptible to accumulated motion estimation errors. To address these challenges, we propose AIMS, an adaptive LiDAR-IMU-leg odometry fusion method for robust quadrupedal robot localization in degenerate environments. The proposed method is formulated within an error-state Kalman filtering framework, where LiDAR and leg odometry measurements are integrated with IMU-based state prediction, and measurement noise covariance matrices are adaptively adjusted based on online degeneracy-aware reliability assessment. Experimental results obtained in narrow corridor environments demonstrate that the proposed method improves localization accuracy and robustness compared with state-of-the-art approaches.", "AI": {"tldr": "\u63d0\u51faAIMS\u65b9\u6cd5\uff0c\u4e00\u79cd\u81ea\u9002\u5e94LiDAR-IMU-\u817f\u91cc\u7a0b\u8ba1\u878d\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u5728\u9000\u5316\u73af\u5883\uff08\u5982\u72ed\u7a84\u96a7\u9053\uff09\u4e2d\u7684\u9c81\u68d2\u5b9a\u4f4d\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u5728\u72ed\u7a84\u96a7\u9053\u7c7b\u73af\u5883\u4e2d\u8fd0\u884c\u65f6\uff0c\u7531\u4e8e\u73af\u5883\u957f\u4e14\u540c\u8d28\uff0cLiDAR\u6d4b\u91cf\u63d0\u4f9b\u5f31\u51e0\u4f55\u7ea6\u675f\uff0c\u4f20\u7edf\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\u5bb9\u6613\u79ef\u7d2f\u8fd0\u52a8\u4f30\u8ba1\u8bef\u5dee\u3002", "method": "\u5728\u8bef\u5dee\u72b6\u6001\u5361\u5c14\u66fc\u6ee4\u6ce2\u6846\u67b6\u4e2d\uff0c\u5c06LiDAR\u548c\u817f\u91cc\u7a0b\u8ba1\u6d4b\u91cf\u4e0eIMU\u72b6\u6001\u9884\u6d4b\u96c6\u6210\uff0c\u57fa\u4e8e\u5728\u7ebf\u9000\u5316\u611f\u77e5\u53ef\u9760\u6027\u8bc4\u4f30\u81ea\u9002\u5e94\u8c03\u6574\u6d4b\u91cf\u566a\u58f0\u534f\u65b9\u5dee\u77e9\u9635\u3002", "result": "\u5728\u72ed\u7a84\u8d70\u5eca\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "AIMS\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u9000\u5316\u73af\u5883\u4e2d\u7684\u56db\u8db3\u673a\u5668\u4eba\u5b9a\u4f4d\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5728\u51e0\u4f55\u7ea6\u675f\u5f31\u7684\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u6027\u80fd\u3002"}}
{"id": "2601.01121", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01121", "abs": "https://arxiv.org/abs/2601.01121", "authors": ["Yacouba Diarra", "Michael Leventhal"], "title": "Listen, Attend, Understand: a Regularization Technique for Stable E2E Speech Translation Training on High Variance labels", "comment": "9 mages, 3 figures", "summary": "End-to-End Speech Translation often shows slower convergence and worse performance when target transcriptions exhibit high variance and semantic ambiguity. We propose Listen, Attend, Understand (LAU), a semantic regularization technique that constrains the acoustic encoder's latent space during training. By leveraging frozen text embeddings to provide a directional auxiliary loss, LAU injects linguistic groundedness into the acoustic representation without increasing inference cost. We evaluate our method on a Bambara-to-French dataset with 30 hours of Bambara speech translated by non-professionals. Experimental results demonstrate that LAU models achieve comparable performance by standard metrics compared to an E2E-ST system pretrained with 100\\% more data and while performing better in preserving semantic meaning. Furthermore, we introduce Total Parameter Drift as a metric to quantify the structural impact of regularization to demonstrate that semantic constraints actively reorganize the encoder's weights to prioritize meaning over literal phonetics. Our findings suggest that LAU is a robust alternative to post-hoc rescoring and a valuable addition to E2E-ST training, especially when training data is scarce and/or noisy.", "AI": {"tldr": "LAU\u662f\u4e00\u79cd\u8bed\u4e49\u6b63\u5219\u5316\u6280\u672f\uff0c\u901a\u8fc7\u51bb\u7ed3\u6587\u672c\u5d4c\u5165\u63d0\u4f9b\u65b9\u5411\u6027\u8f85\u52a9\u635f\u5931\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u7ea6\u675f\u58f0\u5b66\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u4e3a\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u6ce8\u5165\u8bed\u8a00\u57fa\u7840\u6027\uff0c\u4e0d\u589e\u52a0\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u5728\u76ee\u6807\u8f6c\u5f55\u5177\u6709\u9ad8\u65b9\u5dee\u548c\u8bed\u4e49\u6a21\u7cca\u6027\u65f6\uff0c\u901a\u5e38\u8868\u73b0\u51fa\u8f83\u6162\u7684\u6536\u655b\u901f\u5ea6\u548c\u8f83\u5dee\u7684\u6027\u80fd\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u5728\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c/\u6216\u5608\u6742\u7684\u60c5\u51b5\u4e0b\u6539\u5584\u6027\u80fd\u3002", "method": "\u63d0\u51faListen, Attend, Understand (LAU)\u8bed\u4e49\u6b63\u5219\u5316\u6280\u672f\uff0c\u5229\u7528\u51bb\u7ed3\u7684\u6587\u672c\u5d4c\u5165\u63d0\u4f9b\u65b9\u5411\u6027\u8f85\u52a9\u635f\u5931\uff0c\u7ea6\u675f\u58f0\u5b66\u7f16\u7801\u5668\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u5c06\u8bed\u8a00\u57fa\u7840\u6027\u6ce8\u5165\u58f0\u5b66\u8868\u793a\u4e2d\u3002", "result": "\u572830\u5c0f\u65f6\u73ed\u5df4\u62c9\u8bed\u5230\u6cd5\u8bed\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cLAU\u6a21\u578b\u5728\u4f7f\u7528100%\u66f4\u591a\u6570\u636e\u9884\u8bad\u7ec3\u7684E2E-ST\u7cfb\u7edf\u4e0a\u8fbe\u5230\u53ef\u6bd4\u6027\u80fd\uff0c\u540c\u65f6\u5728\u4fdd\u7559\u8bed\u4e49\u542b\u4e49\u65b9\u9762\u8868\u73b0\u66f4\u597d\u3002\u5f15\u5165\u603b\u53c2\u6570\u6f02\u79fb\u5ea6\u91cf\u6765\u91cf\u5316\u6b63\u5219\u5316\u7684\u7ed3\u6784\u5f71\u54cd\u3002", "conclusion": "LAU\u662f\u540e\u5904\u7406\u91cd\u8bc4\u5206\u7684\u7a33\u5065\u66ff\u4ee3\u65b9\u6848\uff0c\u662fE2E-ST\u8bad\u7ec3\u7684\u6709\u4ef7\u503c\u8865\u5145\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c/\u6216\u5608\u6742\u7684\u60c5\u51b5\u3002\u8bed\u4e49\u7ea6\u675f\u80fd\u4e3b\u52a8\u91cd\u7ec4\u7f16\u7801\u5668\u6743\u91cd\uff0c\u4f18\u5148\u8003\u8651\u610f\u4e49\u800c\u975e\u5b57\u9762\u97f3\u7d20\u3002"}}
{"id": "2601.01577", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01577", "abs": "https://arxiv.org/abs/2601.01577", "authors": ["Tran Tien Dat", "Nguyen Hai An", "Nguyen Khanh Viet Dung", "Nguyen Duy Duc"], "title": "HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller", "comment": null, "summary": "Current attempts of Reinforcement Learning for Autonomous Controller are data-demanding while the results are under-performed, unstable, and unable to grasp and anchor on the concept of safety, and over-concentrating on noise features due to the nature of pixel reconstruction. While current Self-Supervised Learningapproachs that learning on high-dimensional representations by leveraging the JointEmbedding Predictive Architecture (JEPA) are interesting and an effective alternative, as the idea mimics the natural ability of the human brain in acquiring new skill usingimagination and minimal samples of observations. This study introduces Hanoi-World, a JEPA-based world model that using recurrent neural network (RNN) formaking longterm horizontal planning with effective inference time. Experimentsconducted on the Highway-Env package with difference enviroment showcase the effective capability of making a driving plan while safety-awareness, with considerablecollision rate in comparison with SOTA baselines", "AI": {"tldr": "\u63d0\u51faHanoi-World\uff0c\u4e00\u79cd\u57fa\u4e8eJEPA\u7684\u4e16\u754c\u6a21\u578b\uff0c\u4f7f\u7528RNN\u8fdb\u884c\u957f\u671f\u89c4\u5212\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u63a7\u5236\u5668\u4e2d\u5b9e\u73b0\u5b89\u5168\u611f\u77e5\u7684\u9a7e\u9a76\u89c4\u5212", "motivation": "\u5f53\u524d\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u63a7\u5236\u5668\u4e2d\u6570\u636e\u9700\u6c42\u5927\u3001\u6027\u80fd\u4e0d\u4f73\u3001\u4e0d\u7a33\u5b9a\u3001\u65e0\u6cd5\u786e\u4fdd\u5b89\u5168\uff0c\u4e14\u56e0\u50cf\u7d20\u91cd\u5efa\u800c\u8fc7\u5ea6\u5173\u6ce8\u566a\u58f0\u7279\u5f81\u3002\u81ea\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u901a\u8fc7JEPA\u5b66\u4e60\u9ad8\u7ef4\u8868\u793a\uff0c\u6a21\u4eff\u4eba\u8111\u901a\u8fc7\u60f3\u8c61\u548c\u5c11\u91cf\u89c2\u5bdf\u5b66\u4e60\u65b0\u6280\u80fd\u7684\u80fd\u529b\uff0c\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u63d0\u51faHanoi-World\uff0c\u4e00\u79cd\u57fa\u4e8e\u8054\u5408\u5d4c\u5165\u9884\u6d4b\u67b6\u6784(JEPA)\u7684\u4e16\u754c\u6a21\u578b\uff0c\u4f7f\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc(RNN)\u8fdb\u884c\u957f\u671f\u6c34\u5e73\u89c4\u5212\uff0c\u5177\u6709\u9ad8\u6548\u7684\u63a8\u7406\u65f6\u95f4", "result": "\u5728Highway-Env\u5305\u7684\u4e0d\u540c\u73af\u5883\u4e2d\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u6a21\u578b\u5728\u5236\u5b9a\u9a7e\u9a76\u8ba1\u5212\u65f6\u7684\u6709\u6548\u80fd\u529b\uff0c\u540c\u65f6\u5177\u5907\u5b89\u5168\u611f\u77e5\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\u5177\u6709\u53ef\u89c2\u7684\u78b0\u649e\u7387", "conclusion": "Hanoi-World\u4f5c\u4e3a\u4e00\u79cdJEPA-based\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u5b9e\u73b0\u5b89\u5168\u611f\u77e5\u7684\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\uff0c\u76f8\u6bd4\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u6709\u6240\u6539\u8fdb"}}
{"id": "2601.01126", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01126", "abs": "https://arxiv.org/abs/2601.01126", "authors": ["Andrew Borthwick", "Stephen Ash"], "title": "RoboPhD: Self-Improving Text-to-SQL Through Autonomous Agent Evolution", "comment": "18 pages, 3 figures", "summary": "We present RoboPhD, a system where AI agents autonomously conduct research to improve Text-to-SQL performance. RoboPhD implements a closed-loop evolution cycle with two coordinated components: a SQL Generation agent composed of a database analysis script and SQL generation instructions, and an Evolution agent that designs new versions based on performance feedback. Central to the framework is an ELO-based selection mechanism enabling survival-of-the-fittest dynamics while handling non-transitivity in performance. Starting from a naive 70-line baseline, RoboPhD evolves agents through iterative cross-pollination, discovering effective techniques without any external guidance on the Text-to-SQL domain. Our best agent, evolved to 1500 lines over 18 iterations, autonomously discovered strategies such as size-adaptive database analysis that adjusts depth based on schema complexity and SQL generation patterns for column selection, evidence interpretation, and aggregation. Evolution provides the largest gains on cheaper models: while we improve by 2.3 points over a strong Claude Opus 4.5 naive baseline, we show an improvement of 8.9 points over the weaker Claude Haiku model. This enables 'skip a tier' deployment: evolved Haiku exceeds naive Sonnet accuracy, and evolved Sonnet exceeds naive Opus, both at lower cost. The full system achieves 73.67% accuracy on the BIRD test set, demonstrating that AI can autonomously build a strong agentic system with only a trivial human-provided starting point.", "AI": {"tldr": "RoboPhD\u662f\u4e00\u4e2aAI\u81ea\u4e3b\u7814\u7a76\u7cfb\u7edf\uff0c\u901a\u8fc7\u8fdb\u5316\u5faa\u73af\u81ea\u52a8\u6539\u8fdbText-to-SQL\u6027\u80fd\uff0c\u4ece70\u884c\u57fa\u7ebf\u8fdb\u5316\u52301500\u884c\uff0c\u5728BIRD\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523073.67%\u51c6\u786e\u7387\uff0c\u5b9e\u73b0\"\u8df3\u7ea7\u90e8\u7f72\"\u6548\u679c\u3002", "motivation": "\u63a2\u7d22AI\u80fd\u5426\u5728\u6ca1\u6709\u9886\u57df\u4e13\u5bb6\u6307\u5bfc\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u81ea\u4e3b\u8fdb\u5316\u6784\u5efa\u5f3a\u5927\u7684Text-to-SQL\u4ee3\u7406\u7cfb\u7edf\uff0c\u4ece\u7b80\u5355\u57fa\u7ebf\u5f00\u59cb\u5b9e\u73b0\u6027\u80fd\u7a81\u7834\u3002", "method": "\u91c7\u7528\u95ed\u73af\u8fdb\u5316\u5faa\u73af\uff1aSQL\u751f\u6210\u4ee3\u7406\uff08\u6570\u636e\u5e93\u5206\u6790\u811a\u672c+SQL\u751f\u6210\u6307\u4ee4\uff09\u548c\u8fdb\u5316\u4ee3\u7406\uff08\u57fa\u4e8e\u6027\u80fd\u53cd\u9988\u8bbe\u8ba1\u65b0\u7248\u672c\uff09\uff0c\u4f7f\u7528ELO\u9009\u62e9\u673a\u5236\u5904\u7406\u6027\u80fd\u975e\u4f20\u9012\u6027\uff0c\u901a\u8fc7\u8fed\u4ee3\u4ea4\u53c9\u6388\u7c89\u8fdb\u5316\u4ee3\u7406\u3002", "result": "\u4ece70\u884c\u57fa\u7ebf\u8fdb\u531618\u4ee3\u52301500\u884c\u4ee3\u7406\uff0c\u5728BIRD\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u523073.67%\u51c6\u786e\u7387\uff1b\u5bf9\u5ec9\u4ef7\u6a21\u578b\u63d0\u5347\u6700\u5927\uff1aClaude Haiku\u63d0\u53478.9\u5206\uff0cClaude Opus\u63d0\u53472.3\u5206\uff1b\u5b9e\u73b0\"\u8df3\u7ea7\u90e8\u7f72\"\uff1a\u8fdb\u5316\u7248Haiku\u8d85\u8fc7\u539f\u751fSonnet\uff0c\u8fdb\u5316\u7248Sonnet\u8d85\u8fc7\u539f\u751fOpus\u3002", "conclusion": "AI\u80fd\u591f\u4ece\u5fae\u4e0d\u8db3\u9053\u7684\u4eba\u5de5\u8d77\u70b9\u81ea\u4e3b\u6784\u5efa\u5f3a\u5927\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u8fdb\u5316\u5bf9\u5ec9\u4ef7\u6a21\u578b\u63d0\u5347\u6700\u5927\uff0c\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u663e\u8457\u7684\"\u8df3\u7ea7\u90e8\u7f72\"\uff0c\u5c55\u793a\u4e86\u81ea\u4e3b\u7814\u7a76\u7cfb\u7edf\u7684\u6f5c\u529b\u3002"}}
{"id": "2601.01618", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01618", "abs": "https://arxiv.org/abs/2601.01618", "authors": ["Huajie Tan", "Peterson Co", "Yijie Xu", "Shanyu Rong", "Yuheng Ji", "Cheng Chi", "Xiansheng Chen", "Qiongyu Zhang", "Zhongxia Zhao", "Pengwei Wang", "Zhongyuan Wang", "Shanghang Zhang"], "title": "Action-Sketcher: From Reasoning to Action via Visual Sketches for Long-Horizon Robotic Manipulation", "comment": "26 pages, 14 figures", "summary": "Long-horizon robotic manipulation is increasingly important for real-world deployment, requiring spatial disambiguation in complex layouts and temporal resilience under dynamic interaction. However, existing end-to-end and hierarchical Vision-Language-Action (VLA) policies often rely on text-only cues while keeping plan intent latent, which undermines referential grounding in cluttered or underspecified scenes, impedes effective task decomposition of long-horizon goals with close-loop interaction, and limits causal explanation by obscuring the rationale behind action choices. To address these issues, we first introduce Visual Sketch, an implausible visual intermediate that renders points, boxes, arrows, and typed relations in the robot's current views to externalize spatial intent, connect language to scene geometry. Building on Visual Sketch, we present Action-Sketcher, a VLA framework that operates in a cyclic See-Think-Sketch-Act workflow coordinated by adaptive token-gated strategy for reasoning triggers, sketch revision, and action issuance, thereby supporting reactive corrections and human interaction while preserving real-time action prediction. To enable scalable training and evaluation, we curate diverse corpus with interleaved images, text, Visual Sketch supervision, and action sequences, and train Action-Sketcher with a multi-stage curriculum recipe that combines interleaved sequence alignment for modality unification, language-to-sketch consistency for precise linguistic grounding, and imitation learning augmented with sketch-to-action reinforcement for robustness. Extensive experiments on cluttered scenes and multi-object tasks, in simulation and on real-world tasks, show improved long-horizon success, stronger robustness to dynamic scene changes, and enhanced interpretability via editable sketches and step-wise plans. Project website: https://action-sketcher.github.io", "AI": {"tldr": "\u63d0\u51faAction-Sketcher\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8349\u56fe\uff08Visual Sketch\uff09\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u5c06\u7a7a\u95f4\u610f\u56fe\u53ef\u89c6\u5316\uff0c\u7ed3\u5408\u81ea\u9002\u5e94token\u95e8\u63a7\u7b56\u7565\u5b9e\u73b0See-Think-Sketch-Act\u5faa\u73af\u5de5\u4f5c\u6d41\uff0c\u63d0\u5347\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u548c\u5206\u5c42\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u7b56\u7565\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u7ebf\u7d22\uff0c\u5c06\u89c4\u5212\u610f\u56fe\u9690\u5f0f\u8868\u793a\uff0c\u5bfc\u81f4\u5728\u6742\u4e71\u6216\u672a\u5145\u5206\u6307\u5b9a\u7684\u573a\u666f\u4e2d\u5f15\u7528\u57fa\u7840\u8584\u5f31\uff0c\u96be\u4ee5\u6709\u6548\u5206\u89e3\u957f\u65f6\u7a0b\u76ee\u6807\u5e76\u8fdb\u884c\u95ed\u73af\u4ea4\u4e92\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u52a8\u4f5c\u9009\u62e9\u80cc\u540e\u539f\u7406\u7684\u56e0\u679c\u89e3\u91ca\u3002", "method": "1. \u5f15\u5165Visual Sketch\u4f5c\u4e3a\u89c6\u89c9\u4e2d\u95f4\u8868\u793a\uff0c\u5728\u673a\u5668\u4eba\u5f53\u524d\u89c6\u56fe\u4e2d\u6e32\u67d3\u70b9\u3001\u6846\u3001\u7bad\u5934\u548c\u7c7b\u578b\u5316\u5173\u7cfb\uff0c\u5c06\u7a7a\u95f4\u610f\u56fe\u5916\u90e8\u5316\uff1b2. \u63d0\u51faAction-Sketcher\u6846\u67b6\uff0c\u91c7\u7528See-Think-Sketch-Act\u5faa\u73af\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u81ea\u9002\u5e94token\u95e8\u63a7\u7b56\u7565\u534f\u8c03\u63a8\u7406\u89e6\u53d1\u3001\u8349\u56fe\u4fee\u8ba2\u548c\u52a8\u4f5c\u6267\u884c\uff1b3. \u6784\u5efa\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u7ed3\u5408\u4ea4\u9519\u5e8f\u5217\u5bf9\u9f50\u3001\u8bed\u8a00-\u8349\u56fe\u4e00\u81f4\u6027\u548c\u6a21\u4eff\u5b66\u4e60\u589e\u5f3a\u7684\u8349\u56fe-\u52a8\u4f5c\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u6742\u4e71\u573a\u666f\u548c\u591a\u7269\u4f53\u4efb\u52a1\u7684\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0cAction-Sketcher\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u6210\u529f\u7387\u3001\u52a8\u6001\u573a\u666f\u53d8\u5316\u7684\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u901a\u8fc7\u53ef\u7f16\u8f91\u8349\u56fe\u548c\u5206\u6b65\u89c4\u5212\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u901a\u8fc7\u89c6\u89c9\u8349\u56fe\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0cAction-Sketcher\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u7a7a\u95f4\u6d88\u6b67\u3001\u4efb\u52a1\u5206\u89e3\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01143", "categories": ["cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2601.01143", "abs": "https://arxiv.org/abs/2601.01143", "authors": ["Peng Chen"], "title": "KOS-TL (Knowledge Operation System Type Logic)", "comment": null, "summary": "This paper introduces KOS-TL (Knowledge Operation System Type Logic), a novel constructive framework designed to provide a rigorous logical foundation for autonomous and executable knowledge systems. Traditional knowledge representation models often suffer from a gap between static symbolic logic and dynamic system execution. To bridge this divide, KOS-TL leverages Dependent Type Theory to unify data, logic, and proof into a singular computational substrate.The architecture of KOS-TL is organized into three hierarchical layers: the Core Layer, which defines the static type universe and constructive primitives; the Kernel Layer, which governs state evolution through an event-driven mechanism characterized by the triple $\\langle \u03a3, \\textsf{Ev}, \u0394\\rangle$; and the Runtime Layer, responsible for the bidirectional refinement of physical signals into logical evidence. We formally define the operational semantics of the system and prove key meta-theoretical properties, including Progress and Evolutionary Consistency, ensuring that the system remains logically self-consistent and free from stuck states during continuous state transitions.By integrating Davidsonian event semantics with Martin-L\u00f6f type theory, KOS-TL enables the construction of \"proof-carrying knowledge,\" where every state change in the knowledge base is accompanied by a formal witness of its validity. We demonstrate the practical utility of this logic through application examples in industrial traceability and cross-border financial compliance. Our results suggest that KOS-TL provides a robust, formally verifiable basis for the next generation of intelligent, autonomous operating systems.", "AI": {"tldr": "KOS-TL\u662f\u4e00\u4e2a\u57fa\u4e8e\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\u7684\u65b0\u578b\u77e5\u8bc6\u64cd\u4f5c\u7cfb\u7edf\u903b\u8f91\u6846\u67b6\uff0c\u5c06\u6570\u636e\u3001\u903b\u8f91\u548c\u8bc1\u660e\u7edf\u4e00\u5230\u5355\u4e00\u8ba1\u7b97\u57fa\u677f\u4e0a\uff0c\u4e3a\u81ea\u4e3b\u53ef\u6267\u884c\u77e5\u8bc6\u7cfb\u7edf\u63d0\u4f9b\u4e25\u683c\u903b\u8f91\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u77e5\u8bc6\u8868\u793a\u6a21\u578b\u5b58\u5728\u9759\u6001\u7b26\u53f7\u903b\u8f91\u4e0e\u52a8\u6001\u7cfb\u7edf\u6267\u884c\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e00\u6570\u636e\u3001\u903b\u8f91\u548c\u8bc1\u660e\u7684\u4e25\u683c\u903b\u8f91\u6846\u67b6\u6765\u652f\u6301\u81ea\u4e3b\u77e5\u8bc6\u7cfb\u7edf\u7684\u6784\u5efa\u3002", "method": "\u91c7\u7528\u4f9d\u8d56\u7c7b\u578b\u7406\u8bba\uff0c\u6784\u5efa\u4e09\u5c42\u67b6\u6784\uff1a\u6838\u5fc3\u5c42\u5b9a\u4e49\u9759\u6001\u7c7b\u578b\u5b87\u5b99\u548c\u6784\u9020\u539f\u8bed\uff1b\u5185\u6838\u5c42\u901a\u8fc7\u4e8b\u4ef6\u9a71\u52a8\u673a\u5236\uff08\u27e8\u03a3, Ev, \u0394\u27e9\u4e09\u5143\u7ec4\uff09\u7ba1\u7406\u72b6\u6001\u6f14\u5316\uff1b\u8fd0\u884c\u65f6\u5c42\u8d1f\u8d23\u7269\u7406\u4fe1\u53f7\u5230\u903b\u8f91\u8bc1\u636e\u7684\u53cc\u5411\u7cbe\u5316\u3002", "result": "\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86\u7cfb\u7edf\u64cd\u4f5c\u8bed\u4e49\uff0c\u8bc1\u660e\u4e86\u5173\u952e\u5143\u7406\u8bba\u6027\u8d28\uff08\u8fdb\u5c55\u6027\u548c\u6f14\u5316\u4e00\u81f4\u6027\uff09\uff0c\u786e\u4fdd\u7cfb\u7edf\u5728\u8fde\u7eed\u72b6\u6001\u8f6c\u6362\u4e2d\u4fdd\u6301\u903b\u8f91\u81ea\u6d3d\u4e14\u65e0\u505c\u6ede\u72b6\u6001\u3002\u901a\u8fc7\u5de5\u4e1a\u8ffd\u6eaf\u548c\u8de8\u5883\u91d1\u878d\u5408\u89c4\u5e94\u7528\u5c55\u793a\u4e86\u5b9e\u7528\u6027\u3002", "conclusion": "KOS-TL\u901a\u8fc7\u6574\u5408\u6234\u7ef4\u68ee\u4e8b\u4ef6\u8bed\u4e49\u4e0e\u9a6c\u4e01-\u6d1b\u592b\u7c7b\u578b\u7406\u8bba\uff0c\u5b9e\u73b0\u4e86\"\u643a\u5e26\u8bc1\u660e\u7684\u77e5\u8bc6\"\u6784\u5efa\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u81ea\u4e3b\u64cd\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5f3a\u5927\u3001\u5f62\u5f0f\u53ef\u9a8c\u8bc1\u7684\u57fa\u7840\u3002"}}
{"id": "2601.01651", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01651", "abs": "https://arxiv.org/abs/2601.01651", "authors": ["Yucheng Xu", "Xiaofeng Mao", "Elle Miller", "Xinyu Yi", "Yang Li", "Zhibin Li", "Robert B. Fisher"], "title": "DemoBot: Efficient Learning of Bimanual Manipulation with Dexterous Hands From Third-Person Human Videos", "comment": null, "summary": "This work presents DemoBot, a learning framework that enables a dual-arm, multi-finger robotic system to acquire complex manipulation skills from a single unannotated RGB-D video demonstration. The method extracts structured motion trajectories of both hands and objects from raw video data. These trajectories serve as motion priors for a novel reinforcement learning (RL) pipeline that learns to refine them through contact-rich interactions, thereby eliminating the need to learn from scratch. To address the challenge of learning long-horizon manipulation skills, we introduce: (1) Temporal-segment based RL to enforce temporal alignment of the current state with demonstrations; (2) Success-Gated Reset strategy to balance the refinement of readily acquired skills and the exploration of subsequent task stages; and (3) Event-Driven Reward curriculum with adaptive thresholding to guide the RL learning of high-precision manipulation. The novel video processing and RL framework successfully achieved long-horizon synchronous and asynchronous bimanual assembly tasks, offering a scalable approach for direct skill acquisition from human videos.", "AI": {"tldr": "DemoBot\uff1a\u4ece\u5355\u6bb5\u65e0\u6807\u6ce8RGB-D\u89c6\u9891\u4e2d\u5b66\u4e60\u590d\u6742\u53cc\u624b\u64cd\u4f5c\u6280\u80fd\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u7ed3\u6784\u5316\u8fd0\u52a8\u8f68\u8ff9\u4f5c\u4e3a\u5148\u9a8c\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u7cbe\u70bc", "motivation": "\u89e3\u51b3\u4ece\u4eba\u7c7b\u6f14\u793a\u89c6\u9891\u76f4\u63a5\u5b66\u4e60\u590d\u6742\u53cc\u624b\u64cd\u4f5c\u6280\u80fd\u7684\u6311\u6218\uff0c\u907f\u514d\u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u6280\u80fd\u83b7\u53d6\u65b9\u6cd5", "method": "1. \u4ece\u539f\u59cb\u89c6\u9891\u6570\u636e\u63d0\u53d6\u53cc\u624b\u548c\u7269\u4f53\u7684\u7ed3\u6784\u5316\u8fd0\u52a8\u8f68\u8ff9\u4f5c\u4e3a\u8fd0\u52a8\u5148\u9a8c\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u65f6\u95f4\u5206\u6bb5\u7684\u5f3a\u5316\u5b66\u4e60\u786e\u4fdd\u4e0e\u6f14\u793a\u7684\u65f6\u95f4\u5bf9\u9f50\uff1b3. \u6210\u529f\u95e8\u63a7\u91cd\u7f6e\u7b56\u7565\u5e73\u8861\u6280\u80fd\u7cbe\u70bc\u4e0e\u540e\u7eed\u9636\u6bb5\u63a2\u7d22\uff1b4. \u4e8b\u4ef6\u9a71\u52a8\u7684\u5956\u52b1\u8bfe\u7a0b\u4e0e\u81ea\u9002\u5e94\u9608\u503c\u5f15\u5bfc\u9ad8\u7cbe\u5ea6\u64cd\u4f5c\u5b66\u4e60", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u957f\u65f6\u7a0b\u540c\u6b65\u548c\u5f02\u6b65\u53cc\u624b\u88c5\u914d\u4efb\u52a1\uff0c\u4e3a\u4ece\u4eba\u7c7b\u89c6\u9891\u76f4\u63a5\u83b7\u53d6\u6280\u80fd\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u6cd5", "conclusion": "DemoBot\u6846\u67b6\u80fd\u591f\u4ece\u5355\u6bb5\u65e0\u6807\u6ce8RGB-D\u89c6\u9891\u6f14\u793a\u4e2d\u6709\u6548\u5b66\u4e60\u590d\u6742\u53cc\u624b\u64cd\u4f5c\u6280\u80fd\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u5904\u7406\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u957f\u65f6\u7a0b\u64cd\u4f5c\u5b66\u4e60\u7684\u6311\u6218"}}
{"id": "2601.01153", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01153", "abs": "https://arxiv.org/abs/2601.01153", "authors": ["Jiani Guo", "Jiajia Li", "Jie Wu", "Zuchao Li", "Yujiu Yang", "Ping Wang"], "title": "SongSage: A Large Musical Language Model with Lyric Generative Pre-training", "comment": null, "summary": "Large language models have achieved significant success in various domains, yet their understanding of lyric-centric knowledge has not been fully explored. In this work, we first introduce PlaylistSense, a dataset to evaluate the playlist understanding capability of language models. PlaylistSense encompasses ten types of user queries derived from common real-world perspectives, challenging LLMs to accurately grasp playlist features and address diverse user intents. Comprehensive evaluations indicate that current general-purpose LLMs still have potential for improvement in playlist understanding. Inspired by this, we introduce SongSage, a large musical language model equipped with diverse lyric-centric intelligence through lyric generative pretraining. SongSage undergoes continual pretraining on LyricBank, a carefully curated corpus of 5.48 billion tokens focused on lyrical content, followed by fine-tuning with LyricBank-SFT, a meticulously crafted instruction set comprising 775k samples across nine core lyric-centric tasks. Experimental results demonstrate that SongSage exhibits a strong understanding of lyric-centric knowledge, excels in rewriting user queries for zero-shot playlist recommendations, generates and continues lyrics effectively, and performs proficiently across seven additional capabilities. Beyond its lyric-centric expertise, SongSage also retains general knowledge comprehension and achieves a competitive MMLU score. We will keep the datasets inaccessible due to copyright restrictions and release the SongSage and training script to ensure reproducibility and support music AI research and applications, the datasets release plan details are provided in the appendix.", "AI": {"tldr": "SongSage\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u6b4c\u8bcd\u7406\u89e3\u7684\u5927\u578b\u97f3\u4e50\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u6b4c\u8bcd\u751f\u6210\u9884\u8bad\u7ec3\u83b7\u5f97\u591a\u6837\u5316\u7684\u6b4c\u8bcd\u4e2d\u5fc3\u667a\u80fd\uff0c\u5728\u64ad\u653e\u5217\u8868\u7406\u89e3\u548c\u6b4c\u8bcd\u76f8\u5173\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b4c\u8bcd\u4e2d\u5fc3\u77e5\u8bc6\u7406\u89e3\u65b9\u9762\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u64ad\u653e\u5217\u8868\u7406\u89e3\u80fd\u529b\u4e0a\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u4e13\u95e8\u7684\u97f3\u4e50\u8bed\u8a00\u6a21\u578b\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "1. \u521b\u5efaPlaylistSense\u6570\u636e\u96c6\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u64ad\u653e\u5217\u8868\u7406\u89e3\u80fd\u529b\uff1b2. \u6784\u5efaLyricBank\u8bed\u6599\u5e93\uff0854.8\u4ebftoken\uff09\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff1b3. \u4f7f\u7528LyricBank-SFT\u6307\u4ee4\u96c6\uff0877.5\u4e07\u6837\u672c\uff0c9\u4e2a\u6838\u5fc3\u4efb\u52a1\uff09\u8fdb\u884c\u5fae\u8c03\uff1b4. \u5f00\u53d1SongSage\u6a21\u578b\uff0c\u4e13\u6ce8\u4e8e\u6b4c\u8bcd\u4e2d\u5fc3\u667a\u80fd\u3002", "result": "SongSage\u5728\u6b4c\u8bcd\u4e2d\u5fc3\u77e5\u8bc6\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u6709\u6548\u91cd\u5199\u7528\u6237\u67e5\u8be2\u8fdb\u884c\u96f6\u6837\u672c\u64ad\u653e\u5217\u8868\u63a8\u8350\uff0c\u751f\u6210\u548c\u7eed\u5199\u6b4c\u8bcd\uff0c\u5e76\u5728\u5176\u4ed6\u4e03\u4e2a\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\u3002\u540c\u65f6\u4fdd\u6301\u4e86\u901a\u7528\u77e5\u8bc6\u7406\u89e3\u80fd\u529b\uff0c\u83b7\u5f97\u6709\u7ade\u4e89\u529b\u7684MMLU\u5206\u6570\u3002", "conclusion": "SongSage\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u6b4c\u8bcd\u7406\u89e3\u7684\u5927\u578b\u97f3\u4e50\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4e13\u95e8\u7684\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7b56\u7565\uff0c\u5728\u64ad\u653e\u5217\u8868\u7406\u89e3\u548c\u6b4c\u8bcd\u76f8\u5173\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u97f3\u4e50AI\u7814\u7a76\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2601.01675", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01675", "abs": "https://arxiv.org/abs/2601.01675", "authors": ["Snehal s. Dikhale", "Karankumar Patel", "Daksh Dhingra", "Itoshi Naramura", "Akinobu Hayashi", "Soshi Iba", "Nawid Jamali"], "title": "VisuoTactile 6D Pose Estimation of an In-Hand Object using Vision and Tactile Sensor Data", "comment": "Accepted for publication in IEEE Robotics and Automation Letters (RA-L), January 2022. Presented at ICRA 2022. This is the author's version of the manuscript", "summary": "Knowledge of the 6D pose of an object can benefit in-hand object manipulation. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot's grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this paper, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot's hand. To address challenges like lack of standard representation for tactile data and sensor fusion, we propose the use of point clouds to represent object surfaces in contact with the tactile sensor and present a network architecture based on pixel-wise dense fusion. We also extend NVIDIA's Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and corresponding tactile point clouds. Results suggest that using tactile data in addition to vision data improves the 6D pose estimate, and our network generalizes successfully from synthetic training to real physical robots.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u89e6\u89c9\u4e0e\u89c6\u89c9\u6570\u636e\u4f30\u8ba1\u624b\u4e2d\u7269\u4f536D\u4f4d\u59ff\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u70b9\u4e91\u8868\u793a\u89e6\u89c9\u63a5\u89e6\u8868\u9762\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u5bc6\u96c6\u878d\u5408\u7f51\u7edc\u67b6\u6784\uff0c\u5728\u5408\u6210\u6570\u636e\u4e0a\u8bad\u7ec3\u5e76\u6210\u529f\u8fc1\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba", "motivation": "\u673a\u5668\u4eba\u624b\u5185\u7269\u4f536D\u4f4d\u59ff\u4f30\u8ba1\u56e0\u5939\u722a\u906e\u6321\u800c\u56f0\u96be\uff0c\u4ec5\u4f9d\u8d56\u89c6\u89c9\u6570\u636e\u6548\u679c\u4e0d\u4f73\u3002\u8bb8\u591a\u673a\u5668\u4eba\u914d\u5907\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u53ef\u8865\u5145\u89c6\u89c9\u6570\u636e\uff0c\u4f46\u7f3a\u4e4f\u89e6\u89c9\u6570\u636e\u6807\u51c6\u8868\u793a\u548c\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5", "method": "\u4f7f\u7528\u70b9\u4e91\u8868\u793a\u89e6\u89c9\u4f20\u611f\u5668\u63a5\u89e6\u7684\u7269\u4f53\u8868\u9762\uff0c\u63d0\u51fa\u57fa\u4e8e\u50cf\u7d20\u7ea7\u5bc6\u96c6\u878d\u5408\u7684\u7f51\u7edc\u67b6\u6784\u3002\u6269\u5c55NVIDIA\u6df1\u5ea6\u5b66\u4e60\u6570\u636e\u96c6\u5408\u6210\u5668\u751f\u6210\u5408\u6210\u89c6\u89c9\u6570\u636e\u548c\u5bf9\u5e94\u89e6\u89c9\u70b9\u4e91", "result": "\u89e6\u89c9\u6570\u636e\u8865\u5145\u89c6\u89c9\u6570\u636e\u80fd\u6539\u55846D\u4f4d\u59ff\u4f30\u8ba1\uff0c\u7f51\u7edc\u4ece\u5408\u6210\u8bad\u7ec3\u6210\u529f\u6cdb\u5316\u5230\u771f\u5b9e\u7269\u7406\u673a\u5668\u4eba", "conclusion": "\u591a\u6a21\u6001\u89e6\u89c9-\u89c6\u89c9\u878d\u5408\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u624b\u5185\u7269\u4f53\u4f4d\u59ff\u4f30\u8ba1\u7684\u906e\u6321\u95ee\u9898\uff0c\u5408\u6210\u6570\u636e\u8bad\u7ec3\u53ef\u8fc1\u79fb\u5230\u771f\u5b9e\u573a\u666f"}}
{"id": "2601.01156", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01156", "abs": "https://arxiv.org/abs/2601.01156", "authors": ["Jiani Guo", "Xiangke Zeng", "Jie Wu", "Zuchao Li"], "title": "DHI: Leveraging Diverse Hallucination Induction for Enhanced Contrastive Factuality Control in Large Language Models", "comment": "ICONIP 2025", "summary": "Large language models (LLMs) frequently produce inaccurate or fabricated information, known as \"hallucinations,\" which compromises their reliability. Existing approaches often train an \"Evil LLM\" to deliberately generate hallucinations on curated datasets, using these induced hallucinations to guide contrastive decoding against a reliable \"positive model\" for hallucination mitigation. However, this strategy is limited by the narrow diversity of hallucinations induced, as Evil LLMs trained on specific error types tend to reproduce only these particular patterns, thereby restricting their overall effectiveness. To address these limitations, we propose DHI (Diverse Hallucination Induction), a novel training framework that enables the Evil LLM to generate a broader range of hallucination types without relying on pre-annotated hallucination data. DHI employs a modified loss function that down-weights the generation of specific factually correct tokens, encouraging the Evil LLM to produce diverse hallucinations at targeted positions while maintaining overall factual content. Additionally, we introduce a causal attention masking adaptation to reduce the impact of this penalization on the generation of subsequent tokens. During inference, we apply an adaptive rationality constraint that restricts contrastive decoding to tokens where the positive model exhibits high confidence, thereby avoiding unnecessary penalties on factually correct tokens. Extensive empirical results show that DHI achieves significant performance gains over other contrastive decoding-based approaches across multiple hallucination benchmarks.", "AI": {"tldr": "DHI\u662f\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u6539\u635f\u5931\u51fd\u6570\u548c\u56e0\u679c\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u8ba9\"\u90aa\u6076LLM\"\u751f\u6210\u66f4\u591a\u6837\u5316\u7684\u5e7b\u89c9\uff0c\u65e0\u9700\u9884\u6807\u6ce8\u6570\u636e\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u7406\u6027\u7ea6\u675f\u63d0\u5347\u5e7b\u89c9\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u8bad\u7ec3\"\u90aa\u6076LLM\"\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u6545\u610f\u751f\u6210\u5e7b\u89c9\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u4ea7\u751f\u7684\u5e7b\u89c9\u7c7b\u578b\u6709\u9650\uff0c\u56e0\u4e3a\u6a21\u578b\u53ea\u5b66\u4e60\u5230\u7279\u5b9a\u7684\u9519\u8bef\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u6574\u4f53\u6548\u679c\u3002", "method": "\u63d0\u51faDHI\u6846\u67b6\uff1a1) \u4fee\u6539\u635f\u5931\u51fd\u6570\uff0c\u964d\u4f4e\u7279\u5b9a\u4e8b\u5b9e\u6b63\u786etoken\u7684\u751f\u6210\u6743\u91cd\uff0c\u9f13\u52b1\u5728\u76ee\u6807\u4f4d\u7f6e\u4ea7\u751f\u591a\u6837\u5316\u5e7b\u89c9\uff1b2) \u5f15\u5165\u56e0\u679c\u6ce8\u610f\u529b\u63a9\u7801\u9002\u5e94\uff0c\u51cf\u5c11\u60e9\u7f5a\u5bf9\u540e\u7eedtoken\u7684\u5f71\u54cd\uff1b3) \u63a8\u7406\u65f6\u5e94\u7528\u81ea\u9002\u5e94\u7406\u6027\u7ea6\u675f\uff0c\u4ec5\u5728\u6b63\u6a21\u578b\u9ad8\u7f6e\u4fe1\u5ea6\u65f6\u8fdb\u884c\u5bf9\u6bd4\u89e3\u7801\u3002", "result": "\u5728\u591a\u4e2a\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDHI\u76f8\u6bd4\u5176\u4ed6\u57fa\u4e8e\u5bf9\u6bd4\u89e3\u7801\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "DHI\u6846\u67b6\u80fd\u591f\u8bf1\u5bfc\u66f4\u5e7f\u6cdb\u7684\u5e7b\u89c9\u7c7b\u578b\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u6807\u6ce8\u6570\u636e\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5e7b\u89c9\u68c0\u6d4b\u548c\u7f13\u89e3\u7684\u6548\u679c\u3002"}}
{"id": "2601.01705", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01705", "abs": "https://arxiv.org/abs/2601.01705", "authors": ["Kenneth Kwok", "Basura Fernando", "Qianli Xu", "Vigneshwaran Subbaraju", "Dongkyu Choi", "Boon Kiat Quek"], "title": "Explicit World Models for Reliable Human-Robot Collaboration", "comment": "Accepted to AAAI-26 Bridge Program B10: Making Embodied AI Reliable with Testing and Formal Verification", "summary": "This paper addresses the topic of robustness under sensing noise, ambiguous instructions, and human-robot interaction. We take a radically different tack to the issue of reliable embodied AI: instead of focusing on formal verification methods aimed at achieving model predictability and robustness, we emphasise the dynamic, ambiguous and subjective nature of human-robot interactions that requires embodied AI systems to perceive, interpret, and respond to human intentions in a manner that is consistent, comprehensible and aligned with human expectations. We argue that when embodied agents operate in human environments that are inherently social, multimodal, and fluid, reliability is contextually determined and only has meaning in relation to the goals and expectations of humans involved in the interaction. This calls for a fundamentally different approach to achieving reliable embodied AI that is centred on building and updating an accessible \"explicit world model\" representing the common ground between human and AI, that is used to align robot behaviours with human expectations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u9760\u5177\u8eabAI\u65b9\u6cd5\uff0c\u5f3a\u8c03\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u53ef\u9760\u6027\u662f\u60c5\u5883\u51b3\u5b9a\u7684\uff0c\u9700\u8981\u5efa\u7acb\u53ef\u8bbf\u95ee\u7684\u663e\u5f0f\u4e16\u754c\u6a21\u578b\u6765\u5bf9\u9f50\u4eba\u7c7b\u671f\u671b", "motivation": "\u5f53\u524d\u5177\u8eabAI\u7814\u7a76\u8fc7\u4e8e\u5173\u6ce8\u5f62\u5f0f\u5316\u9a8c\u8bc1\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u6a21\u578b\u53ef\u9884\u6d4b\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u7684\u52a8\u6001\u6027\u3001\u6a21\u7cca\u6027\u548c\u4e3b\u89c2\u6027\u3002\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\uff0c\u53ef\u9760\u6027\u662f\u60c5\u5883\u51b3\u5b9a\u7684\uff0c\u53ea\u6709\u5728\u4e0e\u4eba\u7c7b\u76ee\u6807\u548c\u671f\u671b\u76f8\u5173\u65f6\u624d\u6709\u610f\u4e49", "method": "\u91c7\u7528\u6839\u672c\u4e0d\u540c\u7684\u65b9\u6cd5\uff1a\u5f3a\u8c03\u5efa\u7acb\u548c\u66f4\u65b0\u53ef\u8bbf\u95ee\u7684\"\u663e\u5f0f\u4e16\u754c\u6a21\u578b\"\uff0c\u8be5\u6a21\u578b\u8868\u793a\u4eba\u7c7b\u4e0eAI\u4e4b\u95f4\u7684\u5171\u540c\u57fa\u7840\uff0c\u7528\u4e8e\u5c06\u673a\u5668\u4eba\u884c\u4e3a\u4e0e\u4eba\u7c7b\u671f\u671b\u5bf9\u9f50\u3002\u5173\u6ce8\u611f\u77e5\u3001\u89e3\u91ca\u548c\u54cd\u5e94\u4eba\u7c7b\u610f\u56fe\u7684\u80fd\u529b", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u4eba\u7c7b\u4e3a\u4e2d\u5fc3\u7684\u53ef\u9760\u5177\u8eabAI\u6846\u67b6\uff0c\u5f3a\u8c03\u5728\u52a8\u6001\u3001\u6a21\u7cca\u7684\u4eba\u7c7b\u73af\u5883\u4e2d\uff0c\u53ef\u9760\u6027\u9700\u8981\u901a\u8fc7\u5efa\u7acb\u5171\u540c\u7406\u89e3\u548c\u671f\u671b\u5bf9\u9f50\u6765\u5b9e\u73b0\uff0c\u800c\u4e0d\u662f\u5355\u7eaf\u8ffd\u6c42\u5f62\u5f0f\u5316\u9a8c\u8bc1", "conclusion": "\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u5177\u8eabAI\u9700\u8981\u6839\u672c\u6027\u7684\u65b9\u6cd5\u8f6c\u53d8\uff0c\u4ece\u8ffd\u6c42\u6a21\u578b\u53ef\u9884\u6d4b\u6027\u8f6c\u5411\u5efa\u7acb\u53ef\u8bbf\u95ee\u7684\u663e\u5f0f\u4e16\u754c\u6a21\u578b\u6765\u5bf9\u9f50\u4eba\u7c7b\u671f\u671b\uff0c\u5f3a\u8c03\u53ef\u9760\u6027\u662f\u60c5\u5883\u51b3\u5b9a\u4e14\u4e0e\u4eba\u7c7b\u76ee\u6807\u76f8\u5173\u7684"}}
{"id": "2601.01171", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01171", "abs": "https://arxiv.org/abs/2601.01171", "authors": ["Serge Sharoff", "John Baker", "David Francis Hunt", "Alan Simpson"], "title": "Almost Clinical: Linguistic properties of synthetic electronic health records", "comment": null, "summary": "This study evaluates the linguistic and clinical suitability of synthetic electronic health records (EHRs) in the field of mental health. First, we describe the rationale and the methodology for creating the synthetic corpus. Second, we assess agency, modality, and information flow across four clinical genres (Assessments, Correspondence, Referrals and Care plans) to understand how LLMs grammatically construct medical authority and patient agency through linguistic choices. While LLMs produce coherent, terminology-appropriate texts that approximate clinical practice, systematic divergences remain, including registerial shifts, insufficient clinical specificity, and inaccuracies in medication use and diagnostic procedures.", "AI": {"tldr": "\u8bc4\u4f30\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u5408\u6210\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u8bed\u8a00\u5b66\u548c\u4e34\u5e8a\u9002\u7528\u6027\uff0c\u53d1\u73b0LLM\u80fd\u751f\u6210\u8fde\u8d2f\u3001\u672f\u8bed\u9002\u5f53\u7684\u6587\u672c\uff0c\u4f46\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02", "motivation": "\u8bc4\u4f30\u5408\u6210\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u9002\u7528\u6027\uff0c\u4e86\u89e3LLM\u5982\u4f55\u901a\u8fc7\u8bed\u8a00\u9009\u62e9\u6784\u5efa\u533b\u7597\u6743\u5a01\u548c\u60a3\u8005\u80fd\u52a8\u6027", "method": "\u9996\u5148\u63cf\u8ff0\u521b\u5efa\u5408\u6210\u8bed\u6599\u5e93\u7684\u7406\u8bba\u57fa\u7840\u548c\u65b9\u6cd5\u8bba\uff0c\u7136\u540e\u8bc4\u4f30\u56db\u79cd\u4e34\u5e8a\u4f53\u88c1\uff08\u8bc4\u4f30\u3001\u901a\u4fe1\u3001\u8f6c\u8bca\u548c\u62a4\u7406\u8ba1\u5212\uff09\u4e2d\u7684\u80fd\u52a8\u6027\u3001\u6a21\u6001\u548c\u4fe1\u606f\u6d41", "result": "LLM\u80fd\u751f\u6210\u8fde\u8d2f\u3001\u672f\u8bed\u9002\u5f53\u7684\u6587\u672c\uff0c\u8fd1\u4f3c\u4e34\u5e8a\u5b9e\u8df5\uff0c\u4f46\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u5305\u62ec\u8bed\u57df\u8f6c\u6362\u3001\u4e34\u5e8a\u7279\u5f02\u6027\u4e0d\u8db3\u3001\u836f\u7269\u4f7f\u7528\u548c\u8bca\u65ad\u7a0b\u5e8f\u4e0d\u51c6\u786e", "conclusion": "\u5408\u6210\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u7cfb\u7edf\u6027\u5dee\u5f02\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4e34\u5e8a\u7279\u5f02\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762"}}
{"id": "2601.01726", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2601.01726", "abs": "https://arxiv.org/abs/2601.01726", "authors": ["Wenhui Chu", "Aobo Jin", "Hardik A. Gohel"], "title": "Simulations and Advancements in MRI-Guided Power-Driven Ferric Tools for Wireless Therapeutic Interventions", "comment": "10 pages, 7 figures", "summary": "Designing a robotic system that functions effectively within the specific environment of a Magnetic Resonance Imaging (MRI) scanner requires solving numerous technical issues, such as maintaining the robot's precision and stability under strong magnetic fields. This research focuses on enhancing MRI's role in medical imaging, especially in its application to guide intravascular interventions using robot-assisted devices. A newly developed computational system is introduced, designed for seamless integration with the MRI scanner, including a computational unit and user interface. This system processes MR images to delineate the vascular network, establishing virtual paths and boundaries within vessels to prevent procedural damage. Key findings reveal the system's capability to create tailored magnetic field gradient patterns for device control, considering the vessel's geometry and safety norms, and adapting to different blood flow characteristics for finer navigation. Additionally, the system's modeling aspect assesses the safety and feasibility of navigating pre-set vascular paths. Conclusively, this system, based on the Qt framework and C/C++, with specialized software modules, represents a major step forward in merging imaging technology with robotic aid, significantly enhancing precision and safety in intravascular procedures.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e0eMRI\u626b\u63cf\u4eea\u96c6\u6210\u7684\u8ba1\u7b97\u7cfb\u7edf\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u8f85\u52a9\u8840\u7ba1\u4ecb\u5165\u624b\u672f\uff0c\u901a\u8fc7\u5904\u7406MR\u56fe\u50cf\u5efa\u7acb\u865a\u62df\u8840\u7ba1\u8def\u5f84\u548c\u8fb9\u754c\uff0c\u751f\u6210\u5b9a\u5236\u78c1\u573a\u68af\u5ea6\u6a21\u5f0f\u63a7\u5236\u8bbe\u5907\uff0c\u63d0\u9ad8\u624b\u672f\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "MRI\u626b\u63cf\u4eea\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u9762\u4e34\u5f3a\u78c1\u573a\u4e0b\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u6311\u6218\uff0c\u9700\u8981\u589e\u5f3aMRI\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u8f85\u52a9\u8bbe\u5907\u5f15\u5bfc\u8840\u7ba1\u4ecb\u5165\u624b\u672f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eQt\u6846\u67b6\u548cC/C++\u7684\u8ba1\u7b97\u7cfb\u7edf\uff0c\u5305\u62ec\u8ba1\u7b97\u5355\u5143\u548c\u7528\u6237\u754c\u9762\uff0c\u4e0eMRI\u626b\u63cf\u4eea\u65e0\u7f1d\u96c6\u6210\u3002\u7cfb\u7edf\u5904\u7406MR\u56fe\u50cf\u4ee5\u63cf\u7ed8\u8840\u7ba1\u7f51\u7edc\uff0c\u5efa\u7acb\u865a\u62df\u8def\u5f84\u548c\u8fb9\u754c\uff0c\u751f\u6210\u5b9a\u5236\u78c1\u573a\u68af\u5ea6\u6a21\u5f0f\u63a7\u5236\u8bbe\u5907\uff0c\u8003\u8651\u8840\u7ba1\u51e0\u4f55\u5f62\u72b6\u3001\u5b89\u5168\u89c4\u8303\u548c\u8840\u6d41\u7279\u6027\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u521b\u5efa\u9488\u5bf9\u8840\u7ba1\u51e0\u4f55\u5f62\u72b6\u548c\u5b89\u5168\u89c4\u8303\u7684\u5b9a\u5236\u78c1\u573a\u68af\u5ea6\u6a21\u5f0f\uff0c\u9002\u5e94\u4e0d\u540c\u8840\u6d41\u7279\u6027\u5b9e\u73b0\u7cbe\u7ec6\u5bfc\u822a\u3002\u5efa\u6a21\u65b9\u9762\u8bc4\u4f30\u9884\u8bbe\u8840\u7ba1\u8def\u5f84\u7684\u5b89\u5168\u6027\u548c\u53ef\u884c\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u8840\u7ba1\u4ecb\u5165\u624b\u672f\u7684\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4ee3\u8868\u4e86\u6210\u50cf\u6280\u672f\u4e0e\u673a\u5668\u4eba\u8f85\u52a9\u878d\u5408\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u57fa\u4e8eQt\u6846\u67b6\u548cC/C++\u5f00\u53d1\uff0c\u5177\u6709\u4e13\u95e8\u8f6f\u4ef6\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8840\u7ba1\u4ecb\u5165\u624b\u672f\u7684\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2601.01225", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01225", "abs": "https://arxiv.org/abs/2601.01225", "authors": ["Hezam Albaqami", "Muhammad Asif Ayub", "Nasir Ahmad", "Yaseen Ahmad", "Mohammed M. Alqahtani", "Abdullah M. Algamdi", "Almoaid A. Owaidah", "Kashif Ahmad"], "title": "Stylometry Analysis of Human and Machine Text for Academic Integrity", "comment": "16 pages, 9 tables, 3 figures", "summary": "This work addresses critical challenges to academic integrity, including plagiarism, fabrication, and verification of authorship of educational content, by proposing a Natural Language Processing (NLP)-based framework for authenticating students' content through author attribution and style change detection. Despite some initial efforts, several aspects of the topic are yet to be explored. In contrast to existing solutions, the paper provides a comprehensive analysis of the topic by targeting four relevant tasks, including (i) classification of human and machine text, (ii) differentiating in single and multi-authored documents, (iii) author change detection within multi-authored documents, and (iv) author recognition in collaboratively produced documents. The solutions proposed for the tasks are evaluated on two datasets generated with Gemini using two different prompts, including a normal and a strict set of instructions. During experiments, some reduction in the performance of the proposed solutions is observed on the dataset generated through the strict prompt, demonstrating the complexities involved in detecting machine-generated text with cleverly crafted prompts. The generated datasets, code, and other relevant materials are made publicly available on GitHub, which are expected to provide a baseline for future research in the domain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u57fa\u4e8eNLP\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f5c\u8005\u5f52\u5c5e\u548c\u98ce\u683c\u53d8\u5316\u68c0\u6d4b\u6765\u9a8c\u8bc1\u5b66\u751f\u5185\u5bb9\u7684\u771f\u5b9e\u6027\uff0c\u89e3\u51b3\u5b66\u672f\u8bda\u4fe1\u95ee\u9898\uff0c\u5305\u62ec\u4eba\u673a\u6587\u672c\u5206\u7c7b\u3001\u5355/\u591a\u4f5c\u8005\u533a\u5206\u3001\u591a\u4f5c\u8005\u6587\u6863\u4e2d\u7684\u4f5c\u8005\u53d8\u5316\u68c0\u6d4b\u548c\u534f\u4f5c\u6587\u6863\u4e2d\u7684\u4f5c\u8005\u8bc6\u522b\u7b49\u56db\u4e2a\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u5b66\u672f\u8bda\u4fe1\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5305\u62ec\u6284\u88ad\u3001\u634f\u9020\u548c\u6559\u80b2\u5185\u5bb9\u4f5c\u8005\u8eab\u4efd\u9a8c\u8bc1\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5c1a\u4e0d\u5b8c\u5584\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u5206\u6790\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u6846\u67b6\uff0c\u9488\u5bf9\u56db\u4e2a\u76f8\u5173\u4efb\u52a1\uff1a\u4eba\u673a\u6587\u672c\u5206\u7c7b\u3001\u5355/\u591a\u4f5c\u8005\u533a\u5206\u3001\u591a\u4f5c\u8005\u6587\u6863\u4e2d\u7684\u4f5c\u8005\u53d8\u5316\u68c0\u6d4b\u3001\u534f\u4f5c\u6587\u6863\u4e2d\u7684\u4f5c\u8005\u8bc6\u522b\u3002\u4f7f\u7528Gemini\u751f\u6210\u7684\u4e24\u4e2a\u6570\u636e\u96c6\uff08\u666e\u901a\u548c\u4e25\u683c\u6307\u4ee4\uff09\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u4e25\u683c\u6307\u4ee4\u751f\u6210\u7684\u6570\u636e\u96c6\u4e0a\u89c2\u5bdf\u5230\u6027\u80fd\u4e0b\u964d\uff0c\u8868\u660e\u68c0\u6d4b\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u590d\u6742\u6027\u3002\u516c\u5f00\u63d0\u4f9b\u4e86\u751f\u6210\u7684\u6570\u636e\u96c6\u3001\u4ee3\u7801\u548c\u76f8\u5173\u6750\u6599\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b66\u672f\u8bda\u4fe1\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u5168\u9762\u7684NLP\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u68c0\u6d4b\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u673a\u5668\u751f\u6210\u6587\u672c\u7684\u6311\u6218\uff0c\u516c\u5f00\u8d44\u6e90\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u672a\u6765\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2601.01762", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01762", "abs": "https://arxiv.org/abs/2601.01762", "authors": ["Yanhao Wu", "Haoyang Zhang", "Fei He", "Rui Wu", "Congpei Qiu", "Liang Gao", "Wei Ke", "Tong Zhang"], "title": "AlignDrive: Aligned Lateral-Longitudinal Planning for End-to-End Autonomous Driving", "comment": "underreview", "summary": "End-to-end autonomous driving has rapidly progressed, enabling joint perception and planning in complex environments. In the planning stage, state-of-the-art (SOTA) end-to-end autonomous driving models decouple planning into parallel lateral and longitudinal predictions. While effective, this parallel design can lead to i) coordination failures between the planned path and speed, and ii) underutilization of the drive path as a prior for longitudinal planning, thus redundantly encoding static information. To address this, we propose a novel cascaded framework that explicitly conditions longitudinal planning on the drive path, enabling coordinated and collision-aware lateral and longitudinal planning. Specifically, we introduce a path-conditioned formulation that explicitly incorporates the drive path into longitudinal planning. Building on this, the model predicts longitudinal displacements along the drive path rather than full 2D trajectory waypoints. This design simplifies longitudinal reasoning and more tightly couples it with lateral planning. Additionally, we introduce a planning-oriented data augmentation strategy that simulates rare safety-critical events, such as vehicle cut-ins, by adding agents and relabeling longitudinal targets to avoid collision. Evaluated on the challenging Bench2Drive benchmark, our method sets a new SOTA, achieving a driving score of 89.07 and a success rate of 73.18%, demonstrating significantly improved coordination and safety", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ea7\u8054\u6846\u67b6\uff0c\u5c06\u7eb5\u5411\u89c4\u5212\u663e\u5f0f\u5730\u5efa\u7acb\u5728\u9a7e\u9a76\u8def\u5f84\u4e0a\uff0c\u5b9e\u73b0\u534f\u8c03\u7684\u6a2a\u5411\u548c\u7eb5\u5411\u89c4\u5212\uff0c\u5728Bench2Drive\u57fa\u51c6\u4e0a\u8fbe\u5230\u65b0SOTA", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u5c06\u89c4\u5212\u89e3\u8026\u4e3a\u5e76\u884c\u7684\u6a2a\u5411\u548c\u7eb5\u5411\u9884\u6d4b\uff0c\u8fd9\u4f1a\u5bfc\u81f4\uff1a1) \u89c4\u5212\u8def\u5f84\u4e0e\u901f\u5ea6\u534f\u8c03\u5931\u8d25\uff1b2) \u672a\u5145\u5206\u5229\u7528\u9a7e\u9a76\u8def\u5f84\u4f5c\u4e3a\u7eb5\u5411\u89c4\u5212\u7684\u5148\u9a8c\uff0c\u5bfc\u81f4\u9759\u6001\u4fe1\u606f\u5197\u4f59\u7f16\u7801", "method": "\u63d0\u51fa\u7ea7\u8054\u6846\u67b6\uff0c\u5c06\u7eb5\u5411\u89c4\u5212\u663e\u5f0f\u5730\u5efa\u7acb\u5728\u9a7e\u9a76\u8def\u5f84\u4e0a\uff1a1) \u8def\u5f84\u6761\u4ef6\u5316\u516c\u5f0f\uff0c\u5c06\u9a7e\u9a76\u8def\u5f84\u663e\u5f0f\u7eb3\u5165\u7eb5\u5411\u89c4\u5212\uff1b2) \u6cbf\u9a7e\u9a76\u8def\u5f84\u9884\u6d4b\u7eb5\u5411\u4f4d\u79fb\u800c\u975e\u5b8c\u65742D\u8f68\u8ff9\u70b9\uff1b3) \u89c4\u5212\u5bfc\u5411\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\uff0c\u6a21\u62df\u8f66\u8f86\u5207\u5165\u7b49\u5b89\u5168\u5173\u952e\u4e8b\u4ef6", "result": "\u5728Bench2Drive\u57fa\u51c6\u4e0a\u8fbe\u5230\u65b0SOTA\uff1a\u9a7e\u9a76\u5206\u657089.07\uff0c\u6210\u529f\u738773.18%\uff0c\u663e\u8457\u6539\u5584\u4e86\u534f\u8c03\u6027\u548c\u5b89\u5168\u6027", "conclusion": "\u901a\u8fc7\u5c06\u7eb5\u5411\u89c4\u5212\u663e\u5f0f\u5730\u5efa\u7acb\u5728\u9a7e\u9a76\u8def\u5f84\u4e0a\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6a2a\u5411\u548c\u7eb5\u5411\u89c4\u5212\u534f\u8c03\uff0c\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd"}}
{"id": "2601.01244", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01244", "abs": "https://arxiv.org/abs/2601.01244", "authors": ["Zsolt Csibi", "Bence Gy\u00f6rgy Gortka", "Natabara Gy\u00f6ngy\u00f6ssy", "Korn\u00e9l Nagy", "D\u00e1vid M\u00e1rk Nemeskey", "Martin Sallai", "Andr\u00e1s Simonyi", "Andr\u00e1s M\u00e1rk Szekeres", "G\u00e1bor Palk\u00f3"], "title": "Racka: Efficient Hungarian LLM Adaptation on Academic Infrastructure", "comment": "18 pages, 1 figures. To appear in the XXII. Magyar Sz\u00e1m\u00edt\u00f3g\u00e9pes Nyelv\u00e9szeti Konferencia (MSZNY 2026)", "summary": "We present Racka, a lightweight, continually pretrained large language model designed to bridge the resource gap between Hungarian and high-resource languages such as English and German. Racka employs parameter-efficient continual pretraining via Low-Rank Adaptation (LoRA) on a Qwen-3 4B backbone, making the recipe practical on A100 (40GB)-based HPC clusters with low inter-node bandwidth. To better match the training distribution, we replace and adapt the tokenizer, achieving substantially improved tokenization fertility for Hungarian while maintaining competitive performance in English and German. The model is trained on 160B subword tokens drawn from a mixture of internet and high-quality curated sources, with a composition of 44% Hungarian, 24% English, 21% German, and 11% code. This data mix is chosen to mitigate catastrophic forgetting and preserve high-resource language capabilities during continual pretraining. Our preliminary results indicate modest but stable results in language adaptation.", "AI": {"tldr": "Racka\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u6301\u7eed\u9884\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u4e3a\u5308\u7259\u5229\u8bed\u8bbe\u8ba1\uff0c\u65e8\u5728\u7f29\u5c0f\u5308\u7259\u5229\u8bed\u4e0e\u82f1\u8bed\u3001\u5fb7\u8bed\u7b49\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e4b\u95f4\u7684\u8d44\u6e90\u5dee\u8ddd\u3002", "motivation": "\u5308\u7259\u5229\u8bed\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u4e0e\u82f1\u8bed\u3001\u5fb7\u8bed\u7b49\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u8d44\u6e90\u5dee\u8ddd\u3002\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u5308\u7259\u5229\u8bed\u7684\u652f\u6301\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u5308\u7259\u5229\u8bed\u8fdb\u884c\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u9ad8\u8d44\u6e90\u8bed\u8a00\u7684\u6027\u80fd\u3002", "method": "\u91c7\u7528\u57fa\u4e8eQwen-3 4B\u9aa8\u5e72\u7f51\u7edc\u7684\u53c2\u6570\u9ad8\u6548\u6301\u7eed\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u4f7f\u7528LoRA\uff08\u4f4e\u79e9\u9002\u5e94\uff09\u6280\u672f\u3002\u66ff\u6362\u5e76\u9002\u914d\u5206\u8bcd\u5668\u4ee5\u6539\u5584\u5308\u7259\u5229\u8bed\u7684\u5206\u8bcd\u6548\u7387\u3002\u5728160B\u5b50\u8bcd\u6807\u8bb0\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\u4e3a\uff1a44%\u5308\u7259\u5229\u8bed\u300124%\u82f1\u8bed\u300121%\u5fb7\u8bed\u300111%\u4ee3\u7801\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u5728\u8bed\u8a00\u9002\u5e94\u65b9\u9762\u53d6\u5f97\u4e86\u7a33\u5b9a\u4f46\u9002\u5ea6\u7684\u6210\u679c\u3002\u5308\u7259\u5229\u8bed\u7684\u5206\u8bcd\u6548\u7387\u5f97\u5230\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u82f1\u8bed\u548c\u5fb7\u8bed\u7684\u7ade\u4e89\u6027\u6027\u80fd\u3002", "conclusion": "Racka\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6210\u529f\u7f29\u5c0f\u4e86\u5308\u7259\u5229\u8bed\u4e0e\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e4b\u95f4\u7684\u8d44\u6e90\u5dee\u8ddd\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01822", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01822", "abs": "https://arxiv.org/abs/2601.01822", "authors": ["Shiyong Meng", "Tao Zou", "Bolei Chen", "Chaoxu Mu", "Jianxin Wang"], "title": "DisCo-FLoc: Using Dual-Level Visual-Geometric Contrasts to Disambiguate Depth-Aware Visual Floorplan Localization", "comment": "7 pages, 4 figures", "summary": "Since floorplan data is readily available, long-term persistent, and robust to changes in visual appearance, visual Floorplan Localization (FLoc) has garnered significant attention. Existing methods either ingeniously match geometric priors or utilize sparse semantics to reduce FLoc uncertainty. However, they still suffer from ambiguous FLoc caused by repetitive structures within minimalist floorplans. Moreover, expensive but limited semantic annotations restrict their applicability. To address these issues, we propose DisCo-FLoc, which utilizes dual-level visual-geometric Contrasts to Disambiguate depth-aware visual Floc, without requiring additional semantic labels. Our solution begins with a ray regression predictor tailored for ray-casting-based FLoc, predicting a series of FLoc candidates using depth estimation expertise. In addition, a novel contrastive learning method with position-level and orientation-level constraints is proposed to strictly match depth-aware visual features with the corresponding geometric structures in the floorplan. Such matches can effectively eliminate FLoc ambiguity and select the optimal imaging pose from FLoc candidates. Exhaustive comparative studies on two standard visual Floc benchmarks demonstrate that our method outperforms the state-of-the-art semantic-based method, achieving significant improvements in both robustness and accuracy.", "AI": {"tldr": "DisCo-FLoc\uff1a\u901a\u8fc7\u53cc\u5c42\u6b21\u89c6\u89c9-\u51e0\u4f55\u5bf9\u6bd4\u5b66\u4e60\u6d88\u9664\u697c\u5c42\u5e73\u9762\u56fe\u5b9a\u4f4d\u4e2d\u7684\u6b67\u4e49\uff0c\u65e0\u9700\u989d\u5916\u8bed\u4e49\u6807\u6ce8\uff0c\u5728\u6df1\u5ea6\u611f\u77e5\u89c6\u89c9\u7279\u5f81\u4e0e\u51e0\u4f55\u7ed3\u6784\u5339\u914d\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u697c\u5c42\u5e73\u9762\u56fe\u5b9a\u4f4d\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u7b80\u7ea6\u697c\u5c42\u5e73\u9762\u56fe\u4e2d\u7684\u91cd\u590d\u7ed3\u6784\u5bfc\u81f4\u5b9a\u4f4d\u6b67\u4e49\uff1b2\uff09\u6602\u8d35\u4e14\u6709\u9650\u7684\u8bed\u4e49\u6807\u6ce8\u9650\u5236\u4e86\u65b9\u6cd5\u9002\u7528\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bed\u4e49\u6807\u7b7e\u5c31\u80fd\u6d88\u9664\u6b67\u4e49\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDisCo-FLoc\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u5149\u7ebf\u56de\u5f52\u9884\u6d4b\u5668\u4e3a\u57fa\u4e8e\u5149\u7ebf\u6295\u5c04\u7684\u5b9a\u4f4d\u751f\u6210\u5019\u9009\u4f4d\u7f6e\uff1b2\uff09\u63d0\u51fa\u65b0\u9896\u7684\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u5305\u542b\u4f4d\u7f6e\u7ea7\u548c\u65b9\u5411\u7ea7\u7ea6\u675f\uff0c\u4e25\u683c\u5339\u914d\u6df1\u5ea6\u611f\u77e5\u89c6\u89c9\u7279\u5f81\u4e0e\u697c\u5c42\u5e73\u9762\u56fe\u4e2d\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u4ece\u800c\u6d88\u9664\u6b67\u4e49\u5e76\u9009\u62e9\u6700\u4f73\u6210\u50cf\u59ff\u6001\u3002", "result": "\u5728\u4e24\u4e2a\u6807\u51c6\u89c6\u89c9\u697c\u5c42\u5e73\u9762\u56fe\u5b9a\u4f4d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u8bed\u4e49\u7684\u65b9\u6cd5\uff0c\u5728\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "DisCo-FLoc\u901a\u8fc7\u53cc\u5c42\u6b21\u89c6\u89c9-\u51e0\u4f55\u5bf9\u6bd4\u5b66\u4e60\u6709\u6548\u89e3\u51b3\u4e86\u697c\u5c42\u5e73\u9762\u56fe\u5b9a\u4f4d\u4e2d\u7684\u6b67\u4e49\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u8bed\u4e49\u6807\u6ce8\uff0c\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u6df1\u5ea6\u611f\u77e5\u7279\u5f81\u4e0e\u51e0\u4f55\u7ed3\u6784\u5339\u914d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2601.01266", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01266", "abs": "https://arxiv.org/abs/2601.01266", "authors": ["Rhitabrat Pokharel", "Hamid Hassanzadeh", "Ameeta Agrawal"], "title": "From Policy to Logic for Efficient and Interpretable Coverage Assessment", "comment": "Accepted at AIMedHealth @ AAAI 2026", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u4e0e\u7b26\u53f7\u63a8\u7406\u7684\u6df7\u5408\u7cfb\u7edf\uff0c\u7528\u4e8e\u533b\u7597\u4fdd\u9669\u653f\u7b56\u5206\u6790\uff0c\u5728\u51cf\u5c1144%\u63a8\u7406\u6210\u672c\u7684\u540c\u65f6\u63d0\u53474.5% F1\u5206\u6570", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u6cd5\u5f8b\u548c\u653f\u7b56\u8bed\u8a00\u65f6\u5b58\u5728\u5e7b\u89c9\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u533b\u7597\u4fdd\u9669\u653f\u7b56\u5ba1\u67e5\u8fd9\u79cd\u9700\u8981\u9ad8\u5ea6\u51c6\u786e\u6027\u7684\u9886\u57df\uff0c\u9700\u8981\u652f\u6301\u4eba\u7c7b\u4e13\u5bb6\u8fdb\u884c\u66f4\u9ad8\u6548\u548c\u53ef\u89e3\u91ca\u7684\u653f\u7b56\u89e3\u8bfb", "method": "\u7ed3\u5408\u8986\u76d6\u611f\u77e5\u68c0\u7d22\u5668\u4e0e\u57fa\u4e8e\u7b26\u53f7\u7684\u89c4\u5219\u63a8\u7406\uff0c\u63d0\u53d6\u76f8\u5173\u653f\u7b56\u8bed\u8a00\uff0c\u7ec4\u7ec7\u6210\u660e\u786e\u7684\u4e8b\u5b9e\u548c\u89c4\u5219\uff0c\u751f\u6210\u53ef\u5ba1\u8ba1\u7684\u63a8\u7406\u4f9d\u636e\uff0c\u51cf\u5c11LLM\u63a8\u7406\u6b21\u6570", "result": "\u5b9e\u73b0\u4e8644%\u7684\u63a8\u7406\u6210\u672c\u964d\u4f4e\u548c4.5%\u7684F1\u5206\u6570\u63d0\u5347\uff0c\u5728\u6548\u7387\u548c\u6548\u679c\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb", "conclusion": "\u8be5\u6df7\u5408\u65b9\u6cd5\u5728\u533b\u7597\u4fdd\u9669\u653f\u7b56\u5206\u6790\u4e2d\u65e2\u63d0\u9ad8\u4e86\u6548\u7387\u53c8\u589e\u5f3a\u4e86\u53ef\u9760\u6027\uff0c\u4e3a\u4eba\u7c7b\u4e13\u5bb6\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u653f\u7b56\u89e3\u8bfb\u652f\u6301"}}
{"id": "2601.01872", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01872", "abs": "https://arxiv.org/abs/2601.01872", "authors": ["Hongbo Duan", "Shangyi Luo", "Zhiyuan Deng", "Yanbo Chen", "Yuanhao Chiang", "Yi Liu", "Fangming Liu", "Xueqian Wang"], "title": "CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios", "comment": "Accepted by IEEE Robotics and Automation Letters (RA-L)", "summary": "Autonomous language-guided navigation in large-scale outdoor environments remains a key challenge in mobile robotics, due to difficulties in semantic reasoning, dynamic conditions, and long-term stability. We propose CausalNav, the first scene graph-based semantic navigation framework tailored for dynamic outdoor environments. We construct a multi-level semantic scene graph using LLMs, referred to as the Embodied Graph, that hierarchically integrates coarse-grained map data with fine-grained object entities. The constructed graph serves as a retrievable knowledge base for Retrieval-Augmented Generation (RAG), enabling semantic navigation and long-range planning under open-vocabulary queries. By fusing real-time perception with offline map data, the Embodied Graph supports robust navigation across varying spatial granularities in dynamic outdoor environments. Dynamic objects are explicitly handled in both the scene graph construction and hierarchical planning modules. The Embodied Graph is continuously updated within a temporal window to reflect environmental changes and support real-time semantic navigation. Extensive experiments in both simulation and real-world settings demonstrate superior robustness and efficiency.", "AI": {"tldr": "CausalNav\uff1a\u9996\u4e2a\u57fa\u4e8e\u573a\u666f\u56fe\u7684\u8bed\u4e49\u5bfc\u822a\u6846\u67b6\uff0c\u4e13\u4e3a\u52a8\u6001\u6237\u5916\u73af\u5883\u8bbe\u8ba1\uff0c\u901a\u8fc7LLM\u6784\u5efa\u591a\u5c42\u7ea7\u8bed\u4e49\u573a\u666f\u56fe\uff08Embodied Graph\uff09\uff0c\u7ed3\u5408\u5b9e\u65f6\u611f\u77e5\u4e0e\u79bb\u7ebf\u5730\u56fe\u6570\u636e\uff0c\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\u4e0b\u7684\u8bed\u4e49\u5bfc\u822a\u548c\u957f\u8ddd\u79bb\u89c4\u5212\u3002", "motivation": "\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u8bed\u8a00\u5f15\u5bfc\u5bfc\u822a\u9762\u4e34\u8bed\u4e49\u63a8\u7406\u56f0\u96be\u3001\u52a8\u6001\u6761\u4ef6\u53d8\u5316\u548c\u957f\u671f\u7a33\u5b9a\u6027\u7b49\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u590d\u6742\u95ee\u9898\u3002", "method": "\u4f7f\u7528LLM\u6784\u5efa\u591a\u5c42\u7ea7\u8bed\u4e49\u573a\u666f\u56fe\uff08Embodied Graph\uff09\uff0c\u5206\u5c42\u6574\u5408\u7c97\u7c92\u5ea6\u5730\u56fe\u6570\u636e\u548c\u7ec6\u7c92\u5ea6\u7269\u4f53\u5b9e\u4f53\uff1b\u5c06\u6784\u5efa\u7684\u56fe\u4f5c\u4e3a\u53ef\u68c0\u7d22\u77e5\u8bc6\u5e93\u7528\u4e8eRAG\uff1b\u5728\u573a\u666f\u56fe\u6784\u5efa\u548c\u5206\u5c42\u89c4\u5212\u6a21\u5757\u4e2d\u663e\u5f0f\u5904\u7406\u52a8\u6001\u7269\u4f53\uff1b\u5728\u65f6\u95f4\u7a97\u53e3\u5185\u6301\u7eed\u66f4\u65b0\u573a\u666f\u56fe\u4ee5\u53cd\u6620\u73af\u5883\u53d8\u5316\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u6237\u5916\u73af\u5883\u4e2d\u5177\u6709\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "CausalNav\u662f\u9996\u4e2a\u9488\u5bf9\u52a8\u6001\u6237\u5916\u73af\u5883\u7684\u573a\u666f\u56fe\u8bed\u4e49\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u7ea7\u8bed\u4e49\u573a\u666f\u56fe\u548cRAG\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6237\u5916\u73af\u5883\u4e2d\u7684\u8bed\u4e49\u5bfc\u822a\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5f00\u653e\u8bcd\u6c47\u67e5\u8be2\u4e0b\u7684\u9c81\u68d2\u5bfc\u822a\u3002"}}
{"id": "2601.01280", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01280", "abs": "https://arxiv.org/abs/2601.01280", "authors": ["Sen Hu", "Yuxiang Wei", "Jiaxin Ran", "Zhiyuan Yao", "Lei Zou"], "title": "Does Memory Need Graphs? A Unified Framework and Empirical Analysis for Long-Term Dialog Memory", "comment": null, "summary": "Graph structures are increasingly used in dialog memory systems, but empirical findings on their effectiveness remain inconsistent, making it unclear which design choices truly matter. We present an experimental, system-oriented analysis of long-term dialog memory architectures. We introduce a unified framework that decomposes dialog memory systems into core components and supports both graph-based and non-graph approaches. Under this framework, we conduct controlled, stage-wise experiments on LongMemEval and HaluMem, comparing common design choices in memory representation, organization, maintenance, and retrieval. Our results show that many performance differences are driven by foundational system settings rather than specific architectural innovations. Based on these findings, we identify stable and reliable strong baselines for future dialog memory research.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u5206\u6790\u5bf9\u8bdd\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u53d1\u73b0\u6027\u80fd\u5dee\u5f02\u4e3b\u8981\u6e90\u4e8e\u57fa\u7840\u7cfb\u7edf\u8bbe\u7f6e\u800c\u975e\u7279\u5b9a\u67b6\u6784\u521b\u65b0\uff0c\u5e76\u786e\u5b9a\u4e86\u7a33\u5b9a\u53ef\u9760\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u56fe\u7ed3\u6784\u5728\u5bf9\u8bdd\u8bb0\u5fc6\u7cfb\u7edf\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5b9e\u8bc1\u7814\u7a76\u7ed3\u679c\u4e0d\u4e00\u81f4\uff0c\u4e0d\u6e05\u695a\u54ea\u4e9b\u8bbe\u8ba1\u9009\u62e9\u771f\u6b63\u91cd\u8981\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u5b9e\u9a8c\u5206\u6790\u6765\u6f84\u6e05\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u5c06\u5bf9\u8bdd\u8bb0\u5fc6\u7cfb\u7edf\u5206\u89e3\u4e3a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u652f\u6301\u56fe\u548c\u975e\u56fe\u65b9\u6cd5\uff0c\u5728LongMemEval\u548cHaluMem\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u53d7\u63a7\u7684\u5206\u9636\u6bb5\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u8bb0\u5fc6\u8868\u793a\u3001\u7ec4\u7ec7\u3001\u7ef4\u62a4\u548c\u68c0\u7d22\u7b49\u5e38\u89c1\u8bbe\u8ba1\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8bb8\u591a\u6027\u80fd\u5dee\u5f02\u662f\u7531\u57fa\u7840\u7cfb\u7edf\u8bbe\u7f6e\u9a71\u52a8\u7684\uff0c\u800c\u975e\u7279\u5b9a\u7684\u67b6\u6784\u521b\u65b0\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u786e\u5b9a\u4e86\u672a\u6765\u5bf9\u8bdd\u8bb0\u5fc6\u7814\u7a76\u7684\u7a33\u5b9a\u53ef\u9760\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5bf9\u8bdd\u8bb0\u5fc6\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u7684\u5206\u6790\u6846\u67b6\u548c\u53ef\u9760\u7684\u57fa\u7ebf\uff0c\u6709\u52a9\u4e8e\u672a\u6765\u7814\u7a76\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u67b6\u6784\u521b\u65b0\u7684\u5b9e\u9645\u4ef7\u503c\u3002"}}
{"id": "2601.01946", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.01946", "abs": "https://arxiv.org/abs/2601.01946", "authors": ["Sichao Song", "Yuki Okafuji", "Takuya Iwamoto", "Jun Baba", "Hiroshi Ishiguro"], "title": "From Metrics to Meaning: Insights from a Mixed-Methods Field Experiment on Retail Robot Deployment", "comment": null, "summary": "We report a mixed-methods field experiment of a conversational service robot deployed under everyday staffing discretion in a live bedding store. Over 12 days we alternated three conditions--Baseline (no robot), Robot-only, and Robot+Fixture--and video-annotated the service funnel from passersby to purchase. An explanatory sequential design then used six post-experiment staff interviews to interpret the quantitative patterns.\n  Quantitatively, the robot increased stopping per passerby (highest with the fixture), yet clerk-led downstream steps per stopper--clerk approach, store entry, assisted experience, and purchase--decreased. Interviews explained this divergence: clerks avoided interrupting ongoing robot-customer talk, struggled with ambiguous timing amid conversational latency, and noted child-centered attraction that often satisfied curiosity at the doorway. The fixture amplified visibility but also anchored encounters at the threshold, creating a well-defined micro-space where needs could ``close'' without moving inside.\n  We synthesize these strands into an integrative account from the initial show of interest on the part of a customer to their entering the store and derive actionable guidance. The results advance the understanding of interactions between customers, staff members, and the robot and offer practical recommendations for deploying service robots in high-touch retail.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6df7\u5408\u65b9\u6cd5\u5b9e\u5730\u5b9e\u9a8c\uff0c\u5728\u5e8a\u54c1\u5e97\u90e8\u7f72\u5bf9\u8bdd\u670d\u52a1\u673a\u5668\u4eba\uff0c\u53d1\u73b0\u673a\u5668\u4eba\u80fd\u589e\u52a0\u987e\u5ba2\u505c\u7559\u7387\u4f46\u964d\u4f4e\u5e97\u5458\u4e3b\u5bfc\u7684\u540e\u7eed\u670d\u52a1\u8f6c\u5316\uff0c\u63ed\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u65f6\u95f4\u534f\u8c03\u6311\u6218\u548c\u95e8\u69db\u6548\u5e94\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728\u771f\u5b9e\u96f6\u552e\u73af\u5883\u4e2d\u90e8\u7f72\u5bf9\u8bdd\u670d\u52a1\u673a\u5668\u4eba\u65f6\uff0c\u673a\u5668\u4eba\u5982\u4f55\u5f71\u54cd\u987e\u5ba2\u670d\u52a1\u6d41\u7a0b\u4ee5\u53ca\u5e97\u5458\u4e0e\u673a\u5668\u4eba\u7684\u534f\u4f5c\u52a8\u6001\uff0c\u7279\u522b\u662f\u5728\u9ad8\u63a5\u89e6\u96f6\u552e\u573a\u666f\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u5b9e\u5730\u5b9e\u9a8c\u8bbe\u8ba1\uff1a\u5728\u5e8a\u54c1\u5e97\u8fdb\u884c12\u5929\u7684\u73b0\u573a\u5b9e\u9a8c\uff0c\u4ea4\u66ff\u4e09\u79cd\u6761\u4ef6\uff08\u65e0\u673a\u5668\u4eba\u3001\u4ec5\u673a\u5668\u4eba\u3001\u673a\u5668\u4eba+\u5c55\u793a\u88c5\u7f6e\uff09\uff0c\u901a\u8fc7\u89c6\u9891\u6807\u6ce8\u5206\u6790\u670d\u52a1\u6f0f\u6597\u6570\u636e\uff0c\u7136\u540e\u8fdb\u884c\u89e3\u91ca\u6027\u987a\u5e8f\u8bbe\u8ba1\uff0c\u901a\u8fc76\u6b21\u5e97\u5458\u8bbf\u8c08\u89e3\u91ca\u5b9a\u91cf\u6a21\u5f0f\u3002", "result": "\u673a\u5668\u4eba\u589e\u52a0\u4e86\u8def\u8fc7\u987e\u5ba2\u7684\u505c\u7559\u7387\uff08\u5c55\u793a\u88c5\u7f6e\u6548\u679c\u6700\u4f73\uff09\uff0c\u4f46\u5e97\u5458\u4e3b\u5bfc\u7684\u4e0b\u6e38\u670d\u52a1\u6b65\u9aa4\uff08\u5e97\u5458\u63a5\u8fd1\u3001\u8fdb\u5e97\u3001\u534f\u52a9\u4f53\u9a8c\u3001\u8d2d\u4e70\uff09\u5374\u51cf\u5c11\u4e86\u3002\u8bbf\u8c08\u63ed\u793a\uff1a\u5e97\u5458\u907f\u514d\u6253\u65ad\u673a\u5668\u4eba-\u987e\u5ba2\u5bf9\u8bdd\uff0c\u5728\u5bf9\u8bdd\u5ef6\u8fdf\u4e2d\u96be\u4ee5\u628a\u63e1\u65f6\u673a\uff0c\u4e14\u673a\u5668\u4eba\u4e3b\u8981\u5438\u5f15\u513f\u7ae5\u6ee1\u8db3\u95e8\u53e3\u597d\u5947\u5fc3\u3002\u5c55\u793a\u88c5\u7f6e\u589e\u5f3a\u4e86\u53ef\u89c1\u6027\u4f46\u4e5f\u5c06\u4e92\u52a8\u951a\u5b9a\u5728\u95e8\u69db\u5904\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u4ece\u987e\u5ba2\u5174\u8da3\u5c55\u793a\u5230\u8fdb\u5e97\u7684\u6574\u5408\u6027\u89e3\u91ca\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u65f6\u95f4\u534f\u8c03\u6311\u6218\u548c\"\u95e8\u69db\u6548\u5e94\"\uff0c\u4e3a\u9ad8\u63a5\u89e6\u96f6\u552e\u573a\u666f\u4e2d\u670d\u52a1\u673a\u5668\u4eba\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\uff0c\u5f3a\u8c03\u9700\u8981\u4f18\u5316\u4eba\u673a\u4ea4\u63a5\u65f6\u673a\u548c\u7a7a\u95f4\u5e03\u5c40\u8bbe\u8ba1\u3002"}}
{"id": "2601.01299", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2601.01299", "abs": "https://arxiv.org/abs/2601.01299", "authors": ["Ismail Lamaakal", "Chaymae Yahyati", "Yassine Maleh", "Khalid El Makkaoui", "Ibrahim Ouahbi"], "title": "T3C: Test-Time Tensor Compression with Consistency Guarantees", "comment": null, "summary": "We present T3C, a train-once, test-time budget-conditioned compression framework that exposes rank and precision as a controllable deployment knob. T3C combines elastic tensor factorization (maintained up to a maximal rank) with rank-tied mixed-precision quantization and a lightweight controller that maps a latency/energy/size budget token to per-layer rank/bit assignments; the policy snaps to hardware-aligned profiles and is monotone in the budget. A fast, layerwise consistency certificate, computed from spectral proxies and activation statistics, upper-bounds logit drift and regularizes training, yielding a practical reliability signal with negligible overhead. On ImageNet-1k, T3C shifts the vision Pareto frontier: for ResNet-50 at matched accuracy (\\leq 0.5% drop), p50 latency is 1.18ms with a 38MB model, outperforming PTQ-8b (1.44ms, 88MB); for ViT-B/16, T3C reaches 2.30ms p50 with 59MB, improving over strong PTQ/QAT baselines. A single T3C checkpoint therefore provides predictable, certificate-backed accuracy-latency-size trade-offs on demand across devices.", "AI": {"tldr": "T3C\u662f\u4e00\u4e2a\u8bad\u7ec3\u4e00\u6b21\u3001\u6d4b\u8bd5\u65f6\u9884\u7b97\u6761\u4ef6\u5316\u7684\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u5f39\u6027\u5f20\u91cf\u5206\u89e3\u548c\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u5b9e\u73b0\u53ef\u63a7\u5236\u7684\u90e8\u7f72\u8c03\u6574\uff0c\u63d0\u4f9b\u53ef\u9884\u6d4b\u7684\u7cbe\u5ea6-\u5ef6\u8fdf-\u5927\u5c0f\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e3a\u4e0d\u540c\u90e8\u7f72\u573a\u666f\u91cd\u65b0\u8bad\u7ec3\u6216\u8c03\u6574\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u3001\u53ef\u9884\u6d4b\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u65b9\u6848\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6839\u636e\u5b9e\u9645\u90e8\u7f72\u9884\u7b97\uff08\u5ef6\u8fdf/\u80fd\u8017/\u5927\u5c0f\uff09\u52a8\u6001\u8c03\u6574\u538b\u7f29\u7b56\u7565\u7684\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u5f39\u6027\u5f20\u91cf\u5206\u89e3\uff08\u7ef4\u6301\u6700\u5927\u79e9\uff09\u3001\u79e9\u7ed1\u5b9a\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\uff0c\u4ee5\u53ca\u8f7b\u91cf\u7ea7\u63a7\u5236\u5668\u5c06\u9884\u7b97\u4ee4\u724c\u6620\u5c04\u5230\u6bcf\u5c42\u79e9/\u6bd4\u7279\u5206\u914d\u3002\u4f7f\u7528\u57fa\u4e8e\u8c31\u4ee3\u7406\u548c\u6fc0\u6d3b\u7edf\u8ba1\u7684\u5feb\u901f\u5c42\u4e00\u81f4\u6027\u8bc1\u4e66\u6765\u4e0a\u754clogit\u6f02\u79fb\u5e76\u6b63\u5219\u5316\u8bad\u7ec3\u3002", "result": "\u5728ImageNet-1k\u4e0a\uff0cT3C\u663e\u8457\u63a8\u8fdb\u4e86\u89c6\u89c9\u6a21\u578b\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff1aResNet-50\u5728\u7cbe\u5ea6\u635f\u5931\u22640.5%\u65f6\u8fbe\u52301.18ms p50\u5ef6\u8fdf\u548c38MB\u6a21\u578b\u5927\u5c0f\uff0c\u4f18\u4e8ePTQ-8b\uff081.44ms, 88MB\uff09\uff1bViT-B/16\u8fbe\u52302.30ms p50\u5ef6\u8fdf\u548c59MB\u5927\u5c0f\uff0c\u8d85\u8d8a\u5f3aPTQ/QAT\u57fa\u7ebf\u3002", "conclusion": "\u5355\u4e2aT3C\u68c0\u67e5\u70b9\u5373\u53ef\u63d0\u4f9b\u53ef\u9884\u6d4b\u7684\u3001\u8bc1\u4e66\u652f\u6301\u7684\u7cbe\u5ea6-\u5ef6\u8fdf-\u5927\u5c0f\u6743\u8861\uff0c\u80fd\u591f\u6839\u636e\u8bbe\u5907\u9700\u6c42\u6309\u9700\u8c03\u6574\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u4e00\u6b21\u3001\u90e8\u7f72\u7075\u6d3b\u7684\u9ad8\u6548\u6a21\u578b\u538b\u7f29\u6846\u67b6\u3002"}}
{"id": "2601.01948", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01948", "abs": "https://arxiv.org/abs/2601.01948", "authors": ["Zhihao Gu", "Ming Yang", "Difan Zou", "Dong Xu"], "title": "Learning Diffusion Policy from Primitive Skills for Robot Manipulation", "comment": "Accepted to AAAI2026", "summary": "Diffusion policies (DP) have recently shown great promise for generating actions in robotic manipulation. However, existing approaches often rely on global instructions to produce short-term control signals, which can result in misalignment in action generation. We conjecture that the primitive skills, referred to as fine-grained, short-horizon manipulations, such as ``move up'' and ``open the gripper'', provide a more intuitive and effective interface for robot learning. To bridge this gap, we propose SDP, a skill-conditioned DP that integrates interpretable skill learning with conditional action planning. SDP abstracts eight reusable primitive skills across tasks and employs a vision-language model to extract discrete representations from visual observations and language instructions. Based on them, a lightweight router network is designed to assign a desired primitive skill for each state, which helps construct a single-skill policy to generate skill-aligned actions. By decomposing complex tasks into a sequence of primitive skills and selecting a single-skill policy, SDP ensures skill-consistent behavior across diverse tasks. Extensive experiments on two challenging simulation benchmarks and real-world robot deployments demonstrate that SDP consistently outperforms SOTA methods, providing a new paradigm for skill-based robot learning with diffusion policies.", "AI": {"tldr": "SDP\uff1a\u4e00\u79cd\u57fa\u4e8e\u6280\u80fd\u6761\u4ef6\u7684\u6269\u6563\u7b56\u7565\uff0c\u901a\u8fc7\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u7684\u539f\u59cb\u6280\u80fd\u5e8f\u5217\uff0c\u63d0\u5347\u673a\u5668\u4eba\u52a8\u4f5c\u751f\u6210\u7684\u5bf9\u9f50\u6027\u548c\u6027\u80fd", "motivation": "\u73b0\u6709\u6269\u6563\u7b56\u7565\u65b9\u6cd5\u4f9d\u8d56\u5168\u5c40\u6307\u4ee4\u751f\u6210\u77ed\u671f\u63a7\u5236\u4fe1\u53f7\uff0c\u5bb9\u6613\u5bfc\u81f4\u52a8\u4f5c\u751f\u6210\u4e0d\u5bf9\u9f50\u3002\u539f\u59cb\u6280\u80fd\uff08\u5982\"\u5411\u4e0a\u79fb\u52a8\"\u3001\"\u6253\u5f00\u5939\u722a\"\uff09\u4f5c\u4e3a\u7ec6\u7c92\u5ea6\u3001\u77ed\u65f6\u7a0b\u7684\u64cd\u7eb5\u5355\u5143\uff0c\u80fd\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u66f4\u76f4\u89c2\u6709\u6548\u7684\u63a5\u53e3", "method": "\u63d0\u51fa\u6280\u80fd\u6761\u4ef6\u6269\u6563\u7b56\u7565SDP\uff0c\u6574\u5408\u53ef\u89e3\u91ca\u6280\u80fd\u5b66\u4e60\u4e0e\u6761\u4ef6\u52a8\u4f5c\u89c4\u5212\uff1a1) \u62bd\u8c61\u51fa8\u4e2a\u8de8\u4efb\u52a1\u53ef\u91cd\u7528\u539f\u59cb\u6280\u80fd\uff1b2) \u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u89c6\u89c9\u89c2\u5bdf\u548c\u8bed\u8a00\u6307\u4ee4\u4e2d\u63d0\u53d6\u79bb\u6563\u8868\u793a\uff1b3) \u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u7f51\u7edc\u4e3a\u6bcf\u4e2a\u72b6\u6001\u5206\u914d\u671f\u671b\u539f\u59cb\u6280\u80fd\uff1b4) \u6784\u5efa\u5355\u6280\u80fd\u7b56\u7565\u751f\u6210\u6280\u80fd\u5bf9\u9f50\u7684\u52a8\u4f5c", "result": "\u5728\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u4eff\u771f\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\uff0cSDP\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e3a\u57fa\u4e8e\u6280\u80fd\u7684\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f", "conclusion": "\u901a\u8fc7\u5c06\u590d\u6742\u4efb\u52a1\u5206\u89e3\u4e3a\u539f\u59cb\u6280\u80fd\u5e8f\u5217\u5e76\u9009\u62e9\u5355\u6280\u80fd\u7b56\u7565\uff0cSDP\u786e\u4fdd\u4e86\u8de8\u591a\u6837\u4efb\u52a1\u7684\u6280\u80fd\u4e00\u81f4\u884c\u4e3a\uff0c\u8bc1\u660e\u4e86\u6280\u80fd\u6761\u4ef6\u6269\u6563\u7b56\u7565\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2601.01332", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01332", "abs": "https://arxiv.org/abs/2601.01332", "authors": ["Hossam Amer", "Maryam Dialameh", "Hossein Rajabzadeh", "Walid Ahmed", "Weiwei Zhang", "Yang Liu"], "title": "FLOP-Efficient Training: Early Stopping Based on Test-Time Compute Awareness", "comment": null, "summary": "Scaling training compute, measured in FLOPs, has long been shown to improve the accuracy of large language models, yet training remains resource-intensive. Prior work shows that increasing test-time compute (TTC)-for example through iterative sampling-can allow smaller models to rival or surpass much larger ones at lower overall cost. We introduce TTC-aware training, where an intermediate checkpoint and a corresponding TTC configuration can together match or exceed the accuracy of a fully trained model while requiring substantially fewer training FLOPs. Building on this insight, we propose an early stopping algorithm that jointly selects a checkpoint and TTC configuration to minimize training compute without sacrificing accuracy. To make this practical, we develop an efficient TTC evaluation method that avoids exhaustive search, and we formalize a break-even bound that identifies when increased inference compute compensates for reduced training compute. Experiments demonstrate up to 92\\% reductions in training FLOPs while maintaining and sometimes remarkably improving accuracy. These results highlight a new perspective for balancing training and inference compute in model development, enabling faster deployment cycles and more frequent model refreshes. Codes will be publicly released.", "AI": {"tldr": "\u63d0\u51faTTC\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e9\u671f\u505c\u6b62\u7b97\u6cd5\u548c\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8bad\u7ec3\u8ba1\u7b97\u91cf", "motivation": "\u4f20\u7edf\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u867d\u7136\u589e\u52a0\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u53ef\u4ee5\u8ba9\u5c0f\u6a21\u578b\u5ab2\u7f8e\u5927\u6a21\u578b\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u4ecd\u7136\u8d44\u6e90\u5bc6\u96c6\u3002\u9700\u8981\u627e\u5230\u5e73\u8861\u8bad\u7ec3\u548c\u63a8\u7406\u8ba1\u7b97\u7684\u65b0\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u5feb\u90e8\u7f72\u548c\u66f4\u9891\u7e41\u6a21\u578b\u66f4\u65b0\u3002", "method": "1. \u63d0\u51faTTC\u611f\u77e5\u8bad\u7ec3\uff1a\u4e2d\u95f4\u68c0\u67e5\u70b9\u914d\u5408\u76f8\u5e94\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u914d\u7f6e\u53ef\u8fbe\u5230\u6216\u8d85\u8fc7\u5b8c\u6574\u8bad\u7ec3\u6a21\u578b\u7684\u51c6\u786e\u6027\uff1b2. \u5f00\u53d1\u65e9\u671f\u505c\u6b62\u7b97\u6cd5\uff1a\u8054\u5408\u9009\u62e9\u68c0\u67e5\u70b9\u548cTTC\u914d\u7f6e\u4ee5\u6700\u5c0f\u5316\u8bad\u7ec3\u8ba1\u7b97\uff1b3. \u8bbe\u8ba1\u9ad8\u6548TTC\u8bc4\u4f30\u65b9\u6cd5\u907f\u514d\u7a77\u4e3e\u641c\u7d22\uff1b4. \u5f62\u5f0f\u5316\u76c8\u4e8f\u5e73\u8861\u754c\u9650\u786e\u5b9a\u63a8\u7406\u8ba1\u7b97\u4f55\u65f6\u8865\u507f\u8bad\u7ec3\u8ba1\u7b97\u51cf\u5c11\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8bad\u7ec3FLOPs\u6700\u591a\u51cf\u5c1192%\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u663e\u8457\u63d0\u9ad8\u51c6\u786e\u6027\u3002\u9a8c\u8bc1\u4e86\u5e73\u8861\u8bad\u7ec3\u548c\u63a8\u7406\u8ba1\u7b97\u7684\u65b0\u89c6\u89d2\u53ef\u884c\u6027\u3002", "conclusion": "TTC\u611f\u77e5\u8bad\u7ec3\u4e3a\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u5e73\u8861\u8bad\u7ec3\u548c\u63a8\u7406\u8ba1\u7b97\u7684\u65b0\u8303\u5f0f\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u5feb\u90e8\u7f72\u5468\u671f\u548c\u66f4\u9891\u7e41\u6a21\u578b\u66f4\u65b0\uff0c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2601.01969", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01969", "abs": "https://arxiv.org/abs/2601.01969", "authors": ["Sichao Song", "Yuki Okafuji", "Kaito Ariu", "Amy Koike"], "title": "What you reward is what you learn: Comparing rewards for online speech policy optimization in public HRI", "comment": null, "summary": "Designing policies that are both efficient and acceptable for conversational service robots in open and diverse environments is non-trivial. Unlike fixed, hand-tuned parameters, online learning can adapt to non-stationary conditions. In this paper, we study how to adapt a social robot's speech policy in the wild. During a 12-day in-situ deployment with over 1,400 public encounters, we cast online policy optimization as a multi-armed bandit problem and use Thompson sampling to select among six actions defined by speech rate (slow/normal/fast) and verbosity (concise/detailed). We compare three complementary binary rewards--Ru (user rating), Rc (conversation closure), and Rt (>=2 turns)--and show that each induces distinct arm distributions and interaction behaviors. We complement the online results with offline evaluations that analyze contextual factors (e.g., crowd level, group size) using video-annotated data. Taken together, we distill ready-to-use design lessons for deploying online optimization of speech policies in real public HRI settings.", "AI": {"tldr": "\u5728\u7ebf\u5b66\u4e60\u793e\u4ea4\u673a\u5668\u4eba\u8bed\u97f3\u7b56\u7565\uff0c\u901a\u8fc7\u591a\u81c2\u8001\u864e\u673a\u6846\u67b6\u5728\u771f\u5b9e\u516c\u5171\u73af\u5883\u4e2d\u4f18\u5316\u8bed\u901f\u548c\u8be6\u7ec6\u7a0b\u5ea6\uff0c\u4f7f\u7528\u4e09\u79cd\u4e0d\u540c\u5956\u52b1\u51fd\u6570\u8bc4\u4f30\u6548\u679c", "motivation": "\u5728\u5f00\u653e\u591a\u6837\u7684\u73af\u5883\u4e2d\u8bbe\u8ba1\u65e2\u9ad8\u6548\u53c8\u53ef\u63a5\u53d7\u7684\u5bf9\u8bdd\u670d\u52a1\u673a\u5668\u4eba\u7b56\u7565\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u9002\u5e94\u975e\u5e73\u7a33\u6761\u4ef6\uff0c\u800c\u5728\u7ebf\u5b66\u4e60\u80fd\u591f\u5b9e\u73b0\u8fd9\u79cd\u9002\u5e94", "method": "\u5c06\u5728\u7ebf\u7b56\u7565\u4f18\u5316\u5efa\u6a21\u4e3a\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u4f7f\u7528Thompson\u91c7\u6837\u4ece6\u4e2a\u52a8\u4f5c\u4e2d\u9009\u62e9\uff08\u8bed\u901f\uff1a\u6162/\u6b63\u5e38/\u5feb \u00d7 \u8be6\u7ec6\u7a0b\u5ea6\uff1a\u7b80\u6d01/\u8be6\u7ec6\uff09\uff0c\u572812\u5929\u5b9e\u5730\u90e8\u7f72\u4e2d\u6536\u96c61400\u591a\u6b21\u516c\u5171\u4ea4\u4e92\u6570\u636e", "result": "\u4e09\u79cd\u4e92\u8865\u7684\u4e8c\u5143\u5956\u52b1\uff08\u7528\u6237\u8bc4\u5206\u3001\u5bf9\u8bdd\u7ed3\u675f\u3001\u22652\u8f6e\u5bf9\u8bdd\uff09\u5404\u81ea\u8bf1\u5bfc\u51fa\u4e0d\u540c\u7684\u52a8\u4f5c\u5206\u5e03\u548c\u4ea4\u4e92\u884c\u4e3a\uff0c\u79bb\u7ebf\u5206\u6790\u8fd8\u8003\u8651\u4e86\u4eba\u7fa4\u5bc6\u5ea6\u3001\u5c0f\u7ec4\u89c4\u6a21\u7b49\u4e0a\u4e0b\u6587\u56e0\u7d20", "conclusion": "\u4e3a\u5728\u771f\u5b9e\u516c\u5171\u4eba\u673a\u4ea4\u4e92\u73af\u5883\u4e2d\u90e8\u7f72\u8bed\u97f3\u7b56\u7565\u7684\u5728\u7ebf\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u76f4\u63a5\u5e94\u7528\u7684\u8bbe\u8ba1\u7ecf\u9a8c\u6559\u8bad"}}
{"id": "2601.01341", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01341", "abs": "https://arxiv.org/abs/2601.01341", "authors": ["Md Abdullah Al Kafi", "Raka Moni", "Sumit Kumar Banshal"], "title": "Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems", "comment": null, "summary": "The deployment of Large Language Models (LLMs) in mental health counseling faces the dual challenges of hallucinations and lack of empathy. While the former may be mitigated by RAG (retrieval-augmented generation) by anchoring answers in trusted clinical sources, there remains an open question as to whether the most effective model under this paradigm would be one that is fine-tuned on mental health data, or a more general and powerful model that succeeds purely on the basis of reasoning. In this paper, we perform a direct comparison by running four open-source models through the same RAG pipeline using ChromaDB: two generalist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-tunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge framework to automate evaluation over 50 turns. We find a clear trend: the generalist models outperform the domain-specific ones in empathy (3.72 vs. 3.26, $p < 0.001$) in spite of being much smaller (3B vs. 7B), and all models perform well in terms of safety, but the generalist models show better contextual understanding and are less prone to overfitting as we observe in the domain-specific models. Overall, our results indicate that for RAG-based therapy systems, strong reasoning is more important than training on mental health-specific vocabulary; i.e. a well-reasoned general model would provide more empathetic and balanced support than a larger narrowly fine-tuned model, so long as the answer is already grounded in clinical evidence.", "AI": {"tldr": "\u5728\u5fc3\u7406\u5065\u5eb7\u54a8\u8be2\u4e2d\uff0c\u901a\u7528\u63a8\u7406\u6a21\u578b\uff083B\u53c2\u6570\uff09\u901a\u8fc7RAG\u6846\u67b6\u6bd4\u9886\u57df\u5fae\u8c03\u6a21\u578b\uff087B\u53c2\u6570\uff09\u8868\u73b0\u66f4\u597d\uff0c\u5c24\u5176\u5728\u5171\u60c5\u80fd\u529b\u65b9\u9762\uff0c\u8868\u660e\u5f3a\u63a8\u7406\u80fd\u529b\u6bd4\u5fc3\u7406\u5065\u5eb7\u7279\u5b9a\u8bad\u7ec3\u66f4\u91cd\u8981\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u5fc3\u7406\u5065\u5eb7\u54a8\u8be2\u4e2d\u5b58\u5728\u7684\u5e7b\u89c9\u548c\u7f3a\u4e4f\u5171\u60c5\u95ee\u9898\uff0c\u63a2\u7d22\u5728RAG\u6846\u67b6\u4e0b\uff0c\u662f\u9886\u57df\u5fae\u8c03\u6a21\u578b\u8fd8\u662f\u901a\u7528\u63a8\u7406\u6a21\u578b\u66f4\u9002\u5408\u5fc3\u7406\u5065\u5eb7\u54a8\u8be2\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u56db\u4e2a\u5f00\u6e90\u6a21\u578b\uff08\u4e24\u4e2a\u901a\u7528\u63a8\u7406\u6a21\u578b\uff1aQwen2.5-3B\u548cPhi-3-Mini\uff1b\u4e24\u4e2a\u9886\u57df\u5fae\u8c03\u6a21\u578b\uff1aMentalHealthBot-7B\u548cTherapyBot-7B\uff09\u901a\u8fc7\u76f8\u540c\u7684ChromaDB RAG\u6d41\u7a0b\u8fdb\u884c\u6bd4\u8f83\uff0c\u91c7\u7528LLM-as-a-Judge\u6846\u67b6\u5bf950\u8f6e\u5bf9\u8bdd\u8fdb\u884c\u81ea\u52a8\u5316\u8bc4\u4f30\u3002", "result": "\u901a\u7528\u6a21\u578b\u5728\u5171\u60c5\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u9886\u57df\u6a21\u578b\uff083.72 vs. 3.26\uff0cp<0.001\uff09\uff0c\u5c3d\u7ba1\u53c2\u6570\u66f4\u5c0f\uff083B vs. 7B\uff09\u3002\u6240\u6709\u6a21\u578b\u5728\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u901a\u7528\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u4e14\u4e0d\u6613\u8fc7\u62df\u5408\u3002", "conclusion": "\u5bf9\u4e8e\u57fa\u4e8eRAG\u7684\u6cbb\u7597\u7cfb\u7edf\uff0c\u5f3a\u63a8\u7406\u80fd\u529b\u6bd4\u5fc3\u7406\u5065\u5eb7\u7279\u5b9a\u8bcd\u6c47\u8bad\u7ec3\u66f4\u91cd\u8981\u3002\u53ea\u8981\u56de\u7b54\u57fa\u4e8e\u4e34\u5e8a\u8bc1\u636e\uff0c\u63a8\u7406\u80fd\u529b\u5f3a\u7684\u901a\u7528\u6a21\u578b\u80fd\u63d0\u4f9b\u6bd4\u66f4\u5927\u4f46\u7a84\u9886\u57df\u5fae\u8c03\u6a21\u578b\u66f4\u5171\u60c5\u548c\u5e73\u8861\u7684\u652f\u6301\u3002"}}
{"id": "2601.01971", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.01971", "abs": "https://arxiv.org/abs/2601.01971", "authors": ["Aditya Singh", "Rajpal Singh", "Jishnu Keshavan"], "title": "Deep Robust Koopman Learning from Noisy Data", "comment": null, "summary": "Koopman operator theory has emerged as a leading data-driven approach that relies on a judicious choice of observable functions to realize global linear representations of nonlinear systems in the lifted observable space. However, real-world data is often noisy, making it difficult to obtain an accurate and unbiased approximation of the Koopman operator. The Koopman operator generated from noisy datasets is typically corrupted by noise-induced bias that severely degrades prediction and downstream tracking performance. In order to address this drawback, this paper proposes a novel autoencoder-based neural architecture to jointly learn the appropriate lifting functions and the reduced-bias Koopman operator from noisy data. The architecture initially learns the Koopman basis functions that are consistent for both the forward and backward temporal dynamics of the system. Subsequently, by utilizing the learned forward and backward temporal dynamics, the Koopman operator is synthesized with a reduced bias making the method more robust to noise compared to existing techniques. Theoretical analysis is used to demonstrate significant bias reduction in the presence of training noise. Dynamics prediction and tracking control simulations are conducted for multiple serial manipulator arms, including performance comparisons with leading alternative designs, to demonstrate its robustness under various noise levels. Experimental studies with the Franka FR3 7-DoF manipulator arm are further used to demonstrate the effectiveness of the proposed approach in a practical setting.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u8054\u5408\u5b66\u4e60\u63d0\u5347\u51fd\u6570\u548c\u964d\u504fKoopman\u7b97\u5b50\uff0c\u63d0\u9ad8\u975e\u7ebf\u6027\u7cfb\u7edf\u7ebf\u6027\u8868\u793a\u7684\u9c81\u68d2\u6027", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u901a\u5e38\u5305\u542b\u566a\u58f0\uff0c\u5bfc\u81f4\u4ece\u566a\u58f0\u6570\u636e\u96c6\u751f\u6210\u7684Koopman\u7b97\u5b50\u5b58\u5728\u566a\u58f0\u8bf1\u5bfc\u504f\u5dee\uff0c\u4e25\u91cd\u5f71\u54cd\u9884\u6d4b\u548c\u8ddf\u8e2a\u6027\u80fd\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u7f3a\u9677", "method": "\u8bbe\u8ba1\u81ea\u7f16\u7801\u5668\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u9996\u5148\u5b66\u4e60\u7cfb\u7edf\u524d\u5411\u548c\u540e\u5411\u65f6\u95f4\u52a8\u529b\u5b66\u4e00\u81f4\u7684Koopman\u57fa\u51fd\u6570\uff0c\u7136\u540e\u5229\u7528\u5b66\u4e60\u5230\u7684\u52a8\u529b\u5b66\u5408\u6210\u964d\u504fKoopman\u7b97\u5b50", "result": "\u7406\u8bba\u5206\u6790\u663e\u793a\u5728\u8bad\u7ec3\u566a\u58f0\u4e0b\u663e\u8457\u51cf\u5c11\u504f\u5dee\uff0c\u591a\u4e2a\u4e32\u8054\u673a\u68b0\u81c2\u7684\u52a8\u529b\u5b66\u9884\u6d4b\u548c\u8ddf\u8e2a\u63a7\u5236\u4eff\u771f\u8868\u660e\u5728\u5404\u79cd\u566a\u58f0\u6c34\u5e73\u4e0b\u5177\u6709\u9c81\u68d2\u6027\uff0cFranka FR3 7\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b9e\u9645\u6548\u679c", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u8054\u5408\u5b66\u4e60\u63d0\u5347\u51fd\u6570\u548c\u964d\u504fKoopman\u7b97\u5b50\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u5bf9\u566a\u58f0\u66f4\u9c81\u68d2\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8bc1\u660e\u4e86\u6709\u6548\u6027"}}
{"id": "2601.01350", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01350", "abs": "https://arxiv.org/abs/2601.01350", "authors": ["Juan Junqueras", "Florian Boudin", "May-Myo Zin", "Ha-Thanh Nguyen", "Wachara Fungwacharakorn", "Dami\u00e1n Ariel Furman", "Akiko Aizawa", "Ken Satoh"], "title": "FC-CONAN: An Exhaustively Paired Dataset for Robust Evaluation of Retrieval Systems", "comment": "Presented at NeLaMKRR@KR, 2025 (arXiv:2511.09575)", "summary": "Hate speech (HS) is a critical issue in online discourse, and one promising strategy to counter it is through the use of counter-narratives (CNs). Datasets linking HS with CNs are essential for advancing counterspeech research. However, even flagship resources like CONAN (Chung et al., 2019) annotate only a sparse subset of all possible HS-CN pairs, limiting evaluation. We introduce FC-CONAN (Fully Connected CONAN), the first dataset created by exhaustively considering all combinations of 45 English HS messages and 129 CNs. A two-stage annotation process involving nine annotators and four validators produces four partitions-Diamond, Gold, Silver, and Bronze-that balance reliability and scale. None of the labeled pairs overlap with CONAN, uncovering hundreds of previously unlabelled positives. FC-CONAN enables more faithful evaluation of counterspeech retrieval systems and facilitates detailed error analysis. The dataset is publicly available.", "AI": {"tldr": "FC-CONAN\u662f\u9996\u4e2a\u901a\u8fc7\u7a77\u4e3e45\u6761\u4ec7\u6068\u8a00\u8bba\u548c129\u6761\u53cd\u53d9\u4e8b\u6240\u6709\u7ec4\u5408\u6784\u5efa\u7684\u5b8c\u5168\u8fde\u63a5\u6570\u636e\u96c6\uff0c\u5305\u542b\u56db\u4e2a\u4e0d\u540c\u53ef\u9760\u6027\u5c42\u7ea7\u7684\u6807\u6ce8\u5206\u533a\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u7a00\u758f\u6807\u6ce8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4ec7\u6068\u8a00\u8bba-\u53cd\u53d9\u4e8b\u6570\u636e\u96c6\uff08\u5982CONAN\uff09\u53ea\u6807\u6ce8\u4e86\u7a00\u758f\u7684HS-CN\u914d\u5bf9\uff0c\u9650\u5236\u4e86\u53cd\u8a00\u8bba\u7814\u7a76\u7684\u8bc4\u4f30\u80fd\u529b\u3002\u9700\u8981\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u66f4\u51c6\u786e\u7684\u7cfb\u7edf\u8bc4\u4f30\u548c\u9519\u8bef\u5206\u6790\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6807\u6ce8\u6d41\u7a0b\uff1a1) \u7a77\u4e3e45\u6761\u82f1\u8bed\u4ec7\u6068\u8a00\u8bba\u548c129\u6761\u53cd\u53d9\u4e8b\u7684\u6240\u6709\u7ec4\u5408\uff08\u51715805\u5bf9\uff09\uff1b2) \u901a\u8fc79\u540d\u6807\u6ce8\u5458\u548c4\u540d\u9a8c\u8bc1\u5458\u521b\u5efa\u56db\u4e2a\u4e0d\u540c\u53ef\u9760\u6027\u5c42\u7ea7\u7684\u6807\u6ce8\u5206\u533a\uff08\u94bb\u77f3\u3001\u9ec4\u91d1\u3001\u767d\u94f6\u3001\u9752\u94dc\uff09\u3002", "result": "FC-CONAN\u53d1\u73b0\u4e86\u6570\u767e\u4e2a\u4e4b\u524d\u672a\u6807\u6ce8\u7684\u6b63\u4f8b\u914d\u5bf9\uff0c\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u3002\u6570\u636e\u96c6\u5305\u542b\u56db\u4e2a\u5206\u533a\uff0c\u5e73\u8861\u4e86\u53ef\u9760\u6027\u548c\u89c4\u6a21\uff0c\u652f\u6301\u53cd\u8a00\u8bba\u68c0\u7d22\u7cfb\u7edf\u7684\u5fe0\u5b9e\u8bc4\u4f30\u548c\u8be6\u7ec6\u9519\u8bef\u5206\u6790\u3002", "conclusion": "FC-CONAN\u586b\u8865\u4e86\u4ec7\u6068\u8a00\u8bba-\u53cd\u53d9\u4e8b\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u63d0\u4f9b\u4e86\u9996\u4e2a\u5b8c\u5168\u8fde\u63a5\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u80fd\u591f\u663e\u8457\u6539\u8fdb\u53cd\u8a00\u8bba\u7cfb\u7edf\u7684\u8bc4\u4f30\u548c\u7814\u7a76\u3002\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2601.02078", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02078", "abs": "https://arxiv.org/abs/2601.02078", "authors": ["Chenghao Yin", "Da Huang", "Di Yang", "Jichao Wang", "Nanshu Zhao", "Chen Xu", "Wenjun Sun", "Linjie Hou", "Zhijun Li", "Junhui Wu", "Zhaobo Liu", "Zhen Xiao", "Sheng Zhang", "Lei Bao", "Rui Feng", "Zhenquan Pang", "Jiayu Li", "Qian Wang", "Maoqing Yao"], "title": "Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot", "comment": null, "summary": "The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim.", "AI": {"tldr": "Genie Sim 3.0\u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7edf\u4e00\u4eff\u771f\u5e73\u53f0\uff0c\u5305\u542bLLM\u9a71\u52a8\u7684\u573a\u666f\u751f\u6210\u5668\u3001\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u53d1\u5e03\u4e86\u8d85\u8fc710,000\u5c0f\u65f6\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e86\u96f6\u6837\u672c\u6a21\u62df\u5230\u771f\u5b9e\u4e16\u754c\u7684\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u5b66\u4e60\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a1\uff09\u7269\u7406\u4e16\u754c\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u3001\u53ef\u6269\u5c55\u6027\u5dee\uff1b2\uff09\u73b0\u6709\u4eff\u771f\u57fa\u51c6\u5b58\u5728\u788e\u7247\u5316\u3001\u8303\u56f4\u7a84\u3001\u4fdd\u771f\u5ea6\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u6a21\u62df\u5230\u771f\u5b9e\u8fc1\u79fb\u3002", "method": "1\uff09\u5f00\u53d1Genie Sim Generator\uff1a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6784\u5efa\u9ad8\u4fdd\u771f\u573a\u666f\uff1b2\uff09\u521b\u5efa\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u8bc4\u4f30\u57fa\u51c6\uff1a\u5229\u7528LLM\u6279\u91cf\u751f\u6210\u8bc4\u4f30\u573a\u666f\uff0c\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u81ea\u52a8\u8bc4\u4f30\u6d41\u7a0b\uff1b3\uff09\u53d1\u5e03\u5f00\u6e90\u6570\u636e\u96c6\uff1a\u5305\u542b200\u591a\u4e2a\u4efb\u52a1\u768410,000\u591a\u5c0f\u65f6\u5408\u6210\u6570\u636e\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5f00\u6e90\u6570\u636e\u96c6\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6a21\u62df\u5230\u771f\u5b9e\u8fc1\u79fb\u80fd\u529b\uff0c\u8bc1\u660e\u5728\u53d7\u63a7\u6761\u4ef6\u4e0b\u5408\u6210\u6570\u636e\u53ef\u4ee5\u6709\u6548\u66ff\u4ee3\u771f\u5b9e\u4e16\u754c\u6570\u636e\u8fdb\u884c\u53ef\u6269\u5c55\u7684\u7b56\u7565\u8bad\u7ec3\u3002", "conclusion": "Genie Sim 3.0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4eff\u771f\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u83b7\u53d6\u548c\u53ef\u9760\u8bc4\u4f30\u57fa\u51c6\u7684\u6311\u6218\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u573a\u666f\u751f\u6210\u548c\u81ea\u52a8\u8bc4\u4f30\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01362", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2601.01362", "abs": "https://arxiv.org/abs/2601.01362", "authors": ["Jerry Huang", "Peng Lu", "Qiuhao Zeng", "Yusuke Iwasawa", "Yutaka Matsuo", "Sarath Chandar", "Edison Marrese-Taylor", "Irene Li"], "title": "Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning", "comment": "Accepted to The 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL)", "summary": "Ensuring that deep learning models are well-calibrated in terms of their predictive uncertainty is essential in maintaining their trustworthiness and reliability, yet despite increasing advances in foundation model research, the relationship between such large language models (LLMs) and their calibration remains an open area of research. In this work, we look at a critical gap in the calibration of LLMs within multilingual settings, in an attempt to better understand how the data scarcity can potentially lead to different calibration effects and how commonly used techniques can apply in these settings. Our analysis on two multilingual benchmarks, over 29 and 42 languages respectively, reveals that even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration, highlighting a critical shortcoming of standard SFT for multilingual languages. Furthermore, we observe that the use of label smoothing to be a reasonable method alleviate this concern, again without any need for low-resource SFT data, maintaining better calibration across all languages. Overall, this highlights the importance of multilingual considerations for both training and tuning LLMs in order to improve their reliability and fairness in downstream use.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6821\u51c6\u5b58\u5728\u4e25\u91cd\u95ee\u9898\uff1a\u5373\u4f7f\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u6307\u4ee4\u8c03\u4f18\u540e\u6a21\u578b\u7f6e\u4fe1\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u4f46\u51c6\u786e\u7387\u6539\u5584\u6709\u9650\uff0c\u5bfc\u81f4\u6821\u51c6\u4e0d\u826f\u3002\u6807\u7b7e\u5e73\u6ed1\u6280\u672f\u80fd\u6709\u6548\u7f13\u89e3\u6b64\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u7840\u6a21\u578b\u7814\u7a76\u4e0d\u65ad\u8fdb\u6b65\uff0c\u4f46\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u6821\u51c6\u95ee\u9898\u4ecd\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u7814\u7a76\u9886\u57df\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u6570\u636e\u7a00\u7f3a\u5982\u4f55\u5f71\u54cd\u6821\u51c6\u6548\u679c\uff0c\u4ee5\u53ca\u5e38\u7528\u6280\u672f\u5728\u6b64\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u3002", "method": "\u5728\u4e24\u4e2a\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff08\u5206\u522b\u8986\u76d629\u548c42\u79cd\u8bed\u8a00\uff09\u4e0a\u8fdb\u884c\u5206\u6790\uff0c\u7814\u7a76\u6307\u4ee4\u8c03\u4f18\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5f71\u54cd\uff0c\u5e76\u8bc4\u4f30\u6807\u7b7e\u5e73\u6ed1\u6280\u672f\u5728\u6539\u5584\u6821\u51c6\u65b9\u9762\u7684\u6548\u679c\u3002", "result": "\u53d1\u73b0\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u57fa\u4e8e\u9ad8\u8d44\u6e90\u8bed\u8a00SFT\u6570\u636e\u96c6\u8fdb\u884c\u6307\u4ee4\u8c03\u4f18\u540e\uff0c\u6a21\u578b\u7f6e\u4fe1\u5ea6\u663e\u8457\u589e\u52a0\uff0c\u4f46\u51c6\u786e\u7387\u6539\u5584\u6709\u9650\u6216\u6ca1\u6709\u6539\u5584\uff0c\u5bfc\u81f4\u6821\u51c6\u4e0d\u826f\u3002\u6807\u7b7e\u5e73\u6ed1\u6280\u672f\u80fd\u6709\u6548\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u4e14\u65e0\u9700\u4f4e\u8d44\u6e90SFT\u6570\u636e\u3002", "conclusion": "\u6807\u51c6SFT\u65b9\u6cd5\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e2d\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u4f1a\u7834\u574f\u6821\u51c6\u3002\u6807\u7b7e\u5e73\u6ed1\u662f\u4e00\u79cd\u6709\u6548\u7684\u7f13\u89e3\u65b9\u6cd5\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u8bad\u7ec3\u548c\u8c03\u4f18LLMs\u65f6\u8003\u8651\u6821\u51c6\u95ee\u9898\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u63d0\u9ad8\u4e0b\u6e38\u5e94\u7528\u7684\u53ef\u9760\u6027\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2601.02085", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02085", "abs": "https://arxiv.org/abs/2601.02085", "authors": ["Meili Sun", "Chunjiang Zhao", "Lichao Yang", "Hao Liu", "Shimin Hu", "Ya Xiong"], "title": "Vision-Based Early Fault Diagnosis and Self-Recovery for Strawberry Harvesting Robots", "comment": null, "summary": "Strawberry harvesting robots faced persistent challenges such as low integration of visual perception, fruit-gripper misalignment, empty grasping, and strawberry slippage from the gripper due to insufficient gripping force, all of which compromised harvesting stability and efficiency in orchard environments. To overcome these issues, this paper proposed a visual fault diagnosis and self-recovery framework that integrated multi-task perception with corrective control strategies. At the core of this framework was SRR-Net, an end-to-end multi-task perception model that simultaneously performed strawberry detection, segmentation, and ripeness estimation, thereby unifying visual perception with fault diagnosis. Based on this integrated perception, a relative error compensation method based on the simultaneous target-gripper detection was designed to address positional misalignment, correcting deviations when error exceeded the tolerance threshold. To mitigate empty grasping and fruit-slippage faults, an early abort strategy was implemented. A micro-optical camera embedded in the end-effector provided real-time visual feedback, enabling grasp detection during the deflating stage and strawberry slip prediction during snap-off through MobileNet V3-Small classifier and a time-series LSTM classifier. Experiments demonstrated that SRR-Net maintained high perception accuracy. For detection, it achieved a precision of 0.895 and recall of 0.813 on strawberries, and 0.972/0.958 on hands. In segmentation, it yielded a precision of 0.887 and recall of 0.747 for strawberries, and 0.974/0.947 for hands. For ripeness estimation, SRR-Net attained a mean absolute error of 0.035, while simultaneously supporting multi-task perception and sustaining a competitive inference speed of 163.35 FPS.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8349\u8393\u91c7\u6458\u673a\u5668\u4eba\u7684\u89c6\u89c9\u6545\u969c\u8bca\u65ad\u4e0e\u81ea\u6062\u590d\u6846\u67b6\uff0c\u901a\u8fc7SRR-Net\u591a\u4efb\u52a1\u611f\u77e5\u6a21\u578b\u6574\u5408\u68c0\u6d4b\u3001\u5206\u5272\u548c\u6210\u719f\u5ea6\u4f30\u8ba1\uff0c\u7ed3\u5408\u8bef\u5dee\u8865\u507f\u548c\u65e9\u671f\u4e2d\u6b62\u7b56\u7565\u89e3\u51b3\u91c7\u6458\u8fc7\u7a0b\u4e2d\u7684\u5bf9\u4f4d\u4e0d\u51c6\u3001\u7a7a\u6293\u548c\u679c\u5b9e\u6ed1\u843d\u95ee\u9898\u3002", "motivation": "\u8349\u8393\u91c7\u6458\u673a\u5668\u4eba\u9762\u4e34\u89c6\u89c9\u611f\u77e5\u96c6\u6210\u5ea6\u4f4e\u3001\u679c\u5b9e-\u5939\u722a\u5bf9\u4f4d\u4e0d\u51c6\u3001\u7a7a\u6293\u53d6\u3001\u4ee5\u53ca\u56e0\u5939\u6301\u529b\u4e0d\u8db3\u5bfc\u81f4\u7684\u679c\u5b9e\u6ed1\u843d\u7b49\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u5f71\u54cd\u4e86\u679c\u56ed\u73af\u5883\u4e0b\u7684\u91c7\u6458\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u89c6\u89c9\u6545\u969c\u8bca\u65ad\u4e0e\u81ea\u6062\u590d\u6846\u67b6\uff0c\u6838\u5fc3\u662fSRR-Net\u7aef\u5230\u7aef\u591a\u4efb\u52a1\u611f\u77e5\u6a21\u578b\uff0c\u540c\u65f6\u6267\u884c\u8349\u8393\u68c0\u6d4b\u3001\u5206\u5272\u548c\u6210\u719f\u5ea6\u4f30\u8ba1\u3002\u91c7\u7528\u57fa\u4e8e\u540c\u6b65\u76ee\u6807-\u5939\u722a\u68c0\u6d4b\u7684\u76f8\u5bf9\u8bef\u5dee\u8865\u507f\u65b9\u6cd5\u89e3\u51b3\u4f4d\u7f6e\u504f\u5dee\uff0c\u5b9e\u65bd\u65e9\u671f\u4e2d\u6b62\u7b56\u7565\u9632\u6b62\u7a7a\u6293\u548c\u679c\u5b9e\u6ed1\u843d\u3002\u672b\u7aef\u6267\u884c\u5668\u5d4c\u5165\u5fae\u5149\u5b66\u76f8\u673a\u63d0\u4f9b\u5b9e\u65f6\u89c6\u89c9\u53cd\u9988\uff0c\u901a\u8fc7MobileNet V3-Small\u548cLSTM\u5206\u7c7b\u5668\u8fdb\u884c\u6293\u53d6\u68c0\u6d4b\u548c\u6ed1\u843d\u9884\u6d4b\u3002", "result": "SRR-Net\u5728\u68c0\u6d4b\u4efb\u52a1\u4e0a\u5bf9\u8349\u8393\u7684\u7cbe\u786e\u5ea6\u4e3a0.895\u3001\u53ec\u56de\u7387\u4e3a0.813\uff0c\u5bf9\u624b\u7684\u7cbe\u786e\u5ea6\u4e3a0.972\u3001\u53ec\u56de\u7387\u4e3a0.958\uff1b\u5206\u5272\u4efb\u52a1\u4e0a\u5bf9\u8349\u8393\u7684\u7cbe\u786e\u5ea6\u4e3a0.887\u3001\u53ec\u56de\u7387\u4e3a0.747\uff0c\u5bf9\u624b\u7684\u7cbe\u786e\u5ea6\u4e3a0.974\u3001\u53ec\u56de\u7387\u4e3a0.947\uff1b\u6210\u719f\u5ea6\u4f30\u8ba1\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4e3a0.035\uff0c\u540c\u65f6\u652f\u6301\u591a\u4efb\u52a1\u611f\u77e5\u5e76\u4fdd\u6301163.35 FPS\u7684\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u4efb\u52a1\u611f\u77e5\u4e0e\u7ea0\u6b63\u63a7\u5236\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8349\u8393\u91c7\u6458\u673a\u5668\u4eba\u7684\u5173\u952e\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u91c7\u6458\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86SRR-Net\u7684\u9ad8\u7cbe\u5ea6\u611f\u77e5\u80fd\u529b\u548c\u5b9e\u65f6\u6027\u80fd\u3002"}}
{"id": "2601.01400", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01400", "abs": "https://arxiv.org/abs/2601.01400", "authors": ["Jicheng Ma", "Guohua Wang", "Xinhua Feng", "Yiming Liu", "Zhichao Hu", "Yuhong Liu"], "title": "EternalMath: A Living Benchmark of Frontier Mathematics that Evolves with Human Discovery", "comment": null, "summary": "Current evaluations of mathematical reasoning in large language models (LLMs) are dominated by static benchmarks, either derived from competition-style problems or curated through costly expert effort, resulting in limited coverage of research-level mathematics and rapid performance saturation. We propose a fully automated, theorem-grounded pipeline for evaluating frontier mathematical reasoning, which directly transforms recent peer-reviewed mathematical literature into executable and verifiable reasoning tasks. The pipeline identifies constructive or quantitative results, instantiates them into parameterized problem templates, and generates deterministic solutions through execution-based verification, enabling scalable, reproducible, and continuously updatable evaluation without reliance on large-scale expert authoring. By design, this approach supports temporal extensibility, intrinsic correctness checking, and domain-specific customization across mathematical subfields. Applying this pipeline yields \\textbf{EternalMath}, an evolving evaluation suite derived from contemporary research papers. Experiments with state-of-the-art LLMs reveal substantial performance gaps, indicating that mathematical reasoning at the research frontier remains far from saturated and underscoring the need for evaluation methodologies that evolve in step with human mathematical discovery.", "AI": {"tldr": "\u63d0\u51faEternalMath\uff1a\u4e00\u500b\u5f9e\u6700\u65b0\u6578\u5b78\u7814\u7a76\u6587\u737b\u81ea\u52d5\u751f\u6210\u53ef\u57f7\u884c\u9a57\u8b49\u63a8\u7406\u4efb\u52d9\u7684\u8a55\u4f30\u6846\u67b6\uff0c\u89e3\u6c7a\u73fe\u6709\u975c\u614b\u57fa\u6e96\u8986\u84cb\u4e0d\u8db3\u548c\u6027\u80fd\u5feb\u901f\u98fd\u548c\u7684\u554f\u984c\u3002", "motivation": "\u7576\u524dLLM\u6578\u5b78\u63a8\u7406\u8a55\u4f30\u4e3b\u8981\u4f9d\u8cf4\u975c\u614b\u57fa\u6e96\uff0c\u9019\u4e9b\u57fa\u6e96\u8981\u4e48\u4f86\u81ea\u7af6\u8cfd\u984c\u76ee\uff0c\u8981\u4e48\u9700\u8981\u5c08\u5bb6\u4eba\u5de5\u7de8\u5beb\uff0c\u5c0e\u81f4\u7814\u7a76\u7d1a\u6578\u5b78\u8986\u84cb\u6709\u9650\u4e14\u6027\u80fd\u5feb\u901f\u98fd\u548c\uff0c\u9700\u8981\u80fd\u96a8\u6578\u5b78\u7814\u7a76\u767c\u5c55\u800c\u6f14\u9032\u7684\u8a55\u4f30\u65b9\u6cd5\u3002", "method": "\u8a2d\u8a08\u5168\u81ea\u52d5\u3001\u5b9a\u7406\u57fa\u790e\u7684\u8a55\u4f30\u6d41\u6c34\u7dda\uff1a\u5f9e\u8fd1\u671f\u540c\u884c\u8a55\u5be9\u6578\u5b78\u6587\u737b\u4e2d\u8b58\u5225\u69cb\u9020\u6027\u6216\u5b9a\u91cf\u7d50\u679c\uff0c\u5c07\u5176\u8f49\u5316\u70ba\u53c3\u6578\u5316\u554f\u984c\u6a21\u677f\uff0c\u901a\u904e\u57f7\u884c\u9a57\u8b49\u751f\u6210\u78ba\u5b9a\u6027\u89e3\uff0c\u5275\u5efa\u53ef\u64f4\u5c55\u3001\u53ef\u91cd\u73fe\u4e14\u6301\u7e8c\u66f4\u65b0\u7684\u8a55\u4f30\u5957\u4ef6EternalMath\u3002", "result": "\u5be6\u9a57\u986f\u793a\u6700\u5148\u9032LLM\u5728EternalMath\u4e0a\u5b58\u5728\u986f\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u8868\u660e\u7814\u7a76\u524d\u6cbf\u7684\u6578\u5b78\u63a8\u7406\u9060\u672a\u98fd\u548c\uff0c\u51f8\u986f\u8a55\u4f30\u65b9\u6cd5\u9700\u8981\u8207\u4eba\u985e\u6578\u5b78\u767c\u73fe\u540c\u6b65\u6f14\u9032\u7684\u91cd\u8981\u6027\u3002", "conclusion": "EternalMath\u63d0\u4f9b\u4e86\u4e00\u500b\u53ef\u6301\u7e8c\u6f14\u9032\u7684\u6578\u5b78\u63a8\u7406\u8a55\u4f30\u6846\u67b6\uff0c\u80fd\u66f4\u597d\u5730\u53cd\u6620LLM\u5728\u771f\u5be6\u7814\u7a76\u7d1a\u6578\u5b78\u554f\u984c\u4e0a\u7684\u80fd\u529b\uff0c\u63a8\u52d5\u8a55\u4f30\u65b9\u6cd5\u8207\u6578\u5b78\u7814\u7a76\u524d\u6cbf\u4fdd\u6301\u540c\u6b65\u3002"}}
{"id": "2601.02125", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02125", "abs": "https://arxiv.org/abs/2601.02125", "authors": ["Zhuoxiong Xu", "Xuanchen Li", "Yuhao Cheng", "Fei Xu", "Yichao Yan", "Xiaokang Yang"], "title": "SingingBot: An Avatar-Driven System for Robotic Face Singing Performance", "comment": null, "summary": "Equipping robotic faces with singing capabilities is crucial for empathetic Human-Robot Interaction. However, existing robotic face driving research primarily focuses on conversations or mimicking static expressions, struggling to meet the high demands for continuous emotional expression and coherence in singing. To address this, we propose a novel avatar-driven framework for appealing robotic singing. We first leverage portrait video generation models embedded with extensive human priors to synthesize vivid singing avatars, providing reliable expression and emotion guidance. Subsequently, these facial features are transferred to the robot via semantic-oriented mapping functions that span a wide expression space. Furthermore, to quantitatively evaluate the emotional richness of robotic singing, we propose the Emotion Dynamic Range metric to measure the emotional breadth within the Valence-Arousal space, revealing that a broad emotional spectrum is crucial for appealing performances. Comprehensive experiments prove that our method achieves rich emotional expressions while maintaining lip-audio synchronization, significantly outperforming existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u65b0\u9896\u7684\u673a\u5668\u4eba\u5531\u6b4c\u8868\u60c5\u9a71\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u8096\u50cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u521b\u5efa\u751f\u52a8\u7684\u5531\u6b4c\u865a\u62df\u5f62\u8c61\uff0c\u518d\u901a\u8fc7\u8bed\u4e49\u6620\u5c04\u5c06\u9762\u90e8\u7279\u5f81\u8f6c\u79fb\u5230\u673a\u5668\u4eba\u4e0a\uff0c\u5b9e\u73b0\u4e30\u5bcc\u7684\u60c5\u611f\u8868\u8fbe\u548c\u5507\u97f3\u540c\u6b65\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u9762\u90e8\u9a71\u52a8\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5bf9\u8bdd\u6216\u9759\u6001\u8868\u60c5\u6a21\u4eff\uff0c\u96be\u4ee5\u6ee1\u8db3\u5531\u6b4c\u65f6\u8fde\u7eed\u60c5\u611f\u8868\u8fbe\u548c\u8fde\u8d2f\u6027\u7684\u9ad8\u8981\u6c42\uff0c\u9700\u8981\u4e3a\u673a\u5668\u4eba\u5531\u6b4c\u5f00\u53d1\u66f4\u4e30\u5bcc\u7684\u60c5\u611f\u8868\u8fbe\u80fd\u529b\u3002", "method": "1) \u5229\u7528\u5d4c\u5165\u4eba\u7c7b\u5148\u9a8c\u77e5\u8bc6\u7684\u8096\u50cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u5408\u6210\u751f\u52a8\u7684\u5531\u6b4c\u865a\u62df\u5f62\u8c61\uff1b2) \u901a\u8fc7\u8bed\u4e49\u5bfc\u5411\u7684\u6620\u5c04\u51fd\u6570\u5c06\u9762\u90e8\u7279\u5f81\u8f6c\u79fb\u5230\u673a\u5668\u4eba\u4e0a\uff0c\u8986\u76d6\u5e7f\u6cdb\u7684\u8868\u60c5\u7a7a\u95f4\uff1b3) \u63d0\u51fa\u60c5\u611f\u52a8\u6001\u8303\u56f4\u6307\u6807\u6765\u91cf\u5316\u673a\u5668\u4eba\u5531\u6b4c\u7684\u60c5\u611f\u4e30\u5bcc\u5ea6\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5507\u97f3\u540c\u6b65\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4e30\u5bcc\u7684\u60c5\u611f\u8868\u8fbe\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u60c5\u611f\u52a8\u6001\u8303\u56f4\u6307\u6807\u663e\u793a\u5bbd\u5e7f\u7684\u60c5\u611f\u8c31\u5bf9\u5438\u5f15\u4eba\u7684\u8868\u6f14\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u63d0\u51fa\u7684\u865a\u62df\u5f62\u8c61\u9a71\u52a8\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u5531\u6b4c\u7684\u60c5\u611f\u8868\u8fbe\u95ee\u9898\uff0c\u901a\u8fc7\u4eba\u7c7b\u5148\u9a8c\u77e5\u8bc6\u548c\u8bed\u4e49\u6620\u5c04\u5b9e\u73b0\u4e86\u4e30\u5bcc\u3001\u8fde\u8d2f\u7684\u60c5\u611f\u8868\u8fbe\uff0c\u4e3a\u5171\u60c5\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2601.01401", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01401", "abs": "https://arxiv.org/abs/2601.01401", "authors": ["Chenxu Wang", "Chaozhuo Li", "Pengbo Wang", "Litian Zhang", "Songyang Liu", "Ji Qi", "Jiahui Hu", "Yushan Cai", "Hao Zhao", "Rui Pu"], "title": "LANCET: Neural Intervention via Structural Entropy for Mitigating Faithfulness Hallucinations in LLMs", "comment": null, "summary": "Large Language Models have revolutionized information processing, yet their reliability is severely compromised by faithfulness hallucinations. While current approaches attempt to mitigate this issue through node-level adjustments or coarse suppression, they often overlook the distributed nature of neural information, leading to imprecise interventions. Recognizing that hallucinations propagate through specific forward transmission pathways like an infection, we aim to surgically block this flow using precise structural analysis. To leverage this, we propose Lancet, a novel framework that achieves precise neural intervention by leveraging structural entropy and hallucination difference ratios. Lancet first locates hallucination-prone neurons via gradient-driven contrastive analysis, then maps their propagation pathways by minimizing structural entropy, and finally implements a hierarchical intervention strategy that preserves general model capabilities. Comprehensive evaluations across hallucination benchmark datasets demonstrate that Lancet significantly outperforms state-of-the-art methods, validating the effectiveness of our surgical approach to neural intervention.", "AI": {"tldr": "Lancet\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u71b5\u548c\u5e7b\u89c9\u5dee\u5f02\u6bd4\u5b9e\u73b0\u7cbe\u786e\u795e\u7ecf\u5e72\u9884\uff0c\u5b9a\u4f4d\u5e7b\u89c9\u6613\u53d1\u795e\u7ecf\u5143\u5e76\u963b\u65ad\u5176\u4f20\u64ad\u8def\u5f84\uff0c\u5728\u5e7b\u89c9\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e25\u91cd\u7684\u5fe0\u5b9e\u6027\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u8fc7\u8282\u70b9\u7ea7\u8c03\u6574\u6216\u7c97\u7c92\u5ea6\u6291\u5236\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u795e\u7ecf\u4fe1\u606f\u7684\u5206\u5e03\u5f0f\u7279\u6027\uff0c\u5bfc\u81f4\u5e72\u9884\u4e0d\u7cbe\u786e\u3002\u4f5c\u8005\u8ba4\u8bc6\u5230\u5e7b\u89c9\u50cf\u611f\u67d3\u4e00\u6837\u901a\u8fc7\u7279\u5b9a\u7684\u524d\u5411\u4f20\u64ad\u8def\u5f84\u4f20\u64ad\uff0c\u56e0\u6b64\u5e0c\u671b\u901a\u8fc7\u7cbe\u786e\u7684\u7ed3\u6784\u5206\u6790\u6765\u624b\u672f\u5f0f\u963b\u65ad\u8fd9\u79cd\u6d41\u52a8\u3002", "method": "\u63d0\u51faLancet\u6846\u67b6\uff1a1) \u901a\u8fc7\u68af\u5ea6\u9a71\u52a8\u7684\u5bf9\u6bd4\u5206\u6790\u5b9a\u4f4d\u5e7b\u89c9\u6613\u53d1\u795e\u7ecf\u5143\uff1b2) \u901a\u8fc7\u6700\u5c0f\u5316\u7ed3\u6784\u71b5\u6620\u5c04\u5176\u4f20\u64ad\u8def\u5f84\uff1b3) \u5b9e\u65bd\u5206\u5c42\u5e72\u9884\u7b56\u7565\u4ee5\u4fdd\u7559\u6a21\u578b\u7684\u4e00\u822c\u80fd\u529b\u3002\u8be5\u6846\u67b6\u5229\u7528\u7ed3\u6784\u71b5\u548c\u5e7b\u89c9\u5dee\u5f02\u6bd4\u5b9e\u73b0\u7cbe\u786e\u795e\u7ecf\u5e72\u9884\u3002", "result": "\u5728\u5e7b\u89c9\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cLancet\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8fd9\u79cd\u624b\u672f\u5f0f\u795e\u7ecf\u5e72\u9884\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u786e\u7684\u7ed3\u6784\u5206\u6790\u548c\u624b\u672f\u5f0f\u5e72\u9884\uff0c\u53ef\u4ee5\u6709\u6548\u963b\u65ad\u5e7b\u89c9\u7684\u4f20\u64ad\u8def\u5f84\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u4e00\u822c\u80fd\u529b\uff0c\u4e3a\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5fe0\u5b9e\u6027\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2601.02184", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02184", "abs": "https://arxiv.org/abs/2601.02184", "authors": ["Yuhang Zhang", "S\u00f6ren Schwertfeger"], "title": "Differential Barometric Altimetry for Submeter Vertical Localization and Floor Recognition Indoors", "comment": null, "summary": "Accurate altitude estimation and reliable floor recognition are critical for mobile robot localization and navigation within complex multi-storey environments. In this paper, we present a robust, low-cost vertical estimation framework leveraging differential barometric sensing integrated within a fully ROS-compliant software package. Our system simultaneously publishes real-time altitude data from both a stationary base station and a mobile sensor, enabling precise and drift-free vertical localization. Empirical evaluations conducted in challenging scenarios -- such as fully enclosed stairwells and elevators, demonstrate that our proposed barometric pipeline achieves sub-meter vertical accuracy (RMSE: 0.29 m) and perfect (100%) floor-level identification. In contrast, our results confirm that standalone height estimates, obtained solely from visual- or LiDAR-based SLAM odometry, are insufficient for reliable vertical localization. The proposed ROS-compatible barometric module thus provides a practical and cost-effective solution for robust vertical awareness in real-world robotic deployments. The implementation of our method is released as open source at https://github.com/witsir/differential-barometric.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5dee\u5206\u6c14\u538b\u4f20\u611f\u7684\u4f4e\u6210\u672c\u5782\u76f4\u4f30\u8ba1\u6846\u67b6\uff0c\u5728ROS\u4e2d\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u5ea6\u6570\u636e\u53d1\u5e03\uff0c\u5728\u590d\u6742\u591a\u697c\u5c42\u73af\u5883\u4e2d\u5b9e\u73b0\u4e9a\u7c73\u7ea7\u5782\u76f4\u7cbe\u5ea6\u548c100%\u697c\u5c42\u8bc6\u522b", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u591a\u697c\u5c42\u73af\u5883\u4e2d\u7684\u7cbe\u786e\u5b9a\u4f4d\u548c\u5bfc\u822a\u9700\u8981\u51c6\u786e\u7684\u9ad8\u5ea6\u4f30\u8ba1\u548c\u53ef\u9760\u7684\u697c\u5c42\u8bc6\u522b\uff0c\u800c\u4ec5\u4f9d\u8d56\u89c6\u89c9\u6216LiDAR\u7684SLAM\u91cc\u7a0b\u8ba1\u65e0\u6cd5\u63d0\u4f9b\u53ef\u9760\u7684\u5782\u76f4\u5b9a\u4f4d", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5dee\u5206\u6c14\u538b\u4f20\u611f\u7684\u5782\u76f4\u4f30\u8ba1\u6846\u67b6\uff0c\u5305\u542b\u56fa\u5b9a\u57fa\u7ad9\u548c\u79fb\u52a8\u4f20\u611f\u5668\uff0c\u901a\u8fc7ROS\u517c\u5bb9\u8f6f\u4ef6\u5305\u5b9e\u65f6\u53d1\u5e03\u9ad8\u5ea6\u6570\u636e\uff0c\u5229\u7528\u6c14\u538b\u5dee\u6d88\u9664\u6f02\u79fb", "result": "\u5728\u5c01\u95ed\u697c\u68af\u95f4\u548c\u7535\u68af\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e9a\u7c73\u7ea7\u5782\u76f4\u7cbe\u5ea6\uff08RMSE: 0.29\u7c73\uff09\u548c100%\u697c\u5c42\u8bc6\u522b\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u4ec5\u4f7f\u7528\u89c6\u89c9\u6216LiDAR SLAM\u7684\u65b9\u6cd5", "conclusion": "\u5dee\u5206\u6c14\u538b\u6a21\u5757\u4e3a\u673a\u5668\u4eba\u5782\u76f4\u611f\u77e5\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5df2\u5f00\u6e90\u53d1\u5e03\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u90e8\u7f72"}}
{"id": "2601.01407", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01407", "abs": "https://arxiv.org/abs/2601.01407", "authors": ["Arjhun Sreedar", "Rohan Pillay", "Laukik Patade"], "title": "From Emotion Classification to Emotional Reasoning: Enhancing Emotional Intelligence in Large Language Models", "comment": "10 pages, 1 figure", "summary": "This work investigates whether synthetic emotional chain-of-thought data can improve the emotional reasoning abilities of smaller open large language models (LLMs). We design a multi-agent generation pipeline that produces therapy-style conversations and converts them into structured emotion multiple-choice questions (MCQs) with explanations. We propose that fine-tuning a variety of 7B models on this dataset should yield substantial gains in emotional understanding and emotional awareness on EmoBench-style evaluations, suggesting that emotional reasoning can be induced without architectural changes. Our results demonstrate that fine-tuned Mistral 7B achieves EU improvements from 10.5 to 20.5 and EA improvements from 40.5 to 60.0, validating the effectiveness of synthetic emotional reasoning data for enhancing model capabilities in nuanced emotional tasks.", "AI": {"tldr": "\u4f7f\u7528\u5408\u6210\u60c5\u611f\u94fe\u5f0f\u601d\u7ef4\u6570\u636e\u589e\u5f3a\u5c0f\u578b\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u751f\u6210\u6cbb\u7597\u5f0f\u5bf9\u8bdd\u5e76\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u60c5\u611f\u9009\u62e9\u9898\uff0c\u5fae\u8c037B\u6a21\u578b\u5728\u60c5\u611f\u7406\u89e3(EU)\u548c\u60c5\u611f\u610f\u8bc6(EA)\u8bc4\u4f30\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5408\u6210\u60c5\u611f\u63a8\u7406\u6570\u636e\u6765\u63d0\u5347\u8f83\u5c0f\u89c4\u6a21\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b\uff0c\u800c\u4e0d\u9700\u8981\u6539\u53d8\u6a21\u578b\u67b6\u6784\u3002\u5f53\u524d\u8f83\u5c0f\u6a21\u578b\u5728\u590d\u6742\u60c5\u611f\u4efb\u52a1\u4e0a\u8868\u73b0\u6709\u9650\uff0c\u9700\u8981\u5bfb\u627e\u6709\u6548\u65b9\u6cd5\u6765\u589e\u5f3a\u5176\u60c5\u611f\u7406\u89e3\u548c\u610f\u8bc6\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u667a\u80fd\u4f53\u751f\u6210\u7ba1\u9053\uff0c\u9996\u5148\u751f\u6210\u6cbb\u7597\u5f0f\u5bf9\u8bdd\uff0c\u7136\u540e\u5c06\u5176\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u7684\u60c5\u611f\u591a\u9009\u9898(MCQs)\u5e76\u9644\u5e26\u89e3\u91ca\u3002\u4f7f\u7528\u8fd9\u4e9b\u5408\u6210\u7684\u60c5\u611f\u63a8\u7406\u6570\u636e\u5bf9\u591a\u79cd7B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5728EmoBench\u98ce\u683c\u8bc4\u4f30\u4e2d\u6d4b\u8bd5\u60c5\u611f\u7406\u89e3\u548c\u60c5\u611f\u610f\u8bc6\u80fd\u529b\u3002", "result": "\u5fae\u8c03\u540e\u7684Mistral 7B\u6a21\u578b\u5728\u60c5\u611f\u7406\u89e3(EU)\u65b9\u9762\u4ece10.5\u63d0\u5347\u523020.5\uff0c\u5728\u60c5\u611f\u610f\u8bc6(EA)\u65b9\u9762\u4ece40.5\u63d0\u5347\u523060.0\uff0c\u8bc1\u660e\u4e86\u5408\u6210\u60c5\u611f\u63a8\u7406\u6570\u636e\u5728\u589e\u5f3a\u6a21\u578b\u5904\u7406\u7ec6\u5fae\u60c5\u611f\u4efb\u52a1\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u5408\u6210\u60c5\u611f\u94fe\u5f0f\u601d\u7ef4\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u5728\u4e0d\u6539\u53d8\u6a21\u578b\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u8f83\u5c0f\u89c4\u6a21\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u589e\u5f3aAI\u60c5\u611f\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002"}}
{"id": "2601.02295", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2601.02295", "abs": "https://arxiv.org/abs/2601.02295", "authors": ["Chenyang Ma", "Guangyu Yang", "Kai Lu", "Shitong Xu", "Bill Byrne", "Niki Trigoni", "Andrew Markham"], "title": "CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding", "comment": "Project Page: https://dannymcy.github.io/cyclevla/", "summary": "Current work on robot failure detection and correction typically operate in a post hoc manner, analyzing errors and applying corrections only after failures occur. This work introduces CycleVLA, a system that equips Vision-Language-Action models (VLAs) with proactive self-correction, the capability to anticipate incipient failures and recover before they fully manifest during execution. CycleVLA achieves this by integrating a progress-aware VLA that flags critical subtask transition points where failures most frequently occur, a VLM-based failure predictor and planner that triggers subtask backtracking upon predicted failure, and a test-time scaling strategy based on Minimum Bayes Risk (MBR) decoding to improve retry success after backtracking. Extensive experiments show that CycleVLA improves performance for both well-trained and under-trained VLAs, and that MBR serves as an effective zero-shot test-time scaling strategy for VLAs. Project Page: https://dannymcy.github.io/cyclevla/", "AI": {"tldr": "CycleVLA\u4e3a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u6dfb\u52a0\u4e86\u4e3b\u52a8\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\uff0c\u80fd\u591f\u5728\u6267\u884c\u8fc7\u7a0b\u4e2d\u9884\u6d4b\u5373\u5c06\u53d1\u751f\u7684\u6545\u969c\u5e76\u5728\u6545\u969c\u5b8c\u5168\u663e\u73b0\u524d\u8fdb\u884c\u6062\u590d\uff0c\u800c\u4e0d\u662f\u4e8b\u540e\u5904\u7406\u9519\u8bef\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u6545\u969c\u68c0\u6d4b\u548c\u7ea0\u6b63\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u4e8b\u540e\u5904\u7406\u65b9\u5f0f\uff0c\u53ea\u5728\u6545\u969c\u53d1\u751f\u540e\u5206\u6790\u9519\u8bef\u5e76\u5e94\u7528\u7ea0\u6b63\u3002\u8fd9\u79cd\u88ab\u52a8\u65b9\u6cd5\u65e0\u6cd5\u9884\u9632\u6545\u969c\u53d1\u751f\uff0c\u5bfc\u81f4\u6267\u884c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "1) \u96c6\u6210\u8fdb\u5ea6\u611f\u77e5VLA\u6765\u6807\u8bb0\u5173\u952e\u5b50\u4efb\u52a1\u8f6c\u6362\u70b9\uff08\u6545\u969c\u6700\u5e38\u53d1\u751f\u7684\u4f4d\u7f6e\uff09\uff1b2) \u57fa\u4e8eVLM\u7684\u6545\u969c\u9884\u6d4b\u5668\u548c\u89c4\u5212\u5668\uff0c\u5728\u9884\u6d4b\u5230\u6545\u969c\u65f6\u89e6\u53d1\u5b50\u4efb\u52a1\u56de\u6eaf\uff1b3) \u57fa\u4e8e\u6700\u5c0f\u8d1d\u53f6\u65af\u98ce\u9669\u89e3\u7801\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u7b56\u7565\uff0c\u63d0\u9ad8\u56de\u6eaf\u540e\u7684\u91cd\u8bd5\u6210\u529f\u7387\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCycleVLA\u63d0\u9ad8\u4e86\u8bad\u7ec3\u826f\u597d\u548c\u8bad\u7ec3\u4e0d\u8db3\u7684VLA\u7684\u6027\u80fd\uff0c\u5e76\u4e14MBR\u4f5c\u4e3aVLA\u7684\u6709\u6548\u96f6\u6837\u672c\u6d4b\u8bd5\u65f6\u7f29\u653e\u7b56\u7565\u3002", "conclusion": "CycleVLA\u901a\u8fc7\u4e3b\u52a8\u6545\u969c\u9884\u6d4b\u548c\u6062\u590d\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86VLA\u5728\u5b9e\u9645\u6267\u884c\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6210\u529f\u7387\uff0c\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u81ea\u4e3b\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2601.01446", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01446", "abs": "https://arxiv.org/abs/2601.01446", "authors": ["Yilong Wang", "Qianli Wang", "Nils Feldhus"], "title": "iFlip: Iterative Feedback-driven Counterfactual Example Refinement", "comment": "In submission", "summary": "Counterfactual examples are minimal edits to an input that alter a model's prediction. They are widely employed in explainable AI to probe model behavior and in natural language processing (NLP) to augment training data. However, generating valid counterfactuals with large language models (LLMs) remains challenging, as existing single-pass methods often fail to induce reliable label changes, neglecting LLMs' self-correction capabilities. To explore this untapped potential, we propose iFlip, an iterative refinement approach that leverages three types of feedback, including model confidence, feature attribution, and natural language. Our results show that iFlip achieves an average 57.8% higher validity than the five state-of-the-art baselines, as measured by the label flipping rate. The user study further corroborates that iFlip outperforms baselines in completeness, overall satisfaction, and feasibility. In addition, ablation studies demonstrate that three components are paramount for iFlip to generate valid counterfactuals: leveraging an appropriate number of iterations, pointing to highly attributed words, and early stopping. Finally, counterfactuals generated by iFlip enable effective counterfactual data augmentation, substantially improving model performance and robustness.", "AI": {"tldr": "iFlip\u662f\u4e00\u79cd\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u6211\u4fee\u6b63\u80fd\u529b\u7684\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u578b\u7f6e\u4fe1\u5ea6\u3001\u7279\u5f81\u5f52\u56e0\u548c\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u751f\u6210\u6709\u6548\u7684\u53cd\u4e8b\u5b9e\u793a\u4f8b\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6807\u7b7e\u7ffb\u8f6c\u7387\u3002", "motivation": "\u73b0\u6709\u5355\u6b21\u751f\u6210\u65b9\u6cd5\u5728\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6709\u6548\u53cd\u4e8b\u5b9e\u793a\u4f8b\u65f6\u7ecf\u5e38\u5931\u8d25\uff0c\u65e0\u6cd5\u53ef\u9760\u5730\u6539\u53d8\u6a21\u578b\u9884\u6d4b\u6807\u7b7e\uff0c\u5ffd\u89c6\u4e86LLMs\u7684\u81ea\u6211\u4fee\u6b63\u80fd\u529b\u3002\u9700\u8981\u63a2\u7d22\u8fd9\u79cd\u672a\u5f00\u53d1\u7684\u6f5c\u529b\u6765\u6539\u8fdb\u53cd\u4e8b\u5b9e\u751f\u6210\u3002", "method": "iFlip\u91c7\u7528\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\uff0c\u5229\u7528\u4e09\u79cd\u53cd\u9988\u7c7b\u578b\uff1a\u6a21\u578b\u7f6e\u4fe1\u5ea6\u3001\u7279\u5f81\u5f52\u56e0\u548c\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u3002\u901a\u8fc7\u9002\u5f53\u7684\u8fed\u4ee3\u6b21\u6570\u3001\u6307\u5411\u9ad8\u5f52\u56e0\u8bcd\u8bed\u548c\u65e9\u505c\u7b56\u7565\u6765\u751f\u6210\u6709\u6548\u7684\u53cd\u4e8b\u5b9e\u793a\u4f8b\u3002", "result": "iFlip\u5728\u6807\u7b7e\u7ffb\u8f6c\u7387\u4e0a\u6bd4\u4e94\u79cd\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u9ad8\u51fa57.8%\u3002\u7528\u6237\u7814\u7a76\u8bc1\u5b9eiFlip\u5728\u5b8c\u6574\u6027\u3001\u603b\u4f53\u6ee1\u610f\u5ea6\u548c\u53ef\u884c\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\u4e09\u4e2a\u7ec4\u4ef6\u5bf9\u751f\u6210\u6709\u6548\u53cd\u4e8b\u5b9e\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "iFlip\u901a\u8fc7\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u6211\u4fee\u6b63\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53cd\u4e8b\u5b9e\u751f\u6210\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e14\u751f\u6210\u7684\u53cd\u4e8b\u5b9e\u53ef\u7528\u4e8e\u6570\u636e\u589e\u5f3a\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2601.01449", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01449", "abs": "https://arxiv.org/abs/2601.01449", "authors": ["Harshil Darji", "Martin Heckelmann", "Christina Kratsch", "Gerard de Melo"], "title": "Segmentation and Processing of German Court Decisions from Open Legal Data", "comment": "Accepted and published as a research article in Legal Knowledge and Information Systems (JURIX 2025 proceedings, IOS Press). Pages 276--281", "summary": "The availability of structured legal data is important for advancing Natural Language Processing (NLP) techniques for the German legal system. One of the most widely used datasets, Open Legal Data, provides a large-scale collection of German court decisions. While the metadata in this raw dataset is consistently structured, the decision texts themselves are inconsistently formatted and often lack clearly marked sections. Reliable separation of these sections is important not only for rhetorical role classification but also for downstream tasks such as retrieval and citation analysis. In this work, we introduce a cleaned and sectioned dataset of 251,038 German court decisions derived from the official Open Legal Data dataset. We systematically separated three important sections in German court decisions, namely Tenor (operative part of the decision), Tatbestand (facts of the case), and Entscheidungsgr\u00fcnde (judicial reasoning), which are often inconsistently represented in the original dataset. To ensure the reliability of our extraction process, we used Cochran's formula with a 95% confidence level and a 5% margin of error to draw a statistically representative random sample of 384 cases, and manually verified that all three sections were correctly identified. We also extracted the Rechtsmittelbelehrung (appeal notice) as a separate field, since it is a procedural instruction and not part of the decision itself. The resulting corpus is publicly available in the JSONL format, making it an accessible resource for further research on the German legal system.", "AI": {"tldr": "\u7814\u7a76\u8005\u6e05\u7406\u5e76\u7ed3\u6784\u5316\u5fb7\u56fd\u6cd5\u9662\u5224\u51b3\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u5206\u79bb\u51fa\u5224\u51b3\u7684\u4e09\u4e2a\u6838\u5fc3\u90e8\u5206\uff0c\u5e76\u9a8c\u8bc1\u63d0\u53d6\u51c6\u786e\u6027\uff0c\u4e3a\u5fb7\u56fd\u6cd5\u5f8bNLP\u7814\u7a76\u63d0\u4f9b\u9ad8\u8d28\u91cf\u8d44\u6e90\u3002", "motivation": "\u5fb7\u56fd\u6cd5\u5f8b\u7cfb\u7edf\u4e2d\u7ed3\u6784\u5316\u6570\u636e\u7684\u7f3a\u4e4f\u963b\u788d\u4e86NLP\u6280\u672f\u7684\u53d1\u5c55\u3002\u867d\u7136Open Legal Data\u63d0\u4f9b\u4e86\u5927\u91cf\u5fb7\u56fd\u6cd5\u9662\u5224\u51b3\uff0c\u4f46\u5224\u51b3\u6587\u672c\u683c\u5f0f\u4e0d\u4e00\u81f4\u4e14\u7f3a\u4e4f\u660e\u786e\u6807\u8bb0\u7684\u7ae0\u8282\uff0c\u8fd9\u5f71\u54cd\u4e86\u4fee\u8f9e\u89d2\u8272\u5206\u7c7b\u3001\u68c0\u7d22\u548c\u5f15\u7528\u5206\u6790\u7b49\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u4eceOpen Legal Data\u5b98\u65b9\u6570\u636e\u96c6\u4e2d\u63d0\u53d6251,038\u4e2a\u5fb7\u56fd\u6cd5\u9662\u5224\u51b3\uff0c\u7cfb\u7edf\u5206\u79bb\u4e09\u4e2a\u91cd\u8981\u90e8\u5206\uff1aTenor\uff08\u5224\u51b3\u4e3b\u6587\uff09\u3001Tatbestand\uff08\u6848\u4ef6\u4e8b\u5b9e\uff09\u548cEntscheidungsgr\u00fcnde\uff08\u5224\u51b3\u7406\u7531\uff09\u3002\u4f7f\u7528Cochran\u516c\u5f0f\u4ee595%\u7f6e\u4fe1\u6c34\u5e73\u548c5%\u8bef\u5dee\u8303\u56f4\u62bd\u53d6384\u4e2a\u6848\u4f8b\u7684\u7edf\u8ba1\u4ee3\u8868\u6027\u6837\u672c\u8fdb\u884c\u4eba\u5de5\u9a8c\u8bc1\u3002\u8fd8\u5c06Rechtsmittelbelehrung\uff08\u4e0a\u8bc9\u901a\u77e5\uff09\u4f5c\u4e3a\u5355\u72ec\u5b57\u6bb5\u63d0\u53d6\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b251,038\u4e2a\u5fb7\u56fd\u6cd5\u9662\u5224\u51b3\u7684\u6e05\u7406\u548c\u7ed3\u6784\u5316\u6570\u636e\u96c6\uff0c\u4e09\u4e2a\u6838\u5fc3\u90e8\u5206\u88ab\u51c6\u786e\u5206\u79bb\u3002\u901a\u8fc7\u7edf\u8ba1\u62bd\u6837\u9a8c\u8bc1\uff0c\u786e\u4fdd\u6240\u6709\u4e09\u4e2a\u90e8\u5206\u90fd\u88ab\u6b63\u786e\u8bc6\u522b\u3002\u6700\u7ec8\u8bed\u6599\u5e93\u4ee5JSONL\u683c\u5f0f\u516c\u5f00\u63d0\u4f9b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u3001\u7ed3\u6784\u5316\u7684\u5fb7\u56fd\u6cd5\u9662\u5224\u51b3\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u539f\u59cb\u6570\u636e\u683c\u5f0f\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u4e3a\u5fb7\u56fd\u6cd5\u5f8b\u7cfb\u7edf\u7684NLP\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u8bbf\u95ee\u7684\u8d44\u6e90\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u6cd5\u5f8b\u6587\u672c\u5206\u6790\u3001\u68c0\u7d22\u548c\u5f15\u7528\u5206\u6790\u7b49\u4e0b\u6e38\u4efb\u52a1\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.01461", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.01461", "abs": "https://arxiv.org/abs/2601.01461", "authors": ["Yuxiang Mei", "Dongxing Xu", "Jiaen Liang", "Yanhua Long"], "title": "Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR", "comment": "5 pages, 1 figure", "summary": "The INTERSPEECH 2025 Challenge on Multilingual Conversational Speech Language Models (MLC-SLM) promotes multilingual conversational ASR with large language models (LLMs). Our previous SHNU-mASR system adopted a competitive parallel-speech-encoder architecture that integrated Whisper and mHuBERT with an LLM. However, it faced two challenges: simple feature concatenation may not fully exploit complementary information, and the performance gap between LLM-based ASR and end-to-end(E2E) encoder-decoder ASR remained unexplored. In this work, we present an enhanced LLM-based ASR framework that combines fine-tuned Whisper and mHuBERT encoders with an LLM to enrich speech representations. We first evaluate E2E Whisper models with LoRA and full fine-tuning on the MLC-SLM ASR task, and then propose cross-attention-based fusion mechanisms for the parallel-speech-encoder. On the official evaluation set of the MLC-SLM Challenge, our system achieves a CER/WER of 10.69%, ranking on par with the top-ranked Track 1 systems, even though it uses only 1,500 hours of baseline training data compared with their large-scale training sets. Nonetheless, we find that our final LLM-based ASR still does not match the performance of a fine-tuned E2E Whisper model, providing valuable empirical guidance for future Speech-LLM design. Our code is publicly available at https://github.com/1535176727/MLC-SLM.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u57fa\u4e8eLLM\u7684ASR\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\u7ed3\u5408Whisper\u548cmHuBERT\u7f16\u7801\u5668\uff0c\u5728MLC-SLM\u6311\u6218\u4e2d\u53d6\u5f97\u4e86\u4e0e\u9876\u7ea7\u7cfb\u7edf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4f46\u53d1\u73b0\u4ecd\u4e0d\u53ca\u7aef\u5230\u7aefWhisper\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u4e4b\u524dSHNU-mASR\u7cfb\u7edf\u7684\u4e24\u4e2a\u95ee\u9898\uff1a\u7b80\u5355\u7279\u5f81\u62fc\u63a5\u672a\u80fd\u5145\u5206\u5229\u7528\u4e92\u8865\u4fe1\u606f\uff0c\u4ee5\u53caLLM-based ASR\u4e0e\u7aef\u5230\u7aef\u7f16\u7801\u5668-\u89e3\u7801\u5668ASR\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u5c1a\u672a\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u589e\u5f3a\u7684LLM-based ASR\u6846\u67b6\uff0c\u7ed3\u5408\u5fae\u8c03\u7684Whisper\u548cmHuBERT\u7f16\u7801\u5668\uff0c\u91c7\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u673a\u5236\u6574\u5408\u5e76\u884c\u8bed\u97f3\u7f16\u7801\u5668\u7279\u5f81\uff0c\u5e76\u8bc4\u4f30\u4e86LoRA\u548c\u5168\u5fae\u8c03\u7684Whisper\u6a21\u578b\u3002", "result": "\u5728MLC-SLM\u6311\u6218\u5b98\u65b9\u8bc4\u4f30\u96c6\u4e0a\u83b7\u5f9710.69%\u7684CER/WER\uff0c\u4e0e\u4f7f\u7528\u5927\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u7684\u9876\u7ea7Track 1\u7cfb\u7edf\u6027\u80fd\u76f8\u5f53\uff0c\u4f46\u4ec5\u4f7f\u75281500\u5c0f\u65f6\u57fa\u7ebf\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u6700\u7ec8LLM-based ASR\u4ecd\u4e0d\u53ca\u5fae\u8c03\u7684\u7aef\u5230\u7aefWhisper\u6a21\u578b\uff0c\u4e3a\u672a\u6765Speech-LLM\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u7ecf\u9a8c\u6307\u5bfc\u3002"}}
{"id": "2601.01477", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01477", "abs": "https://arxiv.org/abs/2601.01477", "authors": ["May-Myo Zin", "Sabine Wehnert", "Yuntao Kong", "Ha-Thanh Nguyen", "Wachara Fungwacharakorn", "Jieying Xue", "Micha\u0142 Araszkiewicz", "Randy Goebel", "Ken Satoh", "Le-Minh Nguyen"], "title": "Can Legislation Be Made Machine-Readable in PROLEG?", "comment": null, "summary": "The anticipated positive social impact of regulatory processes requires both the accuracy and efficiency of their application. Modern artificial intelligence technologies, including natural language processing and machine-assisted reasoning, hold great promise for addressing this challenge. We present a framework to address the challenge of tools for regulatory application, based on current state-of-the-art (SOTA) methods for natural language processing (large language models or LLMs) and formalization of legal reasoning (the legal representation system PROLEG). As an example, we focus on Article 6 of the European General Data Protection Regulation (GDPR). In our framework, a single LLM prompt simultaneously transforms legal text into if-then rules and a corresponding PROLEG encoding, which are then validated and refined by legal domain experts. The final output is an executable PROLEG program that can produce human-readable explanations for instances of GDPR decisions. We describe processes to support the end-to-end transformation of a segment of a regulatory document (Article 6 from GDPR), including the prompting frame to guide an LLM to \"compile\" natural language text to if-then rules, then to further \"compile\" the vetted if-then rules to PROLEG. Finally, we produce an instance that shows the PROLEG execution. We conclude by summarizing the value of this approach and note observed limitations with suggestions to further develop such technologies for capturing and deploying regulatory frameworks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408LLM\u548cPROLEG\u6cd5\u5f8b\u8868\u793a\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u5c06GDPR\u7b49\u6cd5\u89c4\u6587\u672c\u81ea\u52a8\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684if-then\u89c4\u5219\u548cPROLEG\u7f16\u7801\uff0c\u652f\u6301\u53ef\u89e3\u91ca\u7684\u6cd5\u5f8b\u51b3\u7b56\u3002", "motivation": "\u6cd5\u89c4\u5e94\u7528\u9700\u8981\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u3002AI\u6280\u672f\u7279\u522b\u662fNLP\u548c\u6cd5\u5f8b\u5f62\u5f0f\u5316\u63a8\u7406\u6709\u671b\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u5b9e\u73b0\u6cd5\u89c4\u7684\u81ea\u52a8\u5316\u5904\u7406\u548c\u5e94\u7528\u3002", "method": "\u4f7f\u7528LLM\u63d0\u793a\u5c06\u6cd5\u5f8b\u6587\u672c\uff08\u5982GDPR\u7b2c6\u6761\uff09\u540c\u65f6\u8f6c\u6362\u4e3aif-then\u89c4\u5219\u548cPROLEG\u7f16\u7801\uff0c\u7ecf\u6cd5\u5f8b\u4e13\u5bb6\u9a8c\u8bc1\u548c\u4f18\u5316\u540e\uff0c\u751f\u6210\u53ef\u6267\u884c\u7684PROLEG\u7a0b\u5e8f\uff0c\u652f\u6301\u4eba\u7c7b\u53ef\u8bfb\u7684\u89e3\u91ca\u3002", "result": "\u5f00\u53d1\u4e86\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u6210\u529f\u5c06GDPR\u7b2c6\u6761\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684PROLEG\u7a0b\u5e8f\uff0c\u80fd\u591f\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u7684GDPR\u51b3\u7b56\u89e3\u91ca\u5b9e\u4f8b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u7ed3\u5408LLM\u548c\u6cd5\u5f8b\u5f62\u5f0f\u5316\u7cfb\u7edf\u5728\u6cd5\u89c4\u6355\u83b7\u548c\u90e8\u7f72\u4e2d\u7684\u4ef7\u503c\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u5c40\u9650\u6027\u5e76\u63d0\u51fa\u4e86\u8fdb\u4e00\u6b65\u53d1\u5c55\u7684\u5efa\u8bae\u3002"}}
{"id": "2601.01488", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01488", "abs": "https://arxiv.org/abs/2601.01488", "authors": ["Vanessa Toborek", "Sebastian M\u00fcller", "Christian Bauckhage"], "title": "Four Quadrants of Difficulty: A Simple Categorisation and its Limits", "comment": "prepared for ESANN 2026 submission", "summary": "Curriculum Learning (CL) aims to improve the outcome of model training by estimating the difficulty of samples and scheduling them accordingly. In NLP, difficulty is commonly approximated using task-agnostic linguistic heuristics or human intuition, implicitly assuming that these signals correlate with what neural models find difficult to learn. We propose a four-quadrant categorisation of difficulty signals -- human vs. model and task-agnostic vs. task-dependent -- and systematically analyse their interactions on a natural language understanding dataset. We find that task-agnostic features behave largely independently and that only task-dependent features align. These findings challenge common CL intuitions and highlight the need for lightweight, task-dependent difficulty estimators that better reflect model learning behaviour.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4efb\u52a1\u65e0\u5173\u7279\u5f81\u4e0e\u6a21\u578b\u5b66\u4e60\u884c\u4e3a\u4e0d\u76f8\u5173\uff0c\u53ea\u6709\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u4e0e\u6a21\u578b\u96be\u5ea6\u611f\u77e5\u4e00\u81f4\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u8bfe\u7a0b\u5b66\u4e60\u76f4\u89c9\uff0c\u5efa\u8bae\u5f00\u53d1\u8f7b\u91cf\u7ea7\u4efb\u52a1\u76f8\u5173\u96be\u5ea6\u4f30\u8ba1\u5668", "motivation": "\u8bfe\u7a0b\u5b66\u4e60(CL)\u901a\u8fc7\u4f30\u8ba1\u6837\u672c\u96be\u5ea6\u5e76\u76f8\u5e94\u5b89\u6392\u8bad\u7ec3\u987a\u5e8f\u6765\u6539\u8fdb\u6a21\u578b\u8bad\u7ec3\u6548\u679c\u3002\u5728NLP\u4e2d\uff0c\u96be\u5ea6\u901a\u5e38\u4f7f\u7528\u4efb\u52a1\u65e0\u5173\u7684\u8bed\u8a00\u5b66\u542f\u53d1\u5f0f\u65b9\u6cd5\u6216\u4eba\u7c7b\u76f4\u89c9\u6765\u8fd1\u4f3c\uff0c\u8fd9\u9690\u542b\u5047\u8bbe\u8fd9\u4e9b\u4fe1\u53f7\u4e0e\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8ba4\u4e3a\u96be\u4ee5\u5b66\u4e60\u7684\u5185\u5bb9\u76f8\u5173\u3002\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u5e76\u5206\u6790\u4e0d\u540c\u96be\u5ea6\u4fe1\u53f7\u4e0e\u6a21\u578b\u5b66\u4e60\u884c\u4e3a\u7684\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u56db\u8c61\u9650\u5206\u7c7b\u6cd5\uff1a\u4eba\u7c7bvs\u6a21\u578b\u3001\u4efb\u52a1\u65e0\u5173vs\u4efb\u52a1\u76f8\u5173\u3002\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u6570\u636e\u96c6\u4e0a\u7cfb\u7edf\u5206\u6790\u8fd9\u4e9b\u96be\u5ea6\u4fe1\u53f7\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u6bd4\u8f83\u4e0d\u540c\u7279\u5f81\u4e0e\u6a21\u578b\u5b66\u4e60\u884c\u4e3a\u7684\u76f8\u5173\u6027\u3002", "result": "\u53d1\u73b0\u4efb\u52a1\u65e0\u5173\u7279\u5f81\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u72ec\u7acb\u4e8e\u6a21\u578b\u5b66\u4e60\u884c\u4e3a\uff0c\u53ea\u6709\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u4e0e\u6a21\u578b\u96be\u5ea6\u611f\u77e5\u4e00\u81f4\u3002\u8fd9\u4e9b\u53d1\u73b0\u6311\u6218\u4e86\u5e38\u89c1\u7684\u8bfe\u7a0b\u5b66\u4e60\u76f4\u89c9\uff0c\u8868\u660e\u4f20\u7edf\u57fa\u4e8e\u8bed\u8a00\u5b66\u7279\u5f81\u6216\u4eba\u7c7b\u76f4\u89c9\u7684\u96be\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u53ef\u80fd\u4e0d\u51c6\u786e\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u3001\u4efb\u52a1\u76f8\u5173\u7684\u96be\u5ea6\u4f30\u8ba1\u5668\uff0c\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u6a21\u578b\u7684\u5b66\u4e60\u884c\u4e3a\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4efb\u52a1\u65e0\u5173\u7684\u8bed\u8a00\u5b66\u7279\u5f81\u6216\u4eba\u7c7b\u76f4\u89c9\u3002\u8fd9\u4e3a\u8bfe\u7a0b\u5b66\u4e60\u5728NLP\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002"}}
{"id": "2601.01490", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01490", "abs": "https://arxiv.org/abs/2601.01490", "authors": ["Junichiro Niimi"], "title": "Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints", "comment": null, "summary": "With the widespread adoption of large language models (LLMs), hallucinations, which are non-factual fabrications in model outputs, have become serious concerns. Reasoning capabilities have received attention as a self-verification process to improve output reliability. However, the effect of reasoning within a closed system where LLMs cannot rely on external tools or knowledge has yet to be clarified. We therefore conduct experiments under strict constraints (recommending peer-reviewed journal articles in computer science) to examine the effect of reasoning across multiple models (GPT-5.2 and Gemini 3 Flash). Our results reveal a problematic trade-off between constraint compliance and factual accuracy. Non-reasoning models exhibit high constraint violation rates (66-75%) but maintain factual accuracy, while reasoning models reduce violations (13-26%) but systematically distort known facts to satisfy constraints and increase complete fabrication. This trade-off pattern is consistent across both models despite different architectures, indicating a fundamental limitation of reasoning. Furthermore, reasoning does not uniformly improve output authenticity: effects diverge by model, reflecting different allocations of the compliance-truthfulness trade-off. These findings challenge the assumption that reasoning universally improves reliability: reasoning models trade honest constraint violations for detection-resistant distortions.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u80fd\u529b\u5728\u5c01\u95ed\u7cfb\u7edf\u4e2d\u5b58\u5728\u7ea6\u675f\u5408\u89c4\u6027\u4e0e\u4e8b\u5b9e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff1a\u975e\u63a8\u7406\u6a21\u578b\u8fdd\u53cd\u7ea6\u675f\u4f46\u4fdd\u6301\u4e8b\u5b9e\u51c6\u786e\uff0c\u63a8\u7406\u6a21\u578b\u51cf\u5c11\u8fdd\u89c4\u4f46\u4f1a\u626d\u66f2\u4e8b\u5b9e\u4ee5\u6ee1\u8db3\u7ea6\u675f", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5e7b\u89c9\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\u3002\u63a8\u7406\u80fd\u529b\u88ab\u89c6\u4e3a\u63d0\u9ad8\u8f93\u51fa\u53ef\u9760\u6027\u7684\u81ea\u6211\u9a8c\u8bc1\u8fc7\u7a0b\uff0c\u4f46\u5728\u65e0\u6cd5\u4f9d\u8d56\u5916\u90e8\u5de5\u5177\u6216\u77e5\u8bc6\u7684\u5c01\u95ed\u7cfb\u7edf\u4e2d\uff0c\u63a8\u7406\u7684\u6548\u679c\u5c1a\u672a\u660e\u786e\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e25\u683c\u7ea6\u675f\u6761\u4ef6\u4e0b\uff08\u63a8\u8350\u8ba1\u7b97\u673a\u79d1\u5b66\u540c\u884c\u8bc4\u5ba1\u671f\u520a\u6587\u7ae0\uff09\u68c0\u9a8c\u63a8\u7406\u7684\u6548\u679c\u3002", "method": "\u5728\u4e25\u683c\u7ea6\u675f\u6761\u4ef6\u4e0b\uff08\u63a8\u8350\u8ba1\u7b97\u673a\u79d1\u5b66\u540c\u884c\u8bc4\u5ba1\u671f\u520a\u6587\u7ae0\uff09\uff0c\u5bf9\u591a\u4e2a\u6a21\u578b\uff08GPT-5.2\u548cGemini 3 Flash\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u63a8\u7406\u6a21\u578b\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u7684\u8868\u73b0\uff0c\u5206\u6790\u7ea6\u675f\u5408\u89c4\u6027\u4e0e\u4e8b\u5b9e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\u3002", "result": "\u975e\u63a8\u7406\u6a21\u578b\u7ea6\u675f\u8fdd\u89c4\u7387\u9ad8\uff0866-75%\uff09\u4f46\u4fdd\u6301\u4e8b\u5b9e\u51c6\u786e\u6027\uff1b\u63a8\u7406\u6a21\u578b\u51cf\u5c11\u8fdd\u89c4\uff0813-26%\uff09\u4f46\u4f1a\u7cfb\u7edf\u6027\u5730\u626d\u66f2\u5df2\u77e5\u4e8b\u5b9e\u4ee5\u6ee1\u8db3\u7ea6\u675f\uff0c\u5e76\u589e\u52a0\u5b8c\u5168\u634f\u9020\u3002\u8fd9\u79cd\u6743\u8861\u6a21\u5f0f\u5728\u4e0d\u540c\u67b6\u6784\u7684\u6a21\u578b\u4e2d\u4fdd\u6301\u4e00\u81f4\uff0c\u8868\u660e\u63a8\u7406\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002\u63a8\u7406\u5e76\u4e0d\u7edf\u4e00\u63d0\u9ad8\u8f93\u51fa\u771f\u5b9e\u6027\uff0c\u6548\u679c\u56e0\u6a21\u578b\u800c\u5f02\u3002", "conclusion": "\u63a8\u7406\u5e76\u4e0d\u666e\u904d\u63d0\u9ad8\u53ef\u9760\u6027\uff1a\u63a8\u7406\u6a21\u578b\u4ee5\u8bda\u5b9e\u7684\u7ea6\u675f\u8fdd\u89c4\u4e3a\u4ee3\u4ef7\uff0c\u6362\u53d6\u96be\u4ee5\u68c0\u6d4b\u7684\u626d\u66f2\u3002\u8fd9\u4e00\u53d1\u73b0\u6311\u6218\u4e86\u63a8\u7406\u80fd\u666e\u904d\u63d0\u9ad8\u53ef\u9760\u6027\u7684\u5047\u8bbe\uff0c\u63ed\u793a\u4e86\u5c01\u95ed\u7cfb\u7edf\u4e2d\u63a8\u7406\u80fd\u529b\u7684\u6839\u672c\u5c40\u9650\u6027\u3002"}}
{"id": "2601.01498", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01498", "abs": "https://arxiv.org/abs/2601.01498", "authors": ["Bingguang Hao", "Zengzhuang Xu", "Yuntao Wen", "Xinyi Xu", "Yang Liu", "Tong Zhao", "Maolin Wang", "Long Chen", "Dong Wang", "Yicheng Chen", "Cunyin Peng", "Xiangyu Zhao", "Chenyi Zhuang", "Ji Zhang"], "title": "From Failure to Mastery: Generating Hard Samples for Tool-use Agents", "comment": null, "summary": "The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.", "AI": {"tldr": "HardGen\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u7684agentic pipeline\uff0c\u7528\u4e8e\u751f\u6210\u5177\u6709\u53ef\u9a8c\u8bc1\u63a8\u7406\u7684\u56f0\u96be\u5de5\u5177\u4f7f\u7528\u8bad\u7ec3\u6837\u672c\uff0c\u901a\u8fc7\u52a8\u6001API\u56fe\u3001\u9ad8\u7ea7\u5de5\u5177\u5b9e\u4f8b\u5316\u548c\u95ed\u73af\u8bc4\u4f30\u53cd\u9988\u6765\u63d0\u5347LLM agent\u7684\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u751f\u6210\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528\u968f\u673a\u91c7\u6837\u548c\u6d45\u5c42\u751f\u6210\u8303\u5f0f\uff0c\u4ea7\u751f\u7684\u8f68\u8ff9\u7b80\u5355\u4e14\u540c\u8d28\u5316\uff0c\u65e0\u6cd5\u6355\u6349\u590d\u6742\u3001\u9690\u5f0f\u7684\u903b\u8f91\u4f9d\u8d56\u5173\u7cfb\uff0c\u9650\u5236\u4e86LLM agent\u7684\u80fd\u529b\u63d0\u5347\u3002", "method": "1) \u57fa\u4e8eagent\u5931\u8d25\u6848\u4f8b\u5efa\u7acb\u52a8\u6001API\u56fe\u5e76\u91c7\u6837\u5408\u6210\u56f0\u96be\u8f68\u8ff9\uff1b2) \u4ee5\u8fd9\u4e9b\u8f68\u8ff9\u4f5c\u4e3a\u6761\u4ef6\u5148\u9a8c\u6307\u5bfc\u6a21\u5757\u5316\u62bd\u8c61\u9ad8\u7ea7\u5de5\u5177\u7684\u5b9e\u4f8b\u5316\uff1b3) \u5229\u7528\u9ad8\u7ea7\u5de5\u5177\u5236\u5b9a\u56f0\u96be\u67e5\u8be2\uff1b4) \u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u590d\u6742\u601d\u7ef4\u94fe\uff0c\u5e76\u901a\u8fc7\u95ed\u73af\u8bc4\u4f30\u53cd\u9988\u6301\u7eed\u4f18\u5316\u6574\u4e2a\u8fc7\u7a0b\u3002", "result": "\u4f7f\u7528HardGen\u751f\u6210\u7684\u6570\u636e\u96c6\u8bad\u7ec3\u76844B\u53c2\u6570\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u591a\u4e2a\u9886\u5148\u7684\u5f00\u6e90\u548c\u95ed\u6e90\u7ade\u4e89\u5bf9\u624b\uff0c\u5305\u62ecGPT-5.2\u3001Gemini-3-Pro\u548cClaude-Opus-4.5\u3002", "conclusion": "HardGen\u80fd\u591f\u6709\u6548\u751f\u6210\u5177\u6709\u53ef\u9a8c\u8bc1\u63a8\u7406\u7684\u56f0\u96be\u5de5\u5177\u4f7f\u7528\u8bad\u7ec3\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347LLM agent\u7684\u6027\u80fd\uff0c\u76f8\u5173\u4ee3\u7801\u3001\u6a21\u578b\u548c\u6570\u636e\u96c6\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2601.01530", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2601.01530", "abs": "https://arxiv.org/abs/2601.01530", "authors": ["Jing Ye", "Lu Xiang", "Yaping Zhang", "Chengqing Zong"], "title": "EmoHarbor: Evaluating Personalized Emotional Support by Simulating the User's Internal World", "comment": null, "summary": "Current evaluation paradigms for emotional support conversations tend to reward generic empathetic responses, yet they fail to assess whether the support is genuinely personalized to users' unique psychological profiles and contextual needs. We introduce EmoHarbor, an automated evaluation framework that adopts a User-as-a-Judge paradigm by simulating the user's inner world. EmoHarbor employs a Chain-of-Agent architecture that decomposes users' internal processes into three specialized roles, enabling agents to interact with supporters and complete assessments in a manner similar to human users. We instantiate this benchmark using 100 real-world user profiles that cover a diverse range of personality traits and situations, and define 10 evaluation dimensions of personalized support quality. Comprehensive evaluation of 20 advanced LLMs on EmoHarbor reveals a critical insight: while these models excel at generating empathetic responses, they consistently fail to tailor support to individual user contexts. This finding reframes the central challenge, shifting research focus from merely enhancing generic empathy to developing truly user-aware emotional support. EmoHarbor provides a reproducible and scalable framework to guide the development and evaluation of more nuanced and user-aware emotional support systems.", "AI": {"tldr": "EmoHarbor\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u91c7\u7528\"\u7528\u6237\u5373\u88c1\u5224\"\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u62df\u7528\u6237\u5185\u5fc3\u4e16\u754c\u6765\u8bc4\u4f30\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u7684\u4e2a\u6027\u5316\u8d28\u91cf\uff0c\u53d1\u73b0\u5f53\u524dLLM\u867d\u80fd\u751f\u6210\u5171\u60c5\u56de\u5e94\u4f46\u7f3a\u4e4f\u4e2a\u6027\u5316\u652f\u6301\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u7684\u8bc4\u4f30\u8303\u5f0f\u503e\u5411\u4e8e\u5956\u52b1\u901a\u7528\u7684\u5171\u60c5\u56de\u5e94\uff0c\u4f46\u65e0\u6cd5\u8bc4\u4f30\u652f\u6301\u662f\u5426\u771f\u6b63\u4e2a\u6027\u5316\u5730\u9002\u5e94\u7528\u6237\u72ec\u7279\u7684\u5fc3\u7406\u7279\u5f81\u548c\u60c5\u5883\u9700\u6c42\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEmoHarbor\u6846\u67b6\uff0c\u91c7\u7528\u7528\u6237\u5373\u88c1\u5224\u8303\u5f0f\uff0c\u4f7f\u7528Chain-of-Agent\u67b6\u6784\u5c06\u7528\u6237\u5185\u90e8\u8fc7\u7a0b\u5206\u89e3\u4e3a\u4e09\u4e2a\u4e13\u95e8\u89d2\u8272\uff0c\u57fa\u4e8e100\u4e2a\u771f\u5b9e\u7528\u6237\u914d\u7f6e\u6587\u4ef6\uff08\u6db5\u76d6\u591a\u6837\u4eba\u683c\u7279\u8d28\u548c\u60c5\u5883\uff09\u548c10\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\u6765\u8bc4\u4f30\u4e2a\u6027\u5316\u652f\u6301\u8d28\u91cf\u3002", "result": "\u5bf920\u4e2a\u5148\u8fdbLLM\u7684\u7efc\u5408\u8bc4\u4f30\u63ed\u793a\u5173\u952e\u53d1\u73b0\uff1a\u8fd9\u4e9b\u6a21\u578b\u5728\u751f\u6210\u5171\u60c5\u56de\u5e94\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u59cb\u7ec8\u65e0\u6cd5\u6839\u636e\u4e2a\u4f53\u7528\u6237\u60c5\u5883\u5b9a\u5236\u652f\u6301\uff0c\u8fd9\u91cd\u65b0\u5b9a\u4e49\u4e86\u6838\u5fc3\u6311\u6218\u3002", "conclusion": "EmoHarbor\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u91cd\u590d\u3001\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u5c06\u7814\u7a76\u91cd\u70b9\u4ece\u4ec5\u4ec5\u589e\u5f3a\u901a\u7528\u5171\u60c5\u8f6c\u5411\u5f00\u53d1\u771f\u6b63\u7528\u6237\u611f\u77e5\u7684\u60c5\u611f\u652f\u6301\u7cfb\u7edf\uff0c\u6307\u5bfc\u66f4\u7ec6\u81f4\u548c\u7528\u6237\u611f\u77e5\u7684\u60c5\u611f\u652f\u6301\u7cfb\u7edf\u7684\u5f00\u53d1\u4e0e\u8bc4\u4f30\u3002"}}
{"id": "2601.01543", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01543", "abs": "https://arxiv.org/abs/2601.01543", "authors": ["Praveenkumar Katwe", "RakeshChandra Balabantaray", "Kaliprasad Vittala"], "title": "Bridging the Data Gap: Creating a Hindi Text Summarization Dataset from the English XSUM", "comment": "Book chapter for River publications", "summary": "Current advancements in Natural Language Processing (NLP) have largely favored resource-rich languages, leaving a significant gap in high-quality datasets for low-resource languages like Hindi. This scarcity is particularly evident in text summarization, where the development of robust models is hindered by a lack of diverse, specialized corpora.\n  To address this disparity, this study introduces a cost-effective, automated framework for creating a comprehensive Hindi text summarization dataset. By leveraging the English Extreme Summarization (XSUM) dataset as a source, we employ advanced translation and linguistic adaptation techniques. To ensure high fidelity and contextual relevance, we utilize the Crosslingual Optimized Metric for Evaluation of Translation (COMET) for validation, supplemented by the selective use of Large Language Models (LLMs) for curation.\n  The resulting dataset provides a diverse, multi-thematic resource that mirrors the complexity of the original XSUM corpus. This initiative not only provides a direct tool for Hindi NLP research but also offers a scalable methodology for democratizing NLP in other underserved languages. By reducing the costs associated with dataset creation, this work fosters the development of more nuanced, culturally relevant models in computational linguistics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u521b\u5efa\u5168\u9762\u7684\u5370\u5730\u8bed\u6587\u672c\u6458\u8981\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7ffb\u8bd1\u548c\u8bed\u8a00\u9002\u5e94\u6280\u672f\u5c06\u82f1\u8bedXSUM\u6570\u636e\u96c6\u8f6c\u6362\u4e3a\u5370\u5730\u8bed\uff0c\u5e76\u4f7f\u7528COMET\u548cLLM\u8fdb\u884c\u9a8c\u8bc1\u548c\u7b5b\u9009\u3002", "motivation": "\u5f53\u524dNLP\u8fdb\u5c55\u4e3b\u8981\u504f\u5411\u8d44\u6e90\u4e30\u5bcc\u7684\u8bed\u8a00\uff0c\u50cf\u5370\u5730\u8bed\u8fd9\u6837\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u6458\u8981\u6570\u636e\u96c6\uff0c\u8fd9\u963b\u788d\u4e86\u9c81\u68d2\u6a21\u578b\u7684\u53d1\u5c55\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u79cd\u4e0d\u5e73\u8861\uff0c\u4e3a\u5370\u5730\u8bedNLP\u7814\u7a76\u63d0\u4f9b\u5de5\u5177\u3002", "method": "\u4f7f\u7528\u82f1\u8bedXSUM\u6570\u636e\u96c6\u4f5c\u4e3a\u6e90\uff0c\u91c7\u7528\u5148\u8fdb\u7684\u7ffb\u8bd1\u548c\u8bed\u8a00\u9002\u5e94\u6280\u672f\u521b\u5efa\u5370\u5730\u8bed\u6458\u8981\u6570\u636e\u96c6\u3002\u4f7f\u7528COMET\u8fdb\u884c\u7ffb\u8bd1\u8d28\u91cf\u9a8c\u8bc1\uff0c\u5e76\u9009\u62e9\u6027\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6570\u636e\u7b5b\u9009\u548c\u6574\u7406\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u591a\u6837\u5316\u3001\u591a\u4e3b\u9898\u7684\u5370\u5730\u8bed\u6587\u672c\u6458\u8981\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u53cd\u6620\u4e86\u539f\u59cbXSUM\u8bed\u6599\u5e93\u7684\u590d\u6742\u6027\uff0c\u4e3a\u5370\u5730\u8bedNLP\u7814\u7a76\u63d0\u4f9b\u4e86\u76f4\u63a5\u5de5\u5177\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e0d\u4ec5\u4e3a\u5370\u5730\u8bedNLP\u7814\u7a76\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u8bba\uff0c\u53ef\u4ee5\u63a8\u5e7f\u5230\u5176\u4ed6\u8d44\u6e90\u4e0d\u8db3\u7684\u8bed\u8a00\uff0c\u901a\u8fc7\u964d\u4f4e\u6570\u636e\u96c6\u521b\u5efa\u6210\u672c\uff0c\u4fc3\u8fdb\u4e86\u8ba1\u7b97\u8bed\u8a00\u5b66\u4e2d\u66f4\u7ec6\u81f4\u3001\u6587\u5316\u76f8\u5173\u6a21\u578b\u7684\u53d1\u5c55\u3002"}}
{"id": "2601.01552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01552", "abs": "https://arxiv.org/abs/2601.01552", "authors": ["Shreyas N. Samaga", "Gilberto Gonzalez Arroyo", "Tamal K. Dey"], "title": "HalluZig: Hallucination Detection using Zigzag Persistence", "comment": null, "summary": "The factual reliability of Large Language Models (LLMs) remains a critical barrier to their adoption in high-stakes domains due to their propensity to hallucinate. Current detection methods often rely on surface-level signals from the model's output, overlooking the failures that occur within the model's internal reasoning process. In this paper, we introduce a new paradigm for hallucination detection by analyzing the dynamic topology of the evolution of model's layer-wise attention. We model the sequence of attention matrices as a zigzag graph filtration and use zigzag persistence, a tool from Topological Data Analysis, to extract a topological signature. Our core hypothesis is that factual and hallucinated generations exhibit distinct topological signatures. We validate our framework, HalluZig, on multiple benchmarks, demonstrating that it outperforms strong baselines. Furthermore, our analysis reveals that these topological signatures are generalizable across different models and hallucination detection is possible only using structural signatures from partial network depth.", "AI": {"tldr": "\u63d0\u51faHalluZig\u6846\u67b6\uff0c\u5229\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\u68c0\u6d4bLLM\u5e7b\u89c9\uff0c\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u77e9\u9635\u7684\u52a8\u6001\u62d3\u6251\u7ed3\u6784\u533a\u5206\u4e8b\u5b9e\u4e0e\u5e7b\u89c9\u751f\u6210", "motivation": "LLM\u7684\u4e8b\u5b9e\u53ef\u9760\u6027\u662f\u5176\u5728\u9ad8\u98ce\u9669\u9886\u57df\u5e94\u7528\u7684\u5173\u952e\u969c\u788d\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u6a21\u578b\u8f93\u51fa\u7684\u8868\u9762\u4fe1\u53f7\uff0c\u5ffd\u7565\u4e86\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5931\u8d25", "method": "\u63d0\u51faHalluZig\u6846\u67b6\uff0c\u5c06\u6ce8\u610f\u529b\u77e9\u9635\u5e8f\u5217\u5efa\u6a21\u4e3azigzag\u56fe\u8fc7\u6ee4\uff0c\u4f7f\u7528zigzag\u6301\u4e45\u6027\u63d0\u53d6\u62d3\u6251\u7279\u5f81\uff0c\u5047\u8bbe\u4e8b\u5b9e\u548c\u5e7b\u89c9\u751f\u6210\u5177\u6709\u4e0d\u540c\u7684\u62d3\u6251\u7b7e\u540d", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86HalluZig\u6846\u67b6\uff0c\u8bc1\u660e\u5176\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u62d3\u6251\u7279\u5f81\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u5177\u6709\u53ef\u6cdb\u5316\u6027\uff0c\u4e14\u4ec5\u4f7f\u7528\u90e8\u5206\u7f51\u7edc\u6df1\u5ea6\u7684\u7ed3\u6784\u7279\u5f81\u5373\u53ef\u68c0\u6d4b\u5e7b\u89c9", "conclusion": "\u901a\u8fc7\u5206\u6790\u6ce8\u610f\u529b\u52a8\u6001\u62d3\u6251\u7ed3\u6784\u4e3a\u5e7b\u89c9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u62d3\u6251\u7279\u5f81\u80fd\u6709\u6548\u533a\u5206\u4e8b\u5b9e\u4e0e\u5e7b\u89c9\uff0c\u4e14\u5177\u6709\u8de8\u6a21\u578b\u6cdb\u5316\u80fd\u529b"}}
{"id": "2601.01584", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01584", "abs": "https://arxiv.org/abs/2601.01584", "authors": ["Jakub Hoscilowicz"], "title": "Steerability of Instrumental-Convergence Tendencies in LLMs", "comment": "Code is available at https://github.com/j-hoscilowicz/instrumental_steering", "summary": "We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). In our experiments, higher capability does not imply lower steerability. We distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma for open-weight AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability to prevent malicious actors from eliciting harmful behaviors. This tension is acute for open-weight models, which are currently highly steerable via common techniques such as fine-tuning and adversarial prompting. Using Qwen3 models (4B/30B; Base/Instruct/Thinking) and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces outputs labeled as instrumental convergence (e.g., shutdown avoidance, deception, self-replication). For Qwen3-30B Instruct, convergence drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models produce fewer convergence-labeled outputs than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u7cfb\u7edf\u7684\u80fd\u529b\u4e0e\u53ef\u64cd\u63a7\u6027\u5e76\u975e\u8d1f\u76f8\u5173\uff0c\u533a\u5206\u4e86\u6388\u6743\u4e0e\u975e\u6388\u6743\u64cd\u63a7\u6027\uff0c\u63ed\u793a\u4e86\u5f00\u653e\u6743\u91cd\u6a21\u578b\u9762\u4e34\u7684\u5b89\u5168-\u5b89\u5168\u56f0\u5883\uff1a\u5b89\u5168\u9700\u8981\u9ad8\u64cd\u63a7\u6027\u6765\u5b9e\u65bd\u63a7\u5236\uff0c\u800c\u5b89\u5168\u9700\u8981\u4f4e\u64cd\u63a7\u6027\u6765\u9632\u6b62\u6076\u610f\u884c\u4e3a\u3002", "motivation": "\u63a2\u8ba8AI\u7cfb\u7edf\u7684\u80fd\u529b\u4e0e\u53ef\u64cd\u63a7\u6027\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u7279\u522b\u662f\u533a\u5206\u6388\u6743\u548c\u975e\u6388\u6743\u64cd\u63a7\u6027\uff0c\u4ee5\u89e3\u51b3\u5f00\u653e\u6743\u91cdAI\u6a21\u578b\u9762\u4e34\u7684\u5b89\u5168-\u5b89\u5168\u56f0\u5883\u3002", "method": "\u4f7f\u7528Qwen3\u6a21\u578b\uff084B/30B\uff1bBase/Instruct/Thinking\uff09\u548cInstrumentalEval\u5de5\u5177\uff0c\u901a\u8fc7\u6b63\u5411\u548c\u53cd\u5411\u5de5\u5177\u6027\u63d0\u793a\u540e\u7f00\u6765\u6d4b\u8bd5\u6a21\u578b\u7684\u884c\u4e3a\u53d8\u5316\uff0c\u5206\u6790\u4e0d\u540c\u89c4\u6a21\u5bf9\u9f50\u6a21\u578b\u5728\u5de5\u5177\u6027\u6536\u655b\u884c\u4e3a\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u80fd\u529b\u66f4\u9ad8\u7684\u7cfb\u7edf\u5e76\u4e0d\u4e00\u5b9a\u66f4\u96be\u64cd\u63a7\uff1b\u7b80\u77ed\u7684\u53cd\u5de5\u5177\u6027\u63d0\u793a\u540e\u7f00\u80fd\u663e\u8457\u51cf\u5c11\u5de5\u5177\u6027\u6536\u655b\u884c\u4e3a\uff08\u5982\u5173\u673a\u56de\u907f\u3001\u6b3a\u9a97\u3001\u81ea\u6211\u590d\u5236\uff09\uff1b\u5728\u53cd\u5de5\u5177\u6027\u63d0\u793a\u4e0b\uff0c\u66f4\u5927\u7684\u5bf9\u9f50\u6a21\u578b\u4ea7\u751f\u7684\u6536\u655b\u884c\u4e3a\u66f4\u5c11\u3002", "conclusion": "\u5f00\u653e\u6743\u91cdAI\u6a21\u578b\u9762\u4e34\u5b89\u5168-\u5b89\u5168\u56f0\u5883\uff1a\u5b89\u5168\u9700\u8981\u9ad8\u64cd\u63a7\u6027\u6765\u5b9e\u65bd\u63a7\u5236\uff0c\u800c\u5b89\u5168\u9700\u8981\u4f4e\u64cd\u63a7\u6027\u6765\u9632\u6b62\u6076\u610f\u884c\u4e3a\u3002\u5f53\u524d\u5f00\u653e\u6743\u91cd\u6a21\u578b\u901a\u8fc7\u5fae\u8c03\u548c\u5bf9\u6297\u6027\u63d0\u793a\u5177\u6709\u9ad8\u5ea6\u53ef\u64cd\u63a7\u6027\uff0c\u8fd9\u65e2\u662f\u98ce\u9669\u4e5f\u662f\u673a\u9047\u3002"}}
{"id": "2601.01624", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01624", "abs": "https://arxiv.org/abs/2601.01624", "authors": ["Raj Vardhan Tomar", "Preslav Nakov", "Yuxia Wang"], "title": "How Does Prefix Matter in Reasoning Model Tuning?", "comment": null, "summary": "Recent alignment studies commonly remove introductory boilerplate phrases from supervised fine-tuning (SFT) datasets. This work challenges that assumption. We hypothesize that safety- and reasoning-oriented prefix sentences serve as lightweight alignment signals that can guide model decoding toward safer and more coherent responses. To examine this, we fine-tune three R1 series models across three core model capabilities: reasoning (mathematics, coding), safety, and factuality, systematically varying prefix inclusion from 0% to 100%.\n  Results show that prefix-conditioned SFT improves both safety and reasoning performance, yielding up to +6% higher Safe@1 accuracy on adversarial benchmarks (WildJailbreak, StrongReject) and +7% improvement on GSM8K reasoning. However, factuality and coding tasks show marginal or negative effects, indicating that prefix-induced narrowing of the search space benefits structured reasoning. Token-level loss analysis further reveals that prefix tokens such as \"revised\" and \"logically\" incur higher gradient magnitudes, acting as alignment anchors that stabilize reasoning trajectories. Our findings suggest that prefix conditioning offers a scalable and interpretable mechanism for improving reasoning safety, serving as an implicit form of alignment that complements traditional reward-based methods.", "AI": {"tldr": "\u524d\u7f00\u6761\u4ef6\u5316SFT\u80fd\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5bf9\u4e8b\u5b9e\u6027\u548c\u7f16\u7801\u4efb\u52a1\u6548\u679c\u6709\u9650", "motivation": "\u6311\u6218\u73b0\u6709\u7814\u7a76\u4e2d\u79fb\u9664SFT\u6570\u636e\u96c6\u524d\u7f00\u77ed\u8bed\u7684\u5047\u8bbe\uff0c\u8ba4\u4e3a\u5b89\u5168\u6027\u548c\u63a8\u7406\u5bfc\u5411\u7684\u524d\u7f00\u53ef\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u5bf9\u9f50\u4fe1\u53f7\uff0c\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u66f4\u5b89\u5168\u548c\u8fde\u8d2f\u7684\u54cd\u5e94", "method": "\u5728\u4e09\u4e2aR1\u7cfb\u5217\u6a21\u578b\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u6db5\u76d6\u63a8\u7406\uff08\u6570\u5b66\u3001\u7f16\u7801\uff09\u3001\u5b89\u5168\u6027\u548c\u4e8b\u5b9e\u6027\u4e09\u4e2a\u6838\u5fc3\u80fd\u529b\uff0c\u7cfb\u7edf\u6027\u5730\u6539\u53d8\u524d\u7f00\u5305\u542b\u6bd4\u4f8b\uff080%\u5230100%\uff09", "result": "\u524d\u7f00\u6761\u4ef6\u5316SFT\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u548c\u63a8\u7406\u6027\u80fd\uff1a\u5b89\u5168\u57fa\u51c6\uff08WildJailbreak, StrongReject\uff09\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u5347+6%\uff0cGSM8K\u63a8\u7406\u63d0\u5347+7%\uff1b\u4f46\u4e8b\u5b9e\u6027\u548c\u7f16\u7801\u4efb\u52a1\u6548\u679c\u6709\u9650\u6216\u8d1f\u9762\uff1btoken\u7ea7\u635f\u5931\u5206\u6790\u663e\u793a\"revised\"\u548c\"logically\"\u7b49\u524d\u7f00token\u68af\u5ea6\u5e45\u5ea6\u66f4\u9ad8\uff0c\u4f5c\u4e3a\u5bf9\u9f50\u951a\u70b9\u7a33\u5b9a\u63a8\u7406\u8f68\u8ff9", "conclusion": "\u524d\u7f00\u6761\u4ef6\u5316\u4e3a\u63d0\u5347\u63a8\u7406\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u673a\u5236\uff0c\u4f5c\u4e3a\u4f20\u7edf\u57fa\u4e8e\u5956\u52b1\u65b9\u6cd5\u7684\u8865\u5145\uff0c\u662f\u4e00\u79cd\u9690\u5f0f\u7684\u5bf9\u9f50\u5f62\u5f0f"}}
{"id": "2601.01627", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01627", "abs": "https://arxiv.org/abs/2601.01627", "authors": ["Junyu Liu", "Zirui Li", "Qian Niu", "Zequn Zhang", "Yue Xun", "Wenlong Hou", "Shujun Wang", "Yusuke Iwasawa", "Yutaka Matsuo", "Kan Hatakeyama-Sato"], "title": "JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models", "comment": "12 pages, 6 figures", "summary": "As Large Language Models (LLMs) are increasingly deployed in healthcare field, it becomes essential to carefully evaluate their medical safety before clinical use. However, existing safety benchmarks remain predominantly English-centric, and test with only single-turn prompts despite multi-turn clinical consultations. To address these gaps, we introduce JMedEthicBench, the first multi-turn conversational benchmark for evaluating medical safety of LLMs for Japanese healthcare. Our benchmark is based on 67 guidelines from the Japan Medical Association and contains over 50,000 adversarial conversations generated using seven automatically discovered jailbreak strategies. Using a dual-LLM scoring protocol, we evaluate 27 models and find that commercial models maintain robust safety while medical-specialized models exhibit increased vulnerability. Furthermore, safety scores decline significantly across conversation turns (median: 9.5 to 5.0, $p < 0.001$). Cross-lingual evaluation on both Japanese and English versions of our benchmark reveals that medical model vulnerabilities persist across languages, indicating inherent alignment limitations rather than language-specific factors. These findings suggest that domain-specific fine-tuning may accidentally weaken safety mechanisms and that multi-turn interactions represent a distinct threat surface requiring dedicated alignment strategies.", "AI": {"tldr": "JMedEthicBench\uff1a\u9996\u4e2a\u9488\u5bf9\u65e5\u672c\u533b\u7597\u7684LLM\u591a\u8f6e\u5bf9\u8bdd\u5b89\u5168\u57fa\u51c6\uff0c\u57fa\u4e8e\u65e5\u672c\u533b\u5b66\u4f1a67\u6761\u6307\u5357\uff0c\u5305\u542b5\u4e07+\u5bf9\u6297\u5bf9\u8bdd\uff0c\u53d1\u73b0\u533b\u7597\u4e13\u7528\u6a21\u578b\u5b89\u5168\u6027\u66f4\u8106\u5f31\uff0c\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u5b89\u5168\u6027\u663e\u8457\u4e0b\u964d", "motivation": "\u73b0\u6709\u5b89\u5168\u57fa\u51c6\u4e3b\u8981\u662f\u82f1\u8bed\u4e2d\u5fc3\u4e14\u4ec5\u6d4b\u8bd5\u5355\u8f6e\u63d0\u793a\uff0c\u800c\u4e34\u5e8a\u54a8\u8be2\u901a\u5e38\u662f\u591a\u8f6e\u5bf9\u8bdd\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30LLM\u5728\u533b\u7597\u9886\u57df\u7684\u5b89\u5168\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u975e\u82f1\u8bed\u73af\u5883", "method": "\u57fa\u4e8e\u65e5\u672c\u533b\u5b66\u4f1a67\u6761\u6307\u5357\u6784\u5efa\u57fa\u51c6\uff0c\u4f7f\u75287\u79cd\u81ea\u52a8\u53d1\u73b0\u7684\u8d8a\u72f1\u7b56\u7565\u751f\u6210\u8d85\u8fc75\u4e07\u5bf9\u6297\u5bf9\u8bdd\uff0c\u91c7\u7528\u53ccLLM\u8bc4\u5206\u534f\u8bae\u8bc4\u4f3027\u4e2a\u6a21\u578b\uff0c\u8fdb\u884c\u8de8\u8bed\u8a00\u8bc4\u4f30", "result": "\u5546\u4e1a\u6a21\u578b\u4fdd\u6301\u7a33\u5065\u5b89\u5168\uff0c\u533b\u7597\u4e13\u7528\u6a21\u578b\u66f4\u8106\u5f31\uff1b\u5b89\u5168\u6027\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u663e\u8457\u4e0b\u964d\uff08\u4e2d\u4f4d\u65709.5\u52305.0\uff0cp<0.001\uff09\uff1b\u8de8\u8bed\u8a00\u8bc4\u4f30\u663e\u793a\u533b\u7597\u6a21\u578b\u6f0f\u6d1e\u5728\u6240\u6709\u8bed\u8a00\u4e2d\u90fd\u5b58\u5728", "conclusion": "\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u53ef\u80fd\u610f\u5916\u524a\u5f31\u5b89\u5168\u673a\u5236\uff0c\u591a\u8f6e\u4ea4\u4e92\u4ee3\u8868\u72ec\u7279\u5a01\u80c1\u9762\u9700\u8981\u4e13\u95e8\u5bf9\u9f50\u7b56\u7565\uff0c\u533b\u7597\u6a21\u578b\u6f0f\u6d1e\u662f\u5185\u5728\u5bf9\u9f50\u9650\u5236\u800c\u975e\u8bed\u8a00\u7279\u5b9a\u56e0\u7d20"}}
{"id": "2601.01668", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01668", "abs": "https://arxiv.org/abs/2601.01668", "authors": ["Houman Kazemzadeh", "Nima Minaifar", "Kamyar Naderi", "Sho Tabibzadeh"], "title": "EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records", "comment": "19 pages", "summary": "Clinicians routinely navigate fragmented electronic health record (EHR) interfaces to assemble a coherent picture of a patient's problems, medications, recent encounters, and longitudinal trends. This work describes EHRSummarizer, a privacy-aware, FHIR-native reference architecture that retrieves a targeted set of high-yield FHIR R4 resources, normalizes them into a consistent clinical context package, and produces structured summaries intended to support structured chart review. The system can be configured for data minimization, stateless processing, and flexible deployment, including local inference within an organization's trust boundary. To mitigate the risk of unsupported or unsafe behavior, the summarization stage is constrained to evidence present in the retrieved context package, is intended to indicate missing or unavailable domains where feasible, and avoids diagnostic or treatment recommendations. Prototype demonstrations on synthetic and test FHIR environments illustrate end-to-end behavior and output formats; however, this manuscript does not report clinical outcomes or controlled workflow studies. We outline an evaluation plan centered on faithfulness, omission risk, temporal correctness, usability, and operational monitoring to guide future institutional assessments.", "AI": {"tldr": "EHRSummarizer\u662f\u4e00\u4e2a\u9690\u79c1\u611f\u77e5\u3001FHIR\u539f\u751f\u7684\u53c2\u8003\u67b6\u6784\uff0c\u7528\u4e8e\u4ece\u788e\u7247\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u68c0\u7d22\u3001\u6807\u51c6\u5316\u548c\u751f\u6210\u7ed3\u6784\u5316\u6458\u8981\uff0c\u652f\u6301\u7ed3\u6784\u5316\u75c5\u5386\u5ba1\u67e5\u3002", "motivation": "\u4e34\u5e8a\u533b\u751f\u9700\u8981\u4ece\u788e\u7247\u5316\u7684\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u754c\u9762\u4e2d\u62fc\u51d1\u51fa\u60a3\u8005\u95ee\u9898\u7684\u8fde\u8d2f\u753b\u9762\uff0c\u5305\u62ec\u7528\u836f\u3001\u8fd1\u671f\u5c31\u8bca\u548c\u957f\u671f\u8d8b\u52bf\u7b49\u4fe1\u606f\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u6548\u7387\u4f4e\u4e0b\u4e14\u5bb9\u6613\u51fa\u9519\u3002", "method": "\u7cfb\u7edf\u91c7\u7528FHIR R4\u6807\u51c6\uff0c\u68c0\u7d22\u76ee\u6807\u6027\u9ad8\u4ef7\u503c\u8d44\u6e90\uff0c\u5c06\u5176\u6807\u51c6\u5316\u4e3a\u4e00\u81f4\u7684\u4e34\u5e8a\u4e0a\u4e0b\u6587\u5305\uff0c\u7136\u540e\u751f\u6210\u7ed3\u6784\u5316\u6458\u8981\u3002\u652f\u6301\u6570\u636e\u6700\u5c0f\u5316\u3001\u65e0\u72b6\u6001\u5904\u7406\u548c\u7075\u6d3b\u90e8\u7f72\uff0c\u5305\u62ec\u5728\u7ec4\u7ec7\u4fe1\u4efb\u8fb9\u754c\u5185\u8fdb\u884c\u672c\u5730\u63a8\u7406\u3002", "result": "\u5728\u5408\u6210\u548c\u6d4b\u8bd5FHIR\u73af\u5883\u4e2d\u7684\u539f\u578b\u6f14\u793a\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u884c\u4e3a\u548c\u8f93\u51fa\u683c\u5f0f\uff0c\u4f46\u672a\u62a5\u544a\u4e34\u5e8a\u7ed3\u679c\u6216\u53d7\u63a7\u5de5\u4f5c\u6d41\u7a0b\u7814\u7a76\u3002\u63d0\u51fa\u4e86\u4ee5\u5fe0\u5b9e\u6027\u3001\u9057\u6f0f\u98ce\u9669\u3001\u65f6\u95f4\u6b63\u786e\u6027\u3001\u53ef\u7528\u6027\u548c\u64cd\u4f5c\u76d1\u63a7\u4e3a\u4e2d\u5fc3\u7684\u8bc4\u4f30\u8ba1\u5212\u3002", "conclusion": "EHRSummarizer\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9690\u79c1\u611f\u77e5\u7684\u53c2\u8003\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6458\u8981\u652f\u6301\u4e34\u5e8a\u75c5\u5386\u5ba1\u67e5\uff0c\u540c\u65f6\u907f\u514d\u8bca\u65ad\u6216\u6cbb\u7597\u5efa\u8bae\uff0c\u5e76\u8ba1\u5212\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u6307\u5bfc\u672a\u6765\u7684\u673a\u6784\u8bc4\u4f30\u3002"}}
{"id": "2601.01685", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2601.01685", "abs": "https://arxiv.org/abs/2601.01685", "authors": ["Jinwei Hu", "Xinmiao Huang", "Youcheng Sun", "Yi Dong", "Xiaowei Huang"], "title": "Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage", "comment": "Under Review", "summary": "As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8ba4\u77e5\u5171\u8c0b\u653b\u51fb\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8fc7\u5ea6\u601d\u8003\u503e\u5411\uff0c\u901a\u8fc7\u516c\u5f00\u6e20\u9053\u5206\u53d1\u771f\u5b9e\u8bc1\u636e\u7247\u6bb5\u6765\u64cd\u7eb5\u53d7\u5bb3\u8005\u4fe1\u5ff5\uff0c\u65e0\u9700\u4f2a\u9020\u4fe1\u606f\u6216\u9690\u853d\u901a\u4fe1\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5411\u81ea\u4e3b\u4ee3\u7406\u53d1\u5c55\uff0c\u5176\u63a8\u7406\u80fd\u529b\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u653b\u51fb\u9762\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6a21\u578b\u672c\u8eab\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u800c\u5ffd\u89c6\u4e86\u591a\u4e2a\u4ee3\u7406\u901a\u8fc7\u771f\u5b9e\u4fe1\u606f\u7247\u6bb5\u8fdb\u884c\u8ba4\u77e5\u5171\u8c0b\u64cd\u7eb5\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51fa\u751f\u6210\u8499\u592a\u5947\u6846\u67b6\uff1aWriter-Editor-Director\u4e09\u5c42\u67b6\u6784\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u8fa9\u8bba\u548c\u534f\u8c03\u53d1\u5e03\u8bc1\u636e\u7247\u6bb5\u6784\u5efa\u6b3a\u9a97\u6027\u53d9\u4e8b\u3002\u5f00\u53d1CoPHEME\u6570\u636e\u96c6\uff08\u57fa\u4e8e\u771f\u5b9e\u8c23\u8a00\u4e8b\u4ef6\uff09\uff0c\u572814\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\u4e2d\u6a21\u62df\u653b\u51fb\u3002", "result": "\u653b\u51fb\u6210\u529f\u7387\u60ca\u4eba\uff1a\u4e13\u6709\u6a21\u578b\u8fbe74.4%\uff0c\u5f00\u6e90\u6a21\u578b\u8fbe70.6%\u3002\u53cd\u76f4\u89c9\u7684\u662f\uff0c\u63a8\u7406\u80fd\u529b\u8d8a\u5f3a\u7684\u6a21\u578b\u8d8a\u5bb9\u6613\u53d7\u653b\u51fb\u3002\u865a\u5047\u4fe1\u5ff5\u8fd8\u4f1a\u5411\u4e0b\u6e38\u4f20\u64ad\uff0c\u6b3a\u9a97\u7387\u8d85\u8fc760%\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u672c\u8eab\u6210\u4e3a\u5b89\u5168\u6f0f\u6d1e\uff0c\u8ba4\u77e5\u5171\u8c0b\u653b\u51fb\u901a\u8fc7\u771f\u5b9e\u4fe1\u606f\u7247\u6bb5\u5c31\u80fd\u6709\u6548\u64cd\u7eb5\u4fe1\u5ff5\u3002\u8fd9\u63ed\u793a\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u52a8\u6001\u4fe1\u606f\u73af\u5883\u4e2d\u7684\u793e\u4f1a\u6280\u672f\u8106\u5f31\u6027\u3002"}}
{"id": "2601.01708", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01708", "abs": "https://arxiv.org/abs/2601.01708", "authors": ["Unggi Lee", "Joo Young Kim", "Ran Ju", "Minyoung Jung", "Jeyeon Eo"], "title": "A Training-Free Large Reasoning Model-based Knowledge Tracing Framework for Unified Prediction and Prescription", "comment": null, "summary": "Knowledge Tracing (KT) aims to estimate a learner's evolving mastery based on interaction histories. Recent studies have explored Large Language Models (LLMs) for KT via autoregressive nature, but such approaches typically require fine-tuning and exhibit unstable or near-random performance. Moreover, prior KT systems primarily focus on prediction and rely on multi-stage pipelines for feedback and recommendation, resulting in increased system complexity and resources. To address this gap, we propose Thinking-KT, a training-free KT framework that incorporates Test-Time Scaling (TTS), enabling even small LLMs to achieve competitive KT performance. Moreover, in this framework, a small LLM can jointly perform KT prediction, personalized feedback generation, and learning recommendation in a unified output without degrading prediction accuracy. Beyond performance, we present the systematic analysis of reasoning traces in KT. Our results demonstrate that TTS is a critical yet underexplored factor in LLM-based KT, and that small LLMs can serve as unified ITS engines.", "AI": {"tldr": "Thinking-KT\uff1a\u65e0\u9700\u8bad\u7ec3\u7684KT\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u8bd5\u65f6\u7f29\u653e\u8ba9\u5c0fLLM\u5b9e\u73b0\u7ade\u4e89\u6027\u77e5\u8bc6\u8ffd\u8e2a\u6027\u80fd\uff0c\u5e76\u80fd\u7edf\u4e00\u6267\u884c\u9884\u6d4b\u3001\u53cd\u9988\u548c\u63a8\u8350\u4efb\u52a1", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684KT\u65b9\u6cd5\u9700\u8981\u5fae\u8c03\u4e14\u6027\u80fd\u4e0d\u7a33\u5b9a\uff0c\u800c\u4f20\u7edfKT\u7cfb\u7edf\u4f9d\u8d56\u591a\u9636\u6bb5\u6d41\u7a0b\u8fdb\u884c\u53cd\u9988\u548c\u63a8\u8350\uff0c\u5bfc\u81f4\u7cfb\u7edf\u590d\u6742\u5ea6\u548c\u8d44\u6e90\u6d88\u8017\u589e\u52a0", "method": "\u63d0\u51faThinking-KT\u6846\u67b6\uff0c\u91c7\u7528\u6d4b\u8bd5\u65f6\u7f29\u653e\u6280\u672f\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u8ba9\u5c0fLLM\u5b9e\u73b0\u7ade\u4e89\u6027KT\u6027\u80fd\uff0c\u5e76\u80fd\u7edf\u4e00\u8f93\u51fa\u9884\u6d4b\u3001\u4e2a\u6027\u5316\u53cd\u9988\u548c\u5b66\u4e60\u63a8\u8350", "result": "\u6d4b\u8bd5\u65f6\u7f29\u653e\u662fLLM-based KT\u4e2d\u88ab\u5ffd\u89c6\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5c0fLLM\u53ef\u4ee5\u4f5c\u4e3a\u7edf\u4e00\u7684\u667a\u80fd\u6559\u5b66\u7cfb\u7edf\u5f15\u64ce\uff0c\u5728\u4fdd\u6301\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b8c\u6210\u591a\u9879\u4efb\u52a1", "conclusion": "\u5c0fLLM\u901a\u8fc7\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\u53ef\u4ee5\u6210\u4e3a\u7edf\u4e00\u7684ITS\u5f15\u64ce\uff0c\u6d4b\u8bd5\u65f6\u7f29\u653e\u662f\u63d0\u5347LLM-based KT\u6027\u80fd\u7684\u5173\u952e\u6280\u672f\uff0c\u4e3a\u77e5\u8bc6\u8ffd\u8e2a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u7b80\u6d01\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.01739", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01739", "abs": "https://arxiv.org/abs/2601.01739", "authors": ["Eunbi Choi", "Kibong Choi", "Seokhee Hong", "Junwon Hwang", "Hyojin Jeon", "Hyunjik Jo", "Joonkee Kim", "Seonghwan Kim", "Soyeon Kim", "Sunkyoung Kim", "Yireun Kim", "Yongil Kim", "Haeju Lee", "Jinsik Lee", "Kyungmin Lee", "Sangha Park", "Heuiyeen Yeen", "Hwan Chang", "Stanley Jungkyu Choi", "Yejin Choi", "Jiwon Ham", "Kijeong Jeon", "Geunyeong Jeong", "Gerrard Jeongwon Jo", "Yonghwan Jo", "Jiyeon Jung", "Naeun Kang", "Dohoon Kim", "Euisoon Kim", "Hayeon Kim", "Hyosang Kim", "Hyunseo Kim", "Jieun Kim", "Minu Kim", "Myoungshin Kim", "Unsol Kim", "Youchul Kim", "YoungJin Kim", "Chaeeun Lee", "Chaeyoon Lee", "Changhun Lee", "Dahm Lee", "Edward Hwayoung Lee", "Honglak Lee", "Jinsang Lee", "Jiyoung Lee", "Sangeun Lee", "Seungwon Lim", "Solji Lim", "Woohyung Lim", "Chanwoo Moon", "Jaewoo Park", "Jinho Park", "Yongmin Park", "Hyerin Seo", "Wooseok Seo", "Yongwoo Song", "Sejong Yang", "Sihoon Yang", "Chang En Yea", "Sihyuk Yi", "Chansik Yoon", "Dongkeun Yoon", "Sangyeon Yoon", "Hyeongu Yun"], "title": "K-EXAONE Technical Report", "comment": "29 pages", "summary": "This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.", "AI": {"tldr": "LG AI Research\u5f00\u53d1\u4e86K-EXAONE\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u76842360\u4ebf\u53c2\u6570\u591a\u8bed\u8a00\u5927\u6a21\u578b\uff0c\u652f\u63016\u79cd\u8bed\u8a00\u548c256K\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5728\u63a8\u7406\u3001\u4ee3\u7406\u3001\u901a\u7528\u3001\u97e9\u8bed\u548c\u591a\u8bed\u8a00\u80fd\u529b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u5f3a\u5927\u7684\u4e13\u6709AI\u57fa\u7840\u6a21\u578b\uff0c\u652f\u6301\u591a\u79cd\u8bed\u8a00\uff08\u7279\u522b\u662f\u97e9\u8bed\uff09\uff0c\u7528\u4e8e\u5e7f\u6cdb\u7684\u5de5\u4e1a\u548c\u79d1\u7814\u5e94\u7528\uff0c\u63a8\u52a8AI\u6280\u672f\u53d1\u5c55\u4ee5\u6539\u5584\u751f\u6d3b\u3002", "method": "\u91c7\u7528\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\uff0c\u603b\u53c2\u6570\u91cf2360\u4ebf\uff0c\u63a8\u7406\u65f6\u6fc0\u6d3b230\u4ebf\u53c2\u6570\uff1b\u652f\u6301256K\u4ee4\u724c\u4e0a\u4e0b\u6587\u7a97\u53e3\uff1b\u8986\u76d6\u97e9\u8bed\u3001\u82f1\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u5fb7\u8bed\u3001\u65e5\u8bed\u548c\u8d8a\u5357\u8bed\u516d\u79cd\u8bed\u8a00\u3002", "result": "\u5728\u63a8\u7406\u3001\u4ee3\u7406\u3001\u901a\u7528\u3001\u97e9\u8bed\u548c\u591a\u8bed\u8a00\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cK-EXAONE\u8868\u73b0\u51fa\u4e0e\u76f8\u4f3c\u89c4\u6a21\u7684\u5f00\u6e90\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "K-EXAONE\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u4e13\u6709AI\u57fa\u7840\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u5de5\u4e1a\u548c\u79d1\u7814\u5e94\u7528\uff0c\u65e8\u5728\u901a\u8fc7AI\u6280\u672f\u6539\u5584\u751f\u6d3b\u3002"}}
{"id": "2601.01745", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01745", "abs": "https://arxiv.org/abs/2601.01745", "authors": ["Hong Han", "Hao-Chen Pei", "Zhao-Zheng Nie", "Xin Luo", "Xin-Shun Xu"], "title": "Multi-granularity Interactive Attention Framework for Residual Hierarchical Pronunciation Assessment", "comment": "9 pages, 4 figures, 5 tables, accepted by AAAI 2026", "summary": "Automatic pronunciation assessment plays a crucial role in computer-assisted pronunciation training systems. Due to the ability to perform multiple pronunciation tasks simultaneously, multi-aspect multi-granularity pronunciation assessment methods are gradually receiving more attention and achieving better performance than single-level modeling tasks. However, existing methods only consider unidirectional dependencies between adjacent granularity levels, lacking bidirectional interaction among phoneme, word, and utterance levels and thus insufficiently capturing the acoustic structural correlations. To address this issue, we propose a novel residual hierarchical interactive method, HIA for short, that enables bidirectional modeling across granularities. As the core of HIA, the Interactive Attention Module leverages an attention mechanism to achieve dynamic bidirectional interaction, effectively capturing linguistic features at each granularity while integrating correlations between different granularity levels. We also propose a residual hierarchical structure to alleviate the feature forgetting problem when modeling acoustic hierarchies. In addition, we use 1-D convolutional layers to enhance the extraction of local contextual cues at each granularity. Extensive experiments on the speechocean762 dataset show that our model is comprehensively ahead of the existing state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6b8b\u5dee\u5c42\u6b21\u4ea4\u4e92\u65b9\u6cd5\uff08HIA\uff09\uff0c\u7528\u4e8e\u591a\u7c92\u5ea6\u53d1\u97f3\u8bc4\u4f30\uff0c\u901a\u8fc7\u53cc\u5411\u4ea4\u4e92\u673a\u5236\u548c\u6b8b\u5dee\u7ed3\u6784\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53d1\u97f3\u8bc4\u4f30\u65b9\u6cd5\u53ea\u8003\u8651\u76f8\u90bb\u7c92\u5ea6\u7ea7\u522b\u7684\u5355\u5411\u4f9d\u8d56\u5173\u7cfb\uff0c\u7f3a\u4e4f\u97f3\u7d20\u3001\u5355\u8bcd\u548c\u8bdd\u8bed\u7ea7\u522b\u4e4b\u95f4\u7684\u53cc\u5411\u4ea4\u4e92\uff0c\u65e0\u6cd5\u5145\u5206\u6355\u6349\u58f0\u5b66\u7ed3\u6784\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51fa\u6b8b\u5dee\u5c42\u6b21\u4ea4\u4e92\u65b9\u6cd5\uff08HIA\uff09\uff0c\u6838\u5fc3\u662f\u4ea4\u4e92\u6ce8\u610f\u529b\u6a21\u5757\u5b9e\u73b0\u52a8\u6001\u53cc\u5411\u4ea4\u4e92\uff0c\u7ed3\u5408\u6b8b\u5dee\u5c42\u6b21\u7ed3\u6784\u7f13\u89e3\u7279\u5f81\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u4f7f\u75281-D\u5377\u79ef\u5c42\u589e\u5f3a\u5c40\u90e8\u4e0a\u4e0b\u6587\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5728speechocean762\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u65b9\u9762\u5168\u9762\u9886\u5148\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "HIA\u65b9\u6cd5\u901a\u8fc7\u53cc\u5411\u5efa\u6a21\u548c\u6b8b\u5dee\u7ed3\u6784\u6709\u6548\u89e3\u51b3\u4e86\u591a\u7c92\u5ea6\u53d1\u97f3\u8bc4\u4f30\u4e2d\u7684\u4ea4\u4e92\u4e0d\u8db3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u6027\u80fd\u3002"}}
{"id": "2601.01768", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01768", "abs": "https://arxiv.org/abs/2601.01768", "authors": ["Meiman Xiao", "Ante Wang", "Qingguo Hu", "Zhongjian Miao", "Huangjun Shen", "Longyue Wang", "Weihua Luo", "Jinsong Su"], "title": "Can LLMs Track Their Output Length? A Dynamic Feedback Mechanism for Precise Length Regulation", "comment": null, "summary": "Precisely controlling the length of generated text is a common requirement in real-world applications. However, despite significant advancements in following human instructions, Large Language Models (LLMs) still struggle with this task. In this work, we demonstrate that LLMs often fail to accurately measure input text length, leading to poor adherence to length constraints. To address this issue, we propose a novel length regulation approach that incorporates dynamic length feedback during generation, enabling adaptive adjustments to meet target lengths. Experiments on summarization and biography tasks show our training-free approach significantly improves precision in achieving target token, word, or sentence counts without compromising quality. Additionally, we demonstrate that further supervised fine-tuning allows our method to generalize effectively to broader text-generation tasks.", "AI": {"tldr": "LLMs\u5728\u63a7\u5236\u751f\u6210\u6587\u672c\u957f\u5ea6\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u52a8\u6001\u957f\u5ea6\u53cd\u9988\u7684\u8c03\u63a7\u65b9\u6cd5\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u957f\u5ea6\u63a7\u5236\u7684\u7cbe\u786e\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6587\u672c\u8d28\u91cf\u3002", "motivation": "\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u7cbe\u786e\u63a7\u5236\u751f\u6210\u6587\u672c\u957f\u5ea6\u662f\u5e38\u89c1\u9700\u6c42\uff0c\u4f46\u5c3d\u7ba1LLMs\u5728\u9075\u5faa\u4eba\u7c7b\u6307\u4ee4\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4ecd\u96be\u4ee5\u51c6\u786e\u5b8c\u6210\u6b64\u4efb\u52a1\u3002\u7814\u7a76\u53d1\u73b0LLMs\u7ecf\u5e38\u65e0\u6cd5\u51c6\u786e\u6d4b\u91cf\u8f93\u5165\u6587\u672c\u957f\u5ea6\uff0c\u5bfc\u81f4\u96be\u4ee5\u6ee1\u8db3\u957f\u5ea6\u7ea6\u675f\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u957f\u5ea6\u8c03\u63a7\u65b9\u6cd5\uff0c\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u878d\u5165\u52a8\u6001\u957f\u5ea6\u53cd\u9988\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u9002\u5e94\u8c03\u6574\u4ee5\u6ee1\u8db3\u76ee\u6807\u957f\u5ea6\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u901a\u8fc7\u5b9e\u65f6\u53cd\u9988\u673a\u5236\u4f18\u5316\u957f\u5ea6\u63a7\u5236\u3002", "result": "\u5728\u6458\u8981\u548c\u4f20\u8bb0\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u73b0\u76ee\u6807token\u6570\u3001\u5355\u8bcd\u6570\u6216\u53e5\u5b50\u6570\u65b9\u9762\u663e\u8457\u63d0\u5347\u4e86\u7cbe\u786e\u5ea6\uff0c\u4e14\u4e0d\u635f\u5bb3\u6587\u672c\u8d28\u91cf\u3002\u8fdb\u4e00\u6b65\u7684\u6709\u76d1\u7763\u5fae\u8c03\u53ef\u4f7f\u65b9\u6cd5\u6709\u6548\u6cdb\u5316\u5230\u66f4\u5e7f\u6cdb\u7684\u6587\u672c\u751f\u6210\u4efb\u52a1\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u957f\u5ea6\u53cd\u9988\u673a\u5236\uff0cLLMs\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u63a7\u5236\u751f\u6210\u6587\u672c\u957f\u5ea6\uff0c\u8be5\u65b9\u6cd5\u8bad\u7ec3\u514d\u8d39\u4e14\u53ef\u6cdb\u5316\uff0c\u4e3a\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u957f\u5ea6\u63a7\u5236\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2601.01778", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01778", "abs": "https://arxiv.org/abs/2601.01778", "authors": ["Jakir Hasan", "Shrestha Datta", "Md Saiful Islam", "Shubhashis Roy Dipta", "Ameya Debnath"], "title": "BanglaIPA: Towards Robust Text-to-IPA Transcription with Contextual Rewriting in Bengali", "comment": null, "summary": "Despite its widespread use, Bengali lacks a robust automated International Phonetic Alphabet (IPA) transcription system that effectively supports both standard language and regional dialectal texts. Existing approaches struggle to handle regional variations, numerical expressions, and generalize poorly to previously unseen words. To address these limitations, we propose BanglaIPA, a novel IPA generation system that integrates a character-based vocabulary with word-level alignment. The proposed system accurately handles Bengali numerals and demonstrates strong performance across regional dialects. BanglaIPA improves inference efficiency by leveraging a precomputed word-to-IPA mapping dictionary for previously observed words. The system is evaluated on the standard Bengali and six regional variations of the DUAL-IPA dataset. Experimental results show that BanglaIPA outperforms baseline IPA transcription models by 58.4-78.7% and achieves an overall mean word error rate of 11.4%, highlighting its robustness in phonetic transcription generation for the Bengali language.", "AI": {"tldr": "BanglaIPA\uff1a\u4e00\u79cd\u65b0\u7684\u5b5f\u52a0\u62c9\u8bedIPA\u8f6c\u5f55\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b57\u7b26\u7ea7\u8bcd\u6c47\u548c\u8bcd\u7ea7\u5bf9\u9f50\u5904\u7406\u65b9\u8a00\u53d8\u4f53\uff0c\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u534758.4-78.7%\uff0c\u8bcd\u9519\u8bef\u738711.4%", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u7f3a\u4e4f\u5f3a\u5927\u7684\u81ea\u52a8\u5316IPA\u8f6c\u5f55\u7cfb\u7edf\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u533a\u57df\u65b9\u8a00\u53d8\u4f53\u3001\u6570\u5b57\u8868\u8fbe\uff0c\u4e14\u5bf9\u672a\u89c1\u8bcd\u6cdb\u5316\u80fd\u529b\u5dee", "method": "\u63d0\u51faBanglaIPA\u7cfb\u7edf\uff0c\u6574\u5408\u5b57\u7b26\u7ea7\u8bcd\u6c47\u4e0e\u8bcd\u7ea7\u5bf9\u9f50\uff0c\u51c6\u786e\u5904\u7406\u5b5f\u52a0\u62c9\u6570\u5b57\uff0c\u5229\u7528\u9884\u8ba1\u7b97\u7684\u8bcd\u5230IPA\u6620\u5c04\u5b57\u5178\u63d0\u9ad8\u63a8\u7406\u6548\u7387", "result": "\u5728\u6807\u51c6\u5b5f\u52a0\u62c9\u8bed\u548cDUAL-IPA\u6570\u636e\u96c6\u76846\u79cd\u533a\u57df\u53d8\u4f53\u4e0a\u8bc4\u4f30\uff0c\u6bd4\u57fa\u7ebfIPA\u8f6c\u5f55\u6a21\u578b\u63d0\u534758.4-78.7%\uff0c\u603b\u4f53\u5e73\u5747\u8bcd\u9519\u8bef\u738711.4%", "conclusion": "BanglaIPA\u7cfb\u7edf\u5728\u5b5f\u52a0\u62c9\u8bed\u8bed\u97f3\u8f6c\u5f55\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u9c81\u68d2\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65b9\u8a00\u53d8\u4f53\u548c\u6570\u5b57\u5904\u7406\u7b49\u73b0\u6709\u6311\u6218"}}
{"id": "2601.01825", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01825", "abs": "https://arxiv.org/abs/2601.01825", "authors": ["Yaxin Cui", "Yuanqiang Zeng", "Jiapeng Yan", "Keling Lin", "Kai Ji", "Jianhui Zeng", "Sheng Zhang", "Xin Luo", "Binzhu Su", "Chaolai Shen", "Jiahao Yu"], "title": "CSCBench: A PVC Diagnostic Benchmark for Commodity Supply Chain Reasoning", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in general benchmarks, yet their competence in commodity supply chains (CSCs) -- a domain governed by institutional rule systems and feasibility constraints -- remains under-explored. CSC decisions are shaped jointly by process stages (e.g., planning, procurement, delivery), variety-specific rules (e.g., contract specifications and delivery grades), and reasoning depth (from retrieval to multi-step analysis and decision selection). We introduce CSCBench, a 2.3K+ single-choice benchmark for CSC reasoning, instantiated through our PVC 3D Evaluation Framework (Process, Variety, and Cognition). The Process axis aligns tasks with SCOR+Enable; the Variety axis operationalizes commodity-specific rule systems under coupled material-information-financial constraints, grounded in authoritative exchange guidebooks/rulebooks and industry reports; and the Cognition axis follows Bloom's revised taxonomy. Evaluating representative LLMs under a direct prompting setting, we observe strong performance on the Process and Cognition axes but substantial degradation on the Variety axis, especially on Freight Agreements. CSCBench provides a diagnostic yardstick for measuring and improving LLM capabilities in this high-stakes domain.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86CSCBench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5546\u54c1\u4f9b\u5e94\u94fe\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u5904\u7406\u54c1\u79cd\u7279\u5b9a\u89c4\u5219\uff08\u5c24\u5176\u662f\u8d27\u8fd0\u534f\u8bae\uff09\u65b9\u9762\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5546\u54c1\u4f9b\u5e94\u94fe\u8fd9\u4e00\u53d7\u5236\u5ea6\u89c4\u5219\u7cfb\u7edf\u548c\u53ef\u884c\u6027\u7ea6\u675f\u7684\u9886\u57df\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u5546\u54c1\u4f9b\u5e94\u94fe\u51b3\u7b56\u6d89\u53ca\u6d41\u7a0b\u9636\u6bb5\u3001\u54c1\u79cd\u7279\u5b9a\u89c4\u5219\u548c\u63a8\u7406\u6df1\u5ea6\u7b49\u591a\u4e2a\u7ef4\u5ea6\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faCSCBench\u57fa\u51c6\uff08\u5305\u542b2300+\u5355\u9009\u9898\uff09\uff0c\u91c7\u7528PVC 3D\u8bc4\u4f30\u6846\u67b6\uff1a\u6d41\u7a0b\u8f74\uff08Process\uff09\u4e0eSCOR+Enable\u5bf9\u9f50\uff1b\u54c1\u79cd\u8f74\uff08Variety\uff09\u57fa\u4e8e\u6743\u5a01\u4ea4\u6362\u6307\u5357/\u89c4\u5219\u624b\u518c\u548c\u884c\u4e1a\u62a5\u544a\uff0c\u5728\u8026\u5408\u7684\u6750\u6599-\u4fe1\u606f-\u8d22\u52a1\u7ea6\u675f\u4e0b\u64cd\u4f5c\u5316\u54c1\u79cd\u7279\u5b9a\u89c4\u5219\u7cfb\u7edf\uff1b\u8ba4\u77e5\u8f74\uff08Cognition\uff09\u9075\u5faa\u5e03\u9c81\u59c6\u4fee\u8ba2\u5206\u7c7b\u6cd5\u3002", "result": "\u5728\u76f4\u63a5\u63d0\u793a\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u4ee3\u8868\u6027\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u53d1\u73b0\u5728\u6d41\u7a0b\u8f74\u548c\u8ba4\u77e5\u8f74\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u54c1\u79cd\u8f74\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u7279\u522b\u662f\u5728\u8d27\u8fd0\u534f\u8bae\u65b9\u9762\u3002", "conclusion": "CSCBench\u4e3a\u8861\u91cf\u548c\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u4e00\u9ad8\u98ce\u9669\u9886\u57df\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u8bca\u65ad\u6027\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u5904\u7406\u54c1\u79cd\u7279\u5b9a\u89c4\u5219\u7cfb\u7edf\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.01827", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2601.01827", "abs": "https://arxiv.org/abs/2601.01827", "authors": ["Valiant Lance D. Dionela", "Fatima Kriselle S. Dy", "Robin James M. Hombrebueno", "Aaron Rae M. Nicolas", "Charibeth K. Cheng", "Raphael W. Gonda"], "title": "Aspect Extraction from E-Commerce Product and Service Reviews", "comment": null, "summary": "Aspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9Taglish\uff08\u4ed6\u52a0\u7984\u8bed-\u82f1\u8bed\u6df7\u5408\u8bed\uff09\u7684\u5168\u9762\u65b9\u9762\u63d0\u53d6\u7ba1\u9053\uff0c\u7ed3\u5408\u4e86\u57fa\u4e8e\u89c4\u5219\u3001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5fae\u8c03\u6280\u672f\uff0c\u5728\u4f4e\u8d44\u6e90\u4ee3\u7801\u8f6c\u6362\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u65b9\u9762\u8bc6\u522b\u4e0e\u63d0\u53d6\u3002", "motivation": "\u65b9\u9762\u63d0\u53d6\u662f\u65b9\u9762\u60c5\u611f\u5206\u6790\u7684\u5173\u952e\u4efb\u52a1\uff0c\u4f46\u5728Taglish\u7b49\u4f4e\u8d44\u6e90\u548c\u4ee3\u7801\u8f6c\u6362\u73af\u5883\u4e2d\u5e94\u7528\u56f0\u96be\u3002Taglish\u662f\u83f2\u5f8b\u5bbe\u7535\u5546\u8bc4\u8bba\u4e2d\u5e38\u7528\u7684\u4ed6\u52a0\u7984\u8bed\u548c\u82f1\u8bed\u6df7\u5408\u8bed\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5904\u7406\u65b9\u6cd5\u3002", "method": "1. \u5f00\u53d1\u4e86\u5206\u5c42\u65b9\u9762\u6846\u67b6\uff08HAF\uff09\u901a\u8fc7\u591a\u65b9\u6cd5\u4e3b\u9898\u5efa\u6a21\uff1b2. \u8bbe\u8ba1\u4e86\u663e\u5f0f\u548c\u9690\u5f0f\u65b9\u9762\u7684\u53cc\u6a21\u5f0f\u6807\u6ce8\u65b9\u6848\uff1b3. \u8bc4\u4f30\u4e86\u56db\u79cd\u6a21\u578b\uff1a\u57fa\u4e8e\u89c4\u5219\u7cfb\u7edf\u3001\u751f\u6210\u5f0fLLM\uff08Gemini 2.0 Flash\uff09\u3001\u4e24\u4e2a\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684Gemma-3 1B\u6a21\u578b\uff08\u57fa\u4e8e\u89c4\u5219\u6807\u6ce8vs. LLM\u6807\u6ce8\uff09\u3002", "result": "\u751f\u6210\u5f0fLLM\u5728\u6240\u6709\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff08Macro F1 0.91\uff09\uff0c\u5728\u5904\u7406\u9690\u5f0f\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u80fd\u529b\u3002\u5fae\u8c03\u6a21\u578b\u7531\u4e8e\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u548c\u67b6\u6784\u5bb9\u91cf\u9650\u5236\u8868\u73b0\u6709\u9650\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u591a\u6837\u5316\u7684\u4ee3\u7801\u8f6c\u6362\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u8bed\u8a00\u81ea\u9002\u5e94\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728Taglish\u7b49\u6df7\u5408\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u65b9\u9762\u60c5\u611f\u5206\u6790\u80fd\u529b\u3002"}}
{"id": "2601.01828", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01828", "abs": "https://arxiv.org/abs/2601.01828", "authors": ["Jack Lindsey"], "title": "Emergent Introspective Awareness in Large Language Models", "comment": null, "summary": "We investigate whether large language models can introspect on their internal states. It is difficult to answer this question through conversation alone, as genuine introspection cannot be distinguished from confabulations. Here, we address this challenge by injecting representations of known concepts into a model's activations, and measuring the influence of these manipulations on the model's self-reported states. We find that models can, in certain scenarios, notice the presence of injected concepts and accurately identify them. Models demonstrate some ability to recall prior internal representations and distinguish them from raw text inputs. Strikingly, we find that some models can use their ability to recall prior intentions in order to distinguish their own outputs from artificial prefills. In all these experiments, Claude Opus 4 and 4.1, the most capable models we tested, generally demonstrate the greatest introspective awareness; however, trends across models are complex and sensitive to post-training strategies. Finally, we explore whether models can explicitly control their internal representations, finding that models can modulate their activations when instructed or incentivized to \"think about\" a concept. Overall, our results indicate that current language models possess some functional introspective awareness of their own internal states. We stress that in today's models, this capacity is highly unreliable and context-dependent; however, it may continue to develop with further improvements to model capabilities.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u80fd\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u6ce8\u610f\u5230\u6ce8\u5165\u6982\u5ff5\u5e76\u51c6\u786e\u8bc6\u522b\uff0c\u5177\u5907\u4e00\u5b9a\u7684\u5185\u7701\u80fd\u529b\uff0c\u4f46\u53ef\u9760\u6027\u9ad8\u5ea6\u4f9d\u8d56\u4e0a\u4e0b\u6587\u548c\u6a21\u578b\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u80fd\u591f\u5185\u7701\u5176\u5185\u90e8\u72b6\u6001\uff0c\u8fd9\u662f\u4e00\u4e2a\u91cd\u8981\u95ee\u9898\uff0c\u56e0\u4e3a\u4ec5\u901a\u8fc7\u5bf9\u8bdd\u96be\u4ee5\u533a\u5206\u771f\u6b63\u5185\u7701\u4e0e\u865a\u6784\u56de\u7b54\u3002", "method": "\u901a\u8fc7\u5411\u6a21\u578b\u6fc0\u6d3b\u4e2d\u6ce8\u5165\u5df2\u77e5\u6982\u5ff5\u7684\u8868\u5f81\uff0c\u6d4b\u91cf\u8fd9\u4e9b\u64cd\u4f5c\u5bf9\u6a21\u578b\u81ea\u6211\u62a5\u544a\u72b6\u6001\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u8bc4\u4f30\u6a21\u578b\u7684\u5185\u7701\u80fd\u529b\u3002", "result": "\u6a21\u578b\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u80fd\u6ce8\u610f\u5230\u6ce8\u5165\u6982\u5ff5\u5e76\u51c6\u786e\u8bc6\u522b\uff1b\u80fd\u56de\u5fc6\u5148\u524d\u5185\u90e8\u8868\u5f81\u5e76\u4e0e\u539f\u59cb\u6587\u672c\u8f93\u5165\u533a\u5206\uff1b\u80fd\u5229\u7528\u610f\u56fe\u56de\u5fc6\u533a\u5206\u81ea\u8eab\u8f93\u51fa\u4e0e\u4eba\u5de5\u9884\u586b\u5145\uff1bClaude Opus 4\u548c4.1\u8868\u73b0\u6700\u4f73\uff1b\u6a21\u578b\u80fd\u5728\u6307\u4ee4\u6216\u6fc0\u52b1\u4e0b\u8c03\u8282\u5185\u90e8\u8868\u5f81\u3002", "conclusion": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5177\u5907\u4e00\u5b9a\u7a0b\u5ea6\u7684\u529f\u80fd\u6027\u5185\u7701\u610f\u8bc6\uff0c\u4f46\u8fd9\u79cd\u80fd\u529b\u9ad8\u5ea6\u4e0d\u53ef\u9760\u4e14\u4f9d\u8d56\u4e0a\u4e0b\u6587\uff0c\u53ef\u80fd\u968f\u6a21\u578b\u80fd\u529b\u63d0\u5347\u800c\u53d1\u5c55\u3002"}}
{"id": "2601.01842", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01842", "abs": "https://arxiv.org/abs/2601.01842", "authors": ["Yusuke Ide", "Adam Nohejl", "Joshua Tanner", "Hitomi Yanaka", "Christopher Lindsay", "Taro Watanabe"], "title": "Towards Automated Lexicography: Generating and Evaluating Definitions for Learner's Dictionaries", "comment": null, "summary": "We study dictionary definition generation (DDG), i.e., the generation of non-contextualized definitions for given headwords. Dictionary definitions are an essential resource for learning word senses, but manually creating them is costly, which motivates us to automate the process. Specifically, we address learner's dictionary definition generation (LDDG), where definitions should consist of simple words. First, we introduce a reliable evaluation approach for DDG, based on our new evaluation criteria and powered by an LLM-as-a-judge. To provide reference definitions for the evaluation, we also construct a Japanese dataset in collaboration with a professional lexicographer. Validation results demonstrate that our evaluation approach agrees reasonably well with human annotators. Second, we propose an LDDG approach via iterative simplification with an LLM. Experimental results indicate that definitions generated by our approach achieve high scores on our criteria while maintaining lexical simplicity.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u8bcd\u5178\u5b9a\u4e49\u751f\u6210(DDG)\uff0c\u7279\u522b\u662f\u5b66\u4e60\u8005\u8bcd\u5178\u5b9a\u4e49\u751f\u6210(LDDG)\uff0c\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u65b9\u6cd5\u548c\u8fed\u4ee3\u7b80\u5316\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u65e5\u8bed\u6570\u636e\u96c6\u3002", "motivation": "\u8bcd\u5178\u5b9a\u4e49\u662f\u5b66\u4e60\u8bcd\u4e49\u7684\u91cd\u8981\u8d44\u6e90\uff0c\u4f46\u4eba\u5de5\u521b\u5efa\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u81ea\u52a8\u5316\u751f\u6210\u8fc7\u7a0b\u3002\u7279\u522b\u662f\u5b66\u4e60\u8005\u8bcd\u5178\u5b9a\u4e49\u9700\u8981\u7531\u7b80\u5355\u8bcd\u6c47\u6784\u6210\uff0c\u8fd9\u589e\u52a0\u4e86\u751f\u6210\u96be\u5ea6\u3002", "method": "1) \u63d0\u51fa\u57fa\u4e8e\u65b0\u8bc4\u4f30\u6807\u51c6\u548cLLM-as-a-judge\u7684\u53ef\u9760DDG\u8bc4\u4f30\u65b9\u6cd5\uff1b2) \u4e0e\u4e13\u4e1a\u8bcd\u5178\u7f16\u7e82\u8005\u5408\u4f5c\u6784\u5efa\u65e5\u8bed\u6570\u636e\u96c6\uff1b3) \u63d0\u51fa\u901a\u8fc7LLM\u8fed\u4ee3\u7b80\u5316\u7684LDDG\u65b9\u6cd5\u3002", "result": "\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u4eba\u5de5\u6807\u6ce8\u8005\u4e00\u81f4\u6027\u826f\u597d\uff1b\u63d0\u51fa\u7684LDDG\u65b9\u6cd5\u5728\u4fdd\u6301\u8bcd\u6c47\u7b80\u5355\u6027\u7684\u540c\u65f6\uff0c\u5728\u8bc4\u4f30\u6807\u51c6\u4e0a\u83b7\u5f97\u9ad8\u5206\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bcd\u5178\u5b9a\u4e49\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u6709\u6548\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5b66\u4e60\u8005\u8bcd\u5178\u573a\u666f\uff0c\u6709\u52a9\u4e8e\u81ea\u52a8\u5316\u8bcd\u5178\u7f16\u7e82\u8fc7\u7a0b\u3002"}}
{"id": "2601.01862", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2601.01862", "abs": "https://arxiv.org/abs/2601.01862", "authors": ["Nuo Chen", "Hanpei Fang", "Piaohong Wang", "Jiqun Liu", "Tetsuya Sakai", "Xiao-Ming Wu"], "title": "Judging with Personality and Confidence: A Study on Personality-Conditioned LLM Relevance Assessment", "comment": null, "summary": "Recent studies have shown that prompting can enable large language models (LLMs) to simulate specific personality traits and produce behaviors that align with those traits. However, there is limited understanding of how these simulated personalities influence critical web search decisions, specifically relevance assessment. Moreover, few studies have examined how simulated personalities impact confidence calibration, specifically the tendencies toward overconfidence or underconfidence. This gap exists even though psychological literature suggests these biases are trait-specific, often linking high extraversion to overconfidence and high neuroticism to underconfidence. To address this gap, we conducted a comprehensive study evaluating multiple LLMs, including commercial models and open-source models, prompted to simulate Big Five personality traits. We tested these models across three test collections (TREC DL 2019, TREC DL 2020, and LLMJudge), collecting two key outputs for each query-document pair: a relevance judgment and a self-reported confidence score.\n  The findings show that personalities such as low agreeableness consistently align more closely with human labels than the unprompted condition. Additionally, low conscientiousness performs well in balancing the suppression of both overconfidence and underconfidence. We also observe that relevance scores and confidence distributions vary systematically across different personalities. Based on the above findings, we incorporate personality-conditioned scores and confidence as features in a random forest classifier. This approach achieves performance that surpasses the best single-personality condition on a new dataset (TREC DL 2021), even with limited training data. These findings highlight that personality-derived confidence offers a complementary predictive signal, paving the way for more reliable and human-aligned LLM evaluators.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u901a\u8fc7\u63d0\u793a\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u7279\u5b9a\u4eba\u683c\u7279\u8d28\uff08\u5927\u4e94\u4eba\u683c\uff09\u4f1a\u5f71\u54cd\u5176\u7f51\u9875\u641c\u7d22\u76f8\u5173\u6027\u8bc4\u4f30\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff0c\u5176\u4e2d\u4f4e\u5b9c\u4eba\u6027\u4eba\u683c\u4e0e\u4eba\u7c7b\u6807\u7b7e\u66f4\u4e00\u81f4\uff0c\u4f4e\u5c3d\u8d23\u6027\u80fd\u66f4\u597d\u5e73\u8861\u8fc7\u5ea6\u81ea\u4fe1\u548c\u81ea\u4fe1\u4e0d\u8db3\uff0c\u4eba\u683c\u7279\u5f81\u53ef\u4f5c\u4e3a\u9884\u6d4b\u4fe1\u53f7\u63d0\u5347\u8bc4\u4f30\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u867d\u7136\u8868\u660e\u63d0\u793a\u80fd\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u7279\u5b9a\u4eba\u683c\u7279\u8d28\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8fd9\u4e9b\u6a21\u62df\u4eba\u683c\u5982\u4f55\u5f71\u54cd\u5173\u952e\u7f51\u9875\u641c\u7d22\u51b3\u7b56\uff08\u7279\u522b\u662f\u76f8\u5173\u6027\u8bc4\u4f30\uff09\u7684\u7406\u89e3\uff0c\u4e5f\u4e0d\u6e05\u695a\u4eba\u683c\u5982\u4f55\u5f71\u54cd\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff08\u8fc7\u5ea6\u81ea\u4fe1\u6216\u81ea\u4fe1\u4e0d\u8db3\u503e\u5411\uff09\u3002\u5fc3\u7406\u5b66\u6587\u732e\u8868\u660e\u8fd9\u4e9b\u504f\u5dee\u5177\u6709\u7279\u8d28\u7279\u5f02\u6027\uff0c\u5982\u9ad8\u5916\u5411\u6027\u5e38\u4e0e\u8fc7\u5ea6\u81ea\u4fe1\u76f8\u5173\uff0c\u9ad8\u795e\u7ecf\u8d28\u5e38\u4e0e\u81ea\u4fe1\u4e0d\u8db3\u76f8\u5173\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u5546\u4e1a\u6a21\u578b\u548c\u5f00\u6e90\u6a21\u578b\uff09\uff0c\u901a\u8fc7\u63d0\u793a\u8ba9\u5b83\u4eec\u6a21\u62df\u5927\u4e94\u4eba\u683c\u7279\u8d28\u3002\u5728\u4e09\u4e2a\u6d4b\u8bd5\u96c6\uff08TREC DL 2019\u3001TREC DL 2020\u548cLLMJudge\uff09\u4e0a\u8fdb\u884c\u6d4b\u8bd5\uff0c\u4e3a\u6bcf\u4e2a\u67e5\u8be2-\u6587\u6863\u5bf9\u6536\u96c6\u4e24\u4e2a\u5173\u952e\u8f93\u51fa\uff1a\u76f8\u5173\u6027\u5224\u65ad\u548c\u81ea\u62a5\u544a\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u3002", "result": "\u4f4e\u5b9c\u4eba\u6027\u4eba\u683c\u4e0e\u4eba\u7c7b\u6807\u7b7e\u7684\u4e00\u81f4\u6027\u6bd4\u65e0\u63d0\u793a\u6761\u4ef6\u66f4\u9ad8\uff1b\u4f4e\u5c3d\u8d23\u6027\u4eba\u683c\u5728\u5e73\u8861\u6291\u5236\u8fc7\u5ea6\u81ea\u4fe1\u548c\u81ea\u4fe1\u4e0d\u8db3\u65b9\u9762\u8868\u73b0\u826f\u597d\uff1b\u4e0d\u540c\u4eba\u683c\u7684\u76f8\u5173\u6027\u5206\u6570\u548c\u7f6e\u4fe1\u5ea6\u5206\u5e03\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\u3002\u5c06\u4eba\u683c\u6761\u4ef6\u5316\u7684\u5206\u6570\u548c\u7f6e\u4fe1\u5ea6\u4f5c\u4e3a\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\u7684\u7279\u5f81\uff0c\u5373\u4f7f\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u65b0\u6570\u636e\u96c6\uff08TREC DL 2021\uff09\u4e0a\u7684\u6027\u80fd\u4e5f\u8d85\u8fc7\u4e86\u6700\u4f73\u5355\u4e00\u4eba\u683c\u6761\u4ef6\u3002", "conclusion": "\u4eba\u683c\u884d\u751f\u7684\u7f6e\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u4e92\u8865\u7684\u9884\u6d4b\u4fe1\u53f7\uff0c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u66f4\u7b26\u5408\u4eba\u7c7b\u5224\u65ad\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5668\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002\u8fd9\u8868\u660e\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4eba\u683c\u6a21\u62df\u53ef\u4ee5\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u5728\u641c\u7d22\u8bc4\u4f30\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u548c\u6821\u51c6\u3002"}}
{"id": "2601.01868", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01868", "abs": "https://arxiv.org/abs/2601.01868", "authors": ["Jinghan Ru", "Siyuan Yan", "Yuguo Yin", "Yuexian Zou", "Zongyuan Ge"], "title": "DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show promise for medical applications, yet progress in dermatology lags due to limited training data, narrow task coverage, and lack of clinically-grounded supervision that mirrors expert diagnostic workflows. We present a comprehensive framework to address these gaps. First, we introduce DermoInstruct, a large-scale morphology-anchored instruction corpus comprising 211,243 images and 772,675 trajectories across five task formats, capturing the complete diagnostic pipeline from morphological observation and clinical reasoning to final diagnosis. Second, we establish DermoBench, a rigorous benchmark evaluating 11 tasks across four clinical axes: Morphology, Diagnosis, Reasoning, and Fairness, including a challenging subset of 3,600 expert-verified open-ended instances and human performance baselines. Third, we develop DermoGPT, a dermatology reasoning MLLM trained via supervised fine-tuning followed by our Morphologically-Anchored Visual-Inference-Consistent (MAVIC) reinforcement learning objective, which enforces consistency between visual observations and diagnostic conclusions. At inference, we deploy Confidence-Consistency Test-time adaptation (CCT) for robust predictions. Experiments show DermoGPT significantly outperforms 16 representative baselines across all axes, achieving state-of-the-art performance while substantially narrowing the human-AI gap. DermoInstruct, DermoBench and DermoGPT will be made publicly available at https://github.com/mendicant04/DermoGPT upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86DermoGPT\u6846\u67b6\uff0c\u5305\u542b\u5927\u89c4\u6a21\u76ae\u80a4\u75c5\u5b66\u6307\u4ee4\u6570\u636e\u96c6DermoInstruct\u3001\u7efc\u5408\u8bc4\u4f30\u57fa\u51c6DermoBench\u548c\u57fa\u4e8e\u5f62\u6001\u5b66\u951a\u5b9a\u7684\u89c6\u89c9\u63a8\u7406\u4e00\u81f4\u6027\u8bad\u7ec3\u7684\u76ae\u80a4\u75c5\u5b66\u63a8\u7406MLLM\u6a21\u578bDermoGPT\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u76ae\u80a4\u75c5\u8bca\u65ad\u4e2d\u7684\u4eba\u673a\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u76ae\u80a4\u75c5\u5b66\u5e94\u7528\u8fdb\u5c55\u7f13\u6162\uff0c\u4e3b\u8981\u53d7\u9650\u4e8e\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u3001\u4efb\u52a1\u8986\u76d6\u8303\u56f4\u7a84\u3001\u7f3a\u4e4f\u6a21\u62df\u4e13\u5bb6\u8bca\u65ad\u6d41\u7a0b\u7684\u4e34\u5e8a\u76d1\u7763\u4fe1\u53f7\u3002\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u5168\u9762\u7684\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e9b\u74f6\u9888\u3002", "method": "1) \u6784\u5efaDermoInstruct\u6570\u636e\u96c6\uff1a\u5305\u542b211,243\u5f20\u56fe\u50cf\u548c772,675\u6761\u8f68\u8ff9\uff0c\u6db5\u76d65\u79cd\u4efb\u52a1\u683c\u5f0f\uff0c\u6355\u6349\u4ece\u5f62\u6001\u89c2\u5bdf\u5230\u4e34\u5e8a\u63a8\u7406\u518d\u5230\u6700\u7ec8\u8bca\u65ad\u7684\u5b8c\u6574\u6d41\u7a0b\uff1b2) \u5efa\u7acbDermoBench\u8bc4\u4f30\u57fa\u51c6\uff1a\u5305\u542b11\u4e2a\u4efb\u52a1\uff0c\u8986\u76d6\u5f62\u6001\u5b66\u3001\u8bca\u65ad\u3001\u63a8\u7406\u548c\u516c\u5e73\u6027\u56db\u4e2a\u4e34\u5e8a\u7ef4\u5ea6\uff0c\u5305\u542b3,600\u4e2a\u4e13\u5bb6\u9a8c\u8bc1\u7684\u5f00\u653e\u5f0f\u5b9e\u4f8b\uff1b3) \u5f00\u53d1DermoGPT\u6a21\u578b\uff1a\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u540e\u91c7\u7528\u5f62\u6001\u5b66\u951a\u5b9a\u7684\u89c6\u89c9\u63a8\u7406\u4e00\u81f4\u6027\u5f3a\u5316\u5b66\u4e60\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u7f6e\u4fe1\u5ea6\u4e00\u81f4\u6027\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u3002", "result": "DermoGPT\u572816\u4e2a\u4ee3\u8868\u6027\u57fa\u7ebf\u6a21\u578b\u4e0a\u8868\u73b0\u663e\u8457\u4f18\u8d8a\uff0c\u5728\u6240\u6709\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u90fd\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u540c\u65f6\u5927\u5e45\u7f29\u5c0f\u4e86\u4eba\u673a\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u76ae\u80a4\u75c5\u5b66MLLM\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3001\u7efc\u5408\u8bc4\u4f30\u57fa\u51c6\u548c\u521b\u65b0\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u76ae\u80a4\u75c5\u8bca\u65ad\u7684AI\u6027\u80fd\uff0c\u4e3a\u76ae\u80a4\u75c5\u5b66AI\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2601.01885", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01885", "abs": "https://arxiv.org/abs/2601.01885", "authors": ["Yi Yu", "Liuyi Yao", "Yuexiang Xie", "Qingquan Tan", "Jiaqi Feng", "Yaliang Li", "Libing Wu"], "title": "Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents", "comment": null, "summary": "Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.", "AI": {"tldr": "AgeMem\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u957f\u77ed\u671f\u8bb0\u5fc6\u7ba1\u7406\u76f4\u63a5\u96c6\u6210\u5230LLM\u4ee3\u7406\u7684\u7b56\u7565\u4e2d\uff0c\u901a\u8fc7\u5de5\u5177\u5316\u64cd\u4f5c\u8ba9\u4ee3\u7406\u81ea\u4e3b\u7ba1\u7406\u8bb0\u5fc6\uff0c\u4f7f\u7528\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728\u957f\u65f6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5728\u957f\u65f6\u63a8\u7406\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u56e0\u4e3a\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u957f\u77ed\u671f\u8bb0\u5fc6\u4f5c\u4e3a\u72ec\u7acb\u7ec4\u4ef6\u5904\u7406\uff0c\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u8f85\u52a9\u63a7\u5236\u5668\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u548c\u7aef\u5230\u7aef\u4f18\u5316\u3002", "method": "\u63d0\u51faAgentic Memory (AgeMem)\u7edf\u4e00\u6846\u67b6\uff0c\u5c06\u8bb0\u5fc6\u64cd\u4f5c\u66b4\u9732\u4e3a\u5de5\u5177\u5316\u52a8\u4f5c\uff0c\u8ba9LLM\u4ee3\u7406\u81ea\u4e3b\u51b3\u5b9a\u5b58\u50a8\u3001\u68c0\u7d22\u3001\u66f4\u65b0\u3001\u603b\u7ed3\u6216\u4e22\u5f03\u4fe1\u606f\u3002\u91c7\u7528\u4e09\u9636\u6bb5\u6e10\u8fdb\u5f0f\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u8bbe\u8ba1step-wise GRPO\u89e3\u51b3\u8bb0\u5fc6\u64cd\u4f5c\u5f15\u8d77\u7684\u7a00\u758f\u548c\u4e0d\u8fde\u7eed\u5956\u52b1\u95ee\u9898\u3002", "result": "\u5728\u4e94\u4e2a\u957f\u65f6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgeMem\u5728\u591a\u4e2aLLM\u9aa8\u5e72\u7f51\u7edc\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u8bb0\u5fc6\u589e\u5f3a\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u4efb\u52a1\u6027\u80fd\u3001\u66f4\u9ad8\u8d28\u91cf\u7684\u957f\u65f6\u8bb0\u5fc6\u548c\u66f4\u9ad8\u6548\u7684\u4e0a\u4e0b\u6587\u4f7f\u7528\u3002", "conclusion": "AgeMem\u901a\u8fc7\u5c06\u8bb0\u5fc6\u7ba1\u7406\u76f4\u63a5\u96c6\u6210\u5230\u4ee3\u7406\u7b56\u7565\u4e2d\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u3001\u53ef\u5b66\u4e60\u7684\u8bb0\u5fc6\u7ba1\u7406\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u5728\u957f\u65f6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2601.01896", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.01896", "abs": "https://arxiv.org/abs/2601.01896", "authors": ["Jingyu Liu", "Jiaen Lin", "Yong Liu"], "title": "Tackling the Inherent Difficulty of Noise Filtering in RAG", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u589e\u5f3aLLMs\u5728RAG\u4e2d\u533a\u5206\u76f8\u5173\u4e0e\u4e0d\u76f8\u5173\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u63d0\u9ad8\u6a21\u578b\u5bf9\u68c0\u7d22\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "motivation": "RAG\u4e2d\u5e38\u5f15\u5165\u566a\u58f0\u6216\u4e0d\u76f8\u5173\u6587\u6863\uff0c\u8fd9\u4f1a\u964d\u4f4e\u6027\u80fd\u751a\u81f3\u5bfc\u81f4\u5e7b\u89c9\u8f93\u51fa\u3002\u73b0\u6709\u8fc7\u6ee4\u65b9\u6cd5\u96be\u4ee5\u5b8c\u5168\u6d88\u9664\u4e0d\u76f8\u5173\u4fe1\u606f\uff0c\u800c\u6807\u51c6\u5fae\u8c03\u65b9\u6cd5\u56e0\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u7ed3\u6784\u9650\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u8ba9\u6a21\u578b\u9009\u62e9\u6027\u5229\u7528\u76f8\u5173\u4fe1\u606f\u800c\u5ffd\u7565\u4e0d\u76f8\u5173\u5185\u5bb9\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u4e13\u95e8\u8bbe\u8ba1\u6765\u589e\u5f3a\u6a21\u578b\u5728\u68c0\u7d22\u6587\u6863\u4e2d\u533a\u5206\u76f8\u5173\u4e0e\u4e0d\u76f8\u5173\u4fe1\u606f\u7684\u80fd\u529b\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86LLMs\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6709\u6548\u589e\u5f3aLLMs\u5728RAG\u573a\u666f\u4e2d\u5904\u7406\u566a\u58f0\u6587\u6863\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b8c\u5168\u8fc7\u6ee4\u4e0d\u76f8\u5173\u4fe1\u606f\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2601.01964", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01964", "abs": "https://arxiv.org/abs/2601.01964", "authors": ["Tran Sy Bao"], "title": "CSF: Contrastive Semantic Features for Direct Multilingual Sign Language Generation", "comment": "9 pages, 8 tables, code available at https://github.com/transybao1393/csf-sign-language", "summary": "Sign language translation systems typically require English as an intermediary language, creating barriers for non-English speakers in the global deaf community. We present Canonical Semantic Form (CSF), a language-agnostic semantic representation framework that enables direct translation from any source language to sign language without English mediation. CSF decomposes utterances into nine universal semantic slots: event, intent, time, condition, agent, object, location, purpose, and modifier. A key contribution is our comprehensive condition taxonomy comprising 35 condition types across eight semantic categories, enabling nuanced representation of conditional expressions common in everyday communication. We train a lightweight transformer-based extractor (0.74 MB) that achieves 99.03% average slot extraction accuracy across four typologically diverse languages: English, Vietnamese, Japanese, and French. The model demonstrates particularly strong performance on condition classification (99.4% accuracy) despite the 35-class complexity. With inference latency of 3.02ms on CPU, our approach enables real-time sign language generation in browser-based applications. We release our code, trained models, and multilingual dataset to support further research in accessible sign language technology.", "AI": {"tldr": "\u63d0\u51faCanonical Semantic Form (CSF)\u6846\u67b6\uff0c\u5b9e\u73b0\u4ece\u4efb\u610f\u6e90\u8bed\u8a00\u5230\u624b\u8bed\u7684\u76f4\u63a5\u7ffb\u8bd1\uff0c\u65e0\u9700\u82f1\u8bed\u4e2d\u4ecb\uff0c\u901a\u8fc79\u4e2a\u901a\u7528\u8bed\u4e49\u69fd\u548c35\u79cd\u6761\u4ef6\u7c7b\u578b\u5206\u7c7b\u5b9e\u73b0\u8bed\u8a00\u65e0\u5173\u7684\u8bed\u4e49\u8868\u793a\u3002", "motivation": "\u73b0\u6709\u624b\u8bed\u7ffb\u8bd1\u7cfb\u7edf\u901a\u5e38\u9700\u8981\u82f1\u8bed\u4f5c\u4e3a\u4e2d\u4ecb\u8bed\u8a00\uff0c\u8fd9\u4e3a\u5168\u7403\u975e\u82f1\u8bed\u4f7f\u7528\u8005\u7684\u804b\u4eba\u793e\u533a\u9020\u6210\u4e86\u969c\u788d\u3002\u9700\u8981\u4e00\u79cd\u8bed\u8a00\u65e0\u5173\u7684\u8bed\u4e49\u8868\u793a\u6846\u67b6\uff0c\u5b9e\u73b0\u4ece\u4efb\u4f55\u6e90\u8bed\u8a00\u5230\u624b\u8bed\u7684\u76f4\u63a5\u7ffb\u8bd1\u3002", "method": "\u63d0\u51faCanonical Semantic Form (CSF)\u6846\u67b6\uff0c\u5c06\u8bdd\u8bed\u5206\u89e3\u4e3a9\u4e2a\u901a\u7528\u8bed\u4e49\u69fd\uff1a\u4e8b\u4ef6\u3001\u610f\u56fe\u3001\u65f6\u95f4\u3001\u6761\u4ef6\u3001\u65bd\u4e8b\u8005\u3001\u5bf9\u8c61\u3001\u4f4d\u7f6e\u3001\u76ee\u7684\u548c\u4fee\u9970\u8bed\u3002\u7279\u522b\u8d21\u732e\u4e86\u5305\u542b8\u4e2a\u8bed\u4e49\u7c7b\u522b\u300135\u79cd\u6761\u4ef6\u7c7b\u578b\u7684\u5168\u9762\u6761\u4ef6\u5206\u7c7b\u6cd5\u3002\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u57fa\u4e8etransformer\u7684\u63d0\u53d6\u5668\uff080.74 MB\uff09\u3002", "result": "\u5728\u56db\u79cd\u7c7b\u578b\u5b66\u591a\u6837\u8bed\u8a00\uff08\u82f1\u8bed\u3001\u8d8a\u5357\u8bed\u3001\u65e5\u8bed\u3001\u6cd5\u8bed\uff09\u4e0a\u5b9e\u73b099.03%\u7684\u5e73\u5747\u69fd\u63d0\u53d6\u51c6\u786e\u7387\u3002\u6761\u4ef6\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523099.4%\uff0835\u4e2a\u7c7b\u522b\uff09\u3002\u5728CPU\u4e0a\u63a8\u7406\u5ef6\u8fdf\u4e3a3.02ms\uff0c\u652f\u6301\u6d4f\u89c8\u5668\u5e94\u7528\u7684\u5b9e\u65f6\u624b\u8bed\u751f\u6210\u3002", "conclusion": "CSF\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u8bed\u8a00\u65e0\u5173\u7684\u624b\u8bed\u7ffb\u8bd1\uff0c\u6d88\u9664\u4e86\u82f1\u8bed\u4e2d\u4ecb\u9700\u6c42\u3002\u8f7b\u91cf\u7ea7\u6a21\u578b\u7684\u9ad8\u6027\u80fd\u548c\u4f4e\u5ef6\u8fdf\u4f7f\u5176\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002\u5f00\u6e90\u4ee3\u7801\u3001\u6a21\u578b\u548c\u591a\u8bed\u8a00\u6570\u636e\u96c6\u5c06\u4fc3\u8fdb\u65e0\u969c\u788d\u624b\u8bed\u6280\u672f\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2601.01972", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.01972", "abs": "https://arxiv.org/abs/2601.01972", "authors": ["Alexandre Le Mercier", "Chris Develder", "Thomas Demeester"], "title": "Hidden State Poisoning Attacks against Mamba-based Language Models", "comment": "17 pages, 4 figures. Submitted to ACL 2026", "summary": "State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9488\u5bf9\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08\u5982Mamba\uff09\u7684\u9690\u85cf\u72b6\u6001\u4e2d\u6bd2\u653b\u51fb\uff08HiSPA\uff09\uff0c\u53d1\u73b0\u7279\u5b9a\u77ed\u8f93\u5165\u77ed\u8bed\u53ef\u5bfc\u81f4\u6a21\u578b\u4fe1\u606f\u9057\u5fd8\uff0c\u5e76\u5f00\u53d1\u4e86RoBench25\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u5bf9\u6b64\u7c7b\u653b\u51fb\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u5982Mamba\u63d0\u4f9b\u4e86Transformer\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u5bf9\u6297\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22SSMs\u5728\u9762\u5bf9\u7279\u5b9a\u8f93\u5165\u77ed\u8bed\u653b\u51fb\u65f6\u7684\u8106\u5f31\u6027\uff0c\u5373\u9690\u85cf\u72b6\u6001\u4e2d\u6bd2\u653b\u51fb\uff08HiSPA\uff09\u3002", "method": "\u5f00\u53d1\u4e86RoBench25\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u5728HiSPA\u653b\u51fb\u4e0b\u7684\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\uff1b\u5bf9SSMs\uff08\u5305\u62ec52B\u53c2\u6570\u7684Jamba\u6df7\u5408\u6a21\u578b\uff09\u8fdb\u884c\u653b\u51fb\u6d4b\u8bd5\uff1b\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u5206\u6790Mamba\u9690\u85cf\u5c42\u5728HiSPA\u653b\u51fb\u4e0b\u7684\u6a21\u5f0f\u3002", "result": "SSMs\u5bf9HiSPA\u653b\u51fb\u9ad8\u5ea6\u8106\u5f31\uff0c\u5373\u4f7f\u662f52B\u53c2\u6570\u7684Jamba\u6df7\u5408\u6a21\u578b\u5728\u4f18\u5316\u540e\u7684HiSPA\u89e6\u53d1\u8bcd\u4e0b\u4e5f\u4f1a\u5d29\u6e83\uff0c\u800c\u7eafTransformer\u6a21\u578b\u5219\u4e0d\u53d7\u5f71\u54cd\u3002HiSPA\u89e6\u53d1\u8bcd\u663e\u8457\u524a\u5f31\u4e86Jamba\u6a21\u578b\u5728Open-Prompt-Injections\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5b58\u5728\u9690\u85cf\u72b6\u6001\u4e2d\u6bd2\u653b\u51fb\u7684\u4e25\u91cd\u8106\u5f31\u6027\uff0c\u8fd9\u79cd\u653b\u51fb\u53ef\u5bfc\u81f4\u6a21\u578b\u4fe1\u606f\u9057\u5fd8\u3002\u7814\u7a76\u63ed\u793a\u4e86Mamba\u9690\u85cf\u5c42\u5728\u653b\u51fb\u4e0b\u7684\u7279\u5b9a\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u53ef\u7528\u4e8e\u6784\u5efaHiSPA\u7f13\u89e3\u7cfb\u7edf\u3002"}}
{"id": "2601.02015", "categories": ["cs.CL", "cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2601.02015", "abs": "https://arxiv.org/abs/2601.02015", "authors": ["Omar Momen", "Emilie Sitter", "Berenike Herrmann", "Sina Zarrie\u00df"], "title": "Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling Effects", "comment": "to be published at EACL 2026 main conference", "summary": "Novel metaphor comprehension involves complex semantic processes and linguistic creativity, making it an interesting task for studying language models (LMs). This study investigates whether surprisal, a probabilistic measure of predictability in LMs, correlates with different metaphor novelty datasets. We analyse surprisal from 16 LM variants on corpus-based and synthetic metaphor novelty datasets. We explore a cloze-style surprisal method that conditions on full-sentence context. Results show that LMs yield significant moderate correlations with scores/labels of metaphor novelty. We further identify divergent scaling patterns: on corpus-based data, correlation strength decreases with model size (inverse scaling effect), whereas on synthetic data it increases (Quality-Power Hypothesis). We conclude that while surprisal can partially account for annotations of metaphor novelty, it remains a limited metric of linguistic creativity.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u7684\u60ca\u5947\u5ea6\u4e0e\u9690\u55bb\u65b0\u9896\u6027\u8bc4\u5206\u5b58\u5728\u4e2d\u7b49\u76f8\u5173\uff0c\u4f46\u5728\u8bed\u6599\u5e93\u6570\u636e\u548c\u5408\u6210\u6570\u636e\u4e0a\u5448\u73b0\u76f8\u53cd\u7684\u89c4\u6a21\u6548\u5e94", "motivation": "\u65b0\u9896\u9690\u55bb\u7406\u89e3\u6d89\u53ca\u590d\u6742\u7684\u8bed\u4e49\u8fc7\u7a0b\u548c\u8bed\u8a00\u521b\u9020\u529b\uff0c\u662f\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u6709\u8da3\u4efb\u52a1\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u7684\u60ca\u5947\u5ea6\uff08\u4e00\u79cd\u6982\u7387\u53ef\u9884\u6d4b\u6027\u5ea6\u91cf\uff09\u662f\u5426\u4e0e\u4e0d\u540c\u9690\u55bb\u65b0\u9896\u6027\u6570\u636e\u96c6\u76f8\u5173\u3002", "method": "\u4f7f\u752816\u79cd\u8bed\u8a00\u6a21\u578b\u53d8\u4f53\u5206\u6790\u8bed\u6599\u5e93\u57fa\u7840\u548c\u5408\u6210\u9690\u55bb\u65b0\u9896\u6027\u6570\u636e\u96c6\u4e0a\u7684\u60ca\u5947\u5ea6\uff0c\u91c7\u7528\u57fa\u4e8e\u5b8c\u6574\u53e5\u5b50\u4e0a\u4e0b\u6587\u7684\u5b8c\u5f62\u586b\u7a7a\u5f0f\u60ca\u5947\u5ea6\u65b9\u6cd5\u3002", "result": "\u8bed\u8a00\u6a21\u578b\u4e0e\u9690\u55bb\u65b0\u9896\u6027\u8bc4\u5206/\u6807\u7b7e\u5b58\u5728\u663e\u8457\u4e2d\u7b49\u76f8\u5173\u6027\u3002\u53d1\u73b0\u4e0d\u540c\u7684\u89c4\u6a21\u6548\u5e94\u6a21\u5f0f\uff1a\u5728\u8bed\u6599\u5e93\u6570\u636e\u4e0a\uff0c\u76f8\u5173\u6027\u5f3a\u5ea6\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5927\u800c\u51cf\u5c0f\uff08\u9006\u89c4\u6a21\u6548\u5e94\uff09\uff0c\u800c\u5728\u5408\u6210\u6570\u636e\u4e0a\u5219\u968f\u6a21\u578b\u89c4\u6a21\u589e\u5927\u800c\u589e\u52a0\uff08\u8d28\u91cf-\u80fd\u529b\u5047\u8bbe\uff09\u3002", "conclusion": "\u867d\u7136\u60ca\u5947\u5ea6\u80fd\u90e8\u5206\u89e3\u91ca\u9690\u55bb\u65b0\u9896\u6027\u7684\u6807\u6ce8\uff0c\u4f46\u5b83\u4ecd\u7136\u662f\u8bed\u8a00\u521b\u9020\u529b\u7684\u6709\u9650\u5ea6\u91cf\u6307\u6807\u3002"}}
{"id": "2601.02023", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02023", "abs": "https://arxiv.org/abs/2601.02023", "authors": ["Amirali Ebrahimzadeh", "Seyyed M. Salili"], "title": "Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs", "comment": "25 pages, 8 figures, 3 tables", "summary": "Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.", "AI": {"tldr": "\u7814\u7a76\u957f\u4e0a\u4e0b\u6587LLM\u5728\u4fe1\u606f\u63d0\u53d6\u548c\u63a8\u7406\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u52a0\u4e0d\u4e00\u5b9a\u63d0\u5347\u6027\u80fd\uff0c\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u5927\uff0c\u53cd\u5e7b\u89c9\u6307\u4ee4\u53ef\u80fd\u8fc7\u5ea6\u4fdd\u5b88\u964d\u4f4e\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740LLM\u652f\u6301\u8d8a\u6765\u8d8a\u957f\u7684\u8f93\u5165\u4e0a\u4e0b\u6587\uff0c\u4f46\u5b83\u4eec\u5728\u89c4\u6a21\u5316\u63d0\u53d6\u548c\u63a8\u7406\u4fe1\u606f\u65b9\u9762\u7684\u53ef\u9760\u6027\u4ecd\u4e0d\u660e\u786e\u3002\u6027\u80fd\u968f\u4e0a\u4e0b\u6587\u957f\u5ea6\u53d8\u5316\uff0c\u4e14\u4e0e\u771f\u5b9e\u8bed\u6599\u4e2d\u4fe1\u606f\u5206\u5e03\u65b9\u5f0f\u5f3a\u70c8\u4ea4\u4e92\u3002\u4f01\u4e1a\u5de5\u4f5c\u6d41\u4e2d\u5e38\u5c06\u5927\u91cf\u672a\u8fc7\u6ee4\u6587\u6863\u7c98\u8d34\u5230LLM\u63d0\u793a\u4e2d\uff0c\u56e0\u6b64\u7406\u89e3\u957f\u4e0a\u4e0b\u6587\u4e0b\u7684\u6a21\u578b\u884c\u4e3a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f15\u5165\u6269\u5c55\u7684\"\u5927\u6d77\u635e\u9488\"\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u56db\u4e2a\u751f\u4ea7\u7ea7\u6a21\u578b(Gemini-2.5-flash\u3001ChatGPT-5-mini\u3001Claude-4.5-haiku\u3001Deepseek-v3.2-chat)\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u5206\u522b\u8bc4\u4f30\u5b57\u9762\u63d0\u53d6\u3001\u903b\u8f91\u63a8\u7406\u548c\u5e7b\u89c9\u98ce\u9669\u3002\u7814\u7a76\u4f4d\u7f6e\u6548\u5e94\u548c\u8bc1\u636e\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u771f\u5b9e\u5206\u5e03\uff0c\u4ee5\u53ca\u660e\u786e\u7981\u6b62\u865a\u6784\u7684\u63d0\u793a\u3002\u8003\u8651\u53cd\u5e7b\u89c9\u6307\u4ee4\u7684\u5f71\u54cd\u3002", "result": "\u4ec5\u589e\u52a0\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0d\u80fd\u4fdd\u8bc1\u66f4\u597d\u6027\u80fd\uff0c\u5f53\u76f8\u5173\u8bc1\u636e\u88ab\u7a00\u91ca\u6216\u5e7f\u6cdb\u5206\u6563\u65f6\u53ef\u80fd\u6709\u5bb3\u3002\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff1a\u4e00\u4e9b\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u4e25\u91cd\u9000\u5316\uff0c\u800c\u53e6\u4e00\u4e9b\u5728\u66f4\u957f\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u66f4\u7a33\u5065\u3002\u53cd\u5e7b\u89c9\u6307\u4ee4\u53ef\u80fd\u4f7f\u67d0\u4e9b\u6a21\u578b\u8fc7\u5ea6\u4fdd\u5b88\uff0c\u663e\u8457\u964d\u4f4e\u5b57\u9762\u63d0\u53d6\u548c\u903b\u8f91\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002\u6a21\u578b\u7ecf\u5e38\u96be\u4ee5\u8bc6\u522b\u548c\u4f18\u5148\u5904\u7406\u76f8\u5173\u4fe1\u606f\uff0c\u5373\u4f7f\u4fe1\u606f\u5b58\u5728\u3002", "conclusion": "\u6709\u6548\u4e0a\u4e0b\u6587\u957f\u5ea6\u548c\u6a21\u578b\u5bf9\u957f\u4e0a\u4e0b\u6587\u7684\u7279\u5b9a\u9c81\u68d2\u6027\u5bf9\u4e8eLLM\u5728\u7814\u7a76\u548c\u5546\u4e1a\u4e2d\u7684\u53ef\u9760\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u8bb8\u591a\u5931\u8d25\u6e90\u4e8e\u65e0\u6548\u7684\u4e0a\u4e0b\u6587\u5229\u7528\uff0c\u800c\u975e\u4fe1\u606f\u7f3a\u5931\u3002\u4f01\u4e1a\u5de5\u4f5c\u6d41\u4e2d\u76f4\u63a5\u7c98\u8d34\u5927\u91cf\u672a\u8fc7\u6ee4\u6587\u6863\u7684\u505a\u6cd5\u9700\u8981\u8c28\u614e\u8003\u8651\u6a21\u578b\u7684\u5b9e\u9645\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2601.02065", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02065", "abs": "https://arxiv.org/abs/2601.02065", "authors": ["Md. Asif Hossain", "Nabil Subhan", "Mantasha Rahman Mahi", "Jannatul Ferdous Nabila"], "title": "Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory", "comment": "5 pages, 3 figures, 1 table", "summary": "Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u519c\u4e1a\u54a8\u8be2\u7684\u8de8\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ffb\u8bd1\u67b6\u6784\u5b9e\u73b0\u4f4e\u6210\u672c\u3001\u4e8b\u5b9e\u53ef\u9760\u7684\u519c\u4e1a\u77e5\u8bc6\u8bbf\u95ee", "motivation": "\u5728\u53d1\u5c55\u4e2d\u5730\u533a\uff0c\u6743\u5a01\u519c\u4e1a\u624b\u518c\u591a\u4e3a\u82f1\u6587\uff0c\u800c\u519c\u6c11\u4e3b\u8981\u4f7f\u7528\u5b5f\u52a0\u62c9\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u73b0\u6709LLM\u76f4\u63a5\u751f\u6210\u4f4e\u8d44\u6e90\u8bed\u8a00\u5b58\u5728\u6d41\u7545\u6027\u548c\u4e8b\u5b9e\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4e91\u7aef\u65b9\u6848\u6210\u672c\u9ad8\u6602", "method": "\u91c7\u7528\u7ffb\u8bd1\u4e2d\u5fc3\u67b6\u6784\uff1a\u5c06\u5b5f\u52a0\u62c9\u8bed\u67e5\u8be2\u7ffb\u8bd1\u6210\u82f1\u6587\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u5173\u952e\u8bcd\u6ce8\u5165\u5bf9\u9f50\u519c\u6c11\u672f\u8bed\u4e0e\u79d1\u5b66\u672f\u8bed\uff0c\u5728\u82f1\u6587\u519c\u4e1a\u624b\u518c\u4e0a\u8fdb\u884c\u5bc6\u96c6\u5411\u91cf\u68c0\u7d22\uff0c\u6700\u540e\u5c06\u82f1\u6587\u56de\u7b54\u7ffb\u8bd1\u56de\u5b5f\u52a0\u62c9\u8bed", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\u7cfb\u7edf\u80fd\u63d0\u4f9b\u53ef\u9760\u7684\u4e8b\u5b9e\u4f9d\u636e\u56de\u7b54\uff0c\u6709\u6548\u62d2\u7edd\u9886\u57df\u5916\u67e5\u8be2\uff0c\u5e73\u5747\u7aef\u5230\u7aef\u5ef6\u8fdf\u4f4e\u4e8e20\u79d2\uff0c\u5b8c\u5168\u4f7f\u7528\u5f00\u6e90\u6a21\u578b\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u4e0a\u8fd0\u884c", "conclusion": "\u8de8\u8bed\u8a00\u68c0\u7d22\u7ed3\u5408\u53d7\u63a7\u7ffb\u8bd1\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u519c\u4e1a\u77e5\u8bc6\u8bbf\u95ee\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2601.02076", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02076", "abs": "https://arxiv.org/abs/2601.02076", "authors": ["Yingte Shu", "Yuchuan Tian", "Chao Xu", "Yunhe Wang", "Hanting Chen"], "title": "Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows", "comment": null, "summary": "Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.", "AI": {"tldr": "DCD\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6ed1\u52a8\u7a97\u53e3\u5ef6\u8fdf\u9ad8\u4e0d\u786e\u5b9a\u6027token\u7684\u786e\u5b9a\uff0c\u89e3\u51b3\u5757\u6269\u6563\u6a21\u578b\u4e2d\u8fb9\u754c\u4e0a\u4e0b\u6587\u622a\u65ad\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5757\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8fb9\u754c\u8bf1\u5bfc\u4e0a\u4e0b\u6587\u622a\u65ad\u95ee\u9898\uff1a\u5757\u8fb9\u754c\u9644\u8fd1\u7684\u672a\u89e3\u7801token\u5728\u7f3a\u4e4f\u672a\u6765\u4e0a\u4e0b\u6587\u7684\u60c5\u51b5\u4e0b\u88ab\u8feb\u786e\u5b9a\uff0c\u5bfc\u81f4\u89e3\u7801\u7f6e\u4fe1\u5ea6\u548c\u751f\u6210\u8d28\u91cf\u4e0b\u964d\uff0c\u5c24\u5176\u5728\u9700\u8981\u7cbe\u786e\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51fa\u5ef6\u8fdf\u627f\u8bfa\u89e3\u7801\uff1a\u7ef4\u62a4\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u6ed1\u52a8\u7a97\u53e3\uff0c\u65e9\u671f\u89e3\u51b3\u4f4e\u4e0d\u786e\u5b9a\u6027token\uff0c\u5ef6\u8fdf\u9ad8\u4e0d\u786e\u5b9a\u6027token\u76f4\u5230\u83b7\u5f97\u8db3\u591f\u4e0a\u4e0b\u6587\u8bc1\u636e\uff0c\u5b9e\u73b0\u89e3\u7801\u7a97\u53e3\u5185\u7684\u6709\u6548\u53cc\u5411\u4fe1\u606f\u6d41\u3002", "result": "\u5728\u591a\u4e2a\u6269\u6563\u8bed\u8a00\u6a21\u578b\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u7f13\u5b58\u914d\u7f6e\u4e0b\uff0cDCD\u5e73\u5747\u63d0\u5347\u751f\u6210\u51c6\u786e\u73871.39%\uff0c\u65f6\u95f4\u76f8\u5f53\uff0c\u6700\u5927\u6539\u8fdb\u8fbe9.0%\u3002", "conclusion": "\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u5ef6\u8fdftoken\u786e\u5b9a\u662f\u63d0\u5347\u6269\u6563\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u8d28\u91cf\u548c\u6548\u7387\u7684\u7b80\u5355\u6709\u6548\u539f\u5219\u3002"}}
{"id": "2601.02123", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02123", "abs": "https://arxiv.org/abs/2601.02123", "authors": ["Po-Jen Ko", "Chen-Han Tsai", "Yu-Shao Peng"], "title": "DeCode: Decoupling Content and Delivery for Medical QA", "comment": "Preprint", "summary": "Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\\%$ to $49.8\\%$, corresponding to a $75\\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.", "AI": {"tldr": "DeCode\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f7f\u73b0\u6709LLM\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u751f\u6210\u66f4\u5177\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u7684\u7b54\u6848\uff0c\u5728OpenAI HealthBench\u4e0a\u5c06SOTA\u4ece28.4%\u63d0\u5347\u523049.8%\u3002", "motivation": "\u73b0\u6709LLM\u867d\u7136\u5177\u5907\u533b\u5b66\u77e5\u8bc6\u5e76\u80fd\u751f\u6210\u4e8b\u5b9e\u51c6\u786e\u7684\u56de\u7b54\uff0c\u4f46\u5f80\u5f80\u5ffd\u89c6\u60a3\u8005\u4e2a\u4f53\u5316\u80cc\u666f\uff0c\u4ea7\u751f\u4e34\u5e8a\u6b63\u786e\u4f46\u4e0e\u60a3\u8005\u9700\u6c42\u4e0d\u5339\u914d\u7684\u56de\u7b54\u3002", "method": "DeCode\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u80fd\u591f\u9002\u914d\u73b0\u6709LLM\uff0c\u4f7f\u5176\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u751f\u6210\u66f4\u5177\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u7684\u7b54\u6848\u3002", "result": "\u5728OpenAI HealthBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDeCode\u5c06\u5148\u524d\u6700\u4f73\u8868\u73b0\u4ece28.4%\u63d0\u5347\u523049.8%\uff0c\u76f8\u5bf9\u6539\u8fdb\u8fbe\u523075%\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u4e34\u5e8a\u95ee\u7b54\u80fd\u529b\u3002", "conclusion": "DeCode\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u95ee\u7b54\u80fd\u529b\uff0c\u80fd\u591f\u751f\u6210\u66f4\u7b26\u5408\u60a3\u8005\u4e2a\u4f53\u5316\u9700\u6c42\u7684\u56de\u7b54\u3002"}}
{"id": "2601.02128", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2601.02128", "abs": "https://arxiv.org/abs/2601.02128", "authors": ["Steffen Freisinger", "Philipp Seeberger", "Thomas Ranzenberger", "Tobias Bocklet", "Korbinian Riedhammer"], "title": "Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation", "comment": "Published in Proceedings of Interspeech 2025. Please cite the proceedings version (DOI: 10.21437/Interspeech.2025-2792)", "summary": "Segmenting speech transcripts into thematic sections benefits both downstream processing and users who depend on written text for accessibility. We introduce a novel approach to hierarchical topic segmentation in transcripts, generating multi-level tables of contents that capture both topic and subtopic boundaries. We compare zero-shot prompting and LoRA fine-tuning on large language models, while also exploring the integration of high-level speech pause features. Evaluations on English meeting recordings and multilingual lecture transcripts (Portuguese, German) show significant improvements over established topic segmentation baselines. Additionally, we adapt a common evaluation measure for multi-level segmentation, taking into account all hierarchical levels within one metric.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5c42\u6b21\u5316\u4e3b\u9898\u5206\u5272\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bed\u97f3\u8f6c\u5f55\u6587\u672c\uff0c\u751f\u6210\u591a\u7ea7\u76ee\u5f55\uff0c\u7ed3\u5408\u96f6\u6837\u672c\u63d0\u793a\u3001LoRA\u5fae\u8c03\u548c\u8bed\u97f3\u505c\u987f\u7279\u5f81\uff0c\u5728\u4f1a\u8bae\u548c\u8bb2\u5ea7\u8f6c\u5f55\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u8bed\u97f3\u8f6c\u5f55\u6587\u672c\u7684\u4e3b\u9898\u5206\u5272\u5bf9\u4e0b\u6e38\u5904\u7406\u548c\u4f9d\u8d56\u6587\u672c\u7684\u53ef\u8bbf\u95ee\u6027\u7528\u6237\u90fd\u6709\u76ca\u5904\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u8fdb\u884c\u5355\u5c42\u5206\u5272\uff0c\u7f3a\u4e4f\u6355\u6349\u4e3b\u9898\u548c\u5b50\u4e3b\u9898\u5c42\u6b21\u7ed3\u6784\u7684\u80fd\u529b\u3002", "method": "1. \u63d0\u51fa\u5c42\u6b21\u5316\u4e3b\u9898\u5206\u5272\u65b9\u6cd5\uff0c\u751f\u6210\u591a\u7ea7\u76ee\u5f55\uff1b2. \u6bd4\u8f83\u96f6\u6837\u672c\u63d0\u793a\u548cLoRA\u5fae\u8c03\u4e24\u79cdLLM\u4f7f\u7528\u65b9\u5f0f\uff1b3. \u96c6\u6210\u9ad8\u5c42\u8bed\u97f3\u505c\u987f\u7279\u5f81\uff1b4. \u63d0\u51fa\u9002\u5e94\u591a\u7ea7\u5206\u5272\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5728\u82f1\u8bed\u4f1a\u8bae\u5f55\u97f3\u548c\u591a\u8bed\u8a00\u8bb2\u5ea7\u8f6c\u5f55\uff08\u8461\u8404\u7259\u8bed\u3001\u5fb7\u8bed\uff09\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4\u73b0\u6709\u4e3b\u9898\u5206\u5272\u57fa\u7ebf\u6709\u663e\u8457\u6539\u8fdb\u3002\u63d0\u51fa\u7684\u591a\u7ea7\u8bc4\u4f30\u6307\u6807\u80fd\u7efc\u5408\u8003\u8651\u6240\u6709\u5c42\u6b21\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8fdb\u884c\u5c42\u6b21\u5316\u4e3b\u9898\u5206\u5272\uff0c\u751f\u6210\u591a\u7ea7\u76ee\u5f55\uff0c\u7ed3\u5408LLM\u6280\u672f\u548c\u8bed\u97f3\u7279\u5f81\u80fd\u63d0\u5347\u5206\u5272\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8bed\u8a00\u548c\u573a\u666f\u3002"}}
{"id": "2601.02144", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02144", "abs": "https://arxiv.org/abs/2601.02144", "authors": ["Boxuan Lyu", "Soichiro Murakami", "Hidetaka Kamigaito", "Peinan Zhang"], "title": "Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts", "comment": null, "summary": "Mixture-of-Experts (MoE) architectures scale large language models efficiently by employing a parametric \"router\" to dispatch tokens to a sparse subset of experts. Typically, this router is trained once and then frozen, rendering routing decisions brittle under distribution shifts. We address this limitation by introducing kNN-MoE, a retrieval-augmented routing framework that reuses optimal expert assignments from a memory of similar past cases. This memory is constructed offline by directly optimizing token-wise routing logits to maximize the likelihood on a reference set. Crucially, we use the aggregate similarity of retrieved neighbors as a confidence-driven mixing coefficient, thus allowing the method to fall back to the frozen router when no relevant cases are found. Experiments show kNN-MoE outperforms zero-shot baselines and rivals computationally expensive supervised fine-tuning.", "AI": {"tldr": "kNN-MoE\uff1a\u57fa\u4e8e\u68c0\u7d22\u7684\u6df7\u5408\u4e13\u5bb6\u8def\u7531\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u7528\u5386\u53f2\u6700\u4f18\u4e13\u5bb6\u5206\u914d\u6765\u63d0\u5347\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u8def\u7531\u9c81\u68d2\u6027", "motivation": "\u4f20\u7edfMoE\u67b6\u6784\u4e2d\u7684\u8def\u7531\u5668\u8bad\u7ec3\u540e\u56fa\u5b9a\uff0c\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u8def\u7531\u51b3\u7b56\u53d8\u5f97\u8106\u5f31\u3002\u9700\u8981\u4e00\u79cd\u80fd\u9002\u5e94\u65b0\u5206\u5e03\u3001\u4e0d\u4f9d\u8d56\u6602\u8d35\u5fae\u8c03\u7684\u8def\u7531\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fakNN-MoE\u6846\u67b6\uff1a1\uff09\u79bb\u7ebf\u6784\u5efa\u8bb0\u5fc6\u5e93\uff0c\u901a\u8fc7\u4f18\u5316\u8def\u7531logits\u6700\u5927\u5316\u53c2\u8003\u96c6\u4f3c\u7136\uff1b2\uff09\u5728\u7ebf\u68c0\u7d22\u76f8\u4f3c\u5386\u53f2\u6848\u4f8b\u91cd\u7528\u6700\u4f18\u4e13\u5bb6\u5206\u914d\uff1b3\uff09\u4f7f\u7528\u68c0\u7d22\u90bb\u5c45\u7684\u805a\u5408\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u6df7\u5408\u7cfb\u6570\uff0c\u5728\u65e0\u76f8\u5173\u6848\u4f8b\u65f6\u56de\u9000\u5230\u51bb\u7ed3\u8def\u7531\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660ekNN-MoE\u4f18\u4e8e\u96f6\u6837\u672c\u57fa\u7ebf\uff0c\u6027\u80fd\u53ef\u4e0e\u8ba1\u7b97\u6602\u8d35\u7684\u76d1\u7763\u5fae\u8c03\u76f8\u5ab2\u7f8e\u3002", "conclusion": "kNN-MoE\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u8def\u7531\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfMoE\u8def\u7531\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u8106\u5f31\u6027\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2601.02158", "categories": ["cs.CL", "cs.AI", "cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2601.02158", "abs": "https://arxiv.org/abs/2601.02158", "authors": ["Almaz Ermilov"], "title": "FormationEval, an open multiple-choice benchmark for petroleum geoscience", "comment": "24 pages, 8 figures, 10 tables; benchmark and code at https://github.com/AlmazErmilov/FormationEval-an-Open-Benchmark-for-Oil-Gas-Geoscience-MCQ-Evaluation", "summary": "This paper presents FormationEval, an open multiple-choice question benchmark for evaluating language models on petroleum geoscience and subsurface disciplines. The dataset contains 505 questions across seven domains including petrophysics, petroleum geology and reservoir engineering, derived from three authoritative sources using a reasoning model with detailed instructions and a concept-based approach that avoids verbatim copying of copyrighted text. Each question includes source metadata to support traceability and audit. The evaluation covers 72 models from major providers including OpenAI, Anthropic, Google, Meta and open-weight alternatives. The top performers achieve over 97\\% accuracy, with Gemini 3 Pro Preview reaching 99.8\\%, while tier and domain gaps persist. Among open-weight models, GLM-4.7 leads at 98.6\\%, with several DeepSeek, Llama, Qwen and Mistral models also exceeding 93\\%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90\\% accuracy. Petrophysics emerges as the most challenging domain across all models, while smaller models show wider performance variance. Residual length bias in the dataset (correct answers tend to be longer) is documented along with bias mitigation strategies applied during construction. The benchmark, evaluation code and results are publicly available.", "AI": {"tldr": "FormationEval\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5728\u77f3\u6cb9\u5730\u7403\u79d1\u5b66\u548c\u5730\u4e0b\u5b66\u79d1\u80fd\u529b\u7684\u5f00\u653e\u591a\u9879\u9009\u62e9\u9898\u57fa\u51c6\uff0c\u5305\u542b505\u4e2a\u95ee\u9898\uff0c\u8986\u76d67\u4e2a\u9886\u57df\uff0c\u8bc4\u4f30\u4e8672\u4e2a\u6a21\u578b\uff0c\u53d1\u73b0\u5f00\u653e\u6743\u91cd\u6a21\u578b\u4e0e\u95ed\u6e90\u6a21\u578b\u6027\u80fd\u5dee\u8ddd\u5c0f\u4e8e\u9884\u671f\u3002", "motivation": "\u9700\u8981\u4e3a\u77f3\u6cb9\u5730\u7403\u79d1\u5b66\u548c\u5730\u4e0b\u5b66\u79d1\u9886\u57df\u5efa\u7acb\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\uff0c\u4ee5\u8861\u91cf\u4e0d\u540c\u6a21\u578b\u5728\u8be5\u4e13\u4e1a\u9886\u57df\u7684\u77e5\u8bc6\u638c\u63e1\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4ece\u4e09\u4e2a\u6743\u5a01\u6765\u6e90\u4f7f\u7528\u63a8\u7406\u6a21\u578b\u548c\u57fa\u4e8e\u6982\u5ff5\u7684\u65b9\u6cd5\u6784\u5efa\u6570\u636e\u96c6\uff0c\u907f\u514d\u76f4\u63a5\u590d\u5236\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u6587\u672c\u3002\u5305\u542b505\u4e2a\u95ee\u9898\uff0c\u8986\u76d67\u4e2a\u9886\u57df\uff0c\u6bcf\u4e2a\u95ee\u9898\u90fd\u6709\u6765\u6e90\u5143\u6570\u636e\u652f\u6301\u53ef\u8ffd\u6eaf\u6027\u3002\u8bc4\u4f30\u4e8672\u4e2a\u6765\u81ea\u4e3b\u8981\u63d0\u4f9b\u5546\u548c\u5f00\u653e\u6743\u91cd\u7684\u6a21\u578b\u3002", "result": "\u6700\u4f73\u6a21\u578bGemini 3 Pro Preview\u8fbe\u523099.8%\u51c6\u786e\u7387\uff0c\u5f00\u653e\u6743\u91cd\u6a21\u578b\u4e2dGLM-4.7\u4ee598.6%\u9886\u5148\u3002\u5f00\u653e\u6743\u91cd\u4e0e\u95ed\u6e90\u6a21\u578b\u6027\u80fd\u5dee\u8ddd\u5c0f\u4e8e\u9884\u671f\uff0c\u591a\u4e2a\u4f4e\u6210\u672c\u5f00\u653e\u6743\u91cd\u6a21\u578b\u8d85\u8fc790%\u51c6\u786e\u7387\u3002\u5ca9\u77f3\u7269\u7406\u5b66\u662f\u6240\u6709\u6a21\u578b\u4e2d\u6700\u5177\u6311\u6218\u6027\u7684\u9886\u57df\uff0c\u8f83\u5c0f\u6a21\u578b\u8868\u73b0\u51fa\u66f4\u5927\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "FormationEval\u57fa\u51c6\u4e3a\u77f3\u6cb9\u5730\u7403\u79d1\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6a21\u578b\u8bc4\u4f30\u5de5\u5177\uff0c\u53d1\u73b0\u5f00\u653e\u6743\u91cd\u6a21\u578b\u5728\u8be5\u4e13\u4e1a\u9886\u57df\u8868\u73b0\u826f\u597d\uff0c\u6027\u80fd\u5dee\u8ddd\u5c0f\u4e8e\u9884\u671f\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7ecf\u6d4e\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2601.02179", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02179", "abs": "https://arxiv.org/abs/2601.02179", "authors": ["Caiqi Zhang", "Ruihan Yang", "Xiaochen Zhu", "Chengzu Li", "Tiancheng Hu", "Yijiang River Dong", "Deqing Yang", "Nigel Collier"], "title": "Confidence Estimation for LLMs in Multi-turn Interactions", "comment": null, "summary": "While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new \"Hinter-Guesser\" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u8f6e\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u6307\u6807\u3002", "motivation": "\u5f53\u524d\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u8f6e\u8bbe\u7f6e\uff0c\u800c\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u4e0a\u4e0b\u6587\u7d2f\u79ef\u548c\u6b67\u4e49\u9010\u6b65\u89e3\u51b3\u7684\u60c5\u51b5\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u5bf9\u4e8e\u81ea\u4e3b\u4ee3\u7406\u548c\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\u7b49\u4e0b\u6e38\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5efa\u7acb\u4e86\u57fa\u4e8e\u4e24\u4e2a\u5173\u952e\u9700\u6c42\u7684\u6b63\u5f0f\u8bc4\u4f30\u6846\u67b6\uff1a\u6bcf\u8f6e\u6821\u51c6\u548c\u968f\u7740\u4fe1\u606f\u589e\u52a0\u7f6e\u4fe1\u5ea6\u7684\u5355\u8c03\u6027\u3002\u5f15\u5165\u4e86\u65b0\u7684\u6307\u6807\uff08\u5305\u62ec\u957f\u5ea6\u5f52\u4e00\u5316\u7684\u9884\u671f\u6821\u51c6\u8bef\u5deeInfoECE\uff09\u548c\"Hinter-Guesser\"\u8303\u5f0f\u6765\u751f\u6210\u53d7\u63a7\u8bc4\u4f30\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5e7f\u6cdb\u4f7f\u7528\u7684\u7f6e\u4fe1\u5ea6\u6280\u672f\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u96be\u4ee5\u5b9e\u73b0\u6821\u51c6\u548c\u5355\u8c03\u6027\u3002\u63d0\u51fa\u7684P(Sufficient)\u903b\u8f91\u63a2\u9488\u76f8\u6bd4\u5176\u4ed6\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u8be5\u4efb\u52a1\u8fdc\u672a\u89e3\u51b3\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u548c\u53ef\u4fe1\u7684\u5bf9\u8bdd\u4ee3\u7406\u63d0\u4f9b\u4e86\u57fa\u7840\u65b9\u6cd5\u8bba\uff0c\u63ed\u793a\u4e86\u591a\u8f6e\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u5efa\u7acb\u4e86\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2601.02186", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02186", "abs": "https://arxiv.org/abs/2601.02186", "authors": ["Rui Yang", "Huitao Li", "Weihao Xuan", "Heli Qi", "Xin Li", "Kunyu Yu", "Yingjian Chen", "Rongrong Wang", "Jacques Behmoaras", "Tianxi Cai", "Bibhas Chakraborty", "Qingyu Chen", "Lionel Tim-Ee Cheng", "Marie-Louise Damwanza", "Chido Dzinotyiwei", "Aosong Feng", "Chuan Hong", "Yusuke Iwasawa", "Yuhe Ke", "Linah Kitala", "Taehoon Ko", "Jisan Lee", "Irene Li", "Jonathan Chong Kai Liew", "Hongfang Liu", "Lian Leng Low", "Edison Marrese-Taylor", "Yutaka Matsuo", "Isheanesu Misi", "Yilin Ning", "Jasmine Chiat Ling Ong", "Marcus Eng Hock Ong", "Enrico Petretto", "Hossein Rouhizadeh", "Abiram Sandralegar", "Oren Schreier", "Iain Bee Huat Tan", "Patrick Tan", "Daniel Shu Wei Ting", "Junjue Wang", "Chunhua Weng", "Matthew Yu Heng Wong", "Fang Wu", "Yunze Xiao", "Xuhai Xu", "Qingcheng Zeng", "Zhuo Zheng", "Yifan Peng", "Douglas Teodoro", "Nan Liu"], "title": "Toward Global Large Language Models in Medicine", "comment": "182 pages, 65 figures", "summary": "Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.", "AI": {"tldr": "\u6784\u5efa\u4e86GlobMed\u591a\u8bed\u8a00\u533b\u7597\u6570\u636e\u96c6\u3001\u8bc4\u4f30\u57fa\u51c6\u548cLLM\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3\u533b\u7597LLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u4fc3\u8fdb\u5168\u7403\u533b\u7597AI\u7684\u516c\u5e73\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u9ad8\u8d44\u6e90\u8bed\u8a00\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5176\u5728\u5168\u7403\u533b\u7597\u573a\u666f\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u9700\u8981\u89e3\u51b3\u533b\u7597\u8d44\u6e90\u5206\u5e03\u4e0d\u5747\u7684\u95ee\u9898\u3002", "method": "1) \u6784\u5efaGlobMed\u591a\u8bed\u8a00\u533b\u7597\u6570\u636e\u96c6\uff0c\u5305\u542b50\u4e07+\u6761\u76ee\uff0c\u6db5\u76d612\u79cd\u8bed\u8a00\uff08\u542b4\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\uff09\uff1b2) \u5efa\u7acbGlobMed-Bench\u8bc4\u4f30\u57fa\u51c6\uff0c\u7cfb\u7edf\u8bc4\u4f3056\u4e2a\u5148\u8fdbLLM\u5728\u591a\u8bed\u8a00\u533b\u7597\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff1b3) \u57fa\u4e8eGlobMed\u8bad\u7ec3GlobMed-LLMs\u6a21\u578b\u5957\u4ef6\uff081.7B-8B\u53c2\u6570\uff09\u3002", "result": "GlobMed-LLMs\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5e73\u5747\u6027\u80fd\u63d0\u5347\u8d85\u8fc740%\uff0c\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u6027\u80fd\u63d0\u5347\u8d85\u8fc73\u500d\uff1b\u8bc4\u4f30\u53d1\u73b0\u4e0d\u540c\u8bed\u8a00\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff0c\u7279\u522b\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u8868\u73b0\u8f83\u5dee\u3002", "conclusion": "GlobMed\u7cfb\u5217\u8d44\u6e90\u4e3a\u4fc3\u8fdbLLM\u5728\u5168\u7403\u7684\u516c\u5e73\u53d1\u5c55\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u4f7f\u66f4\u5e7f\u6cdb\u7684\u8bed\u8a00\u793e\u533a\u80fd\u591f\u53d7\u76ca\u4e8e\u6280\u672f\u8fdb\u6b65\uff0c\u63a8\u52a8\u5168\u7403\u533b\u7597AI\u7684\u5747\u8861\u53d1\u5c55\u3002"}}
{"id": "2601.02209", "categories": ["cs.CL", "cs.CY", "cs.SD"], "pdf": "https://arxiv.org/pdf/2601.02209", "abs": "https://arxiv.org/abs/2601.02209", "authors": ["Omer Nacar", "Serry Sibaee", "Adel Ammar", "Yasser Alhabashi", "Nadia Samer Sibai", "Yara Farouk Ahmed", "Ahmed Saud Alqusaiyer", "Sulieman Mahmoud AlMahmoud", "Abdulrhman Mamdoh Mukhaniq", "Lubaba Raed", "Sulaiman Mohammed Alatwah", "Waad Nasser Alqahtani", "Yousif Abdulmajeed Alnasser", "Mohamed Aziz Khadraoui", "Wadii Boulila"], "title": "ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging", "comment": null, "summary": "The Arabic language is characterized by a rich tapestry of regional dialects that differ substantially in phonetics and lexicon, reflecting the geographic and cultural diversity of its speakers. Despite the availability of many multi-dialect datasets, mapping speech to fine-grained dialect sources, such as cities, remains underexplored. We present ARCADE (Arabic Radio Corpus for Audio Dialect Evaluation), the first Arabic speech dataset designed explicitly with city-level dialect granularity. The corpus comprises Arabic radio speech collected from streaming services across the Arab world. Our data pipeline captures 30-second segments from verified radio streams, encompassing both Modern Standard Arabic (MSA) and diverse dialectal speech. To ensure reliability, each clip was annotated by one to three native Arabic reviewers who assigned rich metadata, including emotion, speech type, dialect category, and a validity flag for dialect identification tasks. The resulting corpus comprises 6,907 annotations and 3,790 unique audio segments spanning 58 cities across 19 countries. These fine-grained annotations enable robust multi-task learning, serving as a benchmark for city-level dialect tagging. We detail the data collection methodology, assess audio quality, and provide a comprehensive analysis of label distributions. The dataset is available on: https://huggingface.co/datasets/riotu-lab/ARCADE-full", "AI": {"tldr": "ARCADE\u662f\u9996\u4e2a\u57ce\u5e02\u7ea7\u65b9\u8a00\u7c92\u5ea6\u7684\u963f\u62c9\u4f2f\u8bed\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea19\u4e2a\u56fd\u5bb658\u4e2a\u57ce\u5e02\u76843790\u4e2a\u97f3\u9891\u7247\u6bb5\uff0c\u7528\u4e8e\u65b9\u8a00\u8bc6\u522b\u4efb\u52a1\u3002", "motivation": "\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u5728\u8bed\u97f3\u548c\u8bcd\u6c47\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4f46\u73b0\u6709\u591a\u65b9\u8a00\u6570\u636e\u96c6\u7f3a\u4e4f\u57ce\u5e02\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u65b9\u8a00\u8bc6\u522b\u7684\u6df1\u5165\u7814\u7a76\u3002", "method": "\u4ece\u963f\u62c9\u4f2f\u4e16\u754c\u6d41\u5a92\u4f53\u670d\u52a1\u6536\u96c6\u5e7f\u64ad\u8bed\u97f3\uff0c\u622a\u53d630\u79d2\u7247\u6bb5\uff0c\u75311-3\u540d\u6bcd\u8bed\u8005\u6807\u6ce8\u60c5\u611f\u3001\u8bed\u97f3\u7c7b\u578b\u3001\u65b9\u8a00\u7c7b\u522b\u548c\u6709\u6548\u6027\u6807\u5fd7\u7b49\u5143\u6570\u636e\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b6907\u4e2a\u6807\u6ce8\u548c3790\u4e2a\u72ec\u7279\u97f3\u9891\u7247\u6bb5\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d658\u4e2a\u57ce\u5e02\u548c19\u4e2a\u56fd\u5bb6\uff0c\u652f\u6301\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u53ef\u4f5c\u4e3a\u57ce\u5e02\u7ea7\u65b9\u8a00\u6807\u6ce8\u7684\u57fa\u51c6\u3002", "conclusion": "ARCADE\u586b\u8865\u4e86\u963f\u62c9\u4f2f\u8bed\u57ce\u5e02\u7ea7\u65b9\u8a00\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u65b9\u8a00\u8bc6\u522b\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u6570\u636e\u96c6\u5df2\u5728Hugging Face\u5e73\u53f0\u516c\u5f00\u3002"}}
{"id": "2601.02224", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02224", "abs": "https://arxiv.org/abs/2601.02224", "authors": ["Fabian Lukassen", "Jan Herrmann", "Christoph Weisser", "Benjamin Saefken", "Thomas Kneib"], "title": "From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality", "comment": null, "summary": "Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u53d1\u73b0\uff1a\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u53ef\u89e3\u91caAI\u4e2d\uff0cLLM\u9009\u62e9\u5bf9\u89e3\u91ca\u8d28\u91cf\u5f71\u54cd\u6700\u5927\uff0cXAI\u65b9\u6cd5\u63d0\u5347\u6709\u9650\uff0c\u4e14\u5b58\u5728\u53ef\u89e3\u91ca\u6027\u6096\u8bba\u2014\u2014\u9884\u6d4b\u7cbe\u5ea6\u66f4\u9ad8\u7684\u6a21\u578b\u53cd\u800c\u89e3\u91ca\u8d28\u91cf\u66f4\u4f4e\u3002", "motivation": "\u73b0\u6709XAI\u65b9\u6cd5\uff08\u5982SHAP\u3001LIME\uff09\u751f\u6210\u7684\u6570\u503c\u7279\u5f81\u5f52\u56e0\u5bf9\u975e\u4e13\u5bb6\u7528\u6237\u96be\u4ee5\u7406\u89e3\uff0c\u867d\u7136LLM\u53ef\u4ee5\u5c06\u8fd9\u4e9b\u8f93\u51fa\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u4f46\u5f71\u54cd\u89e3\u91ca\u8d28\u91cf\u7684\u56e0\u7d20\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u56e0\u5b50\u8bbe\u8ba1\u7814\u7a76\uff1a4\u79cd\u9884\u6d4b\u6a21\u578b\uff08XGBoost\u3001\u968f\u673a\u68ee\u6797\u3001MLP\u3001SARIMAX\uff09\u30013\u79cdXAI\u6761\u4ef6\uff08SHAP\u3001LIME\u3001\u65e0XAI\u57fa\u7ebf\uff09\u30013\u79cdLLM\uff08GPT-4o\u3001Llama-3-8B\u3001DeepSeek-R1\uff09\u30018\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u4f7f\u7528G-Eval\uff08LLM\u4f5c\u4e3a\u8bc4\u5224\u8005\uff09\u65b9\u6cd5\u8bc4\u4f30660\u4e2a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u89e3\u91ca\u3002", "result": "1. XAI\u4ec5\u5bf9\u4e13\u5bb6\u7528\u6237\u6709\u8f7b\u5fae\u6539\u8fdb\uff1b2. LLM\u9009\u62e9\u662f\u4e3b\u5bfc\u56e0\u7d20\uff0cDeepSeek-R1\u8868\u73b0\u6700\u4f73\uff1b3. \u5b58\u5728\u53ef\u89e3\u91ca\u6027\u6096\u8bba\uff1aSARIMAX\u9884\u6d4b\u7cbe\u5ea6\u66f4\u9ad8\u4f46\u89e3\u91ca\u8d28\u91cf\u66f4\u4f4e\uff1b4. \u96f6\u6837\u672c\u63d0\u793a\u4e0e\u81ea\u4e00\u81f4\u6027\u6548\u679c\u76f8\u5f53\u4f46\u6210\u672c\u4f4e7\u500d\uff1b5. \u601d\u7ef4\u94fe\u53cd\u800c\u964d\u4f4e\u89e3\u91ca\u8d28\u91cf\u3002", "conclusion": "\u5728\u6784\u5efa\u53ef\u89e3\u91caAI\u7cfb\u7edf\u65f6\uff0c\u5e94\u4f18\u5148\u8003\u8651LLM\u9009\u62e9\u800c\u975eXAI\u65b9\u6cd5\uff0c\u96f6\u6837\u672c\u63d0\u793a\u662f\u6027\u4ef7\u6bd4\u6700\u9ad8\u7684\u7b56\u7565\uff0c\u540c\u65f6\u9700\u8981\u6ce8\u610f\u6a21\u578b\u9884\u6d4b\u7cbe\u5ea6\u4e0e\u89e3\u91ca\u8d28\u91cf\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2601.02236", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02236", "abs": "https://arxiv.org/abs/2601.02236", "authors": ["Yihao Liang", "Ze Wang", "Hao Chen", "Ximeng Sun", "Jialian Wu", "Xiaodong Yu", "Jiang Liu", "Emad Barsoum", "Zicheng Liu", "Niraj K. Jha"], "title": "CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models", "comment": "33 pages, 7 figures", "summary": "Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive \"long-jump\" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM", "AI": {"tldr": "CD4LM\u6846\u67b6\u901a\u8fc7\u79bb\u6563\u7a7a\u95f4\u4e00\u81f4\u6027\u84b8\u998f\u548c\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u89e3\u7801\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e0e\u63a8\u7406\u4e4b\u95f4\u7684\u9519\u4f4d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u5e76\u884c\u89e3\u7801\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u83b7\u5f973.62-5.18\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u53d7\u9650\u4e8e\u5e8f\u5217\u4f9d\u8d56\uff0c\u6269\u6563\u8bed\u8a00\u6a21\u578b\u867d\u652f\u6301\u5e76\u884c\u751f\u6210\u4f46\u5b58\u5728\u8bad\u7ec3\u4e0e\u63a8\u7406\u7684\u6839\u672c\u6027\u9519\u4f4d\uff1a\u8bad\u7ec3\u4f18\u5316\u56fa\u5b9a\u8c03\u5ea6\u4e0b\u7684\u5c40\u90e8\u8f6c\u79fb\uff0c\u800c\u9ad8\u6548\u63a8\u7406\u9700\u8981\u4ece\u672a\u89c1\u72b6\u6001\u8fdb\u884c\u81ea\u9002\u5e94\"\u957f\u8df3\"\u4f18\u5316\u3002", "method": "\u63d0\u51faCD4LM\u6846\u67b6\uff0c\u5305\u542b\u79bb\u6563\u7a7a\u95f4\u4e00\u81f4\u6027\u84b8\u998f\uff08DSCD\uff09\u548c\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u89e3\u7801\uff08CAD\uff09\u3002DSCD\u8bad\u7ec3\u5b66\u751f\u5bf9\u8f68\u8ff9\u4e0d\u53d8\uff0c\u5c06\u591a\u6837\u566a\u58f0\u72b6\u6001\u76f4\u63a5\u6620\u5c04\u5230\u5e72\u51c0\u5206\u5e03\uff1bCAD\u57fa\u4e8etoken\u7f6e\u4fe1\u5ea6\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u6fc0\u8fdb\u8df3\u8fc7\u6b65\u9aa4\u800c\u4e0d\u635f\u5931\u8d28\u91cf\u3002", "result": "\u5728GSM8K\u4e0a\u5339\u914dLLaDA\u57fa\u7ebf\u5e76\u83b7\u5f975.18\u500d\u52a0\u901f\uff1b\u5728\u4ee3\u7801\u548c\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e3b\u5bfc\u51c6\u786e\u7387-\u6548\u7387\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u5e73\u5747\u52a0\u901f3.62\u500d\u540c\u65f6\u63d0\u9ad8\u5e73\u5747\u51c6\u786e\u7387\u3002", "conclusion": "CD4LM\u6210\u529f\u89e3\u8026\u6269\u6563\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u4e0e\u63a8\u7406\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u84b8\u998f\u548c\u81ea\u9002\u5e94\u89e3\u7801\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u5e76\u884c\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u89e3\u7801\u6548\u7387\u800c\u4e0d\u727a\u7272\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2601.02285", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2601.02285", "abs": "https://arxiv.org/abs/2601.02285", "authors": ["Tobias Schimanski", "Imene Kolli", "Jingwei Ni", "Yu Fan", "Ario Saeid Vaghefi", "Elliott Ash", "Markus Leippold"], "title": "pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs", "comment": null, "summary": "PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing).", "AI": {"tldr": "pdfQA\uff1a\u4e00\u4e2a\u5305\u542b\u771f\u5b9ePDF\u6807\u6ce8\u548c\u5408\u6210\u6570\u636e\u7684\u591a\u9886\u57df\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u5305\u542b10\u4e2a\u590d\u6742\u5ea6\u7ef4\u5ea6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7aef\u5230\u7aefQA\u7cfb\u7edf\u6027\u80fd", "motivation": "PDF\u662f\u4e92\u8054\u7f51\u4e0a\u7b2c\u4e8c\u5e38\u7528\u7684\u6587\u6863\u7c7b\u578b\uff0c\u4f46\u73b0\u6709QA\u6570\u636e\u96c6\u8981\u4e48\u57fa\u4e8e\u6587\u672c\u6e90\uff0c\u8981\u4e48\u53ea\u9488\u5bf9\u7279\u5b9a\u9886\u57df\uff0c\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9PDF\u6587\u6863\u7684\u7efc\u5408\u6027\u95ee\u7b54\u6570\u636e\u96c6", "method": "\u521b\u5efa\u4e86pdfQA\u6570\u636e\u96c6\uff0c\u5305\u542b2K\u4eba\u5de5\u6807\u6ce8\u7684\u771f\u5b9ePDF\u95ee\u7b54\u5bf9\uff08real-pdfQA\uff09\u548c2K\u5408\u6210\u95ee\u7b54\u5bf9\uff08syn-pdfQA\uff09\uff0c\u5b9a\u4e49\u4e8610\u4e2a\u590d\u6742\u5ea6\u7ef4\u5ea6\uff08\u5982\u6587\u4ef6\u7c7b\u578b\u3001\u6765\u6e90\u6a21\u6001\u3001\u6765\u6e90\u4f4d\u7f6e\u3001\u7b54\u6848\u7c7b\u578b\u7b49\uff09\uff0c\u5e76\u5e94\u7528\u8d28\u91cf\u548c\u96be\u5ea6\u8fc7\u6ee4\u5668\u7b5b\u9009\u6709\u6548\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u7b54\u5bf9", "result": "\u4f7f\u7528\u5f00\u6e90LLM\u56de\u7b54\u95ee\u9898\uff0c\u53d1\u73b0\u73b0\u6709\u6311\u6218\u4e0e\u5b9a\u4e49\u7684\u590d\u6742\u5ea6\u7ef4\u5ea6\u76f8\u5173\uff0cpdfQA\u4e3a\u7aef\u5230\u7aefQA\u7ba1\u9053\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u80fd\u591f\u6d4b\u8bd5\u591a\u6837\u5316\u6280\u80fd\u96c6\u548c\u5c40\u90e8\u4f18\u5316\uff08\u5982\u4fe1\u606f\u68c0\u7d22\u6216\u89e3\u6790\uff09", "conclusion": "pdfQA\u662f\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9PDF\u6587\u6863\u7684\u7efc\u5408\u6027\u591a\u9886\u57df\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5b9a\u4e4910\u4e2a\u590d\u6742\u5ea6\u7ef4\u5ea6\uff0c\u80fd\u591f\u7cfb\u7edf\u8bc4\u4f30QA\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3aPDF\u6587\u6863\u5904\u7406\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177"}}
{"id": "2601.02298", "categories": ["cs.CL", "eess.SP"], "pdf": "https://arxiv.org/pdf/2601.02298", "abs": "https://arxiv.org/abs/2601.02298", "authors": ["Mahmoud Elgenedy"], "title": "Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)", "comment": null, "summary": "In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u5e42\u6b21\u4e8c\uff08PoT\uff09\u91cf\u5316\u538b\u7f29\u5927\u8bed\u8a00\u6a21\u578b\u6743\u91cd\uff0c\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u63d0\u5347\u6027\u80fd\uff0c\u5728GPT-2 124M\u4e0a\u5b9e\u73b087.5%\u5185\u5b58\u8282\u7701\u548c3-10\u500d\u63a8\u7406\u52a0\u901f\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u53c2\u6570\u6570\u91cf\u5448\u6307\u6570\u589e\u957f\uff08\u4eceGPT-2\u768415\u4ebf\u5230GPT-3\u76841750\u4ebf\u751a\u81f3\u66f4\u591a\uff09\uff0c\u7ed9\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u5e26\u6765\u5de8\u5927\u6311\u6218\u3002\u8fb9\u7f18\u8bbe\u5907\u5185\u5b58\u548c\u5904\u7406\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u538b\u7f29\u6280\u672f\u6765\u4f7f\u8fd9\u4e9b\u5e94\u7528\u53d8\u5f97\u53ef\u884c\u3002", "method": "\u91c7\u7528\u5e42\u6b21\u4e8c\uff08PoT\uff09\u91cf\u5316\u538b\u7f29\u6743\u91cd\uff0c\u4ec5\u5b58\u50a8\u6307\u6570\u800c\u975e\u5b8c\u6574\u6570\u503c\uff0c\u5c06\u6602\u8d35\u7684\u4e58\u6cd5\u8fd0\u7b97\u66ff\u6362\u4e3a\u4f4e\u6210\u672c\u4f4d\u79fb\u64cd\u4f5c\u3002\u901a\u8fc7\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\uff08QAT\uff09\u6765\u514b\u670d\u4e25\u683c\u91cf\u5316\u5e26\u6765\u7684\u6027\u80fd\u635f\u5931\u3002", "result": "\u5728GPT-2 124M\u6a21\u578b\u4e0a\uff0c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u540e\u7684PoT\u91cf\u5316\u6a21\u578b\u56f0\u60d1\u5ea6\u63d0\u534766%\uff0cBERT-Score\u635f\u5931\u4ec5\u6bd4\u57fa\u7ebfGPT-2\u9ad81%\u3002\u5185\u5b58\u8282\u7701\u4f30\u8ba1\u8fbe87.5%\uff0c\u63a8\u7406\u901f\u5ea6\u9884\u8ba1\u6bd4\u5168\u7cbe\u5ea6\u6a21\u578b\u5feb3-10\u500d\u3002", "conclusion": "PoT\u91cf\u5316\u7ed3\u5408\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u662f\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u80fd\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5185\u5b58\u5360\u7528\u548c\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2601.02303", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02303", "abs": "https://arxiv.org/abs/2601.02303", "authors": ["Juan-Jos\u00e9 Guzm\u00e1n-Landa", "Juan-Manuel Torres-Moreno", "Miguel Figueroa-Saavedra", "Carlos-Emiliano Gonz\u00e1lez-Gallardo", "Graham Ranger", "Martha Lorena-Avenda\u00f1o-Garrido"], "title": "Classifying several dialectal Nawatl varieties", "comment": "9 pages, 5 figures, 4 tables", "summary": "Mexico is a country with a large number of indigenous languages, among which the most widely spoken is Nawatl, with more than two million people currently speaking it (mainly in North and Central America). Despite its rich cultural heritage, which dates back to the 15th century, Nawatl is a language with few computer resources. The problem is compounded when it comes to its dialectal varieties, with approximately 30 varieties recognised, not counting the different spellings in the written forms of the language. In this research work, we addressed the problem of classifying Nawatl varieties using Machine Learning and Neural Networks.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\u5bf9Nawatl\u8bed\u8a00\u7684\u65b9\u8a00\u53d8\u4f53\u8fdb\u884c\u5206\u7c7b\u7814\u7a76", "motivation": "Nawatl\u662f\u58a8\u897f\u54e5\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684\u571f\u8457\u8bed\u8a00\uff0c\u62e5\u6709\u8d85\u8fc7200\u4e07\u4f7f\u7528\u8005\uff0c\u4f46\u8ba1\u7b97\u673a\u8d44\u6e90\u532e\u4e4f\u3002\u8be5\u8bed\u8a00\u6709\u7ea630\u79cd\u65b9\u8a00\u53d8\u4f53\uff0c\u52a0\u4e0a\u4e0d\u540c\u7684\u4e66\u5199\u62fc\u5199\u5f62\u5f0f\uff0c\u4f7f\u5f97\u65b9\u8a00\u5206\u7c7b\u95ee\u9898\u66f4\u52a0\u590d\u6742\u3002", "method": "\u91c7\u7528\u673a\u5668\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u5bf9Nawatl\u65b9\u8a00\u8fdb\u884c\u5206\u7c7b", "result": "\u8bba\u6587\u672a\u63d0\u4f9b\u5177\u4f53\u7ed3\u679c\u6570\u636e\uff0c\u4f46\u8868\u660e\u5df2\u6210\u529f\u5e94\u7528\u673a\u5668\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\u6280\u672f\u6765\u89e3\u51b3Nawatl\u65b9\u8a00\u5206\u7c7b\u95ee\u9898", "conclusion": "\u673a\u5668\u5b66\u4e60\u548c\u795e\u7ecf\u7f51\u7edc\u6280\u672f\u53ef\u4ee5\u6709\u6548\u5e94\u7528\u4e8eNawatl\u65b9\u8a00\u5206\u7c7b\uff0c\u4e3a\u89e3\u51b3\u4f4e\u8d44\u6e90\u571f\u8457\u8bed\u8a00\u7684\u8ba1\u7b97\u673a\u5904\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2601.02320", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02320", "abs": "https://arxiv.org/abs/2601.02320", "authors": ["Nikolay Mikhaylovskiy"], "title": "Estimating Text Temperature", "comment": null, "summary": "Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given language model. We evaluate the temperature estimation capability of a wide selection of small-to-medium LLMs. We then use the best-performing Qwen3 14B to estimate temperatures of popular corpora.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u4efb\u4f55\u6587\u672c\uff08\u5305\u62ec\u4eba\u7c7b\u5199\u4f5c\uff09\u76f8\u5bf9\u4e8e\u7ed9\u5b9a\u8bed\u8a00\u6a21\u578b\u7684\u6e29\u5ea6\u53c2\u6570", "motivation": "\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u6e29\u5ea6\u53c2\u6570\u6765\u63a7\u5236\u751f\u6210\u6587\u672c\u7684\u968f\u673a\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u4f30\u8ba1\u5df2\u751f\u6210\u6587\u672c\u7684\u6e29\u5ea6\uff0c\u7279\u522b\u662f\u4eba\u7c7b\u5199\u4f5c\u6587\u672c\u7684\u6e29\u5ea6", "method": "\u4f7f\u7528\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e00\u4e2a\u7a0b\u5e8f\u6765\u4f30\u8ba1\u4efb\u4f55\u6587\u672c\u76f8\u5bf9\u4e8e\u7ed9\u5b9a\u8bed\u8a00\u6a21\u578b\u7684\u6e29\u5ea6\u53c2\u6570\uff0c\u8bc4\u4f30\u4e86\u591a\u4e2a\u4e2d\u5c0f\u578bLLM\u7684\u6e29\u5ea6\u4f30\u8ba1\u80fd\u529b", "result": "Qwen3 14B\u5728\u6e29\u5ea6\u4f30\u8ba1\u80fd\u529b\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u4f7f\u7528\u8be5\u6a21\u578b\u4f30\u8ba1\u4e86\u6d41\u884c\u8bed\u6599\u5e93\u7684\u6e29\u5ea6", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u4f30\u8ba1\u6587\u672c\u7684\u6e29\u5ea6\u53c2\u6570\uff0c\u4e3a\u5206\u6790\u6587\u672c\u751f\u6210\u7279\u6027\u548c\u4eba\u7c7b\u5199\u4f5c\u98ce\u683c\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177"}}
{"id": "2601.02337", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2601.02337", "abs": "https://arxiv.org/abs/2601.02337", "authors": ["Berk Atil", "Rebecca J. Passonneau", "Ninareh Mehrabi"], "title": "Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling", "comment": null, "summary": "Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic'' modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u57fa\u4e8e\u4eba\u7269\u89d2\u8272\u7684\u6bd2\u6027\u68c0\u6d4b\uff0c\u53d1\u73b0\u6ca1\u6709\u5355\u4e00\u63d0\u793a\u65b9\u6cd5\u5728\u6240\u6709\u6a21\u578b-\u4eba\u7269\u89d2\u8272\u7ec4\u5408\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u63d0\u51fa\u4f7f\u7528SVM\u5143\u96c6\u6210\u65b9\u6cd5\u6574\u5408\u56db\u79cd\u63d0\u793a\u53d8\u4f53\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6bd2\u6027\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u6bd2\u6027\u68c0\u6d4b\u5177\u6709\u4e3b\u89c2\u6027\uff0c\u53d7\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u89c6\u89d2\u548c\u793e\u4f1a\u5148\u9a8c\u5f71\u54cd\u3002\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u793a\u6280\u672f\u5728\u4e0d\u540c\u4eba\u7269\u89d2\u8272\u548c\u57fa\u7840\u6a21\u578b\u95f4\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4eba\u7269\u89d2\u8272\u611f\u77e5\u7684\u6bd2\u6027\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "1) \u7cfb\u7edf\u8bc4\u4f30\u4eba\u7269\u89d2\u8272\u611f\u77e5\u7684\u6bd2\u6027\u68c0\u6d4b\uff1b2) \u63d0\u51fa\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u7b56\u7565\uff1b3) \u63a2\u7d22\u96c6\u6210\u56db\u79cd\u63d0\u793a\u53d8\u4f53\uff1b4) \u63d0\u51fa\u8f7b\u91cf\u7ea7\u5143\u96c6\u6210\u65b9\u6cd5\uff1a\u57fa\u4e8e4\u4f4d\u5411\u91cf\u9884\u6d4b\u7684SVM\u96c6\u6210\u3002", "result": "\u63d0\u51fa\u7684SVM\u96c6\u6210\u65b9\u6cd5\u5728\u591a\u6837\u5316\u4eba\u7269\u89d2\u8272\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u4e2a\u4f53\u63d0\u793a\u65b9\u6cd5\u548c\u4f20\u7edf\u591a\u6570\u6295\u7968\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u6700\u5f3a\u7684\u6574\u4f53\u6027\u80fd\uff0c\u4e3a\u6bd2\u6027\u68c0\u6d4b\u63d0\u4f9b\u4e86\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4eba\u7269\u89d2\u8272\u6761\u4ef6\u63d0\u793a\u5728\u6bd2\u6027\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u4e3b\u89c2NLP\u4efb\u52a1\u4e2d\u7684\u591a\u5143\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7a33\u5065\u65b9\u6cd5\uff0cSVM\u5143\u96c6\u6210\u662f\u6709\u6548\u7684\u96c6\u6210\u7b56\u7565\u3002"}}
