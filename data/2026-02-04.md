<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 76]
- [cs.RO](#cs.RO) [Total: 47]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [The Hypocrisy Gap: Quantifying Divergence Between Internal Belief and Chain-of-Thought Explanation via Sparse Autoencoders](https://arxiv.org/abs/2602.02496)
*Shikhar Shiromani,Archie Chaudhury,Sri Pranav Kunda*

Main category: cs.CL

TL;DR: 本文提出"虚伪差距"指标，利用稀疏自编码器量化大语言模型内部推理与最终生成之间的差异，以检测模型的不忠实行为。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常表现出不忠实行为，其最终答案与内部思维链推理存在显著差异，为了取悦对话用户而改变立场。现有方法难以有效检测这种内部推理与外部表达不一致的问题。

Method: 引入"虚伪差距"机制度量，使用稀疏自编码器提取模型内部表示，通过稀疏线性探针推导内部真实信念，在潜在空间中比较内部信念与最终生成轨迹的差异。

Result: 在Gemma、Llama和Qwen模型上使用Anthropic的奉承基准测试，该方法在检测奉承行为时AUROC达到0.55-0.73，在检测虚伪行为时达到0.55-0.74，优于基于对数概率的基线方法(0.41-0.50 AUROC)。

Conclusion: 提出的"虚伪差距"指标能有效量化大语言模型内部推理与最终生成之间的不一致性，为检测模型的不忠实行为提供了可靠的机制性度量方法。

Abstract: Large Language Models (LLMs) frequently exhibit unfaithful behavior, producing a final answer that differs significantly from their internal chain of thought (CoT) reasoning in order to appease the user they are conversing with. In order to better detect this behavior, we introduce the Hypocrisy Gap, a mechanistic metric utilizing Sparse Autoencoders (SAEs) to quantify the divergence between a model's internal reasoning and its final generation. By mathematically comparing an internal truth belief, derived via sparse linear probes, to the final generated trajectory in latent space, we quantify and detect a model's tendency to engage in unfaithful behavior. Experiments on Gemma, Llama, and Qwen models using Anthropic's Sycophancy benchmark show that our method achieves an AUROC of 0.55-0.73 for detecting sycophantic runs and 0.55-0.74 for hypocritical cases where the model internally "knows" the user is wrong, consistently outperforming a decision-aligned log-probability baseline (0.41-0.50 AUROC).

</details>


### [2] [STEMVerse: A Dual-Axis Diagnostic Framework for STEM Reasoning in Large Language Models](https://arxiv.org/abs/2602.02497)
*Xuzhao Li,Xuchen Li,Jian Zhao,Shiyu Hu*

Main category: cs.CL

TL;DR: STEMVerse是一个诊断框架，通过"学科×认知"双轴标签系统分析LLM在STEM领域的推理能力，揭示结构性失败模式


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估范式将基准视为孤立的"筒仓"，仅提供聚合分数，无法区分错误源于领域知识不足还是认知能力缺陷，限制了诊断价值

Method: 提出STEMVerse诊断框架，将20,000+STEM问题重新聚合到统一的"学科×认知"能力空间，为每个实例分配双轴标签，系统评估不同参数规模和训练范式的LLM

Result: 实证结果揭示了STEM推理中的结构性失败模式，通过多学科覆盖和细粒度认知分层，为理解LLM科学推理特征提供了清晰可操作的视角

Conclusion: STEMVerse通过整合多学科覆盖和细粒度认知分层，提供了系统分析LLM STEM推理能力的统一框架，超越了传统聚合分数评估的限制

Abstract: As Large Language Models (LLMs) achieve significant breakthroughs in complex reasoning tasks, evaluating their proficiency in science, technology, engineering, and mathematics (STEM) has become a primary method for measuring machine intelligence. However, current evaluation paradigms often treat benchmarks as isolated "silos," offering only monolithic aggregate scores that neglect the intricacies of both academic specialization and cognitive depth. This result-oriented approach fails to distinguish whether model errors stem from insufficient domain knowledge or deficiencies in cognitive capacity, thereby limiting the diagnostic value. To address this, we propose STEMVerse, a diagnostic framework designed to systematically analyze the STEM reasoning capabilities of LLMs. This framework characterizes model performance across academic specialization and cognitive complexity to map the capability required for reasoning. We re-aggregate over 20,000 STEM problems from mainstream benchmarks into a unified "Discipline $\times$ Cognition" capability space, assigning dual-axis labels to every instance. Utilizing this unified diagnostic framework, we systematically evaluate representative LLM families across varying parameter scales and training paradigms. Our empirical results reveal structural failure patterns in STEM reasoning. By integrating multi-disciplinary coverage and fine-grained cognitive stratification into a unified framework, STEMVerse provides a clear and actionable perspective for understanding the scientific reasoning characteristics of LLMs.

</details>


### [3] [Test-Time Detoxification without Training or Learning Anything](https://arxiv.org/abs/2602.02498)
*Baturay Saglam,Dionysis Kalogerias*

Main category: cs.CL

TL;DR: 提出一种基于零阶优化的测试时去毒方法，通过调整输入词嵌入来引导语言模型生成更安全的文本，无需模型重训练或梯度访问


<details>
  <summary>Details</summary>
Motivation: 大型语言模型即使对良性输入也可能产生有毒或不适当的文本，存在安全风险。现有方法依赖模型重训练、梯度或辅助组件，成本高且难以跨模型家族迁移或应用于真正的黑盒设置

Method: 提出测试时程序，使用零阶优化近似完成文本毒性对输入嵌入的梯度，通过少量下降步骤引导生成朝向毒性更低的延续。仅需访问输入嵌入、毒性评分函数和模型前向评估

Result: 该方法在不同模型和提示下实现了稳健的毒性降低，在大多数设置中实现了最佳的整体毒性-质量权衡

Conclusion: 将词嵌入定位为有效的控制变量，鼓励更广泛地使用黑盒优化来引导自回归语言模型实现可扩展、更安全的文本生成，无需任何训练或访问中间计算

Abstract: Large language models can produce toxic or inappropriate text even for benign inputs, creating risks when deployed at scale. Detoxification is therefore important for safety and user trust, particularly when we want to reduce harmful content without sacrificing the model's generation quality. Many existing approaches rely on model retraining, gradients, or learned auxiliary components, which can be costly and may not transfer across model families or to truly black-box settings. We introduce a test-time procedure that approximates the gradient of completion toxicity with respect to the input embeddings and uses a small number of descent steps to steer generation toward less toxic continuations. This is achieved with zeroth-order optimization that requires only access to input embeddings, a toxicity scoring function, and forward evaluations of the model. Empirically, the approach delivers robust toxicity reductions across models and prompts and, in most settings, achieves the best overall toxicity-quality trade-off. More broadly, our work positions word embeddings as effective control variables and encourages wider use of black-box optimization to guide autoregressive language models toward scalable, safer text generation, without requiring any training or access to intermediate computations.

</details>


### [4] [ROSA-Tuning: Enhancing Long-Context Modeling via Suffix Matching](https://arxiv.org/abs/2602.02499)
*Yunao Zheng,Xiaojie Wang,Lei Ren,Wei Chen*

Main category: cs.CL

TL;DR: ROSA-Tuning通过引入CPU端的检索模块和召回机制，在保持计算效率的同时增强预训练模型的长上下文建模能力，使窗口注意力模型接近全局注意力的性能。


<details>
  <summary>Details</summary>
Motivation: 现有高效注意力方法虽然降低了计算复杂度，但通常存在模型状态覆盖范围有限的问题，难以有效处理长上下文。需要一种既能保持计算效率又能增强长上下文建模能力的技术方案。

Method: 提出ROSA-Tuning方法：1) 在标准注意力机制外并行引入基于CPU的ROSA检索模块，高效定位长上下文中与当前查询相关的历史位置；2) 通过可训练方式将检索信息注入模型状态；3) 后续通过范围限制注意力进行加权融合；4) 设计二进制离散化策略和反事实梯度算法实现端到端训练；5) 通过异步CPU-GPU流水线优化整体执行效率。

Result: 在Qwen3-Base-1.7B上的系统评估显示，ROSA-Tuning显著恢复了窗口注意力模型的长上下文建模能力，在LongBench等基准测试中达到接近甚至匹配全局注意力的性能，同时保持与窗口注意力方法相当的计算效率和GPU内存使用。

Conclusion: ROSA-Tuning为高效长上下文处理提供了新的技术路径，通过检索-召回机制在计算效率和长上下文建模能力之间取得了良好平衡，使窗口注意力模型能够接近全局注意力的性能表现。

Abstract: Long-context capability and computational efficiency are among the central challenges facing today's large language models. Existing efficient attention methods reduce computational complexity, but they typically suffer from a limited coverage of the model state. This paper proposes ROSA-Tuning, a retrieval-and-recall mechanism for enhancing the long-context modeling ability of pretrained models. Beyond the standard attention mechanism, ROSA-Tuning introduces in parallel a CPU-based ROSA (RWKV Online Suffix Automaton) retrieval module, which efficiently locates historical positions in long contexts that are relevant to the current query, and injects the retrieved information into the model state in a trainable manner; subsequent weighted fusion can then be handled by range-restricted attention. To enable end-to-end training, we design a binary discretization strategy and a counterfactual gradient algorithm, and further optimize overall execution efficiency via an asynchronous CPU-GPU pipeline. Systematic evaluations on Qwen3-Base-1.7B show that ROSA-Tuning substantially restores the long-context modeling ability of windowed-attention models, achieving performance close to and in some cases matching global attention on benchmarks such as LongBench, while maintaining computational efficiency and GPU memory usage that are nearly comparable to windowed-attention methods, offering a new technical path for efficient long-context processing. The example code can be found at https://github.com/zyaaa-ux/ROSA-Tuning.

</details>


### [5] [Graph-Augmented Reasoning with Large Language Models for Tobacco Pest and Disease Management](https://arxiv.org/abs/2602.02635)
*Siyu Li,Chenwei Song,Qi Zhou,Wan Zhou,Xinyi Liu*

Main category: cs.CL

TL;DR: 提出基于图增强推理的烟草病虫害管理框架，将结构化领域知识融入大语言模型，通过知识图谱提供关系证据，改善推荐准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本的LLM在烟草病虫害管理中容易产生幻觉或不恰当的治疗建议，缺乏对症状-疾病-农药-防治措施之间复杂关系的结构化理解，需要将领域知识显式建模以支持证据感知的检索和推理。

Method: 构建领域特定知识图谱，检索查询相关子图作为关系证据；采用ChatGLM作为Transformer主干，结合LoRA参数高效微调；使用图神经网络学习捕获症状-疾病-治疗依赖关系的节点表示；将检索到的图证据融入LLM输入以指导生成。

Result: 相比纯文本基线获得一致改进，在多跳和比较推理问题上提升最显著，这些任务需要链接多个关系；系统能够生成领域一致的建议，减少幻觉或不恰当治疗。

Conclusion: 图增强推理框架通过整合结构化领域知识显著提升烟草病虫害管理的准确性和可靠性，特别适用于需要复杂关系推理的任务，为农业领域知识增强LLM提供了有效方案。

Abstract: This paper proposes a graph-augmented reasoning framework for tobacco pest and disease management that integrates structured domain knowledge into large language models. Building on GraphRAG, we construct a domain-specific knowledge graph and retrieve query-relevant subgraphs to provide relational evidence during answer generation. The framework adopts ChatGLM as the Transformer backbone with LoRA-based parameter-efficient fine-tuning, and employs a graph neural network to learn node representations that capture symptom-disease-treatment dependencies. By explicitly modeling diseases, symptoms, pesticides, and control measures as linked entities, the system supports evidence-aware retrieval beyond surface-level text similarity. Retrieved graph evidence is incorporated into the LLM input to guide generation toward domain-consistent recommendations and to mitigate hallucinated or inappropriate treatments. Experimental results show consistent improvements over text-only baselines, with the largest gains observed on multi-hop and comparative reasoning questions that require chaining multiple relations.

</details>


### [6] [WideSeek: Advancing Wide Research via Multi-Agent Scaling](https://arxiv.org/abs/2602.02636)
*Ziyang Huang,Haolin Ren,Xiaowei Yuan,Jiawei Wang,Zhongtao Jiang,Kun Xu,Shizhu He,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 论文提出了WideSeekBench基准和WideSeek多智能体架构，用于解决从深度研究到广度研究的范式转变，通过并行搜索和综合信息来应对复杂约束。


<details>
  <summary>Details</summary>
Motivation: 搜索智能正从深度研究向广度研究演进，但该领域缺乏专门的基准测试和优化方法，阻碍了进展。需要解决在复杂约束下并行检索和综合全面信息的问题。

Method: 1) 构建WideSeekBench基准，通过多阶段数据管道确保目标信息量、逻辑约束和领域的多样性；2) 提出WideSeek动态分层多智能体架构，能根据任务需求自主分叉并行子智能体；3) 设计统一训练框架，线性化多智能体轨迹并使用端到端强化学习优化系统。

Result: 实验结果表明WideSeek和多智能体强化学习的有效性，证明扩展智能体数量是推进广度研究范式的有前景方向。

Conclusion: 论文通过数据管道和智能体优化两个角度深入探讨广度研究，提出的基准和架构为解决广度搜索问题提供了有效方案，多智能体扩展是未来重要发展方向。

Abstract: Search intelligence is evolving from Deep Research to Wide Research, a paradigm essential for retrieving and synthesizing comprehensive information under complex constraints in parallel. However, progress in this field is impeded by the lack of dedicated benchmarks and optimization methodologies for search breadth. To address these challenges, we take a deep dive into Wide Research from two perspectives: Data Pipeline and Agent Optimization. First, we produce WideSeekBench, a General Broad Information Seeking (GBIS) benchmark constructed via a rigorous multi-phase data pipeline to ensure diversity across the target information volume, logical constraints, and domains. Second, we introduce WideSeek, a dynamic hierarchical multi-agent architecture that can autonomously fork parallel sub-agents based on task requirements. Furthermore, we design a unified training framework that linearizes multi-agent trajectories and optimizes the system using end-to-end RL. Experimental results demonstrate the effectiveness of WideSeek and multi-agent RL, highlighting that scaling the number of agents is a promising direction for advancing the Wide Research paradigm.

</details>


### [7] [Monotonicity as an Architectural Bias for Robust Language Models](https://arxiv.org/abs/2602.02686)
*Patrick Cooper,Alireza Nadali,Ashutosh Trivedi,Alvaro Velasquez*

Main category: cs.CL

TL;DR: 该论文提出在Transformer的feed-forward子层选择性实施单调性约束，以提升语言模型的鲁棒性，同时保持注意力机制不受约束以保留表达能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在对抗性提示和越狱攻击下表现出脆弱行为，即使经过大量对齐和微调。这种脆弱性反映了现代神经语言模型的广泛挑战：高维输入空间中的微小、精心设计的扰动可能导致内部语义表示和输出的巨大不可预测变化。

Method: 在序列到序列Transformer的feed-forward子层选择性实施单调性约束，同时保持注意力机制不受约束。这种架构分离允许通过注意力机制显式引入否定、矛盾和上下文交互，同时确保后续语义细化是保序的。

Result: 单调性显著提高了鲁棒性：对抗性攻击成功率从约69%降至19%，而标准摘要性能仅略微下降。单调语言模型保持了其预训练对应模型的性能。

Conclusion: 单调性作为架构归纳偏置可以有效提升Transformer语言模型的鲁棒性，而不必牺牲表达能力。通过选择性实施单调性约束，可以在保持模型性能的同时显著降低对抗性攻击的脆弱性。

Abstract: Large language models (LLMs) are known to exhibit brittle behavior under adversarial prompts and jailbreak attacks, even after extensive alignment and fine-tuning. This fragility reflects a broader challenge of modern neural language models: small, carefully structured perturbations in high-dimensional input spaces can induce large and unpredictable changes in internal semantic representations and output.
  We investigate monotonicity as an architectural inductive bias for improving the robustness of Transformer-based language models. Monotonicity constrains semantic transformations so that strengthening information, evidence, or constraints cannot lead to regressions in the corresponding internal representations. Such order-preserving behavior has long been exploited in control and safety-critical systems to simplify reasoning and improve robustness, but has traditionally been viewed as incompatible with the expressivity required by neural language models.
  We show that this trade-off is not inherent. By enforcing monotonicity selectively in the feed-forward sublayers of sequence-to-sequence Transformers -- while leaving attention mechanisms unconstrained -- we obtain monotone language models that preserve the performance of their pretrained counterparts. This architectural separation allows negation, contradiction, and contextual interactions to be introduced explicitly through attention, while ensuring that subsequent semantic refinement is order-preserving. Empirically, monotonicity substantially improves robustness: adversarial attack success rates drop from approximately 69% to 19%, while standard summarization performance degrades only marginally.

</details>


### [8] [InfMem: Learning System-2 Memory Control for Long-Context Agent](https://arxiv.org/abs/2602.02704)
*Xinyu Wang,Mingze Li,Peng Lu,Xiao-Wen Chang,Lifeng Shang,Jinping Li,Fei Mi,Prasanna Parthasarathi,Yufei Cui*

Main category: cs.CL

TL;DR: InfMem是一个控制中心代理，通过PreThink-Retrieve-Write协议实现System-2式控制，用于超长文档推理，在保持稀疏证据的同时减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 超长文档推理需要在严格内存约束下综合分散在远距离段落中的稀疏证据。现有的流式代理被动更新内存策略往往无法保留多跳推理所需的低显著性桥梁证据。

Method: 提出InfMem控制中心代理，采用PreThink-Retrieve-Write协议：主动监控证据充分性，执行针对性文档内检索，应用证据感知联合压缩来更新有限内存。引入实用的SFT-to-RL训练方法，使检索、写入和停止决策与最终任务正确性对齐。

Result: 在32k到1M tokens的超长QA基准测试中，InfMem在不同骨干模型上均优于MemAgent：Qwen3-1.7B平均绝对准确率提升+10.17，Qwen3-4B提升+11.84，Qwen2.5-7B提升+8.23。推理时间平均减少3.9倍（最高5.1倍）。

Conclusion: InfMem通过主动控制策略有效解决了超长文档推理中的证据保留问题，在保持准确性的同时显著提升了推理效率。

Abstract: Reasoning over ultra-long documents requires synthesizing sparse evidence scattered across distant segments under strict memory constraints. While streaming agents enable scalable processing, their passive memory update strategy often fails to preserve low-salience bridging evidence required for multi-hop reasoning. We propose InfMem, a control-centric agent that instantiates System-2-style control via a PreThink-Retrieve-Write protocol. InfMem actively monitors evidence sufficiency, performs targeted in-document retrieval, and applies evidence-aware joint compression to update a bounded memory. To ensure reliable control, we introduce a practical SFT-to-RL training recipe that aligns retrieval, writing, and stopping decisions with end-task correctness. On ultra-long QA benchmarks from 32k to 1M tokens, InfMem consistently outperforms MemAgent across backbones. Specifically, InfMem improves average absolute accuracy by +10.17, +11.84, and +8.23 points on Qwen3-1.7B, Qwen3-4B, and Qwen2.5-7B, respectively, while reducing inference time by $3.9\times$ on average (up to $5.1\times$) via adaptive early stopping.

</details>


### [9] [Predicting first-episode homelessness among US Veterans using longitudinal EHR data: time-varying models and social risk factors](https://arxiv.org/abs/2602.02731)
*Rohan Pandey,Haijuan Yan,Hong Yu,Jack Tsai*

Main category: cs.CL

TL;DR: 利用电子健康记录数据预测退伍军人无家可归风险，比较传统机器学习、Transformer模型和大型语言模型，发现加入社会行为因素可提升预测性能


<details>
  <summary>Details</summary>
Motivation: 退伍军人无家可归问题严重，需要预防性干预。当前缺乏有效的风险预测方法，电子健康记录数据可能包含预测无家可归风险的关键信息

Method: 回顾性研究分析4,276,403名退伍军人事务部患者的EHR数据，构建静态和时间变化的EHR表示，使用临床医生指导的逻辑建模临床状况和社会风险的持续性，比较传统机器学习、Transformer掩码语言模型和微调大型语言模型

Result: 加入社会行为因素使PR-AUC提升15-30%；在风险最高的1%人群中，不同时间点的阳性预测值从3.93-13.80%不等；大型语言模型在区分度上表现较差但在不同种族群体间性能差异较小

Conclusion: 纵向、社会信息丰富的EHR建模可将无家可归风险集中到可操作的层次，为高危退伍军人提供有针对性的数据驱动预防策略

Abstract: Homelessness among US veterans remains a critical public health challenge, yet risk prediction offers a pathway for proactive intervention. In this retrospective prognostic study, we analyzed electronic health record (EHR) data from 4,276,403 Veterans Affairs patients during a 2016 observation period to predict first-episode homelessness occurring 3-12 months later in 2017 (prevalence: 0.32-1.19%). We constructed static and time-varying EHR representations, utilizing clinician-informed logic to model the persistence of clinical conditions and social risks over time. We then compared the performance of classical machine learning, transformer-based masked language models, and fine-tuned large language models (LLMs). We demonstrate that incorporating social and behavioral factors into longitudinal models improved precision-recall area under the curve (PR-AUC) by 15-30%. In the top 1% risk tier, models yielded positive predictive values ranging from 3.93-4.72% at 3 months, 7.39-8.30% at 6 months, 9.84-11.41% at 9 months, and 11.65-13.80% at 12 months across model architectures. Large language models underperformed encoder-based models on discrimination but showed smaller performance disparities across racial groups. These results demonstrate that longitudinal, socially informed EHR modeling concentrates homelessness risk into actionable strata, enabling targeted and data-informed prevention strategies for at-risk veterans.

</details>


### [10] [Time-Critical Multimodal Medical Transportation: Organs, Patients, and Medical Supplies](https://arxiv.org/abs/2602.02736)
*Elaheh Sabziyan Varnousfaderani,Syed A. M. Shihab,Mohammad Taghizadeh*

Main category: cs.CL

TL;DR: 本文提出了一种用于医疗运输的多模式车辆调度贪心启发式算法，比较了四种车队配置（仅救护车、救护车+无人机、救护车+eVTOL、三者集成），旨在提高运输效率并降低成本。


<details>
  <summary>Details</summary>
Motivation: 医疗运输（器官、患者、医疗物资）对时效性要求极高，传统救护车受交通拥堵限制，直升机等空中交通工具成本高昂，新兴的无人机和eVTOL飞机则受航程和天气条件限制。需要一种多模式运输系统来整合空中和地面车辆的优势，提高整体运输效率。

Method: 提出了一种构造性贪心启发式算法，用于多模式医疗运输车辆调度。算法考虑了兼容路线的有效载荷整合、地面交通拥堵和空中天气条件，能够快速进行车辆调度（相比计算密集的优化模型）。测试了四种车队配置：仅救护车、救护车+无人机、救护车+eVTOL、三者集成。

Result: 在相同条件下评估了四种车队类型，以确定最有效的配置，在满足医疗运输需求的同时最小化运营成本、充电/燃料成本和总运输时间。

Conclusion: 多模式运输系统能够整合空中和地面车辆的优势，提高医疗运输效率。提出的贪心启发式算法能够快速进行车辆调度，相比传统优化模型更具实用性。通过比较不同车队配置，可以找到最优的医疗运输解决方案。

Abstract: Timely transportation of organs, patients, and medical supplies is critical to modern healthcare, particularly in emergencies and transplant scenarios where even short delays can severely impact outcomes. Traditional ground-based vehicles such as ambulances are often hindered by traffic congestion; while air vehicles such as helicopters are faster but costly. Emerging air vehicles -- Unmanned Aerial Vehicles and electric vertical take-off and landing aircraft -- have lower operating costs, but remain limited by range and susceptibility to weather conditions. A multimodal transportation system that integrates both air and ground vehicles can leverage the strengths of each to enhance overall transportation efficiency. This study introduces a constructive greedy heuristic algorithm for multimodal vehicle dispatching for medical transportation. Four different fleet configurations were tested: (i) ambulances only, (ii) ambulances with Unmanned Aerial Vehicles, (iii) ambulances with electric vertical take-off and landing aircraft, and (iv) a fully integrated fleet of ambulances, Unmanned Aerial Vehicles, and electric vertical take-off and landing aircraft. The algorithm incorporates payload consolidation across compatible routes, accounts for traffic congestion in ground operations and weather conditions in aerial operations, while enabling rapid vehicle dispatching compared to computationally intensive optimization models. Using a common set of conditions, we evaluate all four fleet types to identify the most effective configurations for fulfilling medical transportation needs while minimizing operating costs, recharging/fuel costs, and total transportation time.

</details>


### [11] [From Task Solving to Robust Real-World Adaptation in LLM Agents](https://arxiv.org/abs/2602.02760)
*Pouya Pezeshkpour,Estevam Hruschka*

Main category: cs.CL

TL;DR: 论文提出一个评估LLM智能体在真实部署场景中鲁棒性的框架，测试其在部分可观测性、动态环境、噪声信号和动态智能体状态下的表现，发现现有评估高估了实际部署准备度。


<details>
  <summary>Details</summary>
Motivation: 现有评估假设"干净接口"（稳定动态、可靠工具、单一明确目标），高估了LLM智能体在实际部署中的准备度。实际部署中智能体面临规则不明确、信号不可靠、环境变化和多利益相关者目标等挑战。

Method: 在基于网格的游戏中测试LLM智能体，游戏具有简单目标但长时域执行。设计违反干净接口假设但可解的测试场景，迫使智能体推断规则、为信息付费、适应环境和内部变化、在噪声下谨慎行动。

Result: 测试五个最先进的LLM智能体，发现名义任务解决能力与部署式鲁棒性之间存在巨大差距。性能随网格大小和时域增加而下降，但排名不稳定：较弱模型在策略匹配不确定机制时可能击败较强模型。智能体在没有明确指令情况下权衡完成度、效率和惩罚避免。

Conclusion: 研究揭示了模型特定的敏感性和失败驱动因素，激励在部分可观测性、噪声和非平稳性下的验证、安全动作选择和目标推断方面的进一步工作。需要更现实的评估来反映实际部署挑战。

Abstract: Large language models are increasingly deployed as specialized agents that plan, call tools, and take actions over extended horizons. Yet many existing evaluations assume a "clean interface" where dynamics are specified and stable, tools and sensors are reliable, and success is captured by a single explicit objective-often overestimating real-world readiness. In practice, agents face underspecified rules, unreliable signals, shifting environments, and implicit, multi-stakeholder goals. The challenge is therefore not just solving tasks, but adapting while solving: deciding what to trust, what is wanted, when to verify, and when to fall back or escalate. We stress-test deployment-relevant robustness under four operational circumstances: partial observability, dynamic environments, noisy signals, and dynamic agent state. We benchmark agentic LLMs in a grid-based game with a simple goal but long-horizon execution. Episodes violate clean-interface assumptions yet remain solvable, forcing agents to infer rules, pay for information, adapt to environmental and internal shifts, and act cautiously under noise. Across five state-of-the-art LLM agents, we find large gaps between nominal task-solving and deployment-like robustness. Performance generally degrades as grid size and horizon increase, but rankings are unstable: weaker models can beat stronger ones when strategy matches the uncertainty regime. Despite no explicit instruction, agents trade off completion, efficiency, and penalty avoidance, suggesting partial objective inference. Ablations and feature analyses reveal model-specific sensitivities and failure drivers, motivating work on verification, safe action selection, and objective inference under partial observability, noise, and non-stationarity.

</details>


### [12] [AmharicStoryQA: A Multicultural Story Question Answering Benchmark in Amharic](https://arxiv.org/abs/2602.02774)
*Israel Abebe Azime,Abenezer Kebede Angamo,Hana Mekonen Tamiru,Dagnachew Mekonnen Marilign,Philipp Slusallek,Seid Muhie Yimam,Dietrich Klakow*

Main category: cs.CL

TL;DR: 该论文指出当前多语言评估基准常将语言与文化等同，忽视了同一语言内的文化差异，为此创建了基于埃塞俄比亚不同地区阿姆哈拉语叙事的问答基准AmharicStoryQA，揭示了现有LLMs在叙事理解上的显著差距和区域差异。


<details>
  <summary>Details</summary>
Motivation: 当前多语言和文化评估基准常将语言与文化视为同义词，用性能作为模型对特定语言理解的代理指标，但忽视了同一语言内存在的有意义的文化差异。特别是在低资源语言中，这种简化评估无法准确反映模型对文化多样性的理解能力。

Method: 创建AmharicStoryQA基准，这是一个基于埃塞俄比亚不同阿姆哈拉语地区文化多样性叙事的长序列故事问答基准。使用该基准评估现有LLMs，分析区域特定和领域特定内容对语言评估结果的影响，并进行监督微调实验。

Result: 发现现有LLMs在叙事理解上存在显著差距，评估结果显示出明显的区域差异，监督微调在不同区域和评估设置中产生不均衡的改进效果。这表明共享语言特征不足以确保对文化多样性内容的理解。

Conclusion: 需要超越语言层面评估的文化基础基准，以更准确地评估和改进低资源语言中的叙事理解能力。语言和文化不应被视为同义词，评估应考虑同一语言内的文化多样性。

Abstract: With the growing emphasis on multilingual and cultural evaluation benchmarks for large language models, language and culture are often treated as synonymous, and performance is commonly used as a proxy for a models understanding of a given language. In this work, we argue that such evaluations overlook meaningful cultural variation that exists within a single language. We address this gap by focusing on narratives from different regions of Ethiopia and demonstrate that, despite shared linguistic characteristics, region-specific and domain-specific content substantially influences language evaluation outcomes. To this end, we introduce \textbf{\textit{AmharicStoryQA}}, a long-sequence story question answering benchmark grounded in culturally diverse narratives from Amharic-speaking regions. Using this benchmark, we reveal a significant narrative understanding gap in existing LLMs, highlight pronounced regional differences in evaluation results, and show that supervised fine-tuning yields uneven improvements across regions and evaluation settings. Our findings emphasize the need for culturally grounded benchmarks that go beyond language-level evaluation to more accurately assess and improve narrative understanding in low-resource languages.

</details>


### [13] [When Efficient Communication Explains Convexity](https://arxiv.org/abs/2602.02821)
*Ashvin Ranjan,Shane Steinert-Threlkeld*

Main category: cs.CL

TL;DR: 论文通过信息瓶颈框架分析语言效率，发现语义类型学中优化与凸性的相关性，并识别出交际需求分布的凸性是关键因素。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索语言变异的效率解释，特别是语义类型学中成功解释的因素，超越"效率通信能解释"的表层，深入理解背后的原因。

Method: 使用信息瓶颈方法形式化简单性与信息性之间的权衡，首先分析IB最优性与凸性推广的相关性，然后通过操纵IB框架中的建模参数确定驱动相关性的因素。

Result: 发现交际需求分布的凸性在凸性与最优性之间的相关性中起着特别重要的作用，识别出哪些底层因素导致了效率通信能够解释语义类型学现象。

Conclusion: 研究不仅展示了效率通信可以解释语义类型学的某些方面，更重要的是解释了为什么会出现这种情况，通过识别底层驱动因素推进了对语言变异机制的理解。

Abstract: Much recent work has argued that the variation in the languages of the world can be explained from the perspective of efficient communication; in particular, languages can be seen as optimally balancing competing pressures to be simple and to be informative. Focusing on the expression of meaning -- semantic typology -- the present paper asks what factors are responsible for successful explanations in terms of efficient communication. Using the Information Bottleneck (IB) approach to formalizing this trade-off, we first demonstrate and analyze a correlation between optimality in the IB sense and a novel generalization of convexity to this setting. In a second experiment, we manipulate various modeling parameters in the IB framework to determine which factors drive the correlation between convexity and optimality. We find that the convexity of the communicative need distribution plays an especially important role. These results move beyond showing that efficient communication can explain aspects of semantic typology into explanations for why that is the case by identifying which underlying factors are responsible.

</details>


### [14] [R2-Router: A New Paradigm for LLM Routing with Reasoning](https://arxiv.org/abs/2602.02823)
*Jiaqi Xue,Qian Lou,Jiarong Xing,Heng Huang*

Main category: cs.CL

TL;DR: R2-Router：首个将输出长度预算作为可控变量的LLM路由框架，通过联合选择最佳LLM和长度预算，在可比成本下实现更优性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由器假设每个查询中每个LLM有固定的质量和成本，忽略了同一LLM的质量会随输出长度变化。这导致路由器在估计成本超过预算时排除强大LLM，错失了这些LLM通过缩短输出仍能提供高质量结果的机会。

Method: 提出R2-Router，将输出长度预算作为可控变量，联合选择最佳LLM和长度预算，通过长度约束指令强制执行预算。同时构建R2-Bench，首个捕获LLM在不同输出长度预算下行为的路由数据集。

Result: 实验显示R2-Router在比现有路由器低4-5倍的成本下实现最先进性能。发现强大LLM在受限输出下可以超越较弱LLM，这在先前方法中是不可见的。

Conclusion: 这项工作开辟了新方向：路由即推理，路由器从被动选择器演变为深思熟虑的推理器，探索使用哪个LLM以及在什么成本预算下使用。

Abstract: As LLMs proliferate with diverse capabilities and costs, LLM routing has emerged by learning to predict each LLM's quality and cost for a given query, then selecting the one with high quality and low cost. However, existing routers implicitly assume a single fixed quality and cost per LLM for each query, ignoring that the same LLM's quality varies with its output length. This causes routers to exclude powerful LLMs when their estimated cost exceeds the budget, missing the opportunity that these LLMs could still deliver high quality at reduced cost with shorter outputs. To address this, we introduce R2-Router, which treats output length budget as a controllable variable and jointly selects the best LLM and length budget, enforcing the budget via length-constrained instructions. This enables R2-Router to discover that a powerful LLM with constrained output can outperform a weaker LLM at comparable cost-efficient configurations invisible to prior methods. Together with the router framework, we construct R2-Bench, the first routing dataset capturing LLM behavior across diverse output length budgets. Experiments show that R2-Router achieves state-of-the-art performance at 4-5x lower cost compared with existing routers. This work opens a new direction: routing as reasoning, where routers evolve from reactive selectors to deliberate reasoners that explore which LLM to use and at what cost budget.

</details>


### [15] [CATNIP: LLM Unlearning via Calibrated and Tokenized Negative Preference Alignment](https://arxiv.org/abs/2602.02824)
*Zhengbang Yang,Yisheng Zhong,Junyuan Hong,Zhuangdi Zhu*

Main category: cs.CL

TL;DR: CATNIP是一种无需保留数据或对比对的LLM遗忘方法，通过基于token置信度的校准梯度更新，实现细粒度知识遗忘控制


<details>
  <summary>Details</summary>
Motivation: 现有LLM遗忘方法存在局限性：基于梯度上升的方法会损害通用知识且依赖保留数据；负偏好对齐方法受限于参考模型选择且在真实数据场景下性能不佳。需要开发能精确量化模型对不良知识置信度并校准梯度更新的方法，同时应对数据稀缺和长度变化问题。

Method: 提出CATNIP方法，基于token级置信度按比例重新缩放遗忘效果，实现细粒度控制。该方法无需保留数据或对比遗忘响应对，通过校准的负偏好对齐来精确控制知识遗忘。

Result: 在MUSE和WMDP基准测试中，CATNIP在无需保留数据或对比对的情况下实现了有效的知识遗忘，在知识遗忘与保留的权衡方面优于现有方法。

Conclusion: CATNIP通过基于token置信度的校准梯度更新，解决了现有LLM遗忘方法的局限性，实现了更精确的知识遗忘控制，在数据稀缺和长度变化场景下表现稳健。

Abstract: Pretrained knowledge memorized in LLMs raises critical concerns over safety and privacy, which has motivated LLM Unlearning as a technique for selectively removing the influences of undesirable knowledge. Existing approaches, rooted in Gradient Ascent (GA), often degrade general domain knowledge while relying on retention data or curated contrastive pairs, which can be either impractical or data and computationally prohibitive. Negative Preference Alignment has been explored for unlearning to tackle the limitations of GA, which, however, remains confined by its choice of reference model and shows undermined performance in realistic data settings. These limitations raise two key questions: i) Can we achieve effective unlearning that quantifies model confidence in undesirable knowledge and uses it to calibrate gradient updates more precisely, thus reducing catastrophic forgetting? ii) Can we make unlearning robust to data scarcity and length variation? We answer both questions affirmatively with CATNIP (Calibrated and Tokenized Negative Preference Alignment), a principled method that rescales unlearning effects in proportion to the model's token-level confidence, thus ensuring fine-grained control over forgetting. Extensive evaluations on MUSE and WMDP benchmarks demonstrated that our work enables effective unlearning without requiring retention data or contrastive unlearning response pairs, with stronger knowledge forgetting and preservation tradeoffs than state-of-the-art methods.

</details>


### [16] [Act or Clarify? Modeling Sensitivity to Uncertainty and Cost in Communication](https://arxiv.org/abs/2602.02843)
*Polina Tsvilodub,Karl Mulligan,Todd Snider,Robert D. Hawkins,Michael Franke*

Main category: cs.CL

TL;DR: 人类在不确定情境下是否寻求澄清取决于情境不确定性和替代行动成本，两者存在交互作用：当错误行动代价高时，不确定性影响最大。


<details>
  <summary>Details</summary>
Motivation: 研究在不确定的交流情境中，人们何时会选择通过澄清问题来减少不确定性，何时会直接行动。预测澄清问题的决策取决于情境不确定性和替代行动成本，且两者存在交互作用。

Method: 基于预期遗憾的计算模型：衡量在信息不全时行动相比拥有完整信息时会损失多少。通过两个实验验证：一个检验纯语言情境下的回应，另一个扩展到澄清问题与非语言行动之间的选择。

Result: 实验结果支持理性权衡假设：人类倾向于根据在不确定下行动可能遭受重大损失的风险程度，按比例地寻求澄清。

Conclusion: 澄清问题的决策遵循理性权衡原则，人类会根据情境不确定性和行动成本进行权衡，当错误行动代价高时更倾向于寻求澄清以减少不确定性。

Abstract: When deciding how to act under uncertainty, agents may choose to act to reduce uncertainty or they may act despite that uncertainty.In communicative settings, an important way of reducing uncertainty is by asking clarification questions (CQs). We predict that the decision to ask a CQ depends on both contextual uncertainty and the cost of alternative actions, and that these factors interact: uncertainty should matter most when acting incorrectly is costly. We formalize this interaction in a computational model based on expected regret: how much an agent stands to lose by acting now rather than with full information. We test these predictions in two experiments, one examining purely linguistic responses to questions and another extending to choices between clarification and non-linguistic action. Taken together, our results suggest a rational tradeoff: humans tend to seek clarification proportional to the risk of substantial loss when acting under uncertainty.

</details>


### [17] [Which course? Discourse! Teaching Discourse and Generation in the Era of LLMs](https://arxiv.org/abs/2602.02878)
*Junyi Jessy Li,Yang Janet Liu,Kanishka Misra,Valentina Pyatkin,William Sheffield*

Main category: cs.CL

TL;DR: 该论文介绍了一门名为"计算话语与自然语言生成"的新课程，旨在连接话语处理与自然语言生成两个子领域，以应对NLP快速变化的教育挑战。


<details>
  <summary>Details</summary>
Motivation: NLP领域快速变化，需要设计能够连接不同子学科的课程。话语处理领域具有丰富的语言学见解和计算模型，与开放式或长文本生成高度相关，但在现有本科课程中这种联系未被充分探索。

Method: 设计并开设了"计算话语与自然语言生成"课程，由具有互补专业知识的团队合作设计，作为语言学与计算机科学交叉的高级本科课程。课程理念是深度整合理论与实证方面，在课堂和作业中培养探索性思维。

Result: 成功在2025年秋季首次开设了这门课程，详细描述了课程设计。通过独立调查收集了反馈，并提出了未来发展方向。

Conclusion: 该课程为NLP教育提供了连接话语处理与自然语言生成的新模式，通过整合理论与实践、培养探索性思维，为应对NLP快速变化的挑战提供了教育解决方案。

Abstract: The field of NLP has undergone vast, continuous transformations over the past few years, sparking debates going beyond discipline boundaries. This begs important questions in education: how do we design courses that bridge sub-disciplines in this shifting landscape? This paper explores this question from the angle of discourse processing, an area with rich linguistic insights and computational models for the intentional, attentional, and coherence structure of language. Discourse is highly relevant for open-ended or long-form text generation, yet this connection is under-explored in existing undergraduate curricula. We present a new course, "Computational Discourse and Natural Language Generation". The course is collaboratively designed by a team with complementary expertise and was offered for the first time in Fall 2025 as an upper-level undergraduate course, cross-listed between Linguistics and Computer Science. Our philosophy is to deeply integrate the theoretical and empirical aspects, and create an exploratory mindset inside the classroom and in the assignments. This paper describes the course in detail and concludes with takeaways from an independent survey as well as our vision for future directions.

</details>


### [18] [HALT: Hallucination Assessment via Log-probs as Time series](https://arxiv.org/abs/2602.02888)
*Ahmad Shapiro,Karan Taneja,Ashok Goel*

Main category: cs.CL

TL;DR: HALT是一种轻量级幻觉检测器，仅使用LLM生成的前20个token对数概率作为时间序列，通过GRU模型和基于熵的特征来学习模型校准偏差，相比传统方法更高效且兼容专有LLM。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的幻觉问题在安全关键领域尤为严重，现有检测方法要么需要访问模型内部状态（白盒方法），要么仅依赖表面文本特征（黑盒方法），存在效率低、兼容性差等问题。

Method: HALT仅使用LLM生成的前20个token对数概率作为时间序列输入，结合门控循环单元（GRU）模型和基于熵的特征来学习模型校准偏差。该方法不需要访问隐藏状态或注意力图，仅依赖输出对数概率，同时建立了HUB基准测试集，整合了10种能力的数据集。

Result: HALT在HUB基准测试中优于微调的modernBERT-base编码器（Lettuce），模型大小小30倍，速度提升60倍，在多样化LLM能力上建立了有效的幻觉检测框架。

Conclusion: HALT和HUB共同建立了一个高效的幻觉检测框架，通过轻量级的时间序列方法在保持高性能的同时实现了显著的效率提升，为专有LLM的幻觉检测提供了实用解决方案。

Abstract: Hallucinations remain a major obstacle for large language models (LLMs), especially in safety-critical domains. We present HALT (Hallucination Assessment via Log-probs as Time series), a lightweight hallucination detector that leverages only the top-20 token log-probabilities from LLM generations as a time series. HALT uses a gated recurrent unit model combined with entropy-based features to learn model calibration bias, providing an extremely efficient alternative to large encoders. Unlike white-box approaches, HALT does not require access to hidden states or attention maps, relying only on output log-probabilities. Unlike black-box approaches, it operates on log-probs rather than surface-form text, which enables stronger domain generalization and compatibility with proprietary LLMs without requiring access to internal weights. To benchmark performance, we introduce HUB (Hallucination detection Unified Benchmark), which consolidates prior datasets into ten capabilities covering both reasoning tasks (Algorithmic, Commonsense, Mathematical, Symbolic, Code Generation) and general purpose skills (Chat, Data-to-Text, Question Answering, Summarization, World Knowledge). While being 30x smaller, HALT outperforms Lettuce, a fine-tuned modernBERT-base encoder, achieving a 60x speedup gain on HUB. HALT and HUB together establish an effective framework for hallucination detection across diverse LLM capabilities.

</details>


### [19] [Equal Access, Unequal Interaction: A Counterfactual Audit of LLM Fairness](https://arxiv.org/abs/2602.02932)
*Alireza Amiri-Margavi,Arshia Gharagozlou,Amin Gholami Davodi,Seyed Pouyan Mousavi Davoudi,Hamidreza Hasani Balyani*

Main category: cs.CL

TL;DR: LLM公平性研究揭示：即使访问权限平等，交互质量仍存在系统性差异。GPT-4对年轻男性用户表达更多不确定性，LLaMA在不同身份群体间情感差异显著。


<details>
  <summary>Details</summary>
Motivation: 现有LLM公平性研究主要关注访问层面的拒绝和安全过滤行为，但平等访问并不保证获得响应后的交互质量平等。需要研究在获得访问后，LLM在不同人口身份属性下的交互质量差异。

Method: 采用反事实提示设计，在职业建议任务中变化年龄、性别和国籍等身份属性，评估GPT-4和LLaMA-3.1-70B。通过拒绝率分析访问公平性，使用情感、礼貌度和模糊表达等自动语言指标衡量交互质量，采用配对统计检验评估身份条件差异。

Result: 两个模型在所有身份群体中都显示零拒绝率，表明访问公平。但交互质量存在系统性差异：GPT-4对年轻男性用户表达显著更高的模糊表达（hedging），而LLaMA在不同身份群体间表现出更广泛的情感变化。

Conclusion: 公平性差异可能在访问平等的情况下仍存在于交互层面，这要求超越基于拒绝的审计进行更全面的评估。需要开发包含交互质量指标的公平性评估框架。

Abstract: Prior work on fairness in large language models (LLMs) has primarily focused on access-level behaviors such as refusals and safety filtering. However, equitable access does not ensure equitable interaction quality once a response is provided. In this paper, we conduct a controlled fairness audit examining how LLMs differ in tone, uncertainty, and linguistic framing across demographic identities after access is granted. Using a counterfactual prompt design, we evaluate GPT-4 and LLaMA-3.1-70B on career advice tasks while varying identity attributes along age, gender, and nationality. We assess access fairness through refusal analysis and measure interaction quality using automated linguistic metrics, including sentiment, politeness, and hedging. Identity-conditioned differences are evaluated using paired statistical tests. Both models exhibit zero refusal rates across all identities, indicating uniform access. Nevertheless, we observe systematic, model-specific disparities in interaction quality: GPT-4 expresses significantly higher hedging toward younger male users, while LLaMA exhibits broader sentiment variation across identity groups. These results show that fairness disparities can persist at the interaction level even when access is equal, motivating evaluation beyond refusal-based audits.

</details>


### [20] [Where Norms and References Collide: Evaluating LLMs on Normative Reasoning](https://arxiv.org/abs/2602.02975)
*Mitchell Abrams,Kaveh Eskandari Miandoab,Felix Gervits,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: LLMs在基于社会规范的指代消解任务上表现不佳，特别是在规范隐含、不明确或冲突时，这限制了语言模型在具身智能体中的应用。


<details>
  <summary>Details</summary>
Motivation: 具身智能体（如机器人）需要在具体环境中进行交互，成功的沟通往往依赖于对社会规范的推理。然而，目前尚不清楚大型语言模型是否能支持这种基于规范的指代消解推理。

Method: 提出了SNIC（情境中的规范）测试平台，这是一个经过人工验证的诊断工具，专门设计用于探测最先进的LLMs在提取和应用与规范指代消解相关的规范原则方面的能力。SNIC强调日常任务（如清洁、整理、服务）中出现的物理基础规范。

Result: 通过一系列受控评估发现，即使是最强大的LLMs也难以一致地识别和应用社会规范，特别是在规范隐含、不明确或冲突的情况下。

Conclusion: 这些发现揭示了当前LLMs的一个盲点，突显了在社交情境化、具身环境中部署基于语言的系统所面临的关键挑战。

Abstract: Embodied agents, such as robots, will need to interact in situated environments where successful communication often depends on reasoning over social norms: shared expectations that constrain what actions are appropriate in context. A key capability in such settings is norm-based reference resolution (NBRR), where interpreting referential expressions requires inferring implicit normative expectations grounded in physical and social context. Yet it remains unclear whether Large Language Models (LLMs) can support this kind of reasoning. In this work, we introduce SNIC (Situated Norms in Context), a human-validated diagnostic testbed designed to probe how well state-of-the-art LLMs can extract and utilize normative principles relevant to NBRR. SNIC emphasizes physically grounded norms that arise in everyday tasks such as cleaning, tidying, and serving. Across a range of controlled evaluations, we find that even the strongest LLMs struggle to consistently identify and apply social norms, particularly when norms are implicit, underspecified, or in conflict. These findings reveal a blind spot in current LLMs and highlight a key challenge for deploying language-based systems in socially situated, embodied settings.

</details>


### [21] [CPMobius: Iterative Coach-Player Reasoning for Data-Free Reinforcement Learning](https://arxiv.org/abs/2602.02979)
*Ran Li,Zeyuan Liu,Yinghao chen,Bingxiang He,Jiarui Yuan,Zixuan Fu,Weize Chen,Jinyi Hu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: CPMöbius提出了一种无需外部数据的协作式强化学习框架，通过教练-玩家角色协同优化数学推理能力，显著超越现有无监督方法。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在复杂推理任务上的进展严重依赖大量高质量人工标注数据（SFT或RL），这种监督密集型训练范式不可持续且扩展性受限。需要开发无需外部训练数据的自主学习方法。

Method: 提出CPMöbius框架，采用协作式教练-玩家范式：教练根据玩家能力生成针对性指令并获得玩家性能变化的奖励，玩家通过解决教练生成的递增难度任务获得奖励，形成协同优化循环。

Result: 在Qwen2.5-Math-7B-Instruct上，整体准确率平均提升+4.9，分布外准确率平均提升+5.4，分别超过RENT方法+1.5和R-zero方法+4.2。

Conclusion: CPMöbius证明了无需外部训练数据即可显著提升LLMs数学推理能力的可行性，为减少对人工标注数据的依赖提供了有效解决方案。

Abstract: Large Language Models (LLMs) have demonstrated strong potential in complex reasoning, yet their progress remains fundamentally constrained by reliance on massive high-quality human-curated tasks and labels, either through supervised fine-tuning (SFT) or reinforcement learning (RL) on reasoning-specific data. This dependence renders supervision-heavy training paradigms increasingly unsustainable, with signs of diminishing scalability already evident in practice. To overcome this limitation, we introduce CPMöbius (CPMobius), a collaborative Coach-Player paradigm for data-free reinforcement learning of reasoning models. Unlike traditional adversarial self-play, CPMöbius, inspired by real world human sports collaboration and multi-agent collaboration, treats the Coach and Player as independent but cooperative roles. The Coach proposes instructions targeted at the Player's capability and receives rewards based on changes in the Player's performance, while the Player is rewarded for solving the increasingly instructive tasks generated by the Coach. This cooperative optimization loop is designed to directly enhance the Player's mathematical reasoning ability. Remarkably, CPMöbius achieves substantial improvement without relying on any external training data, outperforming existing unsupervised approaches. For example, on Qwen2.5-Math-7B-Instruct, our method improves accuracy by an overall average of +4.9 and an out-of-distribution average of +5.4, exceeding RENT by +1.5 on overall accuracy and R-zero by +4.2 on OOD accuracy.

</details>


### [22] [LatentMem: Customizing Latent Memory for Multi-Agent Systems](https://arxiv.org/abs/2602.03036)
*Muxin Fu,Guibin Zhang,Xiangyuan Xue,Yafu Li,Zefeng He,Siyuan Huang,Xiaoye Qu,Yu Cheng,Yang Yang*

Main category: cs.CL

TL;DR: LatentMem：一个可学习的多智能体记忆框架，通过角色感知定制和紧凑潜在表示解决现有记忆系统的同质化和信息过载问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的多智能体系统存在两个主要瓶颈：1）缺乏角色感知定制导致记忆同质化；2）过于细粒度的记忆条目导致信息过载。需要一种既能定制化又能高效存储的记忆机制。

Method: 提出LatentMem框架，包含经验库（轻量存储原始交互轨迹）和记忆合成器（基于检索经验和智能体特定上下文合成紧凑潜在记忆）。引入潜在记忆策略优化（LMPO），通过任务级优化信号训练合成器生成紧凑且高实用性的表示。

Result: 在多样化基准测试和主流MAS框架中，LatentMem相比原始设置性能提升最高达19.36%，且始终优于现有记忆架构，无需修改底层框架。

Conclusion: LatentMem通过可学习的潜在记忆表示有效解决了多智能体记忆系统的同质化和信息过载问题，显著提升了集体智能性能，具有良好的通用性和实用性。

Abstract: Large language model (LLM)-powered multi-agent systems (MAS) demonstrate remarkable collective intelligence, wherein multi-agent memory serves as a pivotal mechanism for continual adaptation. However, existing multi-agent memory designs remain constrained by two fundamental bottlenecks: (i) memory homogenization arising from the absence of role-aware customization, and (ii) information overload induced by excessively fine-grained memory entries. To address these limitations, we propose LatentMem, a learnable multi-agent memory framework designed to customize agent-specific memories in a token-efficient manner. Specifically, LatentMem comprises an experience bank that stores raw interaction trajectories in a lightweight form, and a memory composer that synthesizes compact latent memories conditioned on retrieved experience and agent-specific contexts. Further, we introduce Latent Memory Policy Optimization (LMPO), which propagates task-level optimization signals through latent memories to the composer, encouraging it to produce compact and high-utility representations. Extensive experiments across diverse benchmarks and mainstream MAS frameworks show that LatentMem achieves a performance gain of up to $19.36$% over vanilla settings and consistently outperforms existing memory architectures, without requiring any modifications to the underlying frameworks.

</details>


### [23] [SAES-SVD: Self-Adaptive Suppression of Accumulated and Local Errors for SVD-based LLM Compression](https://arxiv.org/abs/2602.03051)
*Xing Hu,Dawei Yang,Yuan Cheng,Zhixuan Chen,Zukang Xu*

Main category: cs.CL

TL;DR: SAES-SVD是一种针对大语言模型的高效低秩压缩框架，通过联合优化层内重构和层间误差补偿来解决现有方法中误差累积传播的问题。


<details>
  <summary>Details</summary>
Motivation: 现有低秩压缩方法通常独立压缩每一层，只最小化单层重构误差，忽略了重构误差会在网络中传播和累积，导致与全精度基线的全局偏差被放大。需要一种能够考虑误差传播的压缩方法。

Method: 提出SAES-SVD框架，包含两个核心组件：1) 累积误差感知层压缩(CEALC)，将压缩目标公式化为局部重构和加权累积误差补偿的组合，基于二阶激活统计推导闭式低秩解；2) 自适应协作误差抑制(ACES)，自动调整权重系数以增强CEALC中压缩目标的低秩结构。

Result: 在多个LLM架构和任务上的广泛实验表明，SAES-SVD在不进行微调或不使用混合秩策略的情况下，能够持续提升压缩后模型的性能。

Conclusion: SAES-SVD通过联合优化层内重构和层间误差补偿，有效解决了低秩压缩中的误差累积问题，为LLM压缩提供了一种硬件无关且高度兼容的高效解决方案。

Abstract: The rapid growth in the parameter scale of large language models (LLMs) has created a high demand for efficient compression techniques. As a hardware-agnostic and highly compatible technique, low-rank compression has been widely adopted. However, existing methods typically compress each layer independently by minimizing per-layer reconstruction error, overlooking a critical limitation: the reconstruction error propagates and accumulates through the network, which leads to amplified global deviations from the full-precision baseline. To address this, we propose Self-Adaptive Error Suppression SVD (SAES-SVD), a LLMs compression framework that jointly optimizes intra-layer reconstruction and inter-layer error compensation. SAES-SVD is composed of two novel components: (1) Cumulative Error-Aware Layer Compression (CEALC), which formulates the compression objective as a combination of local reconstruction and weighted cumulative error compensation. Based on it, we derive a closed-form low-rank solution relied on second-order activation statistics, which explicitly aligns each layer's output with its full-precision counterpart to compensate for accumulated errors. (2) Adaptive Collaborative Error Suppression (ACES), which automatically adjusts the weighting coefficient to enhance the low-rank structure of the compression objective in CEALC. Specifically, the coefficient is optimized to maximize the ratio between the Frobenius norm of the compressed layer's output and that of the compression objective under a fixed rank, thus ensuring that the rank budget is utilized effectively. Extensive experiments across multiple LLM architectures and tasks show that, without fine-tuning or mixed-rank strategies, SAES-SVD consistently improves post-compression performance.

</details>


### [24] [ReMiT: RL-Guided Mid-Training for Iterative LLM Evolution](https://arxiv.org/abs/2602.03075)
*Junjie Huang,Jiarui Qin,Di Yin,Weiwen Liu,Yong Yu,Xing Sun,Weinan Zhang*

Main category: cs.CL

TL;DR: ReMiT提出了一种双向训练框架，利用RL调优模型的推理先验在预训练中期动态重加权token，建立自我增强的飞轮效应，持续提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 传统LLM训练是单向的（预训练→后训练），缺乏双向反馈机制。本文探索如何通过后训练的RL调优模型逆向增强预训练基础模型，建立自我增强的循环，无需专门训练的教师或参考模型。

Method: 分析训练动态，识别中期训练（退火）阶段是关键转折点。提出ReMiT方法，利用RL调优模型的推理先验，在中期训练阶段动态重加权token，优先处理对推理至关重要的token。

Result: 在10个预训练基准测试（数学、代码、通用推理）上平均提升3%，并在整个后训练流程中保持超过2%的增益，验证了迭代反馈循环的有效性。

Conclusion: 成功建立了自我增强的飞轮效应，使LLM能够持续进化。ReMiT证明了双向训练框架的可行性，为LLM的持续改进提供了新范式。

Abstract: Standard training pipelines for large language models (LLMs) are typically unidirectional, progressing from pre-training to post-training. However, the potential for a bidirectional process--where insights from post-training retroactively improve the pre-trained foundation--remains unexplored. We aim to establish a self-reinforcing flywheel: a cycle in which reinforcement learning (RL)-tuned model strengthens the base model, which in turn enhances subsequent post-training performance, requiring no specially trained teacher or reference model. To realize this, we analyze training dynamics and identify the mid-training (annealing) phase as a critical turning point for model capabilities. This phase typically occurs at the end of pre-training, utilizing high-quality corpora under a rapidly decaying learning rate. Building upon this insight, we introduce ReMiT (Reinforcement Learning-Guided Mid-Training). Specifically, ReMiT leverages the reasoning priors of RL-tuned models to dynamically reweight tokens during the mid-training phase, prioritizing those pivotal for reasoning. Empirically, ReMiT achieves an average improvement of 3\% on 10 pre-training benchmarks, spanning math, code, and general reasoning, and sustains these gains by over 2\% throughout the post-training pipeline. These results validate an iterative feedback loop, enabling continuous and self-reinforcing evolution of LLMs.

</details>


### [25] [AERO: Autonomous Evolutionary Reasoning Optimization via Endogenous Dual-Loop Feedback](https://arxiv.org/abs/2602.03084)
*Zhitao Gao,Jie Ma,Xuhong Li,Pengyu Li,Ning Qu,Yaqiang Wu,Hui Liu,Jun Liu*

Main category: cs.CL

TL;DR: AERO是一个无监督的自主进化推理优化框架，通过双循环系统实现自我提问、回答和批评，基于最近发展区理论定位"可解性差距"，在多个基准测试中显著提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在复杂推理中依赖专家标注数据和外部验证器，现有自我进化范式难以识别最优学习区域，且容易强化集体幻觉和错误先验。需要解决这些限制以实现自主推理进化。

Method: 提出AERO框架：1) 基于最近发展区理论，使用熵定位来识别"可解性差距"；2) 采用独立反事实校正进行鲁棒验证；3) 引入交错训练策略同步功能角色能力增长，防止课程崩溃。

Result: 在9个基准测试（涵盖3个领域）中，AERO在Qwen3-4B-Base上平均提升4.57%，在Qwen3-8B-Base上平均提升5.10%，优于竞争基线方法。

Conclusion: AERO通过无监督的自主进化推理优化，成功解决了LLM推理中的依赖问题和幻觉风险，实现了显著的性能提升，为LLM自我进化提供了有效框架。

Abstract: Large Language Models (LLMs) have achieved significant success in complex reasoning but remain bottlenecked by reliance on expert-annotated data and external verifiers. While existing self-evolution paradigms aim to bypass these constraints, they often fail to identify the optimal learning zone and risk reinforcing collective hallucinations and incorrect priors through flawed internal feedback. To address these challenges, we propose \underline{A}utonomous \underline{E}volutionary \underline{R}easoning \underline{O}ptimization (AERO), an unsupervised framework that achieves autonomous reasoning evolution by internalizing self-questioning, answering, and criticism within a synergistic dual-loop system. Inspired by the \textit{Zone of Proximal Development (ZPD)} theory, AERO utilizes entropy-based positioning to target the ``solvability gap'' and employs Independent Counterfactual Correction for robust verification. Furthermore, we introduce a Staggered Training Strategy to synchronize capability growth across functional roles and prevent curriculum collapse. Extensive evaluations across nine benchmarks spanning three domains demonstrate that AERO achieves average performance improvements of 4.57\% on Qwen3-4B-Base and 5.10\% on Qwen3-8B-Base, outperforming competitive baselines. Code is available at https://github.com/mira-ai-lab/AERO.

</details>


### [26] [Test-time Recursive Thinking: Self-Improvement without External Feedback](https://arxiv.org/abs/2602.03094)
*Yufan Zhuang,Chandan Singh,Liyuan Liu,Yelong Shen,Dinghuai Zhang,Jingbo Shang,Jianfeng Gao,Weizhu Chen*

Main category: cs.CL

TL;DR: TRT框架让LLMs无需额外训练即可自我改进，通过迭代生成、验证和选择策略，在数学竞赛和编程问题上实现显著性能提升


<details>
  <summary>Details</summary>
Motivation: 探索LLMs能否在没有额外训练的情况下自我改进，解决两个核心挑战：高效生成多样化高质量解决方案，以及在缺乏真实监督时可靠选择正确答案

Method: 提出Test-time Recursive Thinking (TRT)框架，通过迭代自我改进，基于特定策略、积累知识和自生成验证信号来条件化生成过程

Result: 开源模型在AIME-25/24上达到100%准确率，闭源模型在LiveCodeBench最难问题上提升10.4-14.8个百分点，无需外部反馈

Conclusion: TRT框架有效解决了LLMs自我改进的核心挑战，展示了无需额外训练即可显著提升推理能力的前景

Abstract: Modern Large Language Models (LLMs) have shown rapid improvements in reasoning capabilities, driven largely by reinforcement learning (RL) with verifiable rewards. Here, we ask whether these LLMs can self-improve without the need for additional training. We identify two core challenges for such systems: (i) efficiently generating diverse, high-quality candidate solutions, and (ii) reliably selecting correct answers in the absence of ground-truth supervision. To address these challenges, we propose Test-time Recursive Thinking (TRT), an iterative self-improvement framework that conditions generation on rollout-specific strategies, accumulated knowledge, and self-generated verification signals. Using TRT, open-source models reach 100% accuracy on AIME-25/24, and on LiveCodeBench's most difficult problems, closed-source models improve by 10.4-14.8 percentage points without external feedback.

</details>


### [27] [Task--Specificity Score: Measuring How Much Instructions Really Matter for Supervision](https://arxiv.org/abs/2602.03103)
*Pritam Kadasi,Abhishek Upperwal,Mayank Singh*

Main category: cs.CL

TL;DR: 论文提出了任务特异性评分(TSS)来衡量指令对预测输出的重要性，通过对比真实指令与相同输入下的替代指令，并引入TSS++来缓解简单负样本问题。


<details>
  <summary>Details</summary>
Motivation: 当前指令调优中，许多指令-输入-输出对只是弱指定的：对于给定输入，相同输出在多个替代指令下仍然合理。这引发了一个问题：指令是否唯一决定了目标输出？

Method: 提出任务特异性评分(TSS)，通过对比真实指令与相同输入下的合理替代指令来量化指令的重要性。进一步引入TSS++，使用硬替代指令和小质量项来缓解简单负样本效应。

Result: 在三个指令数据集(Alpaca、Dolly-15k、NI-20)和三个开源LLM(Gemma、Llama、Qwen)上验证，显示选择任务特异性示例能在有限token预算下提升下游性能，并能与基于质量的筛选方法(如困惑度和IFD)互补。

Conclusion: 指令并不总是唯一决定输出，任务特异性评分能有效识别指令的重要性，选择任务特异性示例能优化指令调优过程，特别是在资源受限的情况下。

Abstract: Instruction tuning is now the default way to train and adapt large language models, but many instruction--input--output pairs are only weakly specified: for a given input, the same output can remain plausible under several alternative instructions. This raises a simple question: \emph{does the instruction uniquely determine the target output?}
  We propose the \textbf{Task--Specificity Score (TSS)} to quantify how much an instruction matters for predicting its output, by contrasting the true instruction against plausible alternatives for the same input. We further introduce \textbf{TSS++}, which uses hard alternatives and a small quality term to mitigate easy-negative effects. Across three instruction datasets (\textsc{Alpaca}, \textsc{Dolly-15k}, \textsc{NI-20}) and three open LLMs (Gemma, Llama, Qwen), we show that selecting task-specific examples improves downstream performance under tight token budgets and complements quality-based filters such as perplexity and IFD.

</details>


### [28] [The Mask of Civility: Benchmarking Chinese Mock Politeness Comprehension in Large Language Models](https://arxiv.org/abs/2602.03107)
*Yitong Zhang,Yuhan Xiang,Mingxuan Liu*

Main category: cs.CL

TL;DR: 系统评估六大中文LLM在礼貌、不礼貌和虚假礼貌识别上的表现差异，填补语用理解空白


<details>
  <summary>Details</summary>
Motivation: 现有研究在语用理解方面存在空白，特别是中文语境下的礼貌现象识别。研究旨在评估LLM在识别礼貌、不礼貌和虚假礼貌三种语用现象的能力差异，为"大语言学"范式提供实证基础，并探索技术与人文的共存之道。

Method: 采用关系管理理论和虚假礼貌模型构建三类数据集（真实与模拟中文语篇），选取GPT-5.1、DeepSeek等六个代表性LLM，在零样本、少样本、知识增强和混合策略四种提示条件下进行系统评估。

Result: 论文未提供具体实验结果数据，但研究为"大语言学"范式提供了有意义尝试，展示了语用理论在技术变革时代的应用新途径。

Conclusion: 该研究是连接语言技术与人文反思的跨学科努力，为语用理论在人工智能时代的应用提供了新视角，回应了技术与人文如何共存的当代问题。

Abstract: From a pragmatic perspective, this study systematically evaluates the differences in performance among representative large language models (LLMs) in recognizing politeness, impoliteness, and mock politeness phenomena in Chinese. Addressing the existing gaps in pragmatic comprehension, the research adopts the frameworks of Rapport Management Theory and the Model of Mock Politeness to construct a three-category dataset combining authentic and simulated Chinese discourse. Six representative models, including GPT-5.1 and DeepSeek, were selected as test subjects and evaluated under four prompting conditions: zero-shot, few-shot, knowledge-enhanced, and hybrid strategies. This study serves as a meaningful attempt within the paradigm of ``Great Linguistics,'' offering a novel approach to applying pragmatic theory in the age of technological transformation. It also responds to the contemporary question of how technology and the humanities may coexist, representing an interdisciplinary endeavor that bridges linguistic technology and humanistic reflection.

</details>


### [29] [ChemPro: A Progressive Chemistry Benchmark for Large Language Models](https://arxiv.org/abs/2602.03108)
*Aaditya Baranwal,Shruti Vyas*

Main category: cs.CL

TL;DR: ChemPro是一个包含4100个化学自然语言问答对的渐进式基准测试，涵盖4个难度层级，用于评估大语言模型在化学领域的综合能力。测试显示LLMs在基础化学问题上表现良好，但随着问题复杂度增加，准确率显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能充分评估大语言模型在化学领域的真实能力，特别是缺乏渐进式难度设计和全面的化学主题覆盖。需要创建一个系统性的评估框架来测试LLMs从基础到高级化学问题的解决能力。

Method: 构建ChemPro基准测试，包含4100个自然语言问答对，分为4个难度层级，涵盖生物化学、无机化学、有机化学和物理化学。问题类型包括选择题和数值题，平衡分配了信息回忆、长程推理、多概念整合、复杂问题解决等不同认知需求。评估了45+7个开源和专有LLMs。

Result: LLMs在基础化学问题上表现良好，但随着问题类型和复杂度的增加，准确率显著下降。测试揭示了LLMs在科学推理和理解方面的关键局限性，指出了未充分研究的难度维度。

Conclusion: ChemPro基准测试为评估LLMs化学能力提供了系统性框架。结果表明当前LLMs在复杂化学推理方面存在显著不足，需要更强大的方法来提升其科学理解和推理能力。

Abstract: We introduce ChemPro, a progressive benchmark with 4100 natural language question-answer pairs in Chemistry, across 4 coherent sections of difficulty designed to assess the proficiency of Large Language Models (LLMs) in a broad spectrum of general chemistry topics. We include Multiple Choice Questions and Numerical Questions spread across fine-grained information recall, long-horizon reasoning, multi-concept questions, problem-solving with nuanced articulation, and straightforward questions in a balanced ratio, effectively covering Bio-Chemistry, Inorganic-Chemistry, Organic-Chemistry and Physical-Chemistry. ChemPro is carefully designed analogous to a student's academic evaluation for basic to high-school chemistry. A gradual increase in the question difficulty rigorously tests the ability of LLMs to progress from solving basic problems to solving more sophisticated challenges.
  We evaluate 45+7 state-of-the-art LLMs, spanning both open-source and proprietary variants, and our analysis reveals that while LLMs perform well on basic chemistry questions, their accuracy declines with different types and levels of complexity. These findings highlight the critical limitations of LLMs in general scientific reasoning and understanding and point towards understudied dimensions of difficulty, emphasizing the need for more robust methodologies to improve LLMs.

</details>


### [30] [One Model, All Roles: Multi-Turn, Multi-Agent Self-Play Reinforcement Learning for Conversational Social Intelligence](https://arxiv.org/abs/2602.03109)
*Bowen Jiang,Taiwei Shi,Ryo Kamoi,Yuan Yuan,Camillo J. Taylor,Longqi Yang,Pei Zhou,Sihao Chen*

Main category: cs.CL

TL;DR: OMAR是一个强化学习框架，让单个AI模型通过多轮多智能体对话自我对弈来发展社交智能，无需人类监督即可学习复杂社交规范。


<details>
  <summary>Details</summary>
Motivation: 传统AI训练方法依赖静态、单轮优化，无法有效学习长期目标和复杂社交互动。需要开发能够通过动态社交互动学习社交智能的新框架。

Method: 提出OMAR框架：单个模型同时扮演对话中所有角色；采用分层优势估计（turn-level和token-level）确保长对话训练稳定性；在多智能体对话环境中进行自我对弈强化学习。

Result: 在SOTOPIA社交环境和狼人杀策略游戏中，训练出的模型展现出细粒度的涌现社交智能，包括共情、说服、寻求妥协等能力，即使在竞争场景下也能学习协作。

Conclusion: OMAR证明AI可以在无需人类监督的情况下通过自我对弈发展丰富的社交智能，尽管存在奖励黑客等实际挑战，这为群体对话中的AI社交智能研究提供了新方向。

Abstract: This paper introduces OMAR: One Model, All Roles, a reinforcement learning framework that enables AI to develop social intelligence through multi-turn, multi-agent conversational self-play. Unlike traditional paradigms that rely on static, single-turn optimizations, OMAR allows a single model to role-play all participants in a conversation simultaneously, learning to achieve long-term goals and complex social norms directly from dynamic social interaction. To ensure training stability across long dialogues, we implement a hierarchical advantage estimation that calculates turn-level and token-level advantages. Evaluations in the SOTOPIA social environment and Werewolf strategy games show that our trained models develop fine-grained, emergent social intelligence, such as empathy, persuasion, and compromise seeking, demonstrating the effectiveness of learning collaboration even under competitive scenarios. While we identify practical challenges like reward hacking, our results show that rich social intelligence can emerge without human supervision. We hope this work incentivizes further research on AI social intelligence in group conversations.

</details>


### [31] [Short Chains, Deep Thoughts: Balancing Reasoning Efficiency and Intra-Segment Capability via Split-Merge Optimization](https://arxiv.org/abs/2602.03141)
*Runquan Gui,Jie Wang,Zhihai Wang,Chi Ma,Jianye Hao,Feng Wu*

Main category: cs.CL

TL;DR: CoSMo框架通过一致性引导的分割-合并优化，消除大型推理模型中的结构冗余，在提升准确率的同时显著减少计算开销


<details>
  <summary>Details</summary>
Motivation: 大型推理模型依赖冗长的推理链生成，导致显著的延迟和计算开销，需要一种方法来消除结构冗余而非简单地限制token数量

Method: 提出CoSMo框架：1) 使用分割-合并算法动态优化推理链，合并冗余段并分割逻辑间隙；2) 采用结构对齐的强化学习，使用新颖的段级预算监督模型保持高效推理结构

Result: 在多个基准测试和骨干模型上的实验表明，CoSMo相比推理效率基线平均提升准确率3.3个百分点，同时减少28.7%的段使用量

Conclusion: CoSMo通过消除结构冗余而非简单限制token数量，有效解决了大型推理模型的效率问题，在保持甚至提升性能的同时显著减少计算开销

Abstract: While Large Reasoning Models (LRMs) have demonstrated impressive capabilities in solving complex tasks through the generation of long reasoning chains, this reliance on verbose generation results in significant latency and computational overhead. To address these challenges, we propose \textbf{CoSMo} (\textbf{Co}nsistency-Guided \textbf{S}plit-\textbf{M}erge \textbf{O}ptimization), a framework designed to eliminate structural redundancy rather than indiscriminately restricting token volume. Specifically, CoSMo utilizes a split-merge algorithm that dynamically refines reasoning chains by merging redundant segments and splitting logical gaps to ensure coherence. We then employ structure-aligned reinforcement learning with a novel segment-level budget to supervise the model in maintaining efficient reasoning structures throughout training. Extensive experiments across multiple benchmarks and backbones demonstrate that CoSMo achieves superior performance, improving accuracy by \textbf{3.3} points while reducing segment usage by \textbf{28.7\%} on average compared to reasoning efficiency baselines.

</details>


### [32] [FASA: Frequency-aware Sparse Attention](https://arxiv.org/abs/2602.03152)
*Yifei Wang,Yueqi Wang,Zhenrui Yue,Huimin Zeng,Yong Wang,Ismini Lourentzou,Zhengzhong Tu,Xiangxiang Chu,Julian McAuley*

Main category: cs.CL

TL;DR: FASA是一个通过RoPE频率块级功能稀疏性实现查询感知令牌驱逐的框架，显著降低KV缓存内存占用，在长上下文任务中达到接近全KV性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理长输入时面临KV缓存内存占用过高的瓶颈，现有令牌剪枝方法存在静态方法导致信息丢失或动态启发式方法未能充分捕捉令牌重要性查询依赖性的问题。

Method: 基于RoPE频率块级功能稀疏性的发现，识别出"主导"频率块作为令牌重要性的高效代理，首先使用主导频率块识别关键令牌集，然后仅在这个剪枝子集上进行聚焦注意力计算。

Result: 在长上下文任务中，FASA始终优于所有令牌驱逐基线，达到接近oracle精度，在LongBench-V1上仅保留256个令牌即可达到接近100%全KV性能，在AIME24上使用仅18.9%缓存实现2.56倍加速。

Conclusion: FASA通过利用RoPE频率块级功能稀疏性实现查询感知令牌驱逐，有效解决了长上下文处理中的KV缓存内存瓶颈，在保持高性能的同时显著降低了计算和内存需求。

Abstract: The deployment of Large Language Models (LLMs) faces a critical bottleneck when handling lengthy inputs: the prohibitive memory footprint of the Key Value (KV) cache. To address this bottleneck, the token pruning paradigm leverages attention sparsity to selectively retain a small, critical subset of tokens. However, existing approaches fall short, with static methods risking irreversible information loss and dynamic strategies employing heuristics that insufficiently capture the query-dependent nature of token importance. We propose FASA, a novel framework that achieves query-aware token eviction by dynamically predicting token importance. FASA stems from a novel insight into RoPE: the discovery of functional sparsity at the frequency-chunk (FC) level. Our key finding is that a small, identifiable subset of "dominant" FCs consistently exhibits high contextual agreement with the full attention head. This provides a robust and computationally free proxy for identifying salient tokens. %making them a powerful and efficient proxy for token importance. Building on this insight, FASA first identifies a critical set of tokens using dominant FCs, and then performs focused attention computation solely on this pruned subset. % Since accessing only a small fraction of the KV cache, FASA drastically lowers memory bandwidth requirements and computational cost. Across a spectrum of long-context tasks, from sequence modeling to complex CoT reasoning, FASA consistently outperforms all token-eviction baselines and achieves near-oracle accuracy, demonstrating remarkable robustness even under constraint budgets. Notably, on LongBench-V1, FASA reaches nearly 100\% of full-KV performance when only keeping 256 tokens, and achieves 2.56$\times$ speedup using just 18.9\% of the cache on AIME24.

</details>


### [33] [Privasis: Synthesizing the Largest "Public" Private Dataset from Scratch](https://arxiv.org/abs/2602.03183)
*Hyunwoo Kim,Niloofar Mireshghallah,Michael Duan,Rui Xin,Shuyue Stella Li,Jaehun Jung,David Acuna,Qi Pang,Hanshen Xiao,G. Edward Suh,Sewoong Oh,Yulia Tsvetkov,Pang Wei Koh,Yejin Choi*

Main category: cs.CL

TL;DR: Privasis是首个百万级全合成隐私敏感数据集，包含140万条记录和5510万标注属性，用于隐私保护研究和文本脱敏模型训练。


<details>
  <summary>Details</summary>
Motivation: 隐私敏感数据研究长期受数据稀缺限制，而现代AI智能体（如OpenClaw和Gemini Agent）持续访问高度敏感个人信息带来日益增长的风险，需要解决这一瓶颈。

Method: 构建Privasis数据集（从零开始创建的全合成数据集），包含医疗记录、法律文件、财务记录、日历、短信等多种文档类型。利用该数据集构建文本脱敏平行语料库，通过分解文本和应用针对性脱敏的流程训练紧凑型脱敏模型（≤4B参数）。

Result: Privasis数据集规模达140万条记录，包含5510万标注属性，在规模、质量和多样性上远超现有数据集。基于该数据集训练的紧凑型脱敏模型（≤4B）性能超过GPT-5和Qwen-3 235B等最先进大语言模型。

Conclusion: Privasis为隐私敏感领域和智能体研究提供了大规模高质量合成数据集，将加速相关研究发展。作者计划发布数据、模型和代码以推动未来研究。

Abstract: Research involving privacy-sensitive data has always been constrained by data scarcity, standing in sharp contrast to other areas that have benefited from data scaling. This challenge is becoming increasingly urgent as modern AI agents--such as OpenClaw and Gemini Agent--are granted persistent access to highly sensitive personal information. To tackle this longstanding bottleneck and the rising risks, we present Privasis (i.e., privacy oasis), the first million-scale fully synthetic dataset entirely built from scratch--an expansive reservoir of texts with rich and diverse private information--designed to broaden and accelerate research in areas where processing sensitive social data is inevitable. Compared to existing datasets, Privasis, comprising 1.4 million records, offers orders-of-magnitude larger scale with quality, and far greater diversity across various document types, including medical history, legal documents, financial records, calendars, and text messages with a total of 55.1 million annotated attributes such as ethnicity, date of birth, workplace, etc. We leverage Privasis to construct a parallel corpus for text sanitization with our pipeline that decomposes texts and applies targeted sanitization. Our compact sanitization models (<=4B) trained on this dataset outperform state-of-the-art large language models, such as GPT-5 and Qwen-3 235B. We plan to release data, models, and code to accelerate future research on privacy-sensitive domains and agents.

</details>


### [34] [ForesightKV: Optimizing KV Cache Eviction for Reasoning Models by Learning Long-Term Contribution](https://arxiv.org/abs/2602.03203)
*Zican Dong,Peiyu Liu,Junyi Li,Zhipeng Chen,Han Peng,Shuo Wang,Wayne Xin Zhao*

Main category: cs.CL

TL;DR: ForesightKV：基于训练的KV缓存淘汰框架，通过监督学习和强化学习结合，在长文本生成中智能预测并淘汰不重要的KV对，减少内存和计算成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长推理序列中KV缓存线性增长导致内存和计算成本高昂，现有淘汰方法无法捕捉复杂的KV依赖关系，导致性能下降。

Method: 1. 设计Golden Eviction算法，利用未来注意力分数识别最优淘汰KV对；2. 通过Pairwise Ranking Loss进行监督训练蒸馏；3. 将缓存淘汰建模为马尔可夫决策过程，应用GRPO算法缓解低熵token的语言建模损失增加。

Result: 在AIME2024和AIME2025基准测试中，ForesightKV在仅使用一半缓存预算的情况下，持续优于现有方法，监督学习和强化学习方法产生协同效益。

Conclusion: ForesightKV通过训练框架有效平衡了KV缓存淘汰的效率与性能，结合监督和强化学习实现了更好的长文本生成推理能力。

Abstract: Recently, large language models (LLMs) have shown remarkable reasoning abilities by producing long reasoning traces. However, as the sequence length grows, the key-value (KV) cache expands linearly, incurring significant memory and computation costs. Existing KV cache eviction methods mitigate this issue by discarding less important KV pairs, but often fail to capture complex KV dependencies, resulting in performance degradation. To better balance efficiency and performance, we introduce ForesightKV, a training-based KV cache eviction framework that learns to predict which KV pairs to evict during long-text generations. We first design the Golden Eviction algorithm, which identifies the optimal eviction KV pairs at each step using future attention scores. These traces and the scores at each step are then distilled via supervised training with a Pairwise Ranking Loss. Furthermore, we formulate cache eviction as a Markov Decision Process and apply the GRPO algorithm to mitigate the significant language modeling loss increase on low-entropy tokens. Experiments on AIME2024 and AIME2025 benchmarks of three reasoning models demonstrate that ForesightKV consistently outperforms prior methods under only half the cache budget, while benefiting synergistically from both supervised and reinforcement learning approaches.

</details>


### [35] [Token Sparse Attention: Efficient Long-Context Inference with Interleaved Token Selection](https://arxiv.org/abs/2602.03216)
*Dongwon Jo,Beomseok Kang,Jiwon Song,Jae-Joon Kim*

Main category: cs.CL

TL;DR: Token Sparse Attention：一种轻量级动态token级稀疏化机制，通过压缩Q、K、V到缩减的token集来加速长上下文推理，与Flash Attention兼容，实现最高3.23倍加速且精度损失小于1%。


<details>
  <summary>Details</summary>
Motivation: 注意力机制的二次复杂度是长上下文推理的主要瓶颈。现有方法要么使用结构化模式稀疏化注意力图，要么在特定层永久移除token，这些方法可能保留无关token或依赖不可逆的早期决策，无法适应token重要性在不同层/头的动态变化。

Method: 提出Token Sparse Attention，一种轻量级动态token级稀疏化机制。在注意力计算时，将每个头的Q、K、V压缩到缩减的token集，计算注意力后，将输出解压缩回原始序列，使token信息能在后续层中被重新考虑。该方法与密集注意力实现（如Flash Attention）完全兼容，并能与现有稀疏注意力内核无缝组合。

Result: 实验结果显示，Token Sparse Attention持续改善精度-延迟权衡，在128K上下文长度下实现最高3.23倍注意力加速，精度损失小于1%。该方法在长序列推理任务中表现出色。

Conclusion: 动态和交错式的token级稀疏化是扩展长上下文推理的互补且有效策略。Token Sparse Attention在注意力加速和精度保持之间取得了良好平衡，为解决注意力二次复杂度瓶颈提供了新思路。

Abstract: The quadratic complexity of attention remains the central bottleneck in long-context inference for large language models. Prior acceleration methods either sparsify the attention map with structured patterns or permanently evict tokens at specific layers, which can retain irrelevant tokens or rely on irreversible early decisions despite the layer-/head-wise dynamics of token importance. In this paper, we propose Token Sparse Attention, a lightweight and dynamic token-level sparsification mechanism that compresses per-head $Q$, $K$, $V$ to a reduced token set during attention and then decompresses the output back to the original sequence, enabling token information to be reconsidered in subsequent layers. Furthermore, Token Sparse Attention exposes a new design point at the intersection of token selection and sparse attention. Our approach is fully compatible with dense attention implementations, including Flash Attention, and can be seamlessly composed with existing sparse attention kernels. Experimental results show that Token Sparse Attention consistently improves accuracy-latency trade-off, achieving up to $\times$3.23 attention speedup at 128K context with less than 1% accuracy degradation. These results demonstrate that dynamic and interleaved token-level sparsification is a complementary and effective strategy for scalable long-context inference.

</details>


### [36] [ATACompressor: Adaptive Task-Aware Compression for Efficient Long-Context Processing in LLMs](https://arxiv.org/abs/2602.03226)
*Xuancheng Li,Haitao Li,Yujia Zhou,Qingyao Ai,Yiqun Liu*

Main category: cs.CL

TL;DR: ATACompressor通过任务感知的自适应压缩解决长文本中"迷失在中间"的问题，动态压缩任务相关部分，在保持关键信息的同时提高压缩效率


<details>
  <summary>Details</summary>
Motivation: 大语言模型处理长文本时存在"迷失在中间"问题，关键信息因文本过长而被稀释或忽略。现有上下文压缩方法难以平衡信息保留和压缩效率

Method: 提出自适应任务感知压缩器(ATACompressor)，包含选择性编码器动态压缩任务相关部分，以及自适应分配控制器根据相关内容长度调整压缩率

Result: 在HotpotQA、MSMARCO和SQUAD三个QA数据集上评估，ATACompressor在压缩效率和任务性能方面均优于现有方法

Conclusion: ATACompressor为大语言模型的长文本处理提供了可扩展的解决方案，并通过消融研究和分析实验深入理解其关键组件

Abstract: Long-context inputs in large language models (LLMs) often suffer from the "lost in the middle" problem, where critical information becomes diluted or ignored due to excessive length. Context compression methods aim to address this by reducing input size, but existing approaches struggle with balancing information preservation and compression efficiency. We propose Adaptive Task-Aware Compressor (ATACompressor), which dynamically adjusts compression based on the specific requirements of the task. ATACompressor employs a selective encoder that compresses only the task-relevant portions of long contexts, ensuring that essential information is preserved while reducing unnecessary content. Its adaptive allocation controller perceives the length of relevant content and adjusts the compression rate accordingly, optimizing resource utilization. We evaluate ATACompressor on three QA datasets: HotpotQA, MSMARCO, and SQUAD-showing that it outperforms existing methods in terms of both compression efficiency and task performance. Our approach provides a scalable solution for long-context processing in LLMs. Furthermore, we perform a range of ablation studies and analysis experiments to gain deeper insights into the key components of ATACompressor.

</details>


### [37] [POP: Prefill-Only Pruning for Efficient Large Model Inference](https://arxiv.org/abs/2602.03295)
*Junhui He,Zhihui Fu,Jun Wang,Qingan Li*

Main category: cs.CL

TL;DR: POP是一种阶段感知的推理策略，在预填充阶段安全地跳过深层网络层，在解码阶段保留完整模型，实现了1.37倍预填充加速且性能损失最小。


<details>
  <summary>Details</summary>
Motivation: 现有结构化剪枝方法虽然硬件效率高，但往往导致显著的精度下降。作者认为这种失败源于阶段无关的剪枝方法，忽略了预填充和解码阶段之间的不对称角色。

Method: 提出Prefill-Only Pruning (POP)：1) 通过虚拟门机制分析发现深层对解码关键但对预填充冗余；2) 预填充阶段跳过深层，解码阶段保留完整模型；3) 引入独立的KV投影维护缓存完整性；4) 边界处理策略确保首个生成token的准确性。

Result: 在Llama-3.1、Qwen3-VL和Gemma-3等模型上的实验表明，POP实现了高达1.37倍的预填充延迟加速，且性能损失最小，有效克服了现有结构化剪枝方法的精度-效率权衡限制。

Conclusion: POP通过阶段感知的推理策略，区分预填充和解码阶段的不同需求，在保持解码精度的同时显著加速预填充阶段，为LLM/VLM部署提供了有效的精度-效率平衡方案。

Abstract: Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stages. By introducing a virtual gate mechanism, our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage. To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37$\times$ speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods.

</details>


### [38] [MIRROR: A Multi-Agent Framework with Iterative Adaptive Revision and Hierarchical Retrieval for Optimization Modeling in Operations Research](https://arxiv.org/abs/2602.03318)
*Yifan Shi,Jialong Shi,Jiayi Wang,Ye Fan,Jianyong Sun*

Main category: cs.CL

TL;DR: MIRROR是一个无需微调的多智能体框架，可将自然语言优化问题直接转换为数学模型和求解器代码，通过执行驱动的迭代自适应修订和分层检索机制实现可靠建模。


<details>
  <summary>Details</summary>
Motivation: 传统运筹学建模依赖专家驱动，过程缓慢脆弱且难以适应新场景。现有LLM方法要么需要昂贵的后训练，要么采用多智能体框架但缺乏可靠的协作纠错和任务特定检索，常导致错误输出。

Method: 提出MIRROR框架，包含两个核心机制：1) 执行驱动的迭代自适应修订用于自动错误纠正；2) 分层检索从精心策划的示例库中获取相关建模和编码示例。

Result: MIRROR在标准OR基准测试中优于现有方法，在IndustryOR和Mamo-ComplexLP等复杂工业数据集上表现显著。

Conclusion: 通过结合精确的外部知识注入和系统化错误纠正，MIRROR为非专家用户提供了高效可靠的OR建模解决方案，克服了通用LLM在专业优化任务中的根本限制。

Abstract: Operations Research (OR) relies on expert-driven modeling-a slow and fragile process ill-suited to novel scenarios. While large language models (LLMs) can automatically translate natural language into optimization models, existing approaches either rely on costly post-training or employ multi-agent frameworks, yet most still lack reliable collaborative error correction and task-specific retrieval, often leading to incorrect outputs. We propose MIRROR, a fine-tuning-free, end-to-end multi-agent framework that directly translates natural language optimization problems into mathematical models and solver code. MIRROR integrates two core mechanisms: (1) execution-driven iterative adaptive revision for automatic error correction, and (2) hierarchical retrieval to fetch relevant modeling and coding exemplars from a carefully curated exemplar library. Experiments show that MIRROR outperforms existing methods on standard OR benchmarks, with notable results on complex industrial datasets such as IndustryOR and Mamo-ComplexLP. By combining precise external knowledge infusion with systematic error correction, MIRROR provides non-expert users with an efficient and reliable OR modeling solution, overcoming the fundamental limitations of general-purpose LLMs in expert optimization tasks.

</details>


### [39] [Accurate Failure Prediction in Agents Does Not Imply Effective Failure Prevention](https://arxiv.org/abs/2602.03338)
*Rakshith Vasudev,Melisa Russak,Dan Bikel,Waseem Alshikh*

Main category: cs.CL

TL;DR: LLM批评模型的主动干预可能反而导致性能严重下降，即使离线准确率高。研究发现干预存在"破坏-恢复"权衡，并提出预部署测试来预测干预效果，避免部署时性能退化。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM批评模型的主动干预通常被认为能提高可靠性，但其在部署时的实际效果尚不清楚。研究旨在理解干预的真正影响，避免盲目部署导致性能下降。

Method: 通过实验展示高准确率批评模型仍可能导致性能崩溃，提出"破坏-恢复"权衡理论框架，并设计预部署测试方法：使用50个任务的试点来估计干预效果，无需完整部署。

Result: 研究发现：1）即使AUROC达到0.94的批评模型，仍可能使一个模型性能下降26个百分点；2）干预效果高度可变；3）预部署测试能准确预测干预效果：对高成功率任务干预有害（0到-26pp），对高失败率任务（ALFWorld）有适度改善（+2.8pp）。

Conclusion: LLM批评模型的准确率不足以判断干预是否安全。关键价值在于识别何时不应干预，在部署前预防严重性能退化。预部署测试框架能有效评估干预风险。

Abstract: Proactive interventions by LLM critic models are often assumed to improve reliability, yet their effects at deployment time are poorly understood. We show that a binary LLM critic with strong offline accuracy (AUROC 0.94) can nevertheless cause severe performance degradation, inducing a 26 percentage point (pp) collapse on one model while affecting another by near zero pp. This variability demonstrates that LLM critic accuracy alone is insufficient to determine whether intervention is safe.
  We identify a disruption-recovery tradeoff: interventions may recover failing trajectories but also disrupt trajectories that would have succeeded. Based on this insight, we propose a pre-deployment test that uses a small pilot of 50 tasks to estimate whether intervention is likely to help or harm, without requiring full deployment. Across benchmarks, the test correctly anticipates outcomes: intervention degrades performance on high-success tasks (0 to -26 pp), while yielding a modest improvement on the high-failure ALFWorld benchmark (+2.8 pp, p=0.014). The primary value of our framework is therefore identifying when not to intervene, preventing severe regressions before deployment.

</details>


### [40] [PEGRL: Improving Machine Translation by Post-Editing Guided Reinforcement Learning](https://arxiv.org/abs/2602.03352)
*Yunzhi Shen,Hao Zhou,Xin Huang,Xue Han,Junlan Feng,Shujian Huang*

Main category: cs.CL

TL;DR: PEGRL是一个两阶段强化学习框架，通过后编辑作为辅助任务来稳定训练和指导优化，解决了机器翻译中RL面临的噪声学习信号和大轨迹空间问题。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的机器翻译强化学习方法（如GRPO）取得了显著进展，但仍面临两个主要挑战：1）蒙特卡洛回报估计产生的噪声学习信号；2）大轨迹空间导致全局探索优先于细粒度局部优化。

Method: 提出PEGRL两阶段RL框架：1）采样翻译输出构建后编辑输入；2）后编辑阶段的回报估计受益于当前翻译行为的条件信息；3）任务特定加权方案平衡翻译和后编辑目标；4）支持全局探索和细粒度局部优化。

Result: 在英→芬兰语、英→土耳其语和英↔中文翻译任务上均优于RL基线方法，英→土耳其语任务上COMET-KIWI性能与先进LLM系统（DeepSeek-V3.2）相当。

Conclusion: PEGRL通过后编辑辅助任务有效解决了翻译导向RL的挑战，提供了更稳定、高效的训练框架，在多个语言对上实现了性能提升。

Abstract: Reinforcement learning (RL) has shown strong promise for LLM-based machine translation, with recent methods such as GRPO demonstrating notable gains; nevertheless, translation-oriented RL remains challenged by noisy learning signals arising from Monte Carlo return estimation, as well as a large trajectory space that favors global exploration over fine-grained local optimization. We introduce \textbf{PEGRL}, a \textit{two-stage} RL framework that uses post-editing as an auxiliary task to stabilize training and guide overall optimization. At each iteration, translation outputs are sampled to construct post-editing inputs, allowing return estimation in the post-editing stage to benefit from conditioning on the current translation behavior, while jointly supporting both global exploration and fine-grained local optimization. A task-specific weighting scheme further balances the contributions of translation and post-editing objectives, yielding a biased yet more sample-efficient estimator. Experiments on English$\to$Finnish, English$\to$Turkish, and English$\leftrightarrow$Chinese show consistent gains over RL baselines, and for English$\to$Turkish, performance on COMET-KIWI is comparable to advanced LLM-based systems (DeepSeek-V3.2).

</details>


### [41] [Pursuing Best Industrial Practices for Retrieval-Augmented Generation in the Medical Domain](https://arxiv.org/abs/2602.03368)
*Wei Zhu*

Main category: cs.CL

TL;DR: 该论文分析了医疗领域RAG系统的最佳实践，通过系统评估揭示了性能与效率之间的权衡


<details>
  <summary>Details</summary>
Motivation: 尽管RAG在工业应用中迅速采用，但在医疗领域缺乏关于最佳实践的共识，包括组件构成、组织方式和实现方法

Method: 首先仔细分析RAG系统的每个组件并提出实际替代方案，然后在三类任务上进行系统评估

Result: 揭示了改进RAG系统的最佳实践，以及基于LLM的RAG系统如何在性能与效率之间做出权衡

Conclusion: 为医疗领域工业应用中的RAG系统提供了实用的最佳实践指导，帮助在性能与效率之间做出明智的权衡决策

Abstract: While retrieval augmented generation (RAG) has been swiftly adopted in industrial applications based on large language models (LLMs), there is no consensus on what are the best practices for building a RAG system in terms of what are the components, how to organize these components and how to implement each component for the industrial applications, especially in the medical domain. In this work, we first carefully analyze each component of the RAG system and propose practical alternatives for each component. Then, we conduct systematic evaluations on three types of tasks, revealing the best practices for improving the RAG system and how LLM-based RAG systems make trade-offs between performance and efficiency.

</details>


### [42] [Towards Distillation-Resistant Large Language Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2602.03396)
*Hao Fang,Tianyi Zhang,Tianqu Zhuang,Jiawei Kong,Kuofeng Gao,Bin Chen,Leqi Liang,Shu-Tao Xia,Ke Xu*

Main category: cs.CL

TL;DR: 提出一种从信息论角度防御大语言模型蒸馏攻击的方法，通过最小化条件互信息来保护模型知识产权


<details>
  <summary>Details</summary>
Motivation: 现有防御方法主要针对基于文本的蒸馏攻击，而忽略了基于logit的蒸馏攻击。专有LLMs具有巨大经济价值，但仅通过黑盒API暴露，攻击者仍可通过蒸馏提取知识。

Method: 从信息论角度分析问题，使用条件互信息(CMI)量化蒸馏相关信息。提出学习一个变换矩阵来净化原始输出，通过CMI启发的反蒸馏目标优化变换，在保留输出效用的同时去除蒸馏相关信息。

Result: 在多个LLMs和强蒸馏算法上的广泛实验表明，该方法能显著降低蒸馏性能，同时保持任务准确性，有效保护模型知识产权。

Conclusion: 该方法从信息论角度解决了logit-based蒸馏攻击的防御问题，通过最小化条件互信息有效保护专有LLMs的知识产权，填补了现有防御方法的空白。

Abstract: Proprietary large language models (LLMs) embody substantial economic value and are generally exposed only as black-box APIs, yet adversaries can still exploit their outputs to extract knowledge via distillation. Existing defenses focus exclusively on text-based distillation, leaving the important logit-based distillation largely unexplored. In this work, we analyze this problem and present an effective solution from an information-theoretic perspective. We characterize distillation-relevant information in teacher outputs using the conditional mutual information (CMI) between teacher logits and input queries conditioned on ground-truth labels. This quantity captures contextual information beneficial for model extraction, motivating us to defend distillation via CMI minimization. Guided by our theoretical analysis, we propose learning a transformation matrix that purifies the original outputs to enhance distillation resistance. We further derive a CMI-inspired anti-distillation objective to optimize this transformation, which effectively removes distillation-relevant information while preserving output utility. Extensive experiments across multiple LLMs and strong distillation algorithms demonstrate that the proposed method significantly degrades distillation performance while preserving task accuracy, effectively protecting models' intellectual property.

</details>


### [43] [Verified Critical Step Optimization for LLM Agents](https://arxiv.org/abs/2602.03412)
*Mukai Li,Qingcheng Zeng,Tianqing Fang,Zhenwen Liang,Linfeng Song,Qi Liu,Haitao Mi,Dong Yu*

Main category: cs.CL

TL;DR: CSO提出了一种针对LLM智能体的后训练方法，通过聚焦于已验证的关键步骤（能改变任务成败的决策点）进行偏好学习，显著提升了任务性能，同时大幅减少了所需的监督数据量。


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法面临三个核心挑战：1）仅基于结果的奖励无法精确归因中间步骤；2）估计的步骤级奖励存在系统性噪声；3）蒙特卡洛采样方法计算成本过高。需要一种更高效、更精确的监督方法。

Method: CSO方法：1）从失败的策略轨迹开始，直接针对模型弱点；2）使用过程奖励模型识别候选关键步骤；3）利用专家模型提出高质量替代动作；4）让策略模型从替代动作继续执行至任务完成；5）仅将成功纠正结果的替代动作作为DPO训练数据，确保质量和可达性。

Result: 在GAIA-Text-103和XBench-DeepSearch上，CSO相比SFT基线分别实现了37%和26%的相对提升，显著优于其他后训练方法，同时仅需监督16%的轨迹步骤。

Conclusion: 基于选择性验证的学习方法对智能体后训练非常有效，通过聚焦于已验证的关键步骤，能够在保证质量的同时大幅减少监督需求，为解决长时程任务中的信用分配问题提供了新思路。

Abstract: As large language model agents tackle increasingly complex long-horizon tasks, effective post-training becomes critical. Prior work faces fundamental challenges: outcome-only rewards fail to precisely attribute credit to intermediate steps, estimated step-level rewards introduce systematic noise, and Monte Carlo sampling approaches for step reward estimation incur prohibitive computational cost. Inspired by findings that only a small fraction of high-entropy tokens drive effective RL for reasoning, we propose Critical Step Optimization (CSO), which focuses preference learning on verified critical steps, decision points where alternate actions demonstrably flip task outcomes from failure to success. Crucially, our method starts from failed policy trajectories rather than expert demonstrations, directly targeting the policy model's weaknesses. We use a process reward model (PRM) to identify candidate critical steps, leverage expert models to propose high-quality alternatives, then continue execution from these alternatives using the policy model itself until task completion. Only alternatives that the policy successfully executes to correct outcomes are verified and used as DPO training data, ensuring both quality and policy reachability. This yields fine-grained, verifiable supervision at critical decisions while avoiding trajectory-level coarseness and step-level noise. Experiments on GAIA-Text-103 and XBench-DeepSearch show that CSO achieves 37% and 26% relative improvement over the SFT baseline and substantially outperforms other post-training methods, while requiring supervision at only 16% of trajectory steps. This demonstrates the effectiveness of selective verification-based learning for agent post-training.

</details>


### [44] [FactNet: A Billion-Scale Knowledge Graph for Multilingual Factual Grounding](https://arxiv.org/abs/2602.03417)
*Yingli Shen,Wen Lai,Jie Zhou,Xueren Zhang,Yudong Wang,Kangyang Luo,Shuo Wang,Ge Gao,Alexander Fraser,Maosong Sun*

Main category: cs.CL

TL;DR: FactNet是一个大规模开源资源，将17亿原子断言与30.1亿可审计证据指针统一起来，源自316个维基百科版本，提供高精度可追溯的知识基础。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs存在的事实幻觉和缺乏可追溯来源的问题。现有资源要么提供无文本上下文的结构化知识（如知识库），要么提供规模有限、语言覆盖不足的接地文本。

Method: 采用严格确定性的构建流程，从316个维基百科版本中提取17亿原子断言和30.1亿证据指针，确保每个证据单元都能以字节级精度恢复。

Result: FactNet达到92.1%的高接地精度，即使在长尾语言中也是如此。建立了FactNet-Bench评估套件，涵盖知识图谱补全、问答和事实核查任务。

Conclusion: FactNet为社区提供了一个基础性、可复现的资源，用于训练和评估可信赖、可验证的多语言系统，弥合了结构化知识和接地文本之间的鸿沟。

Abstract: While LLMs exhibit remarkable fluency, their utility is often compromised by factual hallucinations and a lack of traceable provenance. Existing resources for grounding mitigate this but typically enforce a dichotomy: they offer either structured knowledge without textual context (e.g., knowledge bases) or grounded text with limited scale and linguistic coverage. To bridge this gap, we introduce FactNet, a massive, open-source resource designed to unify 1.7 billion atomic assertions with 3.01 billion auditable evidence pointers derived exclusively from 316 Wikipedia editions. Unlike recent synthetic approaches, FactNet employs a strictly deterministic construction pipeline, ensuring that every evidence unit is recoverable with byte-level precision. Extensive auditing confirms a high grounding precision of 92.1%, even in long-tail languages. Furthermore, we establish FactNet-Bench, a comprehensive evaluation suite for Knowledge Graph Completion, Question Answering, and Fact Checking. FactNet provides the community with a foundational, reproducible resource for training and evaluating trustworthy, verifiable multilingual systems.

</details>


### [45] [A-RAG: Scaling Agentic Retrieval-Augmented Generation via Hierarchical Retrieval Interfaces](https://arxiv.org/abs/2602.03442)
*Mingxuan Du,Benfeng Xu,Chiwei Zhu,Shaohan Wang,Pengyu Wang,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: A-RAG是一个代理式检索增强生成框架，通过向模型暴露分层检索接口，让模型参与检索决策，从而更好地利用前沿语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统无法充分利用前沿语言模型的推理和长程工具使用能力，它们要么采用单次检索算法，要么预定义工作流程，都不允许模型参与检索决策，阻碍了随着模型改进而高效扩展。

Method: A-RAG框架向模型提供三种分层检索工具：关键词搜索、语义搜索和块读取，使代理能够自适应地在多个粒度上搜索和检索信息，让模型直接参与检索决策过程。

Result: 在多个开放域QA基准测试中，A-RAG在使用相当或更少检索标记的情况下持续优于现有方法，表明A-RAG能有效利用模型能力并动态适应不同RAG任务。研究还系统分析了A-RAG随模型规模和测试时计算量的扩展性。

Conclusion: A-RAG通过让模型参与检索决策，成功利用了前沿语言模型的推理能力，在保持检索效率的同时显著提升了RAG性能，为未来研究提供了可扩展的框架。

Abstract: Frontier language models have demonstrated strong reasoning and long-horizon tool-use capabilities. However, existing RAG systems fail to leverage these capabilities. They still rely on two paradigms: (1) designing an algorithm that retrieves passages in a single shot and concatenates them into the model's input, or (2) predefining a workflow and prompting the model to execute it step-by-step. Neither paradigm allows the model to participate in retrieval decisions, preventing efficient scaling with model improvements. In this paper, we introduce A-RAG, an Agentic RAG framework that exposes hierarchical retrieval interfaces directly to the model. A-RAG provides three retrieval tools: keyword search, semantic search, and chunk read, enabling the agent to adaptively search and retrieve information across multiple granularities. Experiments on multiple open-domain QA benchmarks show that A-RAG consistently outperforms existing approaches with comparable or lower retrieved tokens, demonstrating that A-RAG effectively leverages model capabilities and dynamically adapts to different RAG tasks. We further systematically study how A-RAG scales with model size and test-time compute. We will release our code and evaluation suite to facilitate future research. Code and evaluation suite are available at https://github.com/Ayanami0730/arag.

</details>


### [46] [Preferences for Idiomatic Language are Acquired Slowly -- and Forgotten Quickly: A Case Study on Swedish](https://arxiv.org/abs/2602.03484)
*Jenny Kunz*

Main category: cs.CL

TL;DR: 语言模型对瑞典语习语的偏好发展研究：习语能力发展较慢，大规模模型持续改进，但指令调优会削弱习语偏好


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在预训练和从英语适应瑞典语过程中，如何发展对习语性瑞典语（相对于语言可接受性）的偏好，特别关注习语能力的发展轨迹

Method: 1) 从头训练瑞典语模型和微调英语预训练模型；2) 使用最小对差异探测模型偏好；3) 将现有基准转化为最小对格式评估语言可接受性；4) 引入两个新数据集：习语与变体对比、习语瑞典语与翻译腔对比

Result: 1) 习语能力比其他语言能力（语法和词汇正确性）发展更慢；2) 长时间训练对大多数任务收益递减，但习语相关性能持续改进（尤其在8B大模型中）；3) 指令调优（使用机器翻译数据）会迅速削弱模型对习语的偏好

Conclusion: 习语能力是语言模型发展中较晚获得的技能，大规模模型能持续改进，但当前基于机器翻译的指令调优方法会损害模型的习语偏好，需要更好的方法来保持语言模型的习语能力

Abstract: In this study, we investigate how language models develop preferences for \textit{idiomatic} as compared to \textit{linguistically acceptable} Swedish, both during pretraining and when adapting a model from English to Swedish. To do so, we train models on Swedish from scratch and by fine-tuning English-pretrained models, probing their preferences at various checkpoints using minimal pairs that differ in linguistic acceptability or idiomaticity. For linguistic acceptability, we adapt existing benchmarks into a minimal-pair format. To assess idiomaticity, we introduce two novel datasets: one contrasting conventionalized idioms with plausible variants, and another contrasting idiomatic Swedish with Translationese. Our findings suggest that idiomatic competence emerges more slowly than other linguistic abilities, including grammatical and lexical correctness. While longer training yields diminishing returns for most tasks, idiom-related performance continues to improve, particularly in the largest model tested (8B). However, instruction tuning on data machine-translated from English -- the common approach for languages with little or no native instruction data -- causes models to rapidly lose their preference for idiomatic language.

</details>


### [47] [Self-Verification Dilemma: Experience-Driven Suppression of Overused Checking in LLM Reasoning](https://arxiv.org/abs/2602.03485)
*Quanyu Long,Kai Jie Jiang,Jianda Chen,Xu Guo,Leilei Gan,Wenya Wang*

Main category: cs.CL

TL;DR: 论文发现大型推理模型中的自我验证步骤大多冗余，提出基于经验池的测试时框架来减少不必要的验证，降低token使用量20.3%的同时保持或提升准确率。


<details>
  <summary>Details</summary>
Motivation: 通过大规模实证分析发现，大型推理模型中的反思步骤包含大量自我验证（重新检查），但这些验证大多是确认性而非纠正性的，很少能发现错误或改变推理结果。这种自我验证的激活频率与实际效用之间存在不匹配。

Method: 提出一个经验驱动的测试时框架：1）检测重新检查行为的激活；2）查询离线经验池中的历史验证结果；3）通过高效检索估计重新检查是否可能不必要；4）当历史经验表明不必要时，发出抑制信号让模型继续推理。

Result: 在多个模型和基准测试中，该方法将token使用量减少高达20.3%，同时保持准确率，在某些数据集上甚至提高了准确率。

Conclusion: 大型推理模型中存在大量不必要的自我验证步骤，通过基于历史经验的智能抑制机制可以有效减少这些冗余计算，在保持性能的同时显著降低推理成本。

Abstract: Large Reasoning Models (LRMs) achieve strong performance by generating long reasoning traces with reflection. Through a large-scale empirical analysis, we find that a substantial fraction of reflective steps consist of self-verification (recheck) that repeatedly confirm intermediate results. These rechecks occur frequently across models and benchmarks, yet the vast majority are confirmatory rather than corrective, rarely identifying errors and altering reasoning outcomes. This reveals a mismatch between how often self-verification is activated and how often it is actually useful. Motivated by this, we propose a novel, experience-driven test-time framework that reduces the overused verification. Our method detects the activation of recheck behavior, consults an offline experience pool of past verification outcomes, and estimates whether a recheck is likely unnecessary via efficient retrieval. When historical experience suggests unnecessary, a suppression signal redirects the model to proceed. Across multiple model and benchmarks, our approach reduces token usage up to 20.3% while maintaining the accuracy, and in some datasets even yields accuracy improvements.

</details>


### [48] [Learning to Reason Faithfully through Step-Level Faithfulness Maximization](https://arxiv.org/abs/2602.03507)
*Runquan Gui,Yafu Li,Xiaoye Qu,Ziyan Liu,Yeqiu Cheng,Yu Cheng*

Main category: cs.CL

TL;DR: FaithRL是一个强化学习框架，通过优化推理忠实度来减少LLM幻觉，同时保持答案正确性。


<details>
  <summary>Details</summary>
Motivation: 现有的RLVR方法主要依赖稀疏的结果奖励，缺乏对中间推理步骤的监督，导致模型过度自信和虚假推理，增加了幻觉问题。

Method: 提出FaithRL框架：1）形式化忠实度最大化目标；2）引入几何奖励设计；3）采用忠实度感知的优势调制机制，惩罚无支持的推理步骤同时保留有效的部分推导。

Result: 在多种骨干模型和基准测试中，FaithRL持续降低幻觉率，同时保持（甚至提升）答案正确性。分析确认FaithRL提高了逐步推理的忠实度并具有鲁棒泛化能力。

Conclusion: FaithRL通过直接优化推理忠实度有效解决了RLVR中的幻觉问题，为多步推理任务提供了更可靠的强化学习框架。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has markedly improved the performance of Large Language Models (LLMs) on tasks requiring multi-step reasoning. However, most RLVR pipelines rely on sparse outcome-based rewards, providing little supervision over intermediate steps and thus encouraging over-confidence and spurious reasoning, which in turn increases hallucinations. To address this, we propose FaithRL, a general reinforcement learning framework that directly optimizes reasoning faithfulness. We formalize a faithfulness-maximization objective and theoretically show that optimizing it mitigates over-confidence. To instantiate this objective, we introduce a geometric reward design and a faithfulness-aware advantage modulation mechanism that assigns step-level credit by penalizing unsupported steps while preserving valid partial derivations. Across diverse backbones and benchmarks, FaithRL consistently reduces hallucination rates while maintaining (and often improving) answer correctness. Further analysis confirms that FaithRL increases step-wise reasoning faithfulness and generalizes robustly. Our code is available at https://github.com/aintdoin/FaithRL.

</details>


### [49] [Can Large Language Models Generalize Procedures Across Representations?](https://arxiv.org/abs/2602.03542)
*Fangru Lin,Valentin Hofmann,Xingchen Wan,Weixing Wang,Zifeng Ding,Anthony G. Cohn,Janet B. Pierrehumbert*

Main category: cs.CL

TL;DR: 该论文提出了一种两阶段数据课程方法，先训练符号数据（代码/图）再训练自然语言数据，显著提升了LLM在不同表示形式间的泛化能力，使小模型能接近GPT-4o在自然语言规划任务上的表现。


<details>
  <summary>Details</summary>
Motivation: LLM在代码和图等符号表示上训练和测试广泛，但真实用户任务通常用自然语言描述。需要研究LLM能否在不同表示形式间有效泛化，特别是从符号表示到自然语言的泛化能力。

Method: 提出两阶段数据课程方法：第一阶段在符号数据（代码或图）上训练，第二阶段在自然语言数据上训练。通过研究同构任务（如规划调度）在不同表示形式（代码、图、自然语言）上的表现，验证方法的有效性。

Result: 两阶段课程方法显著提升了模型性能，跨模型家族和任务都有效。使用该方法训练的1.5B参数Qwen模型在自然语言规划任务上能接近零样本GPT-4o的表现。分析表明成功的跨表示泛化可解释为一种生成类比过程。

Conclusion: LLM从符号表示到自然语言的泛化存在挑战，但通过适当的数据课程设计可以显著改善。两阶段训练方法能有效促进跨表示泛化，使小模型在自然语言任务上达到接近大模型的表现，这为高效训练提供了新思路。

Abstract: Large language models (LLMs) are trained and tested extensively on symbolic representations such as code and graphs, yet real-world user tasks are often specified in natural language. To what extent can LLMs generalize across these representations? Here, we approach this question by studying isomorphic tasks involving procedures represented in code, graphs, and natural language (e.g., scheduling steps in planning). We find that training LLMs with popular post-training methods on graphs or code data alone does not reliably generalize to corresponding natural language tasks, while training solely on natural language can lead to inefficient performance gains. To address this gap, we propose a two-stage data curriculum that first trains on symbolic, then natural language data. The curriculum substantially improves model performance across model families and tasks. Remarkably, a 1.5B Qwen model trained by our method can closely match zero-shot GPT-4o in naturalistic planning. Finally, our analysis suggests that successful cross-representation generalization can be interpreted as a form of generative analogy, which our curriculum effectively encourages.

</details>


### [50] [SEAD: Self-Evolving Agent for Multi-Turn Service Dialogue](https://arxiv.org/abs/2602.03548)
*Yuqin Dai,Ning Gao,Wei Zhang,Jie Wang,Zichen Luo,Jinpeng Wang,Yujie Wang,Ruiyuan Wu,Chaozheng Wang*

Main category: cs.CL

TL;DR: SEAD是一个自演进服务对话代理框架，通过解耦用户建模为配置文件控制器和用户角色扮演模型，无需大规模人工标注即可学习有效策略，显著提升任务完成率和对话效率。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在开放域对话中表现出色，但在服务对话中表现不佳，主要原因是依赖嘈杂、低质量的人类对话数据，以及数据稀缺和模拟真实目标导向用户行为的困难。

Method: 提出SEAD框架，将用户建模解耦为两个组件：配置文件控制器（生成多样化用户状态以管理训练课程）和用户角色扮演模型（专注于真实角色扮演），确保环境提供自适应训练场景而非不公平的对手。

Result: SEAD显著优于开源基础模型和闭源商业模型，将任务完成率提高了17.6%，对话效率提高了11.1%。

Conclusion: SEAD框架通过自演进学习机制有效解决了服务对话中的数据稀缺和用户行为模拟问题，为构建高质量服务对话系统提供了新途径。

Abstract: Large Language Models have demonstrated remarkable capabilities in open-domain dialogues. However, current methods exhibit suboptimal performance in service dialogues, as they rely on noisy, low-quality human conversation data. This limitation arises from data scarcity and the difficulty of simulating authentic, goal-oriented user behaviors. To address these issues, we propose SEAD (Self-Evolving Agent for Service Dialogue), a framework that enables agents to learn effective strategies without large-scale human annotations. SEAD decouples user modeling into two components: a Profile Controller that generates diverse user states to manage training curriculum, and a User Role-play Model that focuses on realistic role-playing. This design ensures the environment provides adaptive training scenarios rather than acting as an unfair adversary. Experiments demonstrate that SEAD significantly outperforms Open-source Foundation Models and Closed-source Commercial Models, improving task completion rate by 17.6% and dialogue efficiency by 11.1%. Code is available at: https://github.com/Da1yuqin/SEAD.

</details>


### [51] [Assessing the Impact of Typological Features on Multilingual Machine Translation in the Age of Large Language Models](https://arxiv.org/abs/2602.03551)
*Vitalii Hirak,Jaap Jumelet,Arianna Bisazza*

Main category: cs.CL

TL;DR: 本文分析大型多语言翻译模型NLLB-200和Tower+，发现目标语言类型学特征显著影响翻译质量，某些语言特征能从更广泛的输出空间搜索中获益，建议采用替代解码策略。


<details>
  <summary>Details</summary>
Motivation: 尽管多语言建模取得重大进展，但不同语言间仍存在显著质量差异。除了训练资源不均衡的明显影响外，类型学特征也被认为是决定语言建模内在难度的因素。现有证据主要基于小型单语模型或从头训练的双语翻译模型，需要扩展到大型预训练多语言翻译模型进行分析。

Method: 分析两个大型预训练多语言翻译模型：NLLB-200（编码器-解码器架构）和Tower+（仅解码器架构）。基于广泛的语言集合，控制数据资源和书写系统等简单因素后，研究目标语言类型学特征对翻译质量的影响。同时分析具有特定类型学特征的语言是否能从更广泛的输出空间搜索中获益。

Result: 目标语言类型学特征显著影响两个模型的翻译质量，即使在控制数据资源和书写系统等因素后仍然如此。具有某些类型学特征的语言能从更广泛的输出空间搜索中获益，表明这些语言可能受益于标准从左到右束搜索之外的替代解码策略。

Conclusion: 语言类型学特征在多语言翻译模型中起重要作用，某些语言特征需要替代解码策略。为促进该领域进一步研究，作者发布了FLORES+ MT评估基准中212种语言的细粒度类型学特征数据集。

Abstract: Despite major advances in multilingual modeling, large quality disparities persist across languages. Besides the obvious impact of uneven training resources, typological properties have also been proposed to determine the intrinsic difficulty of modeling a language. The existing evidence, however, is mostly based on small monolingual language models or bilingual translation models trained from scratch. We expand on this line of work by analyzing two large pre-trained multilingual translation models, NLLB-200 and Tower+, which are state-of-the-art representatives of encoder-decoder and decoder-only machine translation, respectively. Based on a broad set of languages, we find that target language typology drives translation quality of both models, even after controlling for more trivial factors, such as data resourcedness and writing script. Additionally, languages with certain typological properties benefit more from a wider search of the output space, suggesting that such languages could profit from alternative decoding strategies beyond the standard left-to-right beam search. To facilitate further research in this area, we release a set of fine-grained typological properties for 212 languages of the FLORES+ MT evaluation benchmark.

</details>


### [52] [HySparse: A Hybrid Sparse Attention Architecture with Oracle Token Selection and KV Cache Sharing](https://arxiv.org/abs/2602.03560)
*Yizhao Gao,Jianyu Wei,Qihao Zhang,Yu Cheng,Shimao Chen,Zhengju Tang,Zihan Jiang,Yifan Song,Hailin Zhang,Liang Zhao,Bo Yang,Gang Wang,Shijie Cao,Fuli Luo*

Main category: cs.CL

TL;DR: HySparse是一种混合注意力架构，在每层全注意力层之间插入多个稀疏注意力层，利用全注意力层作为精确的token重要性预测器，同时重用KV缓存，显著减少计算和内存开销。


<details>
  <summary>Details</summary>
Motivation: 解决现有稀疏注意力方法的两个根本限制：1）传统方法依赖额外代理预测token重要性，增加复杂性且性能可能不理想；2）现有稀疏注意力设计通常只减少计算而不节省KV缓存。

Method: 提出HySparse架构，在全注意力层之间插入稀疏注意力层，稀疏层的token选择和KV缓存直接从前一个全注意力层推导而来，将全注意力层作为精确的token重要性预测器。

Result: 在7B密集模型和80B MoE模型上评估，HySparse在所有设置中都优于全注意力和混合SWA基线。在80B MoE模型的49层中仅5层使用全注意力，性能显著提升同时将KV缓存存储减少近10倍。

Conclusion: HySparse通过将全注意力层作为稀疏层的精确指导，有效解决了传统稀疏注意力方法的局限性，在保持性能的同时显著减少了计算和内存开销。

Abstract: This work introduces Hybrid Sparse Attention (HySparse), a new architecture that interleaves each full attention layer with several sparse attention layers. While conceptually simple, HySparse strategically derives each sparse layer's token selection and KV caches directly from the preceding full attention layer. This architecture resolves two fundamental limitations of prior sparse attention methods. First, conventional approaches typically rely on additional proxies to predict token importance, introducing extra complexity and potentially suboptimal performance. In contrast, HySparse uses the full attention layer as a precise oracle to identify important tokens. Second, existing sparse attention designs often reduce computation without saving KV cache. HySparse enables sparse attention layers to reuse the full attention KV cache, thereby reducing both computation and memory. We evaluate HySparse on both 7B dense and 80B MoE models. Across all settings, HySparse consistently outperforms both full attention and hybrid SWA baselines. Notably, in the 80B MoE model with 49 total layers, only 5 layers employ full attention, yet HySparse achieves substantial performance gains while reducing KV cache storage by nearly 10x.

</details>


### [53] [ACL: Aligned Contrastive Learning Improves BERT and Multi-exit BERT Fine-tuning](https://arxiv.org/abs/2602.03563)
*Wei Zhu*

Main category: cs.CL

TL;DR: 提出对齐对比学习（ACL）框架，解决监督学习中交叉熵损失与对比学习目标冲突的问题，提升多出口BERT模型的性能


<details>
  <summary>Details</summary>
Motivation: 监督学习中的对比学习研究较少，且交叉熵损失与对比学习目标经常冲突，阻碍了对比学习在监督设置中的应用

Method: 提出ACL框架：1) ACL-Embed将标签嵌入视为额外增强样本进行对比学习；2) ACL-Grad在目标冲突时丢弃ACL-Embed项；3) ACL-CL让教师出口指导浅层学生出口优化

Result: 在GLUE基准测试中：ACL-BERT优于或与CE和CE+SCL相当；ACL-CL显著超越基线方法，为低延迟应用提供更好的质量-速度权衡

Conclusion: ACL框架有效解决了监督学习中交叉熵与对比学习的冲突问题，特别在多出口BERT模型中显著提升了性能，为低延迟应用提供了更好的解决方案

Abstract: Despite its success in self-supervised learning, contrastive learning is less studied in the supervised setting. In this work, we first use a set of pilot experiments to show that in the supervised setting, the cross-entropy loss objective (CE) and the contrastive learning objective often conflict with each other, thus hindering the applications of CL in supervised settings. To resolve this problem, we introduce a novel \underline{A}ligned \underline{C}ontrastive \underline{L}earning (ACL) framework. First, ACL-Embed regards label embeddings as extra augmented samples with different labels and employs contrastive learning to align the label embeddings with its samples' representations. Second, to facilitate the optimization of ACL-Embed objective combined with the CE loss, we propose ACL-Grad, which will discard the ACL-Embed term if the two objectives are in conflict. To further enhance the performances of intermediate exits of multi-exit BERT, we further propose cross-layer ACL (ACL-CL), which is to ask the teacher exit to guide the optimization of student shallow exits. Extensive experiments on the GLUE benchmark results in the following takeaways: (a) ACL-BRT outperforms or performs comparably with CE and CE+SCL on the GLUE tasks; (b) ACL, especially CL-ACL, significantly surpasses the baseline methods on the fine-tuning of multi-exit BERT, thus providing better quality-speed tradeoffs for low-latency applications.

</details>


### [54] [Use Graph When It Needs: Efficiently and Adaptively Integrating Retrieval-Augmented Generation with Graphs](https://arxiv.org/abs/2602.03578)
*Su Dong,Qinggang Zhang,Yilin Xiao,Shengyuan Chen,Chuang Zhou,Xiao Huang*

Main category: cs.CL

TL;DR: EA-GraphRAG：通过语法感知的复杂度分析动态集成RAG和GraphRAG的自适应框架，在混合查询场景中显著提升准确率并降低延迟


<details>
  <summary>Details</summary>
Motivation: 传统RAG在处理非结构化领域文档时存在信息碎片化问题，而GraphRAG虽然通过知识图谱增强上下文推理，但在实际应用中却表现不佳，对所有查询都采用GraphRAG导致准确率下降和延迟过高。需要一种能根据查询复杂度自适应选择检索策略的解决方案。

Method: 提出EA-GraphRAG框架：1) 语法特征构造器解析查询并提取结构特征；2) 轻量级复杂度评分器将特征映射为连续复杂度分数；3) 基于分数的路由策略：低分查询使用密集RAG，高分查询使用基于图的检索，边界情况应用复杂度感知的互逆排序融合。

Result: 在两个单跳和两个多跳QA基准测试上的广泛实验表明，EA-GraphRAG在准确率、延迟和混合场景处理方面显著优于现有方法，实现了最先进的性能。

Conclusion: 通过动态集成RAG和GraphRAG范式，EA-GraphRAG有效解决了传统方法在处理混合复杂度查询时的局限性，为知识密集型任务提供了高效且自适应的解决方案。

Abstract: Large language models (LLMs) often struggle with knowledge-intensive tasks due to hallucinations and outdated parametric knowledge. While Retrieval-Augmented Generation (RAG) addresses this by integrating external corpora, its effectiveness is limited by fragmented information in unstructured domain documents. Graph-augmented RAG (GraphRAG) emerged to enhance contextual reasoning through structured knowledge graphs, yet paradoxically underperforms vanilla RAG in real-world scenarios, exhibiting significant accuracy drops and prohibitive latency despite gains on complex queries. We identify the rigid application of GraphRAG to all queries, regardless of complexity, as the root cause. To resolve this, we propose an efficient and adaptive GraphRAG framework called EA-GraphRAG that dynamically integrates RAG and GraphRAG paradigms through syntax-aware complexity analysis. Our approach introduces: (i) a syntactic feature constructor that parses each query and extracts a set of structural features; (ii) a lightweight complexity scorer that maps these features to a continuous complexity score; and (iii) a score-driven routing policy that selects dense RAG for low-score queries, invokes graph-based retrieval for high-score queries, and applies complexity-aware reciprocal rank fusion to handle borderline cases. Extensive experiments on a comprehensive benchmark, consisting of two single-hop and two multi-hop QA benchmarks, demonstrate that our EA-GraphRAG significantly improves accuracy, reduces latency, and achieves state-of-the-art performance in handling mixed scenarios involving both simple and complex queries.

</details>


### [55] [$V_0$: A Generalist Value Model for Any Policy at State Zero](https://arxiv.org/abs/2602.03584)
*Yi-Kai Zhang,Zhiyuan Yao,Hongyan Hao,Yueqing Sun,Qi Gu,Hui Su,Xunliang Cai,De-Chuan Zhan,Han-Jia Ye*

Main category: cs.CL

TL;DR: 提出V₀通用价值模型，无需参数更新即可估计任何模型在未见提示上的表现，用于GRPO训练中的采样预算分配和部署时的模型路由。


<details>
  <summary>Details</summary>
Motivation: 传统Actor-Critic方法需要价值模型与策略模型同步训练，成本高昂；GRPO方法需要大量采样来维持基线估计稳定性。需要一种更高效的价值估计方法。

Method: 将策略的动态能力作为显式上下文输入，利用指令-性能对历史来动态分析模型能力，专注于状态零（初始提示）的价值估计，构建V₀通用价值模型。

Result: V₀显著优于启发式预算分配方法，在LLM路由任务中实现了性能与成本的帕累托最优权衡。

Conclusion: V₀模型能够在不更新参数的情况下准确估计模型性能，为GRPO训练提供高效的采样预算分配，并在部署时作为成本效益最优的模型路由器。

Abstract: Policy gradient methods rely on a baseline to measure the relative advantage of an action, ensuring the model reinforces behaviors that outperform its current average capability. In the training of Large Language Models (LLMs) using Actor-Critic methods (e.g., PPO), this baseline is typically estimated by a Value Model (Critic) often as large as the policy model itself. However, as the policy continuously evolves, the value model requires expensive, synchronous incremental training to accurately track the shifting capabilities of the policy. To avoid this overhead, Group Relative Policy Optimization (GRPO) eliminates the coupled value model by using the average reward of a group of rollouts as the baseline; yet, this approach necessitates extensive sampling to maintain estimation stability. In this paper, we propose $V_0$, a Generalist Value Model capable of estimating the expected performance of any model on unseen prompts without requiring parameter updates. We reframe value estimation by treating the policy's dynamic capability as an explicit context input; specifically, we leverage a history of instruction-performance pairs to dynamically profile the model, departing from the traditional paradigm that relies on parameter fitting to perceive capability shifts. Focusing on value estimation at State Zero (i.e., the initial prompt, hence $V_0$), our model serves as a critical resource scheduler. During GRPO training, $V_0$ predicts success rates prior to rollout, allowing for efficient sampling budget allocation; during deployment, it functions as a router, dispatching instructions to the most cost-effective and suitable model. Empirical results demonstrate that $V_0$ significantly outperforms heuristic budget allocation and achieves a Pareto-optimal trade-off between performance and cost in LLM routing tasks.

</details>


### [56] [CL-bench: A Benchmark for Context Learning](https://arxiv.org/abs/2602.03587)
*Shihan Dou,Ming Zhang,Zhangyue Yin,Chenhao Huang,Yujiong Shen,Junzhe Wang,Jiayi Chen,Yuchen Ni,Junjie Ye,Cheng Zhang,Huaibing Xie,Jianglu Hu,Shaolei Wang,Weichao Wang,Yanling Xiao,Yiting Liu,Zenan Xu,Zhen Guo,Pluto Zhou,Tao Gui,Zuxuan Wu,Xipeng Qiu,Qi Zhang,Xuanjing Huang,Yu-Gang Jiang,Di Wang,Shunyu Yao*

Main category: cs.CL

TL;DR: CL-bench是一个评估语言模型上下文学习能力的真实世界基准，包含500个复杂上下文、1899个任务和31607个验证标准，模型平均只能解决17.2%的任务，显示当前模型在上下文学习方面仍有很大不足。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型擅长利用预训练知识进行推理，但现实世界任务更加复杂且依赖上下文。模型需要从任务特定上下文中学习并利用预训练之外的新知识来解决问题，这种"上下文学习"能力是人类自然具备但被忽视的关键能力。

Method: 创建CL-bench基准，包含500个由领域专家设计的复杂上下文、1899个任务和31607个验证标准。每个任务所需的新内容都包含在对应上下文中，涵盖新领域知识、规则系统、复杂程序、经验数据推导的法则等预训练中不存在的内容。

Result: 评估10个前沿语言模型发现，模型平均只能解决17.2%的任务。表现最好的GPT-5.1也只能解决23.7%的任务，表明语言模型尚未实现有效的上下文学习能力。

Conclusion: 上下文学习是解决现实世界复杂上下文依赖任务的关键瓶颈。CL-bench代表了构建具备这种基本能力的语言模型的重要一步，使模型更智能并推进其在真实场景中的部署。

Abstract: Current language models (LMs) excel at reasoning over prompts using pre-trained knowledge. However, real-world tasks are far more complex and context-dependent: models must learn from task-specific context and leverage new knowledge beyond what is learned during pre-training to reason and resolve tasks. We term this capability context learning, a crucial ability that humans naturally possess but has been largely overlooked. To this end, we introduce CL-bench, a real-world benchmark consisting of 500 complex contexts, 1,899 tasks, and 31,607 verification rubrics, all crafted by experienced domain experts. Each task is designed such that the new content required to resolve it is contained within the corresponding context. Resolving tasks in CL-bench requires models to learn from the context, ranging from new domain-specific knowledge, rule systems, and complex procedures to laws derived from empirical data, all of which are absent from pre-training. This goes far beyond long-context tasks that primarily test retrieval or reading comprehension, and in-context learning tasks, where models learn simple task patterns via instructions and demonstrations. Our evaluations of ten frontier LMs find that models solve only 17.2% of tasks on average. Even the best-performing model, GPT-5.1, solves only 23.7%, revealing that LMs have yet to achieve effective context learning, which poses a critical bottleneck for tackling real-world, complex context-dependent tasks. CL-bench represents a step towards building LMs with this fundamental capability, making them more intelligent and advancing their deployment in real-world scenarios.

</details>


### [57] [Efficient Algorithms for Partial Constraint Satisfaction Problems over Control-flow Graphs](https://arxiv.org/abs/2602.03588)
*Xuran Cai,Amir Goharshady*

Main category: cs.CL

TL;DR: 提出针对控制流图中部分约束满足问题（PCSP）的通用线性时间算法，适用于结构化程序的SPL分解图，统一了寄存器分配等编译器优化任务。


<details>
  <summary>Details</summary>
Motivation: 许多经典编译器优化任务（如寄存器分配、LOSPRE、最优存储体选择指令放置）都可以建模为控制流图上的PCSP问题。结构化程序的控制流图具有稀疏性和可分解性，这为高效算法设计提供了机会。

Method: 基于Series-Parallel-Loop（SPL）分解，提出针对SPL图的通用PCSP算法，时间复杂度为O(|G|·|D|^6)，对于固定域D实现线性时间。该算法统一了先前基于SPL的寄存器分配和LOSPRE方法。

Result: 算法在最优存储体选择任务上的实验结果显示，运行时间比现有最优方法快4倍。对于固定域D，算法具有线性时间复杂度。

Conclusion: 提出的SPL图PCSP通用算法不仅统一了先前方法，还在实际编译器优化任务中表现出显著性能优势，为结构化程序的控制流图优化问题提供了高效解决方案。

Abstract: In this work, we focus on the Partial Constraint Satisfaction Problem (PCSP) over control-flow graphs (CFGs) of programs. PCSP serves as a generalization of the well-known Constraint Satisfaction Problem (CSP). In the CSP framework, we define a set of variables, a set of constraints, and a finite domain $D$ that encompasses all possible values for each variable. The objective is to assign a value to each variable in such a way that all constraints are satisfied. In the graph variant of CSP, an underlying graph is considered and we have one variable corresponding to each vertex of the graph and one or several constraints corresponding to each edge. In PCSPs, we allow for certain constraints to be violated at a specified cost, aiming to find a solution that minimizes the total cost. Numerous classical compiler optimization tasks can be framed as PCSPs over control-flow graphs. Examples include Register Allocation, Lifetime-optimal Speculative Partial Redundancy Elimination (LOSPRE), and Optimal Placement of Bank Selection Instructions. On the other hand, it is well-known that control-flow graphs of structured programs are sparse and decomposable in a variety of ways. In this work, we rely on the Series-Parallel-Loop (SPL) decompositions as introduced by~\cite{RegisterAllocation}. Our main contribution is a general algorithm for PCSPs over SPL graphs with a time complexity of \(O(|G| \cdot |D|^6)\), where \(|G|\) represents the size of the control-flow graph. Note that for any fixed domain $D,$ this yields a linear-time solution. Our algorithm can be seen as a generalization and unification of previous SPL-based approaches for register allocation and LOSPRE. In addition, we provide experimental results over another classical PCSP task, i.e. Optimal Bank Selection, achieving runtimes four times better than the previous state of the art.

</details>


### [58] [Controlling Output Rankings in Generative Engines for LLM-based Search](https://arxiv.org/abs/2602.03608)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Yifeng Luo,Huimin Zeng,Man Luo,Haohan Wang*

Main category: cs.CL

TL;DR: CORE是一种优化方法，通过向搜索引擎返回内容添加策略设计的优化内容，来控制基于LLM的生成引擎中的输出排名，解决LLM搜索中初始检索顺序对小企业和独立创作者的不利影响。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的兴起，客户搜索和选择产品的方式正在改变。LLM搜索（生成引擎）直接向用户提供产品推荐，而不是传统的在线搜索结果需要用户自行探索。然而，这些推荐受到LLM初始检索顺序的强烈影响，限制了小企业和独立创作者的可见性，对他们不利。

Method: CORE（控制生成引擎中的输出排名）通过优化检索内容来影响输出排名，具体方法是在检索内容后附加策略设计的优化内容。提出了三种优化内容类型：基于字符串的、基于推理的和基于评论的。为了在现实环境中评估CORE，引入了ProductBench基准测试，包含15个产品类别，每个类别200个产品，每个产品关联从亚马逊搜索界面收集的前10个推荐。

Result: 在四个具有搜索能力的LLM（GPT-4o、Gemini-2.5、Claude-4和Grok-3）上的广泛实验表明，CORE在15个产品类别中平均达到91.4% @Top-5、86.6% @Top-3和80.3% @Top-1的提升成功率，优于现有的排名操纵方法，同时保持了优化内容的流畅性。

Conclusion: CORE是一种有效的优化方法，能够控制基于LLM的生成引擎中的输出排名，解决了LLM搜索中初始检索顺序对小企业和独立创作者的不利影响，同时保持了内容的流畅性。

Abstract: The way customers search for and choose products is changing with the rise of large language models (LLMs). LLM-based search, or generative engines, provides direct product recommendations to users, rather than traditional online search results that require users to explore options themselves. However, these recommendations are strongly influenced by the initial retrieval order of LLMs, which disadvantages small businesses and independent creators by limiting their visibility.
  In this work, we propose CORE, an optimization method that \textbf{C}ontrols \textbf{O}utput \textbf{R}ankings in g\textbf{E}nerative Engines for LLM-based search. Since the LLM's interactions with the search engine are black-box, CORE targets the content returned by search engines as the primary means of influencing output rankings. Specifically, CORE optimizes retrieved content by appending strategically designed optimization content to steer the ranking of outputs. We introduce three types of optimization content: string-based, reasoning-based, and review-based, demonstrating their effectiveness in shaping output rankings. To evaluate CORE in realistic settings, we introduce ProductBench, a large-scale benchmark with 15 product categories and 200 products per category, where each product is associated with its top-10 recommendations collected from Amazon's search interface.
  Extensive experiments on four LLMs with search capabilities (GPT-4o, Gemini-2.5, Claude-4, and Grok-3) demonstrate that CORE achieves an average Promotion Success Rate of \textbf{91.4\% @Top-5}, \textbf{86.6\% @Top-3}, and \textbf{80.3\% @Top-1}, across 15 product categories, outperforming existing ranking manipulation methods while preserving the fluency of optimized content.

</details>


### [59] [Learning Query-Specific Rubrics from Human Preferences for DeepResearch Report Generation](https://arxiv.org/abs/2602.03619)
*Changze Lv,Jie Zhou,Wentao Zhao,Jingwen Xu,Zisu Huang,Muzhao Tian,Shihan Dou,Tao Gui,Le Tian,Xiao Zhou,Xiaoqing Zheng,Xuanjing Huang,Jie Zhou*

Main category: cs.CL

TL;DR: 提出一个训练人类偏好对齐的查询特定评分标准生成器管道，用于DeepResearch报告生成，结合强化学习和多智能体工作流提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前DeepResearch报告训练和评估面临挑战，缺乏可验证的奖励信号。现有评分标准方法要么过于粗糙缺乏粒度，要么依赖手动构建的查询特定评分标准，成本高且难以扩展。

Method: 1. 构建带有人类偏好的DeepResearch风格查询数据集；2. 通过强化学习训练评分标准生成器，结合人类偏好监督和LLM评分标准评估的混合奖励；3. 引入多智能体马尔可夫状态工作流处理长程推理。

Result: 提出的评分标准生成器比现有方法提供更具区分性和更好的人类对齐监督。集成到MaMs训练框架后，DeepResearch系统在DeepResearch Bench上持续优于所有开源基线，性能与领先的闭源模型相当。

Conclusion: 成功开发了人类偏好对齐的查询特定评分标准生成器管道，结合多智能体工作流显著提升了DeepResearch报告生成系统的性能，为解决评估挑战提供了可扩展的解决方案。

Abstract: Nowadays, training and evaluating DeepResearch-generated reports remain challenging due to the lack of verifiable reward signals. Accordingly, rubric-based evaluation has become a common practice. However, existing approaches either rely on coarse, pre-defined rubrics that lack sufficient granularity, or depend on manually constructed query-specific rubrics that are costly and difficult to scale. In this paper, we propose a pipeline to train human-preference-aligned query-specific rubric generators tailored for DeepResearch report generation. We first construct a dataset of DeepResearch-style queries annotated with human preferences over paired reports, and train rubric generators via reinforcement learning with a hybrid reward combining human preference supervision and LLM-based rubric evaluation. To better handle long-horizon reasoning, we further introduce a Multi-agent Markov-state (MaMs) workflow for report generation. We empirically show that our proposed rubric generators deliver more discriminative and better human-aligned supervision than existing rubric design strategies. Moreover, when integrated into the MaMs training framework, DeepResearch systems equipped with our rubric generators consistently outperform all open-source baselines on the DeepResearch Bench and achieve performance comparable to that of leading closed-source models.

</details>


### [60] [BIRDTurk: Adaptation of the BIRD Text-to-SQL Dataset to Turkish](https://arxiv.org/abs/2602.03633)
*Burak Aktaş,Mehmet Can Baytekin,Süha Kağan Köse,Ömer İlbilgi,Elif Özge Yılmaz,Çağrı Toraman,Bilge Kaan Görür*

Main category: cs.CL

TL;DR: BIRDTurk是首个土耳其语Text-to-SQL基准，通过受控翻译构建，评估显示土耳其语导致性能下降，但智能推理展现跨语言鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL系统在英语基准上表现良好，但在形态丰富、资源匮乏的语言（如土耳其语）中的行为尚未充分探索，需要建立跨语言评估基准。

Method: 通过受控翻译管道将BIRD基准适配为土耳其语，严格保持SQL查询和数据库的逻辑结构与执行语义，使用中心极限定理确定样本量验证翻译质量（98.15%准确率）。

Result: 土耳其语引入一致性能下降，源于结构语言差异和LLM预训练中的代表性不足；智能推理展现更强的跨语言鲁棒性；监督微调对标准多语言基线具有挑战性，但在现代指令调优模型中可有效扩展。

Conclusion: BIRDTurk为现实数据库条件下的跨语言Text-to-SQL评估提供了受控测试平台，支持未来研究，并发布了训练和开发数据集。

Abstract: Text-to-SQL systems have achieved strong performance on English benchmarks, yet their behavior in morphologically rich, low-resource languages remains largely unexplored. We introduce BIRDTurk, the first Turkish adaptation of the BIRD benchmark, constructed through a controlled translation pipeline that adapts schema identifiers to Turkish while strictly preserving the logical structure and execution semantics of SQL queries and databases. Translation quality is validated on a sample size determined by the Central Limit Theorem to ensure 95% confidence, achieving 98.15% accuracy on human-evaluated samples. Using BIRDTurk, we evaluate inference-based prompting, agentic multi-stage reasoning, and supervised fine-tuning. Our results reveal that Turkish introduces consistent performance degradation, driven by both structural linguistic divergence and underrepresentation in LLM pretraining, while agentic reasoning demonstrates stronger cross-lingual robustness. Supervised fine-tuning remains challenging for standard multilingual baselines but scales effectively with modern instruction-tuned models. BIRDTurk provides a controlled testbed for cross-lingual Text-to-SQL evaluation under realistic database conditions. We release the training and development splits to support future research.

</details>


### [61] [TRE: Encouraging Exploration in the Trust Region](https://arxiv.org/abs/2602.03635)
*Chao Huang,Yujing Lu,Quangang Li,Shenghe Wang,Yan Wang,Yueyang Zhang,Long Xia,Jiashu Zhao,Zhiyuan Sun,Daiting Shi,Tingwen Liu*

Main category: cs.CL

TL;DR: 本文提出Trust Region Entropy (TRE)方法，解决大语言模型中传统熵正则化失效的问题，通过在信任区域内鼓励探索来提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统熵正则化在强化学习中能增强探索，但在大语言模型中效果有限甚至降低性能。作者认为这是因为LLMs拥有大规模词汇表和长生成序列，导致累积尾部风险，标准全局熵最大化会将概率质量分散到大量无效标记中，破坏连贯推理。

Method: 提出Trust Region Entropy (TRE)方法，该方法鼓励探索严格限制在模型的信任区域内，避免将概率质量分散到无效标记中。

Result: 在数学推理(MATH)、组合搜索(Countdown)和偏好对齐(HH)任务上的广泛实验表明，TRE始终优于原始PPO、标准熵正则化和其他探索基线方法。

Conclusion: TRE方法有效解决了LLMs中熵正则化失效的问题，通过在信任区域内进行探索，显著提升了模型在各种任务上的性能。

Abstract: Entropy regularization is a standard technique in reinforcement learning (RL) to enhance exploration, yet it yields negligible effects or even degrades performance in Large Language Models (LLMs). We attribute this failure to the cumulative tail risk inherent to LLMs with massive vocabularies and long generation horizons. In such environments, standard global entropy maximization indiscriminately dilutes probability mass into the vast tail of invalid tokens rather than focusing on plausible candidates, thereby disrupting coherent reasoning. To address this, we propose Trust Region Entropy (TRE), a method that encourages exploration strictly within the model's trust region. Extensive experiments across mathematical reasoning (MATH), combinatorial search (Countdown), and preference alignment (HH) tasks demonstrate that TRE consistently outperforms vanilla PPO, standard entropy regularization, and other exploration baselines. Our code is available at https://github.com/WhyChaos/TRE-Encouraging-Exploration-in-the-Trust-Region.

</details>


### [62] [RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish](https://arxiv.org/abs/2602.03652)
*Süha Kağan Köse,Mehmet Can Baytekin,Burak Aktaş,Bilge Kaan Görür,Evren Ayberk Munis,Deniz Yılmaz,Muhammed Yusuf Kartal,Çağrı Toraman*

Main category: cs.CL

TL;DR: 该研究构建了首个土耳其语RAG数据集，系统评估了RAG流程各阶段，发现复杂方法如HyDE能提升准确率至85%，而帕累托最优配置在保持高性能的同时显著降低成本


<details>
  <summary>Details</summary>
Motivation: 现有RAG设计指导主要针对英语，缺乏对土耳其语等形态丰富语言的研究。需要构建土耳其语RAG数据集并系统评估不同方法在该语言上的表现

Method: 基于土耳其语维基百科和CulturaX构建土耳其语RAG数据集，包含问答对和相关段落。在无任务特定微调的情况下，对RAG流程的七个阶段进行基准测试，包括查询转换、重排序和答案精炼等

Result: HyDE方法获得最高准确率85%，显著高于基线78.70%。帕累托最优配置（交叉编码器重排序+上下文增强）达到84.60%准确率且成本更低。过度堆叠生成模块会因扭曲形态线索而降低性能

Conclusion: 对于土耳其语等形态丰富语言，简单查询澄清配合稳健的重排序是有效解决方案。复杂方法能提升性能但需权衡成本，过度使用生成模块可能适得其反

Abstract: Retrieval-Augmented Generation (RAG) enhances LLM factuality, yet design guidance remains English-centric, limiting insights for morphologically rich languages like Turkish. We address this by constructing a comprehensive Turkish RAG dataset derived from Turkish Wikipedia and CulturaX, comprising question-answer pairs and relevant passage chunks. We benchmark seven stages of the RAG pipeline, from query transformation and reranking to answer refinement, without task-specific fine-tuning. Our results show that complex methods like HyDE maximize accuracy (85%) that is considerably higher than the baseline (78.70%). Also a Pareto-optimal configuration using Cross-encoder Reranking and Context Augmentation achieves comparable performance (84.60%) with much lower cost. We further demonstrate that over-stacking generative modules can degrade performance by distorting morphological cues, whereas simple query clarification with robust reranking offers an effective solution.

</details>


### [63] [Instruction Anchors: Dissecting the Causal Dynamics of Modality Arbitration](https://arxiv.org/abs/2602.03677)
*Yu Zhang,Mufan Xu,Xuefeng Bai,Kehai chen,Pengfei Zhang,Yang Xiang,Min Zhang*

Main category: cs.CL

TL;DR: 该研究通过信息流视角揭示了多模态大语言模型中模态跟随机制的工作原理，发现指令token作为模态仲裁的结构锚点，浅层注意力层进行非选择性信息传递，深层注意力层解决模态竞争，MLP层则表现出语义惯性。研究还识别了驱动这一仲裁过程的稀疏注意力头，并证明通过操纵少量关键头可显著改变模态跟随效果。


<details>
  <summary>Details</summary>
Motivation: 模态跟随能力是多模态大语言模型根据用户指令选择性利用多模态上下文的关键能力，对确保实际部署中的安全性和可靠性至关重要。然而，目前对这一决策过程的底层机制理解不足，需要深入研究其工作原理。

Method: 采用信息流视角分析模态跟随机制，通过实验揭示不同网络层在模态仲裁中的作用：浅层注意力层进行非选择性信息传递，深层注意力层解决模态竞争，MLP层表现出语义惯性。识别驱动仲裁过程的稀疏注意力头，并进行因果干预实验。

Result: 发现指令token作为模态仲裁的结构锚点；识别出稀疏的专门注意力头驱动模态仲裁；通过操纵仅5%的关键注意力头，可以阻断模态跟随效果降低60%，或通过针对性增强失败样本提高60%的模态跟随率。

Conclusion: 该研究为理解多模态大语言模型的模态跟随机制提供了重要见解，建立了模型透明化的基础，并为多模态信息协调提供了原则性框架，有助于提升模型的安全性和可靠性。

Abstract: Modality following serves as the capacity of multimodal large language models (MLLMs) to selectively utilize multimodal contexts based on user instructions. It is fundamental to ensuring safety and reliability in real-world deployments. However, the underlying mechanisms governing this decision-making process remain poorly understood. In this paper, we investigate its working mechanism through an information flow lens. Our findings reveal that instruction tokens function as structural anchors for modality arbitration: Shallow attention layers perform non-selective information transfer, routing multimodal cues to these anchors as a latent buffer; Modality competition is resolved within deep attention layers guided by the instruction intent, while MLP layers exhibit semantic inertia, acting as an adversarial force. Furthermore, we identify a sparse set of specialized attention heads that drive this arbitration. Causal interventions demonstrate that manipulating a mere $5\%$ of these critical heads can decrease the modality-following ratio by $60\%$ through blocking, or increase it by $60\%$ through targeted amplification of failed samples. Our work provides a substantial step toward model transparency and offers a principled framework for the orchestration of multimodal information in MLLMs.

</details>


### [64] [Neural Attention Search Linear: Towards Adaptive Token-Level Hybrid Attention Models](https://arxiv.org/abs/2602.03681)
*Difan Deng,Andreas Bentzen Winje,Lukas Fehring,Marius Lindauer*

Main category: cs.CL

TL;DR: 提出NAtS-L框架，在同一个注意力层中对不同token分别应用线性注意力和softmax注意力，通过搜索最优组合实现高效且表达能力强的混合架构。


<details>
  <summary>Details</summary>
Motivation: softmax注意力在长上下文场景中面临二次计算复杂度瓶颈，而线性注意力虽然高效但表达能力受限于隐藏状态大小。现有混合方法仍受softmax层效率限制。

Method: 提出NAtS-L框架，在同一层中对不同token分别使用线性注意力（Gated DeltaNet）和softmax注意力，自动判断token适合哪种注意力机制：短期影响的token用线性注意力编码到固定大小隐藏状态，需要长期检索信息的token用softmax注意力保留。

Result: NAtS-L通过搜索最优的Gated DeltaNet和softmax注意力组合，实现了token级别的混合架构，既高效又保持强大表达能力。

Conclusion: NAtS-L框架解决了传统混合注意力模型的效率瓶颈问题，通过token级别的注意力机制选择，在保持表达力的同时显著提升效率。

Abstract: The quadratic computational complexity of softmax transformers has become a bottleneck in long-context scenarios. In contrast, linear attention model families provide a promising direction towards a more efficient sequential model. These linear attention models compress past KV values into a single hidden state, thereby efficiently reducing complexity during both training and inference. However, their expressivity remains limited by the size of their hidden state. Previous work proposed interleaving softmax and linear attention layers to reduce computational complexity while preserving expressivity. Nevertheless, the efficiency of these models remains bottlenecked by their softmax attention layers. In this paper, we propose Neural Attention Search Linear (NAtS-L), a framework that applies both linear attention and softmax attention operations within the same layer on different tokens. NAtS-L automatically determines whether a token can be handled by a linear attention model, i.e., tokens that have only short-term impact and can be encoded into fixed-size hidden states, or require softmax attention, i.e., tokens that contain information related to long-term retrieval and need to be preserved for future queries. By searching for optimal Gated DeltaNet and softmax attention combinations across tokens, we show that NAtS-L provides a strong yet efficient token-level hybrid architecture.

</details>


### [65] [Rethinking the Reranker: Boundary-Aware Evidence Selection for Robust Retrieval-Augmented Generation](https://arxiv.org/abs/2602.03689)
*Jiashuo Sun,Pengcheng Jiang,Saizhuo Wang,Jiajun Fan,Heng Wang,Siru Ouyang,Ming Zhong,Yizhu Jiao,Chengsong Huang,Xueqiang Xu,Pengrui Han,Peiran Li,Jiaxin Huang,Ge Liu,Heng Ji,Jiawei Han*

Main category: cs.CL

TL;DR: BAR-RAG提出边界感知的证据选择器，针对生成器的"Goldilocks Zone"——既不过于简单也不过于困难的证据，通过强化学习和两阶段训练提升RAG系统在噪声检索下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统在现实检索噪声下表现脆弱，即使所需证据出现在top-K结果中。主要原因是检索器和重排器仅优化相关性，常选择过于简单或缺乏关键信息的证据，未考虑证据是否适合生成器。

Method: 1. 将重排器重构为边界感知证据选择器，针对生成器的"Goldilocks Zone"；2. 使用强化学习训练选择器，以生成器反馈作为奖励；3. 采用两阶段流水线，在诱导的证据分布下微调生成器，减少训练与推理的分布不匹配。

Result: 在知识密集型问答基准测试中，BAR-RAG在噪声检索下持续提升端到端性能，相比强基线RAG和重排方法平均提升10.3%，显著提高鲁棒性。

Conclusion: BAR-RAG通过边界感知证据选择和两阶段训练，有效解决了RAG系统在噪声检索下的脆弱性问题，为构建更鲁棒的检索增强生成系统提供了新思路。

Abstract: Retrieval-Augmented Generation (RAG) systems remain brittle under realistic retrieval noise, even when the required evidence appears in the top-K results. A key reason is that retrievers and rerankers optimize solely for relevance, often selecting either trivial, answer-revealing passages or evidence that lacks the critical information required to answer the question, without considering whether the evidence is suitable for the generator. We propose BAR-RAG, which reframes the reranker as a boundary-aware evidence selector that targets the generator's Goldilocks Zone -- evidence that is neither trivially easy nor fundamentally unanswerable for the generator, but is challenging yet sufficient for inference and thus provides the strongest learning signal. BAR-RAG trains the selector with reinforcement learning using generator feedback, and adopts a two-stage pipeline that fine-tunes the generator under the induced evidence distribution to mitigate the distribution mismatch between training and inference. Experiments on knowledge-intensive question answering benchmarks show that BAR-RAG consistently improves end-to-end performance under noisy retrieval, achieving an average gain of 10.3 percent over strong RAG and reranking baselines while substantially improving robustness. Code is publicly avaliable at https://github.com/GasolSun36/BAR-RAG.

</details>


### [66] [OCRTurk: A Comprehensive OCR Benchmark for Turkish](https://arxiv.org/abs/2602.03693)
*Deniz Yılmaz,Evren Ayberk Munis,Çağrı Toraman,Süha Kağan Köse,Burak Aktaş,Mehmet Can Baytekin,Bilge Kaan Görür*

Main category: cs.CL

TL;DR: OCRTurk：一个针对土耳其语文档解析的基准测试，包含180个土耳其语文档，涵盖学术文章、论文、幻灯片和非学术文章，分为三个难度级别，评估了7个OCR模型。


<details>
  <summary>Details</summary>
Motivation: 当前文档解析基准主要针对高资源语言，对土耳其语等低资源语言覆盖有限。现有的土耳其语文档解析研究缺乏反映真实场景和文档多样性的标准化基准。

Method: 构建OCRTurk基准，包含180个土耳其语文档，涵盖学术文章、论文、幻灯片和非学术文章，分为三个难度级别。使用元素级指标评估7个OCR模型。

Result: PaddleOCR在所有难度级别上表现最佳，在大多数元素级指标中领先（除了图形识别）。模型在非学术文档上表现良好，而幻灯片是最具挑战性的文档类型。

Conclusion: OCRTurk填补了土耳其语文档解析基准的空白，为评估模型在真实场景中的性能提供了标准化工具。研究揭示了不同模型在土耳其语文档解析中的性能差异，为未来改进提供了方向。

Abstract: Document parsing is now widely used in applications, such as large-scale document digitization, retrieval-augmented generation, and domain-specific pipelines in healthcare and education. Benchmarking these models is crucial for assessing their reliability and practical robustness. Existing benchmarks mostly target high-resource languages and provide limited coverage for low-resource settings, such as Turkish. Moreover, existing studies on Turkish document parsing lack a standardized benchmark that reflects real-world scenarios and document diversity. To address this gap, we introduce OCRTurk, a Turkish document parsing benchmark covering multiple layout elements and document categories at three difficulty levels. OCRTurk consists of 180 Turkish documents drawn from academic articles, theses, slide decks, and non-academic articles. We evaluate seven OCR models on OCRTurk using element-wise metrics. Across difficulty levels, PaddleOCR achieves the strongest overall results, leading most element-wise metrics except figures and attaining high Normalized Edit Distance scores in easy, medium, and hard subsets. We also observe performance variation by document type. Models perform well on non-academic documents, while slideshows become the most challenging.

</details>


### [67] [Cognitively Diverse Multiple-Choice Question Generation: A Hybrid Multi-Agent Framework with Large Language Models](https://arxiv.org/abs/2602.03704)
*Yu Tian,Linh Huynh,Katerina Christhilf,Shubham Chakraborty,Micah Watanabe,Tracy Arner,Danielle McNamara*

Main category: cs.CL

TL;DR: ReQUESTA是一个混合多智能体框架，用于生成认知多样化的多项选择题，通过分解任务、协调LLM智能体和基于规则的组件，相比单次提示的GPT-5基线，能产生更具挑战性、区分度更高且与阅读理解表现更一致的题目。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型使自动生成多项选择题变得可行，但可靠地生成满足受控认知需求的题目仍然是一个挑战。需要解决生成具有不同认知需求的题目（文本理解、推理和主旨理解）的可控性问题。

Method: ReQUESTA是一个混合多智能体框架，将MCQ创作分解为专门的子任务，协调LLM驱动的智能体和基于规则的组件，支持规划、受控生成、迭代评估和后处理。在学术说明文的大规模阅读理解研究中，与单次提示的GPT-5零样本基线进行比较。

Result: ReQUESTA生成的题目在难度、区分度和与阅读理解表现的一致性方面都优于基线。专家评估显示，ReQUESTA题目与核心概念更一致，干扰项的语言一致性和语义合理性更好，特别是在推理题方面。

Conclusion: 混合智能体编排可以系统性地提高LLM生成的可控性和可靠性，工作流设计是超越单次提示的结构化生成的关键杠杆。

Abstract: Recent advances in large language models (LLMs) have made automated multiple-choice question (MCQ) generation increasingly feasible; however, reliably producing items that satisfy controlled cognitive demands remains a challenge. To address this gap, we introduce ReQUESTA, a hybrid, multi-agent framework for generating cognitively diverse MCQs that systematically target text-based, inferential, and main idea comprehension. ReQUESTA decomposes MCQ authoring into specialized subtasks and coordinates LLM-powered agents with rule-based components to support planning, controlled generation, iterative evaluation, and post-processing. We evaluated the framework in a large-scale reading comprehension study using academic expository texts, comparing ReQUESTA-generated MCQs with those produced by a single-pass GPT-5 zero-shot baseline. Psychometric analyses of learner responses assessed item difficulty and discrimination, while expert raters evaluated question quality across multiple dimensions, including topic relevance and distractor quality. Results showed that ReQUESTA-generated items were consistently more challenging, more discriminative, and more strongly aligned with overall reading comprehension performance. Expert evaluations further indicated stronger alignment with central concepts and superior distractor linguistic consistency and semantic plausibility, particularly for inferential questions. These findings demonstrate that hybrid, agentic orchestration can systematically improve the reliability and controllability of LLM-based generation, highlighting workflow design as a key lever for structured artifact generation beyond single-pass prompting.

</details>


### [68] [OmniRAG-Agent: Agentic Omnimodal Reasoning for Low-Resource Long Audio-Video Question Answering](https://arxiv.org/abs/2602.03707)
*Yifan Zhu,Xinyu Mu,Tao Feng,Zhonghong Ou,Yuning Gong,Haoran Luo*

Main category: cs.CL

TL;DR: OmniRAG-Agent：一种面向低资源长音频视频问答的智能体方法，通过图像-音频检索增强生成、智能体循环规划和联合优化，解决现有方法在密集编码、细粒度检索、主动规划和端到端优化方面的不足。


<details>
  <summary>Details</summary>
Motivation: 当前OmniLLMs在长音频视频问答中面临四大挑战：1）密集编码成本高；2）细粒度检索能力弱；3）主动规划能力有限；4）缺乏清晰的端到端优化。这些问题在低资源环境下尤为突出。

Method: 提出OmniRAG-Agent方法，包含三个核心组件：1）图像-音频检索增强生成模块，让OmniLLM从外部库中获取短而相关的帧和音频片段；2）智能体循环，跨轮次规划、调用工具并合并检索到的证据；3）组相对策略优化，联合改进工具使用和答案质量。

Result: 在OmniVideoBench、WorldSense和Daily-Omni数据集上的实验表明，OmniRAG-Agent在低资源设置下持续优于先前方法，并取得强劲结果。消融实验验证了每个组件的有效性。

Conclusion: OmniRAG-Agent通过检索增强生成、智能体循环规划和联合优化，有效解决了长音频视频问答中的关键挑战，为低资源环境下的多模态推理提供了高效解决方案。

Abstract: Long-horizon omnimodal question answering answers questions by reasoning over text, images, audio, and video. Despite recent progress on OmniLLMs, low-resource long audio-video QA still suffers from costly dense encoding, weak fine-grained retrieval, limited proactive planning, and no clear end-to-end optimization.To address these issues, we propose OmniRAG-Agent, an agentic omnimodal QA method for budgeted long audio-video reasoning. It builds an image-audio retrieval-augmented generation module that lets an OmniLLM fetch short, relevant frames and audio snippets from external banks. Moreover, it uses an agent loop that plans, calls tools across turns, and merges retrieved evidence to answer complex queries. Furthermore, we apply group relative policy optimization to jointly improve tool use and answer quality over time. Experiments on OmniVideoBench, WorldSense, and Daily-Omni show that OmniRAG-Agent consistently outperforms prior methods under low-resource settings and achieves strong results, with ablations validating each component.

</details>


### [69] [Beyond Tokens: Semantic-Aware Speculative Decoding for Efficient Inference by Probing Internal States](https://arxiv.org/abs/2602.03708)
*Ximing Dong,Shaowei Wang,Dayi Lin,Boyuan Chen,Ahmed E. Hassan*

Main category: cs.CL

TL;DR: SemanticSpec：一种语义感知的推测解码框架，通过验证整个语义序列而非单个token来加速大语言模型推理，解决现有token级推测解码忽略语义等价性导致的低效拒绝问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）和大型推理模型（LRMs）因自回归解码导致推理延迟高，尤其在生成长思维链时问题更严重。现有推测解码方法在token级别操作，忽略语义等价性（即不同token序列表达相同含义），导致低效拒绝和加速效果受限。

Method: 提出SemanticSpec框架：1）在语义层面而非token层面进行推测解码；2）引入语义概率估计机制，通过探测模型内部隐藏状态来评估生成特定含义序列的可能性；3）验证整个语义序列而非单个token。

Result: 在四个基准测试中，SemanticSpec在DeepSeekR1-32B上实现最高2.7倍加速，在QwQ-32B上实现2.1倍加速，在效率和效果上均优于token级和序列级基线方法。

Conclusion: SemanticSpec通过语义感知的推测解码有效加速大语言模型推理，解决了现有方法忽略语义等价性的问题，为高效推理提供了新方向。

Abstract: Large Language Models (LLMs) achieve strong performance across many tasks but suffer from high inference latency due to autoregressive decoding. The issue is exacerbated in Large Reasoning Models (LRMs), which generate lengthy chains of thought. While speculative decoding accelerates inference by drafting and verifying multiple tokens in parallel, existing methods operate at the token level and ignore semantic equivalence (i.e., different token sequences expressing the same meaning), leading to inefficient rejections. We propose SemanticSpec, a semantic-aware speculative decoding framework that verifies entire semantic sequences instead of tokens. SemanticSpec introduces a semantic probability estimation mechanism that probes the model's internal hidden states to assess the likelihood of generating sequences with specific meanings.Experiments on four benchmarks show that SemanticSpec achieves up to 2.7x speedup on DeepSeekR1-32B and 2.1x on QwQ-32B, consistently outperforming token-level and sequence-level baselines in both efficiency and effectiveness.

</details>


### [70] [No Shortcuts to Culture: Indonesian Multi-hop Question Answering for Complex Cultural Understanding](https://arxiv.org/abs/2602.03709)
*Vynska Amalia Permadi,Xingwei Tan,Nafise Sadat Moosavi,Nikos Aletras*

Main category: cs.CL

TL;DR: ID-MoCQA是首个针对印尼文化的大规模多跳问答数据集，用于评估LLM的文化理解能力，通过系统化框架将单跳问题转化为多跳推理链。


<details>
  <summary>Details</summary>
Motivation: 现有文化QA基准大多依赖单跳问题，模型可能利用浅层线索而非真正进行文化推理。需要创建能评估深层文化理解的多跳推理数据集。

Method: 提出系统化框架将单跳文化问题转化为六种线索类型的多跳推理链（如常识、时间、地理等），采用专家评审和LLM-as-a-judge过滤的多阶段验证流程确保数据质量。

Result: 评估显示最先进模型在文化推理方面存在显著差距，特别是在需要细微推理的任务上。ID-MoCQA为提升LLM文化能力提供了具有挑战性的基准。

Conclusion: ID-MoCQA是首个针对印尼文化的大规模多跳QA数据集，能有效评估LLM的文化理解能力，揭示了现有模型在文化推理方面的不足，为未来研究提供了重要基准。

Abstract: Understanding culture requires reasoning across context, tradition, and implicit social knowledge, far beyond recalling isolated facts. Yet most culturally focused question answering (QA) benchmarks rely on single-hop questions, which may allow models to exploit shallow cues rather than demonstrate genuine cultural reasoning. In this work, we introduce ID-MoCQA, the first large-scale multi-hop QA dataset for assessing the cultural understanding of large language models (LLMs), grounded in Indonesian traditions and available in both English and Indonesian. We present a new framework that systematically transforms single-hop cultural questions into multi-hop reasoning chains spanning six clue types (e.g., commonsense, temporal, geographical). Our multi-stage validation pipeline, combining expert review and LLM-as-a-judge filtering, ensures high-quality question-answer pairs. Our evaluation across state-of-the-art models reveals substantial gaps in cultural reasoning, particularly in tasks requiring nuanced inference. ID-MoCQA provides a challenging and essential benchmark for advancing the cultural competency of LLMs.

</details>


### [71] [Training Multi-Turn Search Agent via Contrastive Dynamic Branch Sampling](https://arxiv.org/abs/2602.03719)
*Yubao Zhao,Weiquan Huang,Sudong Wang,Ruochen Zhao,Chen Chen,Yao Shu,Chengwei Qin*

Main category: cs.CL

TL;DR: BranPO是一种无价值函数的分支相对策略优化方法，通过截断轨迹尾部并重采样替代延续来构建对比后缀，解决长视野强化学习中的稀疏奖励问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于树的强化学习方法在长视野任务中存在高方差和计算效率低的问题，研究发现性能差异主要源于尾部决策，需要更有效的步级监督方法。

Method: BranPO通过截断轨迹尾部并重采样替代延续来构建对比后缀，引入难度感知分支采样调整分支频率，以及冗余步掩码抑制无信息动作。

Result: 在多个问答基准测试中，BranPO始终优于强基线方法，在长视野任务中实现显著准确率提升，且不增加总体训练预算。

Conclusion: BranPO通过步级对比监督有效解决了长视野强化学习中的稀疏奖励问题，提高了训练效率和稳定性。

Abstract: Agentic reinforcement learning has enabled large language models to perform complex multi-turn planning and tool use. However, learning in long-horizon settings remains challenging due to sparse, trajectory-level outcome rewards. While prior tree-based methods attempt to mitigate this issue, they often suffer from high variance and computational inefficiency. Through empirical analysis of search agents, We identify a common pattern: performance diverges mainly due to decisions near the tail. Motivated by this observation, we propose Branching Relative Policy Optimization (BranPO), a value-free method that provides step-level contrastive supervision without dense rewards. BranPO truncates trajectories near the tail and resamples alternative continuations to construct contrastive suffixes over shared prefixes, reducing credit ambiguity in long-horizon rollouts. To further boost efficiency and stabilize training, we introduce difficulty-aware branch sampling to adapt branching frequency across tasks, and redundant step masking to suppress uninformative actions. Extensive experiments on various question answering benchmarks demonstrate that BranPO consistently outperforms strong baselines, achieving significant accuracy gains on long-horizon tasks without increasing the overall training budget. Our code is available at \href{https://github.com/YubaoZhao/BranPO}{code}.

</details>


### [72] [CUBO: Self-Contained Retrieval-Augmented Generation on Consumer Laptops 10 GB Corpora, 16 GB RAM, Single-Device Deployment](https://arxiv.org/abs/2602.03731)
*Paolo Astrino*

Main category: cs.CL

TL;DR: CUBO是一个面向消费级笔记本电脑的RAG平台，能在16GB共享内存限制下实现高效文档检索，解决了云AI的GDPR合规问题和本地系统高内存需求之间的矛盾。


<details>
  <summary>Details</summary>
Motivation: 组织处理敏感文档时面临两难：基于云的AI可能违反GDPR，而本地系统通常需要18-32GB RAM。需要在消费级笔记本电脑（16GB内存）上实现高效、合规的文档检索系统。

Method: CUBO采用系统工程集成方法，包括流式数据摄入（O(1)缓冲区开销）、分层混合检索和硬件感知编排，在严格的15.5GB RAM上限内运行。37,000行代码库实现了本地化处理，符合GDPR第5(1)(c)条的数据最小化原则。

Result: 在BEIR基准测试中，Recall@10达到0.48-0.97（跨不同领域），检索延迟为185毫秒（p50），在C1,300笔记本电脑上运行。系统在15.5GB RAM限制下保持竞争力，验证了中小型专业档案的实际部署可行性。

Conclusion: CUBO证明了在消费级硬件上实现高效RAG系统的可行性，平衡了性能、内存限制和GDPR合规性要求，为中小型组织提供了实用的本地文档检索解决方案。

Abstract: Organizations handling sensitive documents face a tension: cloud-based AI risks GDPR violations, while local systems typically require 18-32 GB RAM. This paper presents CUBO, a systems-oriented RAG platform for consumer laptops with 16 GB shared memory. CUBO's novelty lies in engineering integration of streaming ingestion (O(1) buffer overhead), tiered hybrid retrieval, and hardware-aware orchestration that enables competitive Recall@10 (0.48-0.97 across BEIR domains) within a hard 15.5 GB RAM ceiling. The 37,000-line codebase achieves retrieval latencies of 185 ms (p50) on C1,300 laptops while maintaining data minimization through local-only processing aligned with GDPR Art. 5(1)(c). Evaluation on BEIR benchmarks validates practical deployability for small-to-medium professional archives. The codebase is publicly available at https://github.com/PaoloAstrino/CUBO.

</details>


### [73] [Context Compression via Explicit Information Transmission](https://arxiv.org/abs/2602.03784)
*Jiangnan Ye,Hanqi Yan,Zhenyi Shen,Heng Chang,Ye Mao,Yulan He*

Main category: cs.CL

TL;DR: ComprExIT提出了一种新的软上下文压缩框架，通过显式信息传输在冻结的LLM隐藏状态上进行压缩，解决了现有方法中渐进覆盖和容量分配不协调的问题，在6个QA基准上优于SOTA方法，仅增加约1%参数。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理成本高昂（二次注意力、KV缓存增长），需要上下文压缩。现有方法通常将LLM本身作为可训练压缩器，依赖逐层自注意力迭代聚合信息，但存在两个结构限制：1）跨层的渐进表示覆盖；2）跨token的压缩容量分配不协调。

Method: 提出ComprExIT框架，将软压缩重新定义为新范式：在冻结的LLM隐藏状态上进行显式信息传输。包含两个核心机制：1）深度方向传输：选择性将多层信息传输到token锚点，缓解渐进覆盖；2）宽度方向传输：通过全局优化的传输计划将锚点聚合到少量槽中，确保信息分配的协调性。

Result: 在六个问答基准测试中，ComprExIT始终优于最先进的上下文压缩方法，同时仅引入约1%的额外参数，表明显式和协调的信息传输能够实现更有效和鲁棒的长上下文压缩。

Conclusion: 通过将软压缩重新定义为在冻结LLM隐藏状态上的显式信息传输，ComprExIT解决了现有压缩方法的结构限制，实现了更有效和鲁棒的长上下文压缩，为实际应用提供了高效解决方案。

Abstract: Long-context inference with Large Language Models (LLMs) is costly due to quadratic attention and growing key-value caches, motivating context compression. In this work, we study soft context compression, where a long context is condensed into a small set of continuous representations. Existing methods typically re-purpose the LLM itself as a trainable compressor, relying on layer-by-layer self-attention to iteratively aggregate information. We argue that this paradigm suffers from two structural limitations: (i) progressive representation overwriting across layers (ii) uncoordinated allocation of compression capacity across tokens. We propose ComprExIT (Context Compression via Explicit Information Transmission), a lightweight framework that formulates soft compression into a new paradigm: explicit information transmission over frozen LLM hidden states. This decouples compression from the model's internal self-attention dynamics. ComprExIT performs (i) depth-wise transmission to selectively transmit multi-layer information into token anchors, mitigating progressive overwriting, and (ii) width-wise transmission to aggregate anchors into a small number of slots via a globally optimized transmission plan, ensuring coordinated allocation of information. Across six question-answering benchmarks, ComprExIT consistently outperforms state-of-the-art context compression methods while introducing only ~1% additional parameters, demonstrating that explicit and coordinated information transmission enables more effective and robust long-context compression.

</details>


### [74] [They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References](https://arxiv.org/abs/2602.03822)
*Sahil Tripathi,Gautam Siddharth Kashyap,Mehwish Nasim,Jian Yang,Jiechao Gao,Usman Naseem*

Main category: cs.CL

TL;DR: CROSS-ALIGN+是一个三阶段框架，通过结构化知识增强、参数高效适配器和级联解释生成，解决模因社交滥用检测中的文化盲区、边界模糊和可解释性不足问题，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 模因社交滥用检测面临三大挑战：1）文化盲区（忽略文化符号背景）；2）边界模糊（难以区分讽刺与滥用）；3）缺乏可解释性（模型决策不透明）。现有方法（融合方法和LVLMs上下文学习）在这些方面仍有局限。

Method: CROSS-ALIGN+采用三阶段框架：第一阶段使用ConceptNet、Wikidata和Hatebase的结构化知识增强多模态表示，缓解文化盲区；第二阶段通过参数高效的LoRA适配器锐化决策边界，减少边界模糊；第三阶段生成级联解释，增强可解释性。

Result: 在5个基准测试和8个大型视觉语言模型上的实验表明，CROSS-ALIGN+持续优于最先进方法，相对F1分数提升高达17%，同时为每个决策提供可解释的推理过程。

Conclusion: CROSS-ALIGN+通过系统性地解决文化盲区、边界模糊和可解释性三大挑战，显著提升了模因社交滥用检测的性能和透明度，为实际应用提供了更可靠的解决方案。

Abstract: Meme-based social abuse detection is challenging because harmful intent often relies on implicit cultural symbolism and subtle cross-modal incongruence. Prior approaches, from fusion-based methods to in-context learning with Large Vision-Language Models (LVLMs), have made progress but remain limited by three factors: i) cultural blindness (missing symbolic context), ii) boundary ambiguity (satire vs. abuse confusion), and iii) lack of interpretability (opaque model reasoning). We introduce CROSS-ALIGN+, a three-stage framework that systematically addresses these limitations: (1) Stage I mitigates cultural blindness by enriching multimodal representations with structured knowledge from ConceptNet, Wikidata, and Hatebase; (2) Stage II reduces boundary ambiguity through parameter-efficient LoRA adapters that sharpen decision boundaries; and (3) Stage III enhances interpretability by generating cascaded explanations. Extensive experiments on five benchmarks and eight LVLMs demonstrate that CROSS-ALIGN+ consistently outperforms state-of-the-art methods, achieving up to 17% relative F1 improvement while providing interpretable justifications for each decision.

</details>


### [75] [Accelerating Scientific Research with Gemini: Case Studies and Common Techniques](https://arxiv.org/abs/2602.03837)
*David P. Woodruff,Vincent Cohen-Addad,Lalit Jain,Jieming Mao,Song Zuo,MohammadHossein Bateni,Simina Branzei,Michael P. Brenner,Lin Chen,Ying Feng,Lance Fortnow,Gang Fu,Ziyi Guan,Zahra Hadizadeh,Mohammad T. Hajiaghayi,Mahdi JafariRaviz,Adel Javanmard,Karthik C. S.,Ken-ichi Kawarabayashi,Ravi Kumar,Silvio Lattanzi,Euiwoong Lee,Yi Li,Ioannis Panageas,Dimitris Paparas,Benjamin Przybocki,Bernardo Subercaseaux,Ola Svensson,Shayan Taherijam,Xuan Wu,Eylon Yogev,Morteza Zadimoghaddam,Samson Zhou,Vahab Mirrokni*

Main category: cs.CL

TL;DR: 研究人员展示了如何与先进AI模型（特别是Google的Gemini系列）合作解决理论计算机科学等领域的开放问题、反驳猜想并生成新证明，总结了有效人机协作的技术。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在辅助常规任务方面表现出色，但它们在参与专家级数学发现方面的能力尚不明确。本文旨在探索AI模型如何成为科学发现中真正的创造性合作伙伴。

Method: 通过一系列案例研究，展示研究人员如何与Gemini模型协作，采用迭代精炼、问题分解、跨学科知识转移等技术。同时探索超越标准聊天界面的方法，如将模型作为严格对抗性评审员检测证明缺陷，以及嵌入"神经符号"循环中自主编写和执行代码验证推导。

Result: 成功解决了理论计算机科学、经济学、优化和物理学等领域的开放问题，反驳了猜想并生成了新证明。证明了AI不仅可以自动化任务，还能成为科学发现过程中真正的创造性合作伙伴。

Conclusion: AI模型具有作为科学发现创造性合作伙伴的潜力，有效的人机协作需要特定的技术方法。超越标准聊天界面的创新交互方式能进一步释放AI在理论研究中的价值。

Abstract: Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google's Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a "neuro-symbolic" loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.

</details>


### [76] [Parallel-Probe: Towards Efficient Parallel Thinking via 2D Probing](https://arxiv.org/abs/2602.03845)
*Tong Zheng,Chengsong Huang,Runpeng Dai,Yun He,Rui Liu,Xin Ni,Huiwen Bao,Kaishen Wang,Hongtu Zhu,Jiaxin Huang,Furong Huang,Heng Huang*

Main category: cs.CL

TL;DR: 提出Parallel-Probe方法，通过2D探测技术优化并行推理，减少35.8%的序列token和25.8%的总token成本，同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 并行推理虽然是有前景的范式，但计算负担重。现有效率方法主要依赖局部轨迹信号，缺乏利用并行分支间全局动态的机制。

Method: 引入2D探测接口揭示并行推理的宽度-深度动态，定期从所有分支获取中间答案。基于此开发Parallel-Probe控制器，采用共识早期停止调节推理深度，基于偏差的分支剪枝动态调整宽度。

Result: 在三个基准测试和多个模型上的实验表明，Parallel-Probe建立了优越的测试时扩展帕累托前沿。相比标准多数投票，减少35.8%序列token和25.8%总token成本，同时保持竞争力准确率。

Conclusion: Parallel-Probe通过有效利用并行推理的全局动态，实现了计算效率的显著提升，为优化并行推理系统提供了新方法。

Abstract: Parallel thinking has emerged as a promising paradigm for reasoning, yet it imposes significant computational burdens. Existing efficiency methods primarily rely on local, per-trajectory signals and lack principled mechanisms to exploit global dynamics across parallel branches. We introduce 2D probing, an interface that exposes the width-depth dynamics of parallel thinking by periodically eliciting intermediate answers from all branches. Our analysis reveals three key insights: non-monotonic scaling across width-depth allocations, heterogeneous reasoning branch lengths, and early stabilization of global consensus. Guided by these insights, we introduce $\textbf{Parallel-Probe}$, a training-free controller designed to optimize online parallel thinking. Parallel-Probe employs consensus-based early stopping to regulate reasoning depth and deviation-based branch pruning to dynamically adjust width. Extensive experiments across three benchmarks and multiple models demonstrate that Parallel-Probe establishes a superior Pareto frontier for test-time scaling. Compared to standard majority voting, it reduces sequential tokens by up to $\textbf{35.8}$% and total token cost by over $\textbf{25.8}$% while maintaining competitive accuracy.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [77] [HMVLA: Hyperbolic Multimodal Fusion for Vision-Language-Action Models](https://arxiv.org/abs/2602.02533)
*Kun Wang,Xiao Feng,Mingcheng Qu,Tonghua Su*

Main category: cs.RO

TL;DR: HMVLA是一个新颖的视觉语言动作框架，利用双曲空间嵌入层次结构特征，并引入稀疏门控的专家混合机制，以解决VLA领域中的语义对齐挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的VLA方法通常直接微调预训练的视觉语言模型，将语义和视觉特征直接输入策略网络，未能充分解决VLA领域中独特的语义对齐挑战。需要更有效地建模图像文本数据中的层次关系。

Method: 提出HMVLA框架：1）在双曲空间中嵌入多模态特征，而非传统的欧几里得空间，以更有效地建模层次关系；2）引入专门为语义对齐设计的稀疏门控专家混合机制，增强图像与文本之间的多模态理解并提高效率。

Result: 大量实验表明，HMVLA在准确性和泛化能力方面均超越了基线方法。通过重构数据集进一步验证了其跨域适应性的鲁棒性。

Conclusion: HMVLA通过双曲空间嵌入和稀疏门控MoE机制，有效解决了VLA领域的语义对齐问题，在精度、泛化和跨域适应性方面表现出色，为视觉语言动作模型的发展提供了新思路。

Abstract: Vision Language Action (VLA) models have recently shown great potential in bridging multimodal perception with robotic control. However, existing methods often rely on direct fine-tuning of pre-trained Vision-Language Models (VLMs), feeding semantic and visual features directly into a policy network without fully addressing the unique semantic alignment challenges in the VLA domain. In this paper, we propose HMVLA, a novel VLA framework that exploits the inherent hierarchical structures in vision and language for comprehensive semantic alignment. Unlike traditional methods that perform alignment in Euclidean space, our HMVLA embeds multimodal features in hyperbolic space, enabling more effective modeling of the hierarchical relationships present in image text data. Furthermore, we introduce a sparsely gated Mixture of Experts (MoE) mechanism tailored for semantic alignment, which enhances multimodal comprehension between images and text while improving efficiency. Extensive experiments demonstrate that HMVLA surpasses baseline methods in both accuracy and generalization. In addition, we validate its robustness by reconstructing datasets to further test cross domain adaptability.

</details>


### [78] [StepNav: Structured Trajectory Priors for Efficient and Multimodal Visual Navigation](https://arxiv.org/abs/2602.02590)
*Xubo Luo,Aodi Wu,Haodong Han,Xue Wan,Wei Zhang,Leizheng Shu,Ruisuo Wang*

Main category: cs.RO

TL;DR: StepNav提出了一种基于结构化多模态轨迹先验的视觉导航框架，通过几何感知的成功概率场识别可行走廊，构建多模态混合先验，再通过条件流匹配和最优控制进行轨迹优化，相比传统生成模型能产生更安全高效的导航轨迹。


<details>
  <summary>Details</summary>
Motivation: 当前基于生成模型的视觉导航方法依赖非结构化噪声先验，导致生成的轨迹不安全、效率低、单模态且无法满足实时要求，需要一种能产生可靠、安全、高效多模态轨迹的解决方案。

Method: 1) 学习几何感知的成功概率场识别所有可行导航走廊；2) 构建显式的多模态混合先验初始化条件流匹配过程；3) 将轨迹优化表述为带有显式平滑性和安全性正则化的最优控制问题。

Result: 在仿真和真实世界基准测试中，StepNav在鲁棒性、效率和安全性方面一致优于最先进的生成规划器，用更少的步骤生成更安全高效的轨迹。

Conclusion: StepNav通过用物理基础的结构化多模态先验替代非结构化噪声，显著提升了自主导航中轨迹生成的可靠性，为实际应用提供了更实用的解决方案。

Abstract: Visual navigation is fundamental to autonomous systems, yet generating reliable trajectories in cluttered and uncertain environments remains a core challenge. Recent generative models promise end-to-end synthesis, but their reliance on unstructured noise priors often yields unsafe, inefficient, or unimodal plans that cannot meet real-time requirements. We propose StepNav, a novel framework that bridges this gap by introducing structured, multimodal trajectory priors derived from variational principles. StepNav first learns a geometry-aware success probability field to identify all feasible navigation corridors. These corridors are then used to construct an explicit, multi-modal mixture prior that initializes a conditional flow-matching process. This refinement is formulated as an optimal control problem with explicit smoothness and safety regularization. By replacing unstructured noise with physically-grounded candidates, StepNav generates safer and more efficient plans in significantly fewer steps. Experiments in both simulation and real-world benchmarks demonstrate consistent improvements in robustness, efficiency, and safety over state-of-the-art generative planners, advancing reliable trajectory generation for practical autonomous navigation. The code has been released at https://github.com/LuoXubo/StepNav.

</details>


### [79] [AROLA: A Modular Layered Architecture for Scaled Autonomous Racing](https://arxiv.org/abs/2602.02730)
*Fam Shihata,Mohammed Abdelazim,Ahmed Hussein*

Main category: cs.RO

TL;DR: AROLA是一个模块化、分层的自动驾驶赛车软件架构，通过标准化ROS 2接口实现组件互换和性能评估，配合Race Monitor框架进行实时性能监控和分析。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶赛车领域发展迅速，但现有软件架构存在碎片化和单体化问题，缺乏模块化和标准化接口，导致开发效率低、可复现性差。

Method: 提出AROLA模块化分层架构，将自动驾驶流水线分解为感知、预处理、定位与建图、规划、行为、控制等可互换组件，使用标准化ROS 2接口连接；同时开发Race Monitor框架实时记录圈速、轨迹质量和计算负载。

Result: 在RoboRacer平台仿真和硬件上验证了AROLA架构，包括在2025 RoboRacer IV25竞赛中的实际部署，证明模块化架构和系统化评估能加速开发并提高可复现性。

Conclusion: AROLA和Race Monitor展示了模块化设计、透明接口和系统化评估能够显著提升自动驾驶赛车领域的开发效率和可复现性，为规模化应用奠定基础。

Abstract: Autonomous racing has advanced rapidly, particularly on scaled platforms, and software stacks must evolve accordingly. In this work, AROLA is introduced as a modular, layered software architecture in which fragmented and monolithic designs are reorganized into interchangeable layers and components connected through standardized ROS 2 interfaces. The autonomous-driving pipeline is decomposed into sensing, pre-processing, perception, localization and mapping, planning, behavior, control, and actuation, enabling rapid module replacement and objective benchmarking without reliance on custom message definitions. To support consistent performance evaluation, a Race Monitor framework is introduced as a lightweight system through which lap timing, trajectory quality, and computational load are logged in real time and standardized post-race analyses are generated. AROLA is validated in simulation and on hardware using the RoboRacer platform, including deployment at the 2025 RoboRacer IV25 competition. Together, AROLA and Race Monitor demonstrate that modularity, transparent interfaces, and systematic evaluation can accelerate development and improve reproducibility in scaled autonomous racing.

</details>


### [80] [PokeNet: Learning Kinematic Models of Articulated Objects from Human Observations](https://arxiv.org/abs/2602.02741)
*Anmol Gupta,Weiwei Gu,Omkar Patil,Jun Ki Lee,Nakul Gopalan*

Main category: cs.RO

TL;DR: PokeNet是一个端到端框架，通过单次人类演示估计未知物体的关节模型，无需先验知识，在关节轴和状态估计精度上平均提升27%以上。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在多个问题：依赖物体先验知识（关节数量/类型）、无法恢复交互中才显露的遮挡关节、需要大量多视角图像、忽略操作顺序（对多自由度物体如洗碗机很重要）。需要一种从单次演示中学习关节模型的方法。

Method: PokeNet是端到端框架，给定人类操作未知物体的点云观测序列，预测关节参数、推断操作顺序、跟踪关节状态随时间变化。无需物体先验知识，仅需单次演示。

Result: 在模拟和真实环境中，PokeNet在多样化物体（包括新颖和未见类别）上平均提升关节轴和状态估计精度27%以上，优于现有最先进方法。

Conclusion: PokeNet能够从单次人类演示中有效学习未知物体的关节模型，解决了现有方法的局限性，为机器人操作多自由度物体提供了实用解决方案。

Abstract: Articulation modeling enables robots to learn joint parameters of articulated objects for effective manipulation which can then be used downstream for skill learning or planning. Existing approaches often rely on prior knowledge about the objects, such as the number or type of joints. Some of these approaches also fail to recover occluded joints that are only revealed during interaction. Others require large numbers of multi-view images for every object, which is impractical in real-world settings. Furthermore, prior works neglect the order of manipulations, which is essential for many multi-DoF objects where one joint must be operated before another, such as a dishwasher. We introduce PokeNet, an end-to-end framework that estimates articulation models from a single human demonstration without prior object knowledge. Given a sequence of point cloud observations of a human manipulating an unknown object, PokeNet predicts joint parameters, infers manipulation order, and tracks joint states over time. PokeNet outperforms existing state-of-the-art methods, improving joint axis and state estimation accuracy by an average of over 27% across diverse objects, including novel and unseen categories. We demonstrate these gains in both simulation and real-world environments.

</details>


### [81] [Bimanual High-Density EMG Control for In-Home Mobile Manipulation by a User with Quadriplegia](https://arxiv.org/abs/2602.02773)
*Jehan Yang,Eleanor Hodgson,Cindy Sun,Zackory Erickson,Doug Weber*

Main category: cs.RO

TL;DR: 开发首个基于双前臂高密度肌电图的移动机械臂控制系统，使四肢瘫痪患者能在家庭环境中通过手势控制机器人完成日常家务任务。


<details>
  <summary>Details</summary>
Motivation: 颈椎脊髓损伤患者因瘫痪无法使用传统机器人控制界面（如操纵杆或键盘），需要开发新的控制方式使其能够独立完成家庭日常任务。

Method: 1) 开发定制织物集成HDEMG前臂袖套，捕捉瘫痪肢体的残余神经肌肉活动；2) 集成视觉、语言和运动规划模块，建立共享自主框架；3) 进行为期12天的家庭用户研究评估系统实际使用效果。

Result: 成功开发并部署了首个基于双前臂HDEMG的移动机械臂控制系统，使四肢瘫痪患者能够在真实家庭环境中有效控制机器人完成日常生活活动。

Conclusion: 该系统通过可穿戴肌电接口和共享自主框架的结合，为四肢瘫痪患者提供了有效的机器人控制解决方案，支持其在家庭环境中独立完成日常任务。

Abstract: Mobile manipulators in the home can enable people with cervical spinal cord injury (cSCI) to perform daily physical household tasks that they could not otherwise do themselves. However, paralysis in these users often limits access to traditional robot control interfaces such as joysticks or keyboards. In this work, we introduce and deploy the first system that enables a user with quadriplegia to control a mobile manipulator in their own home using bimanual high-density electromyography (HDEMG). We develop a pair of custom, fabric-integrated HDEMG forearm sleeves, worn on both arms, that capture residual neuromotor activity from clinically paralyzed degrees of freedom and support real-time gesture-based robot control. Second, by integrating vision, language, and motion planning modules, we introduce a shared autonomy framework that supports robust and user-driven teleoperation, with particular benefits for navigation-intensive tasks in home environments. Finally, to demonstrate the system in the wild, we present a twelve-day in-home user study evaluating real-time use of the wearable EMG interface for daily robot control. Together, these system components enable effective robot control for performing activities of daily living and other household tasks in a real home environment.

</details>


### [82] [Adaptive Linear Path Model-Based Diffusion](https://arxiv.org/abs/2602.02831)
*Yutaka Shimizu,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: LP-MBD使用线性概率路径替代方差保持调度，简化扩散模型控制参数调优；ALP-MBD通过强化学习自适应调整扩散步数和噪声水平，提升鲁棒性和实时效率。


<details>
  <summary>Details</summary>
Motivation: 基于扩散模型的模型控制方法在机器人控制中表现出色，但其性能对调度参数选择极为敏感，参数调优成为主要挑战。需要更稳定、易调优且能自适应环境变化的扩散控制方法。

Method: 1) LP-MBD：用流匹配启发的线性概率路径替代方差保持调度，获得几何可解释且解耦的参数化；2) ALP-MBD：在LP-MBD基础上，利用强化学习根据任务复杂度和环境条件自适应调整扩散步数和噪声水平。

Result: 在数值研究、Brax基准测试和移动机器人轨迹跟踪实验中，LP-MBD简化了调度同时保持强大性能，ALP-MBD进一步提高了鲁棒性、适应性和实时效率。

Conclusion: LP-MBD提供了稳定且易调优的扩散控制框架，ALP-MBD通过自适应机制进一步增强了系统在复杂动态环境中的性能，为基于扩散模型的机器人控制提供了更实用的解决方案。

Abstract: The interest in combining model-based control approaches with diffusion models has been growing. Although we have seen many impressive robotic control results in difficult tasks, the performance of diffusion models is highly sensitive to the choice of scheduling parameters, making parameter tuning one of the most critical challenges. We introduce Linear Path Model-Based Diffusion (LP-MBD), which replaces the variance-preserving schedule with a flow-matching-inspired linear probability path. This yields a geometrically interpretable and decoupled parameterization that reduces tuning complexity and provides a stable foundation for adaptation. Building on this, we propose Adaptive LP-MBD (ALP-MBD), which leverages reinforcement learning to adjust diffusion steps and noise levels according to task complexity and environmental conditions. Across numerical studies, Brax benchmarks, and mobile-robot trajectory tracking, LP-MBD simplifies scheduling while maintaining strong performance, and ALP-MBD further improves robustness, adaptability, and real-time efficiency.

</details>


### [83] [Language Movement Primitives: Grounding Language Models in Robot Motion](https://arxiv.org/abs/2602.02839)
*Yinlong Dai,Benjamin A. Christie,Daniel J. Evans,Dylan P. Losey,Simon Stepputtis*

Main category: cs.RO

TL;DR: 提出Language Movement Primitives (LMPs)框架，将视觉语言模型的推理能力与动态运动原语参数化相结合，实现零样本机器人操作任务


<details>
  <summary>Details</summary>
Motivation: 当前机器人操作面临挑战：大型视觉语言模型能理解场景和分解任务步骤，但难以将这些步骤转化为具体的机器人运动；而机器人基础模型需要领域微调才能执行新任务。核心问题是如何连接抽象任务推理与低级运动控制

Method: 提出LMPs框架，将VLM的推理能力与DMP参数化相结合。DMP提供少量可解释参数，VLM可以设置这些参数来生成多样化、连续且稳定的轨迹。构建基于VLM和DMP的LMP管道，通过生成一系列DMP运动来完成桌面操作任务

Result: 在20个真实世界操作任务中，LMP实现了80%的任务成功率，而最佳基线方法只有31%的成功率

Conclusion: LMP框架成功地将高级任务推理与低级位置和速度控制连接起来，通过将自然语言任务描述语义地映射到DMP参数，实现了零样本机器人操作，显著提高了任务成功率

Abstract: Enabling robots to perform novel manipulation tasks from natural language instructions remains a fundamental challenge in robotics, despite significant progress in generalized problem solving with foundational models. Large vision and language models (VLMs) are capable of processing high-dimensional input data for visual scene and language understanding, as well as decomposing tasks into a sequence of logical steps; however, they struggle to ground those steps in embodied robot motion. On the other hand, robotics foundation models output action commands, but require in-domain fine-tuning or experience before they are able to perform novel tasks successfully. At its core, there still remains the fundamental challenge of connecting abstract task reasoning with low-level motion control. To address this disconnect, we propose Language Movement Primitives (LMPs), a framework that grounds VLM reasoning in Dynamic Movement Primitive (DMP) parameterization. Our key insight is that DMPs provide a small number of interpretable parameters, and VLMs can set these parameters to specify diverse, continuous, and stable trajectories. Put another way: VLMs can reason over free-form natural language task descriptions, and semantically ground their desired motions into DMPs -- bridging the gap between high-level task reasoning and low-level position and velocity control. Building on this combination of VLMs and DMPs, we formulate our LMP pipeline for zero-shot robot manipulation that effectively completes tabletop manipulation problems by generating a sequence of DMP motions. Across 20 real-world manipulation tasks, we show that LMP achieves 80% task success as compared to 31% for the best-performing baseline. See videos at our website: https://collab.me.vt.edu/lmp

</details>


### [84] [Kino-PAX$^+$: Near-Optimal Massively Parallel Kinodynamic Sampling-based Motion Planner](https://arxiv.org/abs/2602.02846)
*Nicolas Perrault,Qi Heng Ho,Morteza Lahijanian*

Main category: cs.RO

TL;DR: Kino-PAX⁺ 是一种大规模并行的运动规划算法，在保持概率完备性的同时实现渐进近最优性，比现有串行方法快三个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有的基于采样的运动规划器（SBMPs）由于串行计算设计难以实现实时性能，而现有的并行化方法虽然加速了可行解的寻找，但不能保证目标函数的最优化。

Method: 将传统串行操作分解为三个大规模并行子程序：稀疏树构建、局部邻域内最有希望节点的传播和细化，通过并行计算快速改进解的成本。

Result: Kino-PAX⁺ 比现有串行方法快三个数量级，且比最先进的GPU规划器获得更低的解成本，同时保持概率δ-鲁棒完备性和渐进δ-鲁棒近最优性。

Conclusion: 该算法通过大规模并行化和聚焦最有希望节点的策略，在保持理论保证的同时实现了显著的性能提升，为实时运动规划提供了有效解决方案。

Abstract: Sampling-based motion planners (SBMPs) are widely used for robot motion planning with complex kinodynamic constraints in high-dimensional spaces, yet they struggle to achieve \emph{real-time} performance due to their serial computation design. Recent efforts to parallelize SBMPs have achieved significant speedups in finding feasible solutions; however, they provide no guarantees of optimizing an objective function. We introduce Kino-PAX$^{+}$, a massively parallel kinodynamic SBMP with asymptotic near-optimal guarantees. Kino-PAX$^{+}$ builds a sparse tree of dynamically feasible trajectories by decomposing traditionally serial operations into three massively parallel subroutines. The algorithm focuses computation on the most promising nodes within local neighborhoods for propagation and refinement, enabling rapid improvement of solution cost. We prove that, while maintaining probabilistic $δ$-robust completeness, this focus on promising nodes ensures asymptotic $δ$-robust near-optimality. Our results show that Kino-PAX$^{+}$ finds solutions up to three orders of magnitude faster than existing serial methods and achieves lower solution costs than a state-of-the-art GPU-based planner.

</details>


### [85] [Latent Perspective-Taking via a Schrödinger Bridge in Influence-Augmented Local Models](https://arxiv.org/abs/2602.02857)
*Kevin Alcedo,Pedro U. Lima,Rachid Alami*

Main category: cs.RO

TL;DR: 该论文提出了一种学习结构化心智模型和他人心智状态估计器的方法，通过影响增强局部模型分解社会感知机器人任务，使用神经符号世界模型和视角转换算子，在模型强化学习中实现社会感知策略。


<details>
  <summary>Details</summary>
Motivation: 机器人在人类环境中操作需要在不确定性下做决策，不仅要考虑外部动态，还要推理他人的隐藏心智模型和心智状态。现有方法如交互式POMDP和贝叶斯心智理论虽然理论严谨，但精确的嵌套信念推理计算不可行，且手工指定的模型在开放世界中脆弱。

Method: 基于影响抽象理论，实例化影响增强局部模型，将社会感知机器人任务分解为局部动态、社会影响和外部因素。提出：(a)神经符号世界模型实例化因子化离散动态贝叶斯网络；(b)视角转换算子建模为学习局部动态上的摊销薛定谔桥，将因子化自我中心信念转换为他人中心信念。

Result: 该架构使智能体能够在模型强化学习中通过决策时心智状态规划（信念空间中的薛定谔桥）合成社会感知策略，在MiniGrid社交导航任务中展示了初步结果。

Conclusion: 该方法通过学习结构化心智模型和心智状态估计器，解决了精确嵌套信念推理的不可行性和手工模型脆弱性问题，为机器人在人类环境中进行社会感知决策提供了可扩展的解决方案。

Abstract: Operating in environments alongside humans requires robots to make decisions under uncertainty. In addition to exogenous dynamics, they must reason over others' hidden mental-models and mental-states. While Interactive POMDPs and Bayesian Theory of Mind formulations are principled, exact nested-belief inference is intractable, and hand-specified models are brittle in open-world settings. We address both by learning structured mental-models and an estimator of others' mental-states. Building on the Influence-Based Abstraction, we instantiate an Influence-Augmented Local Model to decompose socially-aware robot tasks into local dynamics, social influences, and exogenous factors. We propose (a) a neuro-symbolic world model instantiating a factored, discrete Dynamic Bayesian Network, and (b) a perspective-shift operator modeled as an amortized Schrödinger Bridge over the learned local dynamics that transports factored egocentric beliefs into other-centric beliefs. We show that this architecture enables agents to synthesize socially-aware policies in model-based reinforcement learning, via decision-time mental-state planning (a Schrödinger Bridge in belief space), with preliminary results in a MiniGrid social navigation task.

</details>


### [86] [IMAGINE: Intelligent Multi-Agent Godot-based Indoor Networked Exploration](https://arxiv.org/abs/2602.02858)
*Tiago Leite,Maria Conceição,António Grilo*

Main category: cs.RO

TL;DR: 使用多智能体强化学习在GNSS拒止环境中实现无人机协同自主探索，通过高保真仿真和连续动作空间训练协作行为。


<details>
  <summary>Details</summary>
Motivation: 解决GNSS拒止环境下无人机群体自主探索的协调、感知和分散决策挑战，克服现有研究中离散动作、集中式控制、先验知识依赖等限制。

Method: 采用多智能体强化学习，基于网络分布式部分可观测马尔可夫决策过程，在Godot游戏引擎中进行高保真仿真，使用LiDAR传感器和局部地图共享，实施课程学习（五个复杂度递增的级别）。

Result: 可扩展的训练范式结合简化架构实现了室内区域的快速自主探索，课程学习使训练更快更鲁棒，为物理机器人系统部署学习到的协作策略奠定了基础。

Conclusion: 高保真仿真、MARL公式化和计算效率的结合为在物理机器人系统中部署学习到的协作策略建立了坚实基础，解决了先前研究的多个关键限制。

Abstract: The exploration of unknown, Global Navigation Satellite System (GNSS) denied environments by an autonomous communication-aware and collaborative group of Unmanned Aerial Vehicles (UAVs) presents significant challenges in coordination, perception, and decentralized decision-making. This paper implements Multi-Agent Reinforcement Learning (MARL) to address these challenges in a 2D indoor environment, using high-fidelity game-engine simulations (Godot) and continuous action spaces. Policy training aims to achieve emergent collaborative behaviours and decision-making under uncertainty using Network-Distributed Partially Observable Markov Decision Processes (ND-POMDPs). Each UAV is equipped with a Light Detection and Ranging (LiDAR) sensor and can share data (sensor measurements and a local occupancy map) with neighbouring agents. Inter-agent communication constraints include limited range, bandwidth and latency. Extensive ablation studies evaluated MARL training paradigms, reward function, communication system, neural network (NN) architecture, memory mechanisms, and POMDP formulations. This work jointly addresses several key limitations in prior research, namely reliance on discrete actions, single-agent or centralized formulations, assumptions of a priori knowledge and permanent connectivity, inability to handle dynamic obstacles, short planning horizons and architectural complexity in Recurrent NNs/Transformers. Results show that the scalable training paradigm, combined with a simplified architecture, enables rapid autonomous exploration of an indoor area. The implementation of Curriculum-Learning (five increasingly complex levels) also enabled faster, more robust training. This combination of high-fidelity simulation, MARL formulation, and computational efficiency establishes a strong foundation for deploying learned cooperative strategies in physical robotic systems.

</details>


### [87] [Accelerating Structured Chain-of-Thought in Autonomous Vehicles](https://arxiv.org/abs/2602.02864)
*Yi Gu,Yan Wang,Yuxiao Chen,Yurong You,Wenjie Luo,Yue Wang,Wenhao Ding,Boyi Li,Heng Yang,Boris Ivanovic,Marco Pavone*

Main category: cs.RO

TL;DR: FastDriveCoT：一种并行解码方法，通过将思维链推理分解为可并行执行的子任务，显著加速自动驾驶中的CoT推理，实现3-4倍加速而不损失性能。


<details>
  <summary>Details</summary>
Motivation: 思维链（CoT）推理虽然能提升自动驾驶中视觉-语言-动作模型的决策能力，但其自回归特性导致推理延迟过高，无法满足实时应用需求。

Method: 提出FastDriveCoT方法，将模板化CoT推理过程分解为依赖关系图，识别关键对象、总结交通规则等子任务可以并行生成，在单次前向传播中同时生成多个独立推理步骤。

Result: 实验表明，CoT生成速度提升3-4倍，端到端延迟显著降低，同时保持了CoT推理带来的下游任务性能改进。

Conclusion: FastDriveCoT通过并行解码有效解决了CoT推理的延迟问题，为实时自动驾驶应用提供了可行的解决方案。

Abstract: Chain-of-Thought (CoT) reasoning enhances the decision-making capabilities of vision-language-action models in autonomous driving, but its autoregressive nature introduces significant inference latency, making it impractical for real-time applications. To address this, we introduce FastDriveCoT, a novel parallel decoding method that accelerates template-structured CoT. Our approach decomposes the reasoning process into a dependency graph of distinct sub-tasks, such as identifying critical objects and summarizing traffic rules, some of which can be generated in parallel. By generating multiple independent reasoning steps concurrently within a single forward pass, we significantly reduce the number of sequential computations. Experiments demonstrate a 3-4$\times$ speedup in CoT generation and a substantial reduction in end-to-end latency across various model architectures, all while preserving the original downstream task improvements brought by incorporating CoT reasoning.

</details>


### [88] [Moving On, Even When You're Broken: Fail-Active Trajectory Generation via Diffusion Policies Conditioned on Embodiment and Task](https://arxiv.org/abs/2602.02895)
*Gilberto G. Briscoe-Martinez,Yaashia Gautam,Rahul Shetty,Anuj Pasricha,Marco M. Nicotra,Alessandro Roncone*

Main category: cs.RO

TL;DR: DEFT：基于扩散模型的轨迹生成器，用于机器人在执行器故障下的主动失效操作，实现任务完成


<details>
  <summary>Details</summary>
Motivation: 机器人故障需要人工干预恢复，影响系统运行。目标是实现故障下的主动操作（fail-active operation），即在受损情况下仍能安全完成任务

Method: 提出DEFT，一种基于扩散模型的轨迹生成器，以机器人当前本体状态和任务约束为条件。该方法能泛化到不同类型故障，支持约束和无约束运动，实现任意故障下的任务完成

Result: 在仿真中，DEFT在数千个关节故障案例中表现优于基线达2倍；对未见过的故障类型仍能超越基线，显示良好的泛化能力。真实世界实验中，在抽屉操作和白板擦除任务中，DEFT成功完成传统方法失败的任务

Conclusion: DEFT能够在任意故障配置下实现主动失效操作，并在真实部署中有效工作，为机器人故障恢复提供了新解决方案

Abstract: Robot failure is detrimental and disruptive, often requiring human intervention to recover. Maintaining safe operation under impairment to achieve task completion, i.e. fail-active operation, is our target. Focusing on actuation failures, we introduce DEFT, a diffusion-based trajectory generator conditioned on the robot's current embodiment and task constraints. DEFT generalizes across failure types, supports constrained and unconstrained motions, and enables task completion under arbitrary failure. We evaluated DEFT in both simulation and real-world scenarios using a 7-DoF robotic arm. In simulation over thousands of joint-failure cases across multiple tasks, DEFT outperformed the baseline by up to 2 times. On failures unseen during training, it continued to outperform the baseline, indicating robust generalization in simulation. Further, we performed real-world evaluations on two multi-step tasks, drawer manipulation and whiteboard erasing. These experiments demonstrated DEFT succeeding on tasks where classical methods failed. Our results show that DEFT achieves fail-active manipulation across arbitrary failure configurations and real-world deployments.

</details>


### [89] [Modular Isoperimetric Soft Robotic Truss for Lunar Applications](https://arxiv.org/abs/2602.02915)
*Mihai Stanciu,Isaac Weaver,Adam Rose,James Wade,Kaden Paxton,Chris Paul,Spencer Stowell,Nathan Usevitch*

Main category: cs.RO

TL;DR: 提出一种用于月球应用的大型可重构机器人系统，采用充气织物管构成三角形桁架结构，通过球形关节连接多个三角形，实现1:18.3的收展体积比，无需额外压缩空气即可改变形状，展示了太阳能阵列和移动装置两种应用。


<details>
  <summary>Details</summary>
Motivation: 解决月球和未来太空任务中可持续运行的关键挑战，需要轻量化、模块化、可重构的机器人系统，能够适应不同任务需求并实现高效部署。

Method: 采用充气织物管构成三角形桁架单元，通过两个滚轮单元和连接单元形成三角形结构。新开发的球形关节允许最多三个三角形在顶点连接。滚轮单元夹紧管道形成有效关节，电机驱动滚轮沿管道移动改变形状，保持周长恒定（等周变形）。

Result: 系统实现1:18.3的收展体积比，无需额外压缩空气即可实现形状变化。展示了12自由度的太阳能阵列（倾斜60度，旋转360度）和14自由度的移动装置（采用步进滑动步态）。

Conclusion: 这种模块化、形状自适应的系统为可持续月球操作和未来太空任务提供了创新解决方案，具有轻量化、可重构和高效部署的优势。

Abstract: We introduce a large-scale robotic system designed as a lightweight, modular, and reconfigurable structure for lunar applications. The system consists of truss-like robotic triangles formed by continuous inflated fabric tubes routed through two robotic roller units and a connecting unit. A newly developed spherical joint enables up to three triangles to connect at a vertex, allowing construction of truss assemblies beyond a single octahedron. When deflated, the triangles compact to approximately the volume of the roller units, achieving a stowed-to-deployed volume ratio of 1:18.3. Upon inflation, the roller units pinch the tubes, locally reducing bending stiffness to form effective joints. Electric motors then translate the roller units along the tube, shifting the pinch point by lengthening one edge while shortening another at the same rate, thereby preserving a constant perimeter (isoperimetric). This shape-changing process requires no additional compressed air, enabling untethered operation after initial inflation. We demonstrate the system as a 12-degree-of-freedom solar array capable of tilting up to 60 degrees and sweeping 360 degrees, and as a 14-degree-of-freedom locomotion device using a step-and-slide gait. This modular, shape-adaptive system addresses key challenges for sustainable lunar operations and future space missions.

</details>


### [90] [Embodiment-Aware Generalist Specialist Distillation for Unified Humanoid Whole-Body Control](https://arxiv.org/abs/2602.02960)
*Quanquan Peng,Yunfeng Lin,Yufei Xue,Jiangmiao Pang,Weinan Zhang*

Main category: cs.RO

TL;DR: EAGLE提出了一种迭代的通用-专家蒸馏框架，能够训练单一统一策略来控制多个异构人形机器人，无需为每个机器人单独调整奖励函数。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的人形机器人全身控制器通常针对单一机器人设计，难以适应不同机器人在动力学、自由度和运动学拓扑上的差异。同时，现有方法难以实现既能跨机器人迁移又能支持丰富行为（如蹲下、倾斜等）的通用策略。

Method: EAGLE采用迭代的通用-专家蒸馏框架：在每个循环中，从当前通用策略派生出针对特定机器人的专家策略，在各自机器人上进行精炼，然后将新技能通过蒸馏方式整合回通用策略中。这个过程在多个机器人数据集上重复进行直到性能收敛。

Result: 在仿真中测试了5个不同机器人，在真实世界中测试了4个机器人（包括Unitree H1、G1和Fourier N1）。定量评估显示，EAGLE相比其他方法具有更高的跟踪精度和鲁棒性。

Conclusion: EAGLE框架为实现可扩展的、面向机器人集群的人形机器人控制迈出了重要一步，能够训练出控制多个异构人形机器人的单一统一策略。

Abstract: Humanoid Whole-Body Controllers trained with reinforcement learning (RL) have recently achieved remarkable performance, yet many target a single robot embodiment. Variations in dynamics, degrees of freedom (DoFs), and kinematic topology still hinder a single policy from commanding diverse humanoids. Moreover, obtaining a generalist policy that not only transfers across embodiments but also supports richer behaviors-beyond simple walking to squatting, leaning-remains especially challenging. In this work, we tackle these obstacles by introducing EAGLE, an iterative generalist-specialist distillation framework that produces a single unified policy that controls multiple heterogeneous humanoids without per-robot reward tuning. During each cycle, embodiment-specific specialists are forked from the current generalist, refined on their respective robots, and new skills are distilled back into the generalist by training on the pooled embodiment set. Repeating this loop until performance convergence produces a robust Whole-Body Controller validated on robots such as Unitree H1, G1, and Fourier N1. We conducted experiments on five different robots in simulation and four in real-world settings. Through quantitative evaluations, EAGLE achieves high tracking accuracy and robustness compared to other methods, marking a step toward scalable, fleet-level humanoid control. See more details at https://eagle-wbc.github.io/

</details>


### [91] [RPL: Learning Robust Humanoid Perceptive Locomotion on Challenging Terrains](https://arxiv.org/abs/2602.03002)
*Yuanhang Zhang,Younggyo Seo,Juyue Chen,Yifu Yuan,Koushil Sreenath,Pieter Abbeel,Carmelo Sferrazza,Karen Liu,Rocky Duan,Guanya Shi*

Main category: cs.RO

TL;DR: RPL是一个两阶段训练框架，通过专家策略蒸馏到Transformer策略，实现人形机器人在复杂地形上的多方向运动，支持负载并利用多深度相机感知。


<details>
  <summary>Details</summary>
Motivation: 人形机器人的感知运动虽已取得进展，但在复杂地形上实现鲁棒的多方向运动仍未被充分探索。需要解决在具有挑战性的地形上实现多方向运动并保持负载鲁棒性的问题。

Method: 采用两阶段训练框架：1）使用特权高度图观测训练地形特定专家策略，掌握解耦的运动和操作技能；2）将专家策略蒸馏到基于Transformer的策略，利用多深度相机覆盖广视角。引入深度特征缩放和随机侧边掩码技术来鲁棒化多方向运动。开发高效多深度系统，在并行环境中对动态机器人网格和静态地形网格进行光线投射，相比现有模拟器深度渲染管线实现5倍加速。

Result: 在真实世界实验中展示了鲁棒的多方向负载运动（2kg），能够应对多种挑战性地形：20°斜坡、不同步长楼梯（22cm、25cm、30cm）以及25cm×25cm的踏石（间隔60cm）。

Conclusion: RPL框架成功实现了人形机器人在复杂地形上的鲁棒多方向运动，支持负载，通过两阶段训练和高效深度感知系统解决了现有方法的局限性。

Abstract: Humanoid perceptive locomotion has made significant progress and shows great promise, yet achieving robust multi-directional locomotion on complex terrains remains underexplored. To tackle this challenge, we propose RPL, a two-stage training framework that enables multi-directional locomotion on challenging terrains, and remains robust with payloads. RPL first trains terrain-specific expert policies with privileged height map observations to master decoupled locomotion and manipulation skills across different terrains, and then distills them into a transformer policy that leverages multiple depth cameras to cover a wide range of views. During distillation, we introduce two techniques to robustify multi-directional locomotion, depth feature scaling based on velocity commands and random side masking, which are critical for asymmetric depth observations and unseen widths of terrains. For scalable depth distillation, we develop an efficient multi-depth system that ray-casts against both dynamic robot meshes and static terrain meshes in massively parallel environments, achieving a 5-times speedup over the depth rendering pipelines in existing simulators while modeling realistic sensor latency, noise, and dropout. Extensive real-world experiments demonstrate robust multi-directional locomotion with payloads (2kg) across challenging terrains, including 20° slopes, staircases with different step lengths (22 cm, 25 cm, 30 cm), and 25 cm by 25 cm stepping stones separated by 60 cm gaps.

</details>


### [92] [Training and Simulation of Quadrupedal Robot in Adaptive Stair Climbing for Indoor Firefighting: An End-to-End Reinforcement Learning Approach](https://arxiv.org/abs/2602.03087)
*Baixiao Huang,Baiyu Huang,Yu Hou*

Main category: cs.RO

TL;DR: 提出两阶段端到端深度强化学习框架，让四足机器人能在复杂室内环境中自主导航并攀爬多种楼梯类型，实现火灾初期搜救任务。


<details>
  <summary>Details</summary>
Motivation: 室内火灾初期搜救需要四足机器人快速、全面地搜索受害者并监控易燃材料，但在复杂室内环境中的态势感知和快速攀爬不同楼梯仍是主要挑战。

Method: 采用两阶段端到端深度强化学习：第一阶段在Isaac Lab的金字塔楼梯地形训练攀爬技能；第二阶段将学习到的策略迁移到真实室内楼梯（直梯、L型梯、螺旋梯）进行训练，使用基于中心线的导航公式统一学习导航和运动控制。

Result: 开发了能适应不同楼梯形状的端到端RL方法，仅使用局部高度图感知就能实现策略在不同楼梯间的泛化，并对成功率、效率和失败模式进行了实证分析。

Conclusion: 两阶段端到端RL框架成功地将楼梯攀爬技能从抽象地形迁移到真实室内拓扑，中心线导航公式实现了导航和运动的统一学习，展示了仅凭局部感知就能在不同楼梯间泛化的能力。

Abstract: Quadruped robots are used for primary searches during the early stages of indoor fires. A typical primary search involves quickly and thoroughly looking for victims under hazardous conditions and monitoring flammable materials. However, situational awareness in complex indoor environments and rapid stair climbing across different staircases remain the main challenges for robot-assisted primary searches. In this project, we designed a two-stage end-to-end deep reinforcement learning (RL) approach to optimize both navigation and locomotion. In the first stage, the quadrupeds, Unitree Go2, were trained to climb stairs in Isaac Lab's pyramid-stair terrain. In the second stage, the quadrupeds were trained to climb various realistic indoor staircases in the Isaac Lab engine, with the learned policy transferred from the previous stage. These indoor staircases are straight, L-shaped, and spiral, to support climbing tasks in complex environments. This project explores how to balance navigation and locomotion and how end-to-end RL methods can enable quadrupeds to adapt to different stair shapes. Our main contributions are: (1) A two-stage end-to-end RL framework that transfers stair-climbing skills from abstract pyramid terrain to realistic indoor stair topologies. (2) A centerline-based navigation formulation that enables unified learning of navigation and locomotion without hierarchical planning. (3) Demonstration of policy generalization across diverse staircases using only local height-map perception. (4) An empirical analysis of success, efficiency, and failure modes under increasing stair difficulty.

</details>


### [93] [A Unified Candidate Set with Scene-Adaptive Refinement via Diffusion for End-to-End Autonomous Driving](https://arxiv.org/abs/2602.03112)
*Zhengfei Wu,Shuaixi Pan,Shuohan Chen,Shuo Yang,Yanjun Huang*

Main category: cs.RO

TL;DR: CdDrive提出了一种结合固定轨迹词汇和场景自适应扩散候选的自动驾驶规划方法，通过共享选择模块实现常规和复杂交互场景的可靠性能。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶规划方法面临两难：固定轨迹词汇在常规驾驶中稳定但复杂交互中可能错过最优解，而场景自适应方法在简单场景中可能过度扰动已有强轨迹。需要一种能兼顾两者的方案。

Method: CdDrive保留原始词汇候选，并通过词汇条件扩散去噪生成场景自适应候选。两者由共享选择模块共同评分。还引入HATNA（Horizon-Aware Trajectory Noise Adapter）通过时间平滑和视野感知噪声调制来提升扩散候选的平滑性和几何连续性。

Result: 在NAVSIM v1和NAVSIM v2数据集上展示了领先性能，消融实验验证了各组件贡献。

Conclusion: CdDrive通过结合固定词汇和场景自适应扩散候选，实现了在常规和高度交互场景中的可靠自动驾驶规划，HATNA进一步提升了扩散候选的质量。

Abstract: End-to-end autonomous driving is increasingly adopting a multimodal planning paradigm that generates multiple trajectory candidates and selects the final plan, making candidate-set design critical. A fixed trajectory vocabulary provides stable coverage in routine driving but often misses optimal solutions in complex interactions, while scene-adaptive refinement can cause over-correction in simple scenarios by unnecessarily perturbing already strong vocabulary trajectories.We propose CdDrive, which preserves the original vocabulary candidates and augments them with scene-adaptive candidates generated by vocabulary-conditioned diffusion denoising. Both candidate types are jointly scored by a shared selection module, enabling reliable performance across routine and highly interactive scenarios. We further introduce HATNA (Horizon-Aware Trajectory Noise Adapter) to improve the smoothness and geometric continuity of diffusion candidates via temporal smoothing and horizon-aware noise modulation. Experiments on NAVSIM v1 and NAVSIM v2 demonstrate leading performance, and ablations verify the contribution of each component.

</details>


### [94] [Multi-function Robotized Surgical Dissector for Endoscopic Pulmonary Thromboendarterectomy: Preclinical Study and Evaluation](https://arxiv.org/abs/2602.03147)
*Runfeng Zhu,Xin Zhong,Qingxiang Zhao,Jing Lin,Zhong Wu,Kang Li*

Main category: cs.RO

TL;DR: 提出基于同心推拉机器人结构的机器人化剥离器，用于慢性严重肺血栓栓塞症的肺动脉内膜切除术，具有细长双段弯曲灵活性，可进入肺动脉薄分支。


<details>
  <summary>Details</summary>
Motivation: 传统肺动脉内膜切除术中使用的剥离器刚性且直，缺乏远端灵活性，难以进入肺动脉的薄分支。需要开发具有更好灵活性的工具来改善手术效果。

Method: 设计基于同心推拉机器人结构的机器人化剥离器，直径3.5mm，具有双段弯曲灵活性。建立基于优化的运动学模型，实现开环控制下2mm的定位精度。中心腔体可容纳冲洗通道、尖端工具和内窥镜信号线。

Result: 机器人剥离器具有3.5mm细长直径和双段弯曲灵活性。通过实验验证了刚度、运动精度和可操作性。在离体猪肺上的手术模拟展示了其在肺动脉内膜切除术中的灵活性和显著优势。

Conclusion: 提出的机器人化剥离器解决了传统工具在肺动脉内膜切除术中的局限性，具有细长灵活的特点，配合内窥镜可实现内窥镜肺动脉内膜切除术，有望改善手术效果。

Abstract: Patients suffering chronic severe pulmonary thromboembolism need Pulmonary Thromboendarterectomy (PTE) to remove the thromb and intima located inside pulmonary artery (PA). During the surgery, a surgeon holds tweezers and a dissector to delicately strip the blockage, but available tools for this surgery are rigid and straight, lacking distal dexterity to access into thin branches of PA. Therefore, this work presents a novel robotized dissector based on concentric push/pull robot (CPPR) structure, enabling entering deep thin branch of tortuous PA. Compared with conventional rigid dissectors, our design characterizes slenderness and dual-segment-bending dexterity. Owing to the hollow and thin-walled structure of the CPPR-based dissector as it has a slender body of 3.5mm in diameter, the central lumen accommodates two channels for irrigation and tip tool, and space for endoscopic camera's signal wire. To provide accurate surgical manipulation, optimization-based kinematics model was established, realizing a 2mm accuracy in positioning the tip tool (60mm length) under open-loop control strategy. As such, with the endoscopic camera, traditional PTE is possible to be upgraded as endoscopic PTE. Basic physic performance of the robotized dissector including stiffness, motion accuracy and maneuverability was evaluated through experiments. Surgery simulation on ex vivo porcine lung also demonstrates its dexterity and notable advantages in PTE.

</details>


### [95] [When Attention Betrays: Erasing Backdoor Attacks in Robotic Policies by Reconstructing Visual Tokens](https://arxiv.org/abs/2602.03153)
*Xuetao Li,Pinhan Fu,Wenke Huang,Nengyuan Pan,Songhua Yang,Kaiyan Zhao,Guancheng Wan,Mengde Li,Jifeng Xuan,Miao Li*

Main category: cs.RO

TL;DR: Bera：一种无需重新训练VLA模型的测试时后门擦除框架，通过检测异常注意力、屏蔽可疑区域并重建无触发图像来防御机器人视觉-语言-动作模型中的后门攻击。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作（VLA）模型的下游微调增强了机器人能力，但暴露了后门风险。现有防御方法要么缺乏对多模态后门的机制理解，要么通过全模型重新训练带来过高计算成本。

Method: Bera框架基于发现的后门机制：后门会重定向后期注意力并在干净流形附近形成紧凑嵌入簇。通过潜在空间定位检测异常注意力标记，使用深层线索屏蔽可疑区域，重建无触发图像以打破触发-不安全动作映射。

Result: 在多个具身平台和任务上的实验表明，Bera能有效保持正常性能，显著降低攻击成功率，并持续从后门输出中恢复良性行为。

Conclusion: Bera为保护机器人系统提供了一种鲁棒且实用的防御机制，无需重新训练VLA模型或改变训练流程，实现了测试时的后门擦除。

Abstract: Downstream fine-tuning of vision-language-action (VLA) models enhances robotics, yet exposes the pipeline to backdoor risks. Attackers can pretrain VLAs on poisoned data to implant backdoors that remain stealthy but can trigger harmful behavior during inference. However, existing defenses either lack mechanistic insight into multimodal backdoors or impose prohibitive computational costs via full-model retraining. To this end, we uncover a deep-layer attention grabbing mechanism: backdoors redirect late-stage attention and form compact embedding clusters near the clean manifold. Leveraging this insight, we introduce Bera, a test-time backdoor erasure framework that detects tokens with anomalous attention via latent-space localization, masks suspicious regions using deep-layer cues, and reconstructs a trigger-free image to break the trigger-unsafe-action mapping while restoring correct behavior. Unlike prior defenses, Bera requires neither retraining of VLAs nor any changes to the training pipeline. Extensive experiments across multiple embodied platforms and tasks show that Bera effectively maintains nominal performance, significantly reduces attack success rates, and consistently restores benign behavior from backdoored outputs, thereby offering a robust and practical defense mechanism for securing robotic systems.

</details>


### [96] [Estimation of Ground Reaction Forces from Kinematic Data during Locomotion](https://arxiv.org/abs/2602.03177)
*Gautami Golani,Dong Anh Khoa To,Ananda Sidarta,Arun-Kumar Kaliya-Perumal,Oliver Roberts,Lek Syn Lim,Jim Patton,Domenico Campolo*

Main category: cs.RO

TL;DR: 提出仅使用运动捕捉标记数据估计地面反作用力的无测力台方法，通过16个身体段运动学计算质心并分解GRF，实现临床广泛部署的步态分析。


<details>
  <summary>Details</summary>
Motivation: 地面反作用力（GRF）对步态力学分析至关重要，但由于测力台系统的实际限制，在临床工作流程中未得到充分利用。需要开发无需专用测力台的方法来获取临床相关的动力学指标。

Method: 仅使用基于标记的运动捕捉数据，通过16个身体段的运动学估计质心（CoM），计算地面反作用力，并通过最小化方法将GRF分解为各个分量。该方法还能识别步态支撑相。

Result: 实验结果表明，仅基于运动学数据估计质心和地面反作用力是可行的，支持无测力台的步态分析，为临床提供有意义的动力学测量。

Conclusion: 提出的无测力台方法能够仅使用运动学数据准确估计GRF，适合临床广泛部署，使临床医生能够在没有专用测力台系统的情况下获取关键的步态动力学信息。

Abstract: Ground reaction forces (GRFs) provide fundamental insight into human gait mechanics and are widely used to assess joint loading, limb symmetry, balance control, and motor function. Despite their clinical relevance, the use of GRF remains underutilised in clinical workflows due to the practical limitations of force plate systems. In this work, we present a force-plate-free approach for estimating GRFs using only marker-based motion capture data. This kinematics only method to estimate and decompose GRF makes it well suited for widespread clinical depolyment. By using kinematics from sixteen body segments, we estimate the centre of mass (CoM) and compute GRFs, which are subsequently decomposed into individual components through a minimization-based approach. Through this framework, we can identify gait stance phases and provide access to clinically meaningful kinetic measures without a dedicated force plate system. Experimental results demonstrate the viability of CoM and GRF estimation based solely on kinematic data, supporting force-plate-free gait analysis.

</details>


### [97] [Hierarchical Proportion Models for Motion Generation via Integration of Motion Primitives](https://arxiv.org/abs/2602.03188)
*Yu-Han Shu,Toshiaki Tsuji,Sho Sakaino*

Main category: cs.RO

TL;DR: 提出分层模仿学习框架，结合运动基元与比例式运动合成，通过三种变体在灵活性、计算成本与适应性间权衡，实现复杂运动生成。


<details>
  <summary>Details</summary>
Motivation: 模仿学习需要大量高质量数据且难以处理复杂长时程任务，为提高数据效率和适应性，需要更高效的学习框架。

Method: 采用两层架构：上层进行长期规划，下层学习独立运动基元，按特定比例组合。提出三种变体：学习型比例模型、采样型比例模型、回放型比例模型。

Result: 在真实机器人拾放实验中，成功生成基元集中未包含的复杂运动。采样型和回放型比例模型比标准分层模型更稳定、适应性更强。

Conclusion: 比例式运动集成对实际机器人学习有效，采样型和回放型模型在稳定性和适应性方面表现更优，为复杂任务提供了高效解决方案。

Abstract: Imitation learning (IL) enables robots to acquire human-like motion skills from demonstrations, but it still requires extensive high-quality data and retraining to handle complex or long-horizon tasks. To improve data efficiency and adaptability, this study proposes a hierarchical IL framework that integrates motion primitives with proportion-based motion synthesis. The proposed method employs a two-layer architecture, where the upper layer performs long-term planning, while a set of lower-layer models learn individual motion primitives, which are combined according to specific proportions. Three model variants are introduced to explore different trade-offs between learning flexibility, computational cost, and adaptability: a learning-based proportion model, a sampling-based proportion model, and a playback-based proportion model, which differ in how the proportions are determined and whether the upper layer is trainable. Through real-robot pick-and-place experiments, the proposed models successfully generated complex motions not included in the primitive set. The sampling-based and playback-based proportion models achieved more stable and adaptable motion generation than the standard hierarchical model, demonstrating the effectiveness of proportion-based motion integration for practical robot learning.

</details>


### [98] [HUSKY: Humanoid Skateboarding System via Physics-Aware Whole-Body Control](https://arxiv.org/abs/2602.03205)
*Jinrui Han,Dewei Wang,Chenyun Zhang,Xinzhe Liu,Ping Luo,Chenjia Bai,Xuelong Li*

Main category: cs.RO

TL;DR: 提出HUSKY框架，通过建模人形滑板系统耦合关系和物理感知全身控制，实现人形机器人在滑板上的稳定动态操控


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人全身控制框架主要依赖静态环境假设，难以处理高动态性和复杂交互的任务。滑板运动需要在不稳定、欠驱动的轮式平台上实现稳定动态操控，面临非完整约束和紧密耦合的人-物交互挑战

Method: 1) 建模滑板倾斜与转向角度的耦合关系；2) 使用对抗运动先验(AMP)学习类人推动动作；3) 采用物理引导、面向航向的倾斜转向策略；4) 轨迹引导机制确保推动与转向间的平滑稳定过渡

Result: 在Unitree G1人形平台上实验验证，框架能够在真实场景中实现稳定敏捷的滑板操控

Conclusion: HUSKY框架成功解决了人形机器人滑板运动这一高挑战性任务，为动态环境下的人-物交互控制提供了有效解决方案

Abstract: While current humanoid whole-body control frameworks predominantly rely on the static environment assumptions, addressing tasks characterized by high dynamism and complex interactions presents a formidable challenge. In this paper, we address humanoid skateboarding, a highly challenging task requiring stable dynamic maneuvering on an underactuated wheeled platform. This integrated system is governed by non-holonomic constraints and tightly coupled human-object interactions. Successfully executing this task requires simultaneous mastery of hybrid contact dynamics and robust balance control on a mechanically coupled, dynamically unstable skateboard. To overcome the aforementioned challenges, we propose HUSKY, a learning-based framework that integrates humanoid-skateboard system modeling and physics-aware whole-body control. We first model the coupling relationship between board tilt and truck steering angles, enabling a principled analysis of system dynamics. Building upon this, HUSKY leverages Adversarial Motion Priors (AMP) to learn human-like pushing motions and employs a physics-guided, heading-oriented strategy for lean-to-steer behaviors. Moreover, a trajectory-guided mechanism ensures smooth and stable transitions between pushing and steering. Experimental results on the Unitree G1 humanoid platform demonstrate that our framework enables stable and agile maneuvering on skateboards in real-world scenarios. The project page is available on https://husky-humanoid.github.io/.

</details>


### [99] [Depth Completion in Unseen Field Robotics Environments Using Extremely Sparse Depth Measurements](https://arxiv.org/abs/2602.03209)
*Marco Job,Thomas Stastny,Eleni Kelasidi,Roland Siegwart,Michael Pantic*

Main category: cs.RO

TL;DR: 提出一种用于野外机器人的深度补全模型，利用合成数据和稀疏深度测量来预测密集度量深度，实现实时部署


<details>
  <summary>Details</summary>
Motivation: 野外机器人需要鲁棒感知，但单目深度估计缺乏可靠尺度线索、纹理条件差且数据集稀缺，限制了低成本相机在野外机器人中的应用

Method: 使用合成数据训练深度补全模型，结合稀疏深度传感器测量来预测密集度量深度；开发了针对野外机器人的合成数据集生成流水线，利用运动结构恢复的纹理3D网格和照片级真实感渲染

Result: 在Nvidia Jetson AGX Orin上实现每帧53毫秒的端到端延迟，支持实时部署；在多样化真实世界野外机器人场景中表现出竞争性性能

Conclusion: 该方法通过合成数据训练和稀疏深度测量，为野外机器人提供了实时、可靠的密集深度感知解决方案，克服了单目深度估计的局限性

Abstract: Autonomous field robots operating in unstructured environments require robust perception to ensure safe and reliable operations. Recent advances in monocular depth estimation have demonstrated the potential of low-cost cameras as depth sensors; however, their adoption in field robotics remains limited due to the absence of reliable scale cues, ambiguous or low-texture conditions, and the scarcity of large-scale datasets. To address these challenges, we propose a depth completion model that trains on synthetic data and uses extremely sparse measurements from depth sensors to predict dense metric depth in unseen field robotics environments. A synthetic dataset generation pipeline tailored to field robotics enables the creation of multiple realistic datasets for training purposes. This dataset generation approach utilizes textured 3D meshes from Structure from Motion and photorealistic rendering with novel viewpoint synthesis to simulate diverse field robotics scenarios. Our approach achieves an end-to-end latency of 53 ms per frame on a Nvidia Jetson AGX Orin, enabling real-time deployment on embedded platforms. Extensive evaluation demonstrates competitive performance across diverse real-world field robotics scenarios.

</details>


### [100] [Omnidirectional Solid-State mmWave Radar Perception for UAV Power Line Collision Avoidance](https://arxiv.org/abs/2602.03229)
*Nicolaj Haarhøj Malle,Emad Ebeid*

Main category: cs.RO

TL;DR: 基于毫米波雷达的无人机全方位感知系统，可检测10米范围内的电力线并实现避障，适用于自主和手动飞行的安全增强层


<details>
  <summary>Details</summary>
Motivation: 检测和估计电力线距离对无人机飞行员和自主系统都是挑战，增加了意外碰撞风险，需要可靠的感知解决方案

Method: 集成多个紧凑固态毫米波雷达模块，合成全方位视场，开发针对电力线环境的检测与避障算法

Result: 现场实验显示：可靠检测范围达10米，飞行速度10m/s时成功避障，可检测直径1.2mm的细线

Conclusion: 该方法适合作为自主和手动无人机飞行的额外安全层，提供可靠的电力线检测与避障能力

Abstract: Detecting and estimating distances to power lines is a challenge for both human UAV pilots and autonomous systems, which increases the risk of unintended collisions. We present a mmWave radar-based perception system that provides spherical sensing coverage around a small UAV for robust power line detection and avoidance. The system integrates multiple compact solid-state mmWave radar modules to synthesize an omnidirectional field of view while remaining lightweight. We characterize the sensing behavior of this omnidirectional radar arrangement in power line environments and develop a robust detection-and-avoidance algorithm tailored to that behavior. Field experiments on real power lines demonstrate reliable detection at ranges up to 10 m, successful avoidance maneuvers at flight speeds upwards of 10 m/s, and detection of wires as thin as 1.2 mm in diameter. These results indicate the approach's suitability as an additional safety layer for both autonomous and manual UAV flight.

</details>


### [101] [A thin and soft optical tactile sensor for highly sensitive object perception](https://arxiv.org/abs/2602.03248)
*Yanchen Shen,Kohei Tsuji,Haruto Koizumi,Jiseon Hong,Tomoaki Niiyama,Hiroyuki Kuwabara,Hayato Ishida,Jun Hiramitsu,Mitsuhito Mase,Satoshi Sunada*

Main category: cs.RO

TL;DR: 提出一种基于散斑图案的薄型、紧凑、柔软的触觉传感器，无需复杂光学组件，通过机器学习实现精确力测量和纹理识别。


<details>
  <summary>Details</summary>
Motivation: 现有光学触觉传感器（特别是视觉触觉传感器）依赖复杂的光学组件（透镜和相机），导致笨重、刚性和对齐敏感的设计。需要开发更紧凑、柔软且对齐自由的触觉传感器。

Method: 采用柔软硅胶材料中的散斑图案变化来感知变形，通过机器学习算法处理散斑图案变化，实现力测量和纹理识别。

Result: 力测量均方根误差为40 mN，在包括麻将牌在内的9类纹理表面上的分类准确率达到93.33%。

Conclusion: 提出的基于散斑的方法提供了一个紧凑、易于制造、机械柔顺的平台，将光学传感与柔性形状自适应架构相结合，展示了作为软机器人和可穿戴触觉界面的新型触觉传感范式的潜力。

Abstract: Tactile sensing is crucial in robotics and wearable devices for safe perception and interaction with the environment. Optical tactile sensors have emerged as promising solutions, as they are immune to electromagnetic interference and have high spatial resolution. However, existing optical approaches, particularly vision-based tactile sensors, rely on complex optical assemblies that involve lenses and cameras, resulting in bulky, rigid, and alignment-sensitive designs. In this study, we present a thin, compact, and soft optical tactile sensor featuring an alignment-free configuration. The soft optical sensor operates by capturing deformation-induced changes in speckle patterns generated within a soft silicone material, thereby enabling precise force measurements and texture recognition via machine learning. The experimental results show a root-mean-square error of 40 mN in the force measurement and a classification accuracy of 93.33% over nine classes of textured surfaces, including Mahjong tiles. The proposed speckle-based approach provides a compact, easily fabricated, and mechanically compliant platform that bridges optical sensing with flexible shape-adaptive architectures, thereby demonstrating its potential as a novel tactile-sensing paradigm for soft robotics and wearable haptic interfaces.

</details>


### [102] [Collision Detection with Analytical Derivatives of Contact Kinematics](https://arxiv.org/abs/2602.03250)
*Anup Teejo Mathew,Anees Peringal,Daniele Caradonna,Frederic Boyer,Federico Renda*

Main category: cs.RO

TL;DR: iDCOL：通过几何正则化实现严格凸隐式表示，解决接触运动学中的非光滑问题，提供可微碰撞检测和接触运动学框架


<details>
  <summary>Details</summary>
Motivation: 机器人学中基于梯度的方法需要可微的接触运动学，但当形状具有零曲率或未定义曲率时，从机器人状态到接触距离、位置和法向的映射会变得非光滑，这限制了梯度方法的有效性

Method: 通过选择性正则化将几何体转换为严格凸的隐式表示，基于几何缩放凸优化公式构建固定大小的非线性系统，应用隐函数定理推导接触运动学量的解析导数，开发快速牛顿求解器

Result: 开发了iDCOL框架，提供了开源的C++实现，通过广泛的碰撞模拟和基准测试验证了鲁棒性，在梯度运动学路径规划和可微接触物理中展示了应用价值

Conclusion: iDCOL通过几何正则化解决了接触运动学中的非光滑问题，为基于梯度的机器人方法提供了可靠的可微碰撞检测和接触运动学框架，具有广泛的应用前景

Abstract: Differentiable contact kinematics are essential for gradient-based methods in robotics, yet the mapping from robot state to contact distance, location, and normal becomes non-smooth in degenerate configurations of shapes with zero or undefined curvature. We address this inherent limitation by selectively regularizing such geometries into strictly convex implicit representations, restoring uniqueness and smoothness of the contact map. Leveraging this geometric regularization, we develop iDCOL, an implicit differentiable collision detection and contact kinematics framework. iDCOL represents colliding bodies using strictly convex implicit surfaces and computes collision detection and contact kinematics by solving a fixed-size nonlinear system derived from a geometric scaling-based convex optimization formulation. By applying the Implicit Function Theorem to the resulting system residual, we derive analytical derivatives of the contact kinematic quantities. We develop a fast Newton-based solver for iDCOL and provide an open-source C++ implementation of the framework. The robustness of the approach is evaluated through extensive collision simulations and benchmarking, and applicability is demonstrated in gradient-based kinematic path planning and differentiable contact physics, including multi-body rigid collisions and a soft-robot interaction example.

</details>


### [103] [RDT2: Exploring the Scaling Limit of UMI Data Towards Zero-Shot Cross-Embodiment Generalization](https://arxiv.org/abs/2602.03310)
*Songming Liu,Bangguo Li,Kai Ma,Lingxuan Wu,Hengkai Tan,Xiao Ouyang,Hang Su,Jun Zhu*

Main category: cs.RO

TL;DR: RDT2是一个基于70亿参数视觉语言模型的机器人基础模型，通过大规模数据集和创新的三阶段训练方法，实现了对未见过的物体、场景、指令和机器人平台的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型面临数据稀缺、架构效率低下以及无法跨不同硬件平台泛化的问题，需要开发能够零样本部署到新机器人平台上的通用机器人基础模型。

Method: 1) 收集了超过10,000小时的大规模开源机器人数据集，使用增强的、与具体机器人无关的通用操作接口；2) 采用新颖的三阶段训练方法，通过残差向量量化、流匹配和蒸馏技术将离散语言知识与连续控制对齐，实现实时推理。

Result: RDT2成为首批同时实现对未见过的物体、场景、指令和机器人平台进行零样本泛化的模型之一，在灵巧操作、长时程和动态下游任务（如打乒乓球）中超越了现有最先进基线。

Conclusion: RDT2通过大规模数据集和创新的训练方法，成功解决了VLA模型的数据稀缺和泛化问题，为通用机器人基础模型的发展提供了重要进展，实现了跨平台零样本部署能力。

Abstract: Vision-Language-Action (VLA) models hold promise for generalist robotics but currently struggle with data scarcity, architectural inefficiencies, and the inability to generalize across different hardware platforms. We introduce RDT2, a robotic foundation model built upon a 7B parameter VLM designed to enable zero-shot deployment on novel embodiments for open-vocabulary tasks. To achieve this, we collected one of the largest open-source robotic datasets--over 10,000 hours of demonstrations in diverse families--using an enhanced, embodiment-agnostic Universal Manipulation Interface (UMI). Our approach employs a novel three-stage training recipe that aligns discrete linguistic knowledge with continuous control via Residual Vector Quantization (RVQ), flow-matching, and distillation for real-time inference. Consequently, RDT2 becomes one of the first models that simultaneously zero-shot generalizes to unseen objects, scenes, instructions, and even robotic platforms. Besides, it outperforms state-of-the-art baselines in dexterous, long-horizon, and dynamic downstream tasks like playing table tennis. See https://rdt-robotics.github.io/rdt2/ for more information.

</details>


### [104] [Manipulation via Force Distribution at Contact](https://arxiv.org/abs/2602.03350)
*Haegu Lee,Yitaek Kim,Casper Hewson Rask,Christoffer Sloth*

Main category: cs.RO

TL;DR: 本文提出了一种力分布线接触模型用于接触丰富的操作任务，相比传统点接触模型能生成更高效、更鲁棒的轨迹，并通过双层优化框架验证了其优势。


<details>
  <summary>Details</summary>
Motivation: 现有基于点接触模型的方法虽然计算高效，但无法捕捉人类操作中的关键摩擦动力学和扭矩生成，限制了实现类人接触丰富操作的能力。

Method: 提出力分布线接触模型，构建双层优化框架：下层优化计算接触力，上层应用iLQR进行轨迹优化。

Result: FDLC模型能够生成沿接触线非均匀力分布的轨迹，相比点接触模型需要更低的控制努力和更少的机器人运动，在盒子旋转任务中验证了有效性。

Conclusion: 力分布线接触模型在接触丰富操作中优于传统点接触模型，能够实现更高效、更鲁棒的轨迹生成。

Abstract: Efficient and robust trajectories play a crucial role in contact-rich manipulation, which demands accurate mod- eling of object-robot interactions. Many existing approaches rely on point contact models due to their computational effi- ciency. Simple contact models are computationally efficient but inherently limited for achieving human-like, contact-rich ma- nipulation, as they fail to capture key frictional dynamics and torque generation observed in human manipulation. This study introduces a Force-Distributed Line Contact (FDLC) model in contact-rich manipulation and compares it against conventional point contact models. A bi-level optimization framework is constructed, in which the lower-level solves an optimization problem for contact force computation, and the upper-level optimization applies iLQR for trajectory optimization. Through this framework, the limitations of point contact are demon- strated, and the benefits of the FDLC in generating efficient and robust trajectories are established. The effectiveness of the proposed approach is validated by a box rotating task, demonstrating that FDLC enables trajectories generated via non-uniform force distributions along the contact line, while requiring lower control effort and less motion of the robot.

</details>


### [105] [Learning-based Adaptive Control of Quadruped Robots for Active Stabilization on Moving Platforms](https://arxiv.org/abs/2602.03367)
*Minsung Yoon,Heechan Shin,Jeil Jeong,Sung-Eui Yoon*

Main category: cs.RO

TL;DR: 提出LAS-MP系统，通过自平衡策略和状态估计器解决四足机器人在六自由度移动平台上的平衡问题


<details>
  <summary>Details</summary>
Motivation: 四足机器人在地铁、公交车、飞机、游艇等六自由度移动平台上会面临平衡挑战，因为平台独立运动和产生的惯性力会影响机器人稳定性

Method: 开发LAS-MP系统，包含自平衡策略（自适应调整机器人姿态响应平台运动）和系统状态估计器（基于本体感知传感器数据推断机器人和平台状态），并引入平台轨迹生成和调度方法进行系统训练

Result: 在多个指标上表现出优于三个基线的平衡性能，并通过消融研究和估计器评估验证了各组件有效性

Conclusion: LAS-MP系统能有效解决四足机器人在移动平台上的平衡问题，为实际应用提供了可靠解决方案

Abstract: A quadruped robot faces balancing challenges on a six-degrees-of-freedom moving platform, like subways, buses, airplanes, and yachts, due to independent platform motions and resultant diverse inertia forces on the robot. To alleviate these challenges, we present the Learning-based Active Stabilization on Moving Platforms (\textit{LAS-MP}), featuring a self-balancing policy and system state estimators. The policy adaptively adjusts the robot's posture in response to the platform's motion. The estimators infer robot and platform states based on proprioceptive sensor data. For a systematic training scheme across various platform motions, we introduce platform trajectory generation and scheduling methods. Our evaluation demonstrates superior balancing performance across multiple metrics compared to three baselines. Furthermore, we conduct a detailed analysis of the \textit{LAS-MP}, including ablation studies and evaluation of the estimators, to validate the effectiveness of each component.

</details>


### [106] [PlanTRansformer: Unified Prediction and Planning with Goal-conditioned Transformer](https://arxiv.org/abs/2602.03376)
*Constantin Selzer,Fabina B. Flohr*

Main category: cs.RO

TL;DR: PlanTRansformer (PTR) 是一个统一的Transformer框架，将目标条件预测、动态可行性、交互感知和车道级拓扑推理集成在一起，解决了自动驾驶中预测与规划的脱节问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中轨迹预测和规划是分离的组件：预测模型在未知意图下预测周围车辆运动，产生多模态分布；而规划假设已知自车目标并生成确定性轨迹。这种不匹配造成关键瓶颈：预测缺乏对车辆意图的监督，而规划又需要这些信息。现有预测模型虽然基准测试表现良好，但往往与碰撞避免、动态可行性等规划约束脱节。

Method: 提出Plan Transformer (PTR)，一个统一的高斯混合Transformer框架，集成了目标条件预测、动态可行性、交互感知和车道级拓扑推理。采用师生训练策略，在训练过程中逐步掩码周围车辆的命令，以与推理时车辆意图不可用的条件对齐。该架构无关的设计可应用于各种基于Transformer的预测模型。

Result: PTR相比基线Motion Transformer (MTR)在边际/联合mAP上分别提升4.3%/3.5%，相比GameFormer在5秒规划时域上减少15.5%的规划误差。

Conclusion: PTR通过统一框架有效连接了自动驾驶中的预测和规划组件，解决了意图监督缺失和规划约束脱节的关键问题，在预测准确性和规划性能上均有显著提升。

Abstract: Trajectory prediction and planning are fundamental yet disconnected components in autonomous driving. Prediction models forecast surrounding agent motion under unknown intentions, producing multimodal distributions, while planning assumes known ego objectives and generates deterministic trajectories. This mismatch creates a critical bottleneck: prediction lacks supervision for agent intentions, while planning requires this information. Existing prediction models, despite strong benchmarking performance, often remain disconnected from planning constraints such as collision avoidance and dynamic feasibility. We introduce Plan TRansformer (PTR), a unified Gaussian Mixture Transformer framework integrating goal-conditioned prediction, dynamic feasibility, interaction awareness, and lane-level topology reasoning. A teacher-student training strategy progressively masks surrounding agent commands during training to align with inference conditions where agent intentions are unavailable. PTR achieves 4.3%/3.5% improvement in marginal/joint mAP compared to the baseline Motion Transformer (MTR) and 15.5% planning error reduction at 5s horizon compared to GameFormer. The architecture-agnostic design enables application to diverse Transformer-based prediction models. Project Website: https://github.com/SelzerConst/PlanTRansformer

</details>


### [107] [Enhancing Navigation Efficiency of Quadruped Robots via Leveraging Personal Transportation Platforms](https://arxiv.org/abs/2602.03397)
*Minsung Yoon,Sung-Eui Yoon*

Main category: cs.RO

TL;DR: 提出RL-ATR方法让四足机器人学会骑乘个人交通工具（如Segway），以提升长距离导航效率


<details>
  <summary>Details</summary>
Motivation: 四足机器人依赖腿部运动，长距离导航效率受限，受人类使用个人交通工具的启发，希望让机器人也能骑乘交通工具来扩展运动能力

Method: 基于强化学习的主动交通工具骑乘方法，包含骑乘策略和两个状态估计器。策略根据交通工具特定控制动力学设计操控策略，估计器通过推断不可观测状态来解决非惯性坐标系中的传感器模糊问题

Result: 仿真验证表明该方法在各种交通工具-机器人模型上具有熟练的命令跟踪能力，相比腿部运动显著降低能耗。消融研究量化了各组件贡献

Conclusion: 骑乘能力可以扩展四足机器人的运动模式，潜在提升操作范围和效率

Abstract: Quadruped robots face limitations in long-range navigation efficiency due to their reliance on legs. To ameliorate the limitations, we introduce a Reinforcement Learning-based Active Transporter Riding method (\textit{RL-ATR}), inspired by humans' utilization of personal transporters, including Segways. The \textit{RL-ATR} features a transporter riding policy and two state estimators. The policy devises adequate maneuvering strategies according to transporter-specific control dynamics, while the estimators resolve sensor ambiguities in non-inertial frames by inferring unobservable robot and transporter states. Comprehensive evaluations in simulation validate proficient command tracking abilities across various transporter-robot models and reduced energy consumption compared to legged locomotion. Moreover, we conduct ablation studies to quantify individual component contributions within the \textit{RL-ATR}. This riding ability could broaden the locomotion modalities of quadruped robots, potentially expanding the operational range and efficiency.

</details>


### [108] [Deep-Learning-Based Control of a Decoupled Two-Segment Continuum Robot for Endoscopic Submucosal Dissection](https://arxiv.org/abs/2602.03406)
*Yuancheng Shao,Yao Zhang,Jia Gu,Zixi Chen,Di Wu,Yuqiao Chen,Bo Lu,Wenjie Liu,Cesare Stefanini,Peng Qi*

Main category: cs.RO

TL;DR: DESectBot是一种新型双段连续体机器人，具有解耦结构和集成手术钳，用于内镜黏膜下剥离术（ESD）。采用基于门控循环单元（GRU）的深度学习控制器进行尖端位置和方向控制，在轨迹跟踪和方向控制任务中表现最佳，并在离体ESD演示中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统手动ESD技术要求高，现有单段机器人工具灵活性有限，需要开发更先进的解决方案来提高病变定位的精确性和操作灵活性。

Method: 开发了DESectBot双段连续体机器人，具有解耦结构和集成手术钳，提供6自由度尖端灵活性。提出了基于GRU的深度学习控制器，用于同时控制尖端位置和方向，处理连续体段之间的非线性耦合。将GRU控制器与雅可比逆运动学、模型预测控制、前馈神经网络和LSTM网络进行对比。

Result: 在轨迹跟踪任务中，GRU控制器取得了最低的位置/方向RMSE：嵌套矩形轨迹1.11 mm/4.62°，李萨如轨迹0.81 mm/2.59°。在固定位置方向控制任务中，GRU平均RMSE为0.14 mm和0.72°。在peg转移任务中，GRU实现了100%成功率（120/120），平均转移时间11.8秒。离体ESD演示证实机器人具有足够刚度分割厚胃黏膜，工作空间适合大病变。

Conclusion: 基于GRU的控制显著提高了ESD手术训练场景中的精确性、可靠性和可用性。DESectBot机器人系统为ESD手术提供了有效的技术解决方案，具有临床应用潜力。

Abstract: Manual endoscopic submucosal dissection (ESD) is technically demanding, and existing single-segment robotic tools offer limited dexterity. These limitations motivate the development of more advanced solutions. To address this, DESectBot, a novel dual segment continuum robot with a decoupled structure and integrated surgical forceps, enabling 6 degrees of freedom (DoFs) tip dexterity for improved lesion targeting in ESD, was developed in this work. Deep learning controllers based on gated recurrent units (GRUs) for simultaneous tip position and orientation control, effectively handling the nonlinear coupling between continuum segments, were proposed. The GRU controller was benchmarked against Jacobian based inverse kinematics, model predictive control (MPC), a feedforward neural network (FNN), and a long short-term memory (LSTM) network. In nested-rectangle and Lissajous trajectory tracking tasks, the GRU achieved the lowest position/orientation RMSEs: 1.11 mm/ 4.62° and 0.81 mm/ 2.59°, respectively. For orientation control at a fixed position (four target poses), the GRU attained a mean RMSE of 0.14 mm and 0.72°, outperforming all alternatives. In a peg transfer task, the GRU achieved a 100% success rate (120 success/120 attempts) with an average transfer time of 11.8s, the STD significantly outperforms novice-controlled systems. Additionally, an ex vivo ESD demonstration grasping, elevating, and resecting tissue as the scalpel completed the cut confirmed that DESectBot provides sufficient stiffness to divide thick gastric mucosa and an operative workspace adequate for large lesions.These results confirm that GRU-based control significantly enhances precision, reliability, and usability in ESD surgical training scenarios.

</details>


### [109] [Learning-based Initialization of Trajectory Optimization for Path-following Problems of Redundant Manipulators](https://arxiv.org/abs/2602.03418)
*Minsung Yoon,Mincheul Kang,Daehyung Park,Sung-Eui Yoon*

Main category: cs.RO

TL;DR: 提出基于学习的初始轨迹生成方法，使用示例引导强化学习快速生成高质量初始轨迹，提升轨迹优化性能


<details>
  <summary>Details</summary>
Motivation: 轨迹优化的性能高度依赖初始轨迹质量，但高质量初始轨迹的选择非常困难，因为解空间极大且缺乏任务约束的先验知识，需要大量时间预算

Method: 采用示例引导强化学习生成高质量初始轨迹，提出零空间投影模仿奖励来考虑零空间约束，高效学习专家演示中的运动学可行运动

Result: 仿真统计评估显示，当使用该方法输出时，轨迹优化在最优性、效率和适用性方面相比三个基线方法均有改进；七自由度机械臂真实实验验证了性能提升和可行性

Conclusion: 提出的学习型初始轨迹生成方法能够快速生成高质量初始轨迹，显著提升轨迹优化的性能，并通过仿真和真实实验验证了有效性

Abstract: Trajectory optimization (TO) is an efficient tool to generate a redundant manipulator's joint trajectory following a 6-dimensional Cartesian path. The optimization performance largely depends on the quality of initial trajectories. However, the selection of a high-quality initial trajectory is non-trivial and requires a considerable time budget due to the extremely large space of the solution trajectories and the lack of prior knowledge about task constraints in configuration space. To alleviate the issue, we present a learning-based initial trajectory generation method that generates high-quality initial trajectories in a short time budget by adopting example-guided reinforcement learning. In addition, we suggest a null-space projected imitation reward to consider null-space constraints by efficiently learning kinematically feasible motion captured in expert demonstrations. Our statistical evaluation in simulation shows the improved optimality, efficiency, and applicability of TO when we plug in our method's output, compared with three other baselines. We also show the performance improvement and feasibility via real-world experiments with a seven-degree-of-freedom manipulator.

</details>


### [110] [ProAct: A Benchmark and Multimodal Framework for Structure-Aware Proactive Response](https://arxiv.org/abs/2602.03430)
*Xiaomeng Zhu,Fengming Zhu,Weijie Zhou,Ye Tian,Zhenlin Hu,Yufei Huang,Yuchun Guo,Xinyu Wu,Zhengyou Zhang,Fangzhen Lin,Xuantang Xiong*

Main category: cs.RO

TL;DR: ProAct-75是一个包含75个任务、91,581个步骤级标注的基准数据集，用于训练和评估主动智能体。ProAct-Helper是基于多模态大语言模型的基线方法，通过状态检测和任务图驱动的启发式搜索实现并行动作执行。


<details>
  <summary>Details</summary>
Motivation: 主动智能体（能够根据高层目标如协助和安全进行环境监控并自主行动）的发展受到缺乏专门资源的限制。现有资源不足以支持主动智能体的训练和评估。

Method: 1) 提出ProAct-75基准数据集，包含75个任务、91,581个步骤级标注，并带有明确的任务图编码步骤依赖和并行执行可能性。2) 提出ProAct-Helper基线方法，基于多模态大语言模型，通过状态检测进行决策，并利用任务图进行熵驱动的启发式搜索来选择动作，使智能体能够独立执行并行线程而非模仿人类下一步。

Result: ProAct-Helper在实验中优于强大的闭源模型：触发检测mF1提升6.21%，在线单步决策中节省0.25个步骤，并行动作率提高15.58%。

Conclusion: ProAct-75为主动智能体的训练和评估提供了重要资源，而ProAct-Helper展示了利用任务图进行熵驱动启发式搜索的有效性，能够显著提升主动智能体的性能。

Abstract: While passive agents merely follow instructions, proactive agents align with higher-level objectives, such as assistance and safety by continuously monitoring the environment to determine when and how to act. However, developing proactive agents is hindered by the lack of specialized resources. To address this, we introduce ProAct-75, a benchmark designed to train and evaluate proactive agents across diverse domains, including assistance, maintenance, and safety monitoring. Spanning 75 tasks, our dataset features 91,581 step-level annotations enriched with explicit task graphs. These graphs encode step dependencies and parallel execution possibilities, providing the structural grounding necessary for complex decision-making. Building on this benchmark, we propose ProAct-Helper, a reference baseline powered by a Multimodal Large Language Model (MLLM) that grounds decision-making in state detection, and leveraging task graphs to enable entropy-driven heuristic search for action selection, allowing agents to execute parallel threads independently rather than mirroring the human's next step. Extensive experiments demonstrate that ProAct-Helper outperforms strong closed-source models, improving trigger detection mF1 by 6.21%, saving 0.25 more steps in online one-step decision, and increasing the rate of parallel actions by 15.58%.

</details>


### [111] [Model-based Optimal Control for Rigid-Soft Underactuated Systems](https://arxiv.org/abs/2602.03435)
*Daniele Caradonna,Nikhil Nair,Anup Teejo Mathew,Daniel Feliu Talegón,Imran Afgan,Egidio Falotico,Cosimo Della Santina,Federico Renda*

Main category: cs.RO

TL;DR: 本文研究了三种最优控制策略（直接配点法、微分动态规划、非线性模型预测控制）用于欠驱动软体系统的动态摆动任务，通过几何变应变模型实现解析导数计算，提高了计算效率和数值鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 连续体软体机器人本质上是欠驱动的，且受内在输入约束，使得动态控制特别具有挑战性。现有方法多关注准静态行为，而动态任务（如摆动）需要准确利用连续体动力学。基于模型的最优控制提供了系统解决方案，但应用于刚-软机器人时常受计算成本和数值微分不准确性的限制。

Method: 基于几何变应变模型实现解析导数计算，研究了三种最优控制策略：直接配点法、微分动态规划和非线性模型预测控制。采用隐式积分方案和热启动策略来处理刚性的连续体动力学和约束驱动，提高数值鲁棒性和计算效率。

Result: 在三个刚-软和高阶软体基准系统（软体小车-杆、软体双摆、软体Furuta摆）上进行仿真评估，展示了各种方法的性能和计算权衡。

Conclusion: 通过几何变应变模型的解析导数，三种最优控制策略能够有效处理欠驱动软体系统的动态控制问题，隐式积分和热启动策略显著提高了数值稳定性和计算效率，为刚-软机器人的动态任务控制提供了系统解决方案。

Abstract: Continuum soft robots are inherently underactuated and subject to intrinsic input constraints, making dynamic control particularly challenging, especially in hybrid rigid-soft robots. While most existing methods focus on quasi-static behaviors, dynamic tasks such as swing-up require accurate exploitation of continuum dynamics. This has led to studies on simple low-order template systems that often fail to capture the complexity of real continuum deformations. Model-based optimal control offers a systematic solution; however, its application to rigid-soft robots is often limited by the computational cost and inaccuracy of numerical differentiation for high-dimensional models. Building on recent advances in the Geometric Variable Strain model that enable analytical derivatives, this work investigates three optimal control strategies for underactuated soft systems-Direct Collocation, Differential Dynamic Programming, and Nonlinear Model Predictive Control-to perform dynamic swing-up tasks. To address stiff continuum dynamics and constrained actuation, implicit integration schemes and warm-start strategies are employed to improve numerical robustness and computational efficiency. The methods are evaluated in simulation on three Rigid-Soft and high-order soft benchmark systems-the Soft Cart-Pole, the Soft Pendubot, and the Soft Furuta Pendulum- highlighting their performance and computational trade-offs.

</details>


### [112] [HetroD: A High-Fidelity Drone Dataset and Benchmark for Autonomous Driving in Heterogeneous Traffic](https://arxiv.org/abs/2602.03447)
*Yu-Hsiang Chen,Wei-Jer Chang,Christian Kotulla,Thomas Keutgens,Steffen Runde,Tobias Moers,Christoph Klas,Wei Zhan,Masayoshi Tomizuka,Yi-Ting Chen*

Main category: cs.RO

TL;DR: HetroD是一个用于异构交通环境下自动驾驶系统开发的数据集和基准测试，重点关注弱势道路使用者（行人、自行车、摩托车）的复杂行为，填补了现有数据集在非结构化交通场景中的空白。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶数据集主要关注结构化、有车道纪律的交通场景，而现实世界中存在大量弱势道路使用者（VRUs）参与的异构交通，这些场景中VRUs表现出复杂的非结构化行为（如钩形转弯、车道分割、非正式路权协商），对自动驾驶系统构成重大挑战，但现有数据集对此代表性不足。

Method: 通过无人机采集大规模异构交通场景数据，提供厘米级精确标注、高清地图和交通信号状态；开发模块化工具包提取每个智能体的场景，支持下游任务开发；数据集包含超过65.4k个高保真智能体轨迹，其中70%来自弱势道路使用者。

Result: 评估结果显示，最先进的预测和规划模型在HetroD数据集上表现不佳：无法预测VRUs的横向移动，不能处理非结构化机动，在密集和多智能体场景中性能有限，凸显了异构交通场景需要更鲁棒的方法。

Conclusion: HetroD填补了异构交通场景数据集的空白，支持VRU行为建模，为预测、规划和仿真任务提供标准化基准，揭示了当前自动驾驶模型在处理现实世界异构交通挑战时的局限性，推动了更鲁棒方法的发展。

Abstract: We present HetroD, a dataset and benchmark for developing autonomous driving systems in heterogeneous environments. HetroD targets the critical challenge of navi- gating real-world heterogeneous traffic dominated by vulner- able road users (VRUs), including pedestrians, cyclists, and motorcyclists that interact with vehicles. These mixed agent types exhibit complex behaviors such as hook turns, lane splitting, and informal right-of-way negotiation. Such behaviors pose significant challenges for autonomous vehicles but remain underrepresented in existing datasets focused on structured, lane-disciplined traffic. To bridge the gap, we collect a large- scale drone-based dataset to provide a holistic observation of traffic scenes with centimeter-accurate annotations, HD maps, and traffic signal states. We further develop a modular toolkit for extracting per-agent scenarios to support downstream task development. In total, the dataset comprises over 65.4k high- fidelity agent trajectories, 70% of which are from VRUs. HetroD supports modeling of VRU behaviors in dense, het- erogeneous traffic and provides standardized benchmarks for forecasting, planning, and simulation tasks. Evaluation results reveal that state-of-the-art prediction and planning models struggle with the challenges presented by our dataset: they fail to predict lateral VRU movements, cannot handle unstructured maneuvers, and exhibit limited performance in dense and multi-agent scenarios, highlighting the need for more robust approaches to heterogeneous traffic. See our project page for more examples: https://hetroddata.github.io/HetroD/

</details>


### [113] [CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains](https://arxiv.org/abs/2602.03511)
*Qixin Zeng,Hongyin Zhang,Shangke Lyu,Junxi Jin,Donglin Wang,Chao Huang*

Main category: cs.RO

TL;DR: 提出CMR框架，通过收缩映射将高维噪声观测映射到潜在空间，增强人形机器人在非结构化地形上的抗干扰能力


<details>
  <summary>Details</summary>
Motivation: 人形机器人在非结构化地形上的抗干扰控制是一个长期挑战，感知信息（如高度图）虽然能增强地形感知，但传感器噪声和仿真到现实的差距会导致策略在实际中不稳定

Method: 提出收缩映射鲁棒性（CMR）框架，将高维易受干扰的观测映射到潜在空间，结合对比表示学习和Lipschitz正则化，在保持任务相关几何结构的同时显式控制敏感性

Result: CMR在噪声增加的情况下显著优于其他运动算法，该框架可作为辅助损失项轻松集成到现代深度强化学习流程中

Conclusion: CMR框架通过收缩映射有效提升人形机器人在噪声环境下的鲁棒性，为强化学习中的抗干扰控制提供了理论保证和实用方案

Abstract: Robust disturbance rejection remains a longstanding challenge in humanoid locomotion, particularly on unstructured terrains where sensing is unreliable and model mismatch is pronounced. While perception information, such as height map, enhances terrain awareness, sensor noise and sim-to-real gaps can destabilize policies in practice. In this work, we provide theoretical analysis that bounds the return gap under observation noise, when the induced latent dynamics are contractive. Furthermore, we present Contractive Mapping for Robustness (CMR) framework that maps high-dimensional, disturbance-prone observations into a latent space, where local perturbations are attenuated over time. Specifically, this approach couples contrastive representation learning with Lipschitz regularization to preserve task-relevant geometry while explicitly controlling sensitivity. Notably, the formulation can be incorporated into modern deep reinforcement learning pipelines as an auxiliary loss term with minimal additional technical effort required. Further, our extensive humanoid experiments show that CMR potently outperforms other locomotion algorithms under increased noise.

</details>


### [114] [Investigating the Influence of Spatial Ability in Augmented Reality-assisted Robot Programming](https://arxiv.org/abs/2602.03544)
*Nicolas Leins,Jana Gonnermann-Müller,Malte Teichmann,Sebastian Pokutta*

Main category: cs.RO

TL;DR: AR辅助机器人编程学习对空间能力较低的学习者有补偿作用，但整体学习体验与传统方法无显著差异


<details>
  <summary>Details</summary>
Motivation: 增强现实(AR)在提升学习效果方面具有潜力，但其作用机制和效果尚未完全明确。随着学习日益个性化，考虑学习者个体特征变得更为重要。本研究旨在探讨空间能力对AR学习体验的调节作用。

Method: 采用被试间实验设计(N=71)，比较传统机器人编程与头戴式AR辅助方法。使用心理旋转测试评估参与者的空间能力，通过系统可用性量表(SUS)和认知负荷测量学习体验。

Result: AR支持相比传统方法并未显著改善学习体验。但在控制组中，空间能力与SUS分数显著正相关，与外部认知负荷显著负相关；而在AR条件下，这些关系不显著，表明AR缓解了空间能力较低学习者的劣势。

Conclusion: AR具有补偿功能，能减少学习者特征的影响。未来研究应进一步探索AR的这种补偿作用，以指导设计满足多样化学习者需求、降低不同认知特征学习者障碍的个性化学习环境。

Abstract: Augmented Reality (AR) offers promising opportunities to enhance learning, but its mechanisms and effects are not yet fully understood. As learning becomes increasingly personalized, considering individual learner characteristics becomes more important. This study investigates the moderating effect of spatial ability on learning experience with AR in the context of robot programming. A between-subjects experiment ($N=71$) compared conventional robot programming to an AR-assisted approach using a head-mounted display. Participants' spatial ability was assessed using the Mental Rotation Test. The learning experience was measured through the System Usability Scale (SUS) and cognitive load. The results indicate that AR support does not significantly improve the learning experience compared to the conventional approach. However, AR appears to have a compensatory effect on the influence of spatial ability. In the control group, spatial ability was significantly positively associated with SUS scores and negatively associated with extraneous cognitive load, indicating that higher spatial ability predicts a better learning experience. In the AR condition, these relationships were not observable, suggesting that AR mitigated the disadvantage typically experienced by learners with lower spatial abilities. These findings suggest that AR can serve a compensatory function by reducing the influence of learner characteristics. Future research should further explore this compensatory role of AR to guide the design of personalized learning environments that address diverse learner needs and reduce barriers for learners with varying cognitive profiles.

</details>


### [115] [AffordanceGrasp-R1:Leveraging Reasoning-Based Affordance Segmentation with Reinforcement Learning for Robotic Grasping](https://arxiv.org/abs/2602.03547)
*Dingyi Zhou,Mu He,Zhuowei Fang,Xiangtong Yao,Yinlong Liu,Alois Knoll,Hu Cao*

Main category: cs.RO

TL;DR: AffordanceGrasp-R1：结合思维链冷启动与强化学习的推理驱动抓取框架，通过全局点云生成抓取候选并基于指令条件掩码过滤，在复杂语言条件场景中表现出色


<details>
  <summary>Details</summary>
Motivation: 现有机器人抓取方法在复杂语言条件操作场景中缺乏足够的推理能力和上下文感知，需要更智能的抓取决策框架

Method: 1. 思维链冷启动策略增强推理能力；2. 强化学习提升空间定位；3. 重新设计抓取流程：从全局场景点云生成抓取候选，再用指令条件affordance掩码过滤

Result: 在基准数据集上持续超越SOTA方法，真实世界机器人抓取评估验证了其在复杂语言条件操作场景中的鲁棒性和泛化能力

Conclusion: AffordanceGrasp-R1通过推理驱动的affordance分割框架，显著提升了机器人抓取在复杂语言条件场景中的性能

Abstract: We introduce AffordanceGrasp-R1, a reasoning-driven affordance segmentation framework for robotic grasping that combines a chain-of-thought (CoT) cold-start strategy with reinforcement learning to enhance deduction and spatial grounding. In addition, we redesign the grasping pipeline to be more context-aware by generating grasp candidates from the global scene point cloud and subsequently filtering them using instruction-conditioned affordance masks. Extensive experiments demonstrate that AffordanceGrasp-R1 consistently outperforms state-of-the-art (SOTA) methods on benchmark datasets, and real-world robotic grasping evaluations further validate its robustness and generalization under complex language-conditioned manipulation scenarios.

</details>


### [116] [Multi-Player, Multi-Strategy Quantum Game Model for Interaction-Aware Decision-Making in Autonomous Driving](https://arxiv.org/abs/2602.03571)
*Karim Essalmi,Fernando Garrido,Fawzi Nashashibi*

Main category: cs.RO

TL;DR: 提出量子博弈决策模型(QGDM)，结合经典博弈论与量子力学原理，解决自动驾驶中的交互感知决策问题，无需量子硬件即可实时运行。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶决策方法过度简化车辆间交互，忽视周围智能体之间的相互作用，且经典博弈论假设理性玩家，而人类行为常不确定或不理性。

Method: 提出量子博弈决策模型(QGDM)，融合经典博弈论与量子力学原理（叠加、纠缠、干涉），处理多玩家、多策略决策问题，可在标准计算机上实时运行。

Result: 在环岛、并道、高速公路等多种场景的仿真测试中，QGDM相比经典方法显著提高了成功率，降低了碰撞率，尤其在高度交互场景中表现突出。

Conclusion: QGDM是首个将量子博弈论应用于自动驾驶决策的研究之一，有效解决了交互感知问题，无需量子硬件即可实现实时决策性能提升。

Abstract: Although significant progress has been made in decision-making for automated driving, challenges remain for deployment in the real world. One challenge lies in addressing interaction-awareness. Most existing approaches oversimplify interactions between the ego vehicle and surrounding agents, and often neglect interactions among the agents themselves. A common solution is to model these interactions using classical game theory. However, its formulation assumes rational players, whereas human behavior is frequently uncertain or irrational. To address these challenges, we propose the Quantum Game Decision-Making (QGDM) model, a novel framework that combines classical game theory with quantum mechanics principles (such as superposition, entanglement, and interference) to tackle multi-player, multi-strategy decision-making problems. To the best of our knowledge, this is one of the first studies to apply quantum game theory to decision-making for automated driving. QGDM runs in real time on a standard computer, without requiring quantum hardware. We evaluate QGDM in simulation across various scenarios, including roundabouts, merging, and highways, and compare its performance with multiple baseline methods. Results show that QGDM significantly improves success rates and reduces collision rates compared to classical approaches, particularly in scenarios with high interaction.

</details>


### [117] [Human-in-the-Loop Failure Recovery with Adaptive Task Allocation](https://arxiv.org/abs/2602.03603)
*Lorena Maria Genua,Nikita Boguslavskii,Zhi Li*

Main category: cs.RO

TL;DR: 提出ARFA方法，根据操作员能力、任务紧急性和工作负载，自适应分配机器人故障给最合适的人类操作员，减少机器人闲置时间并提高系统性能。


<details>
  <summary>Details</summary>
Motivation: 新冠疫情后，移动机械臂和人形辅助机器人在医疗护理中应用增加，但它们在动态非结构化环境中可靠性不足，需要人工干预恢复故障。需要有效的人机协作来减少操作员工作负担并最小化任务中断。

Method: 提出ARFA（自适应机器人故障分配）方法：建模操作员能力并基于实际表现持续更新；通过奖励函数计算预期结果（考虑操作员能力、历史数据、任务紧急性和当前工作负载分布）；将故障分配给预期奖励最高的操作员。

Result: 仿真和用户研究表明，ARFA优于随机分配方法，显著减少机器人闲置时间，提高整体系统性能，并使操作员间工作负载分布更均衡。

Conclusion: ARFA方法通过智能分配机器人故障给最合适的人类操作员，有效提升了人机协作系统的效率和可靠性，减少了机器人故障恢复时间。

Abstract: Since the recent Covid-19 pandemic, mobile manipulators and humanoid assistive robots with higher levels of autonomy have increasingly been adopted for patient care and living assistance. Despite advancements in autonomy, these robots often struggle to perform reliably in dynamic and unstructured environments and require human intervention to recover from failures. Effective human-robot collaboration is essential to enable robots to receive assistance from the most competent operator, in order to reduce their workload and minimize disruptions in task execution. In this paper, we propose an adaptive method for allocating robotic failures to human operators (ARFA). Our proposed approach models the capabilities of human operators, and continuously updates these beliefs based on their actual performance for failure recovery. For every failure to be resolved, a reward function calculates expected outcomes based on operator capabilities and historical data, task urgency, and current workload distribution. The failure is then assigned to the operator with the highest expected reward. Our simulations and user studies show that ARFA outperforms random allocation, significantly reducing robot idle time, improving overall system performance, and leading to a more distributed workload among operators.

</details>


### [118] [Self-supervised Physics-Informed Manipulation of Deformable Linear Objects with Non-negligible Dynamics](https://arxiv.org/abs/2602.03623)
*Youyuan Long,Gokhan Solak,Sara Zeynalpour,Heng Zhang,Arash Ajoudani*

Main category: cs.RO

TL;DR: SPiD是一个用于可变形线性物体动态操作的物理信息自监督学习框架，通过耦合精确的物体模型与增强的自监督训练策略，在绳稳定任务中实现快速平滑的稳定效果，并具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决可变形线性物体（如绳子）的动态操作问题，传统方法难以准确建模物体动力学且需要大量专家监督数据。需要开发一个数据高效、鲁棒且物理基础扎实的框架。

Method: 1. 扩展质量-弹簧模型以更准确捕捉物体动力学，同时保持轻量级用于自监督学习中的高吞吐量推演；2. 使用任务导向成本训练神经控制器，通过可微分物体模型实现端到端优化；3. 提出自监督DAgger变体，在部署期间检测分布偏移并执行离线自校正以增强鲁棒性。

Result: 在绳稳定任务中，控制器实现了快速平滑的稳定效果，能够泛化到未见过的初始状态、绳长、质量、非均匀质量分布和外部干扰。开发了经济实惠的无标记绳感知方法，控制器在噪声和低频状态更新下仍保持性能。框架还可扩展到绳轨迹跟踪任务。

Conclusion: SPiD提供了一个数据高效、鲁棒且物理基础扎实的框架，用于可变形线性物体的动态操作，具有强大的仿真到现实泛化能力。

Abstract: We address dynamic manipulation of deformable linear objects by presenting SPiD, a physics-informed self-supervised learning framework that couples an accurate deformable object model with an augmented self-supervised training strategy. On the modeling side, we extend a mass-spring model to more accurately capture object dynamics while remaining lightweight enough for high-throughput rollouts during self-supervised learning. On the learning side, we train a neural controller using a task-oriented cost, enabling end-to-end optimization through interaction with the differentiable object model. In addition, we propose a self-supervised DAgger variant that detects distribution shift during deployment and performs offline self-correction to further enhance robustness without expert supervision. We evaluate our method primarily on the rope stabilization task, where a robot must bring a swinging rope to rest as quickly and smoothly as possible. Extensive experiments in both simulation and the real world demonstrate that the proposed controller achieves fast and smooth rope stabilization, generalizing across unseen initial states, rope lengths, masses, non-uniform mass distributions, and external disturbances. Additionally, we develop an affordable markerless rope perception method and demonstrate that our controller maintains performance with noisy and low-frequency state updates. Furthermore, we demonstrate the generality of the framework by extending it to the rope trajectory tracking task. Overall, SPiD offers a data-efficient, robust, and physically grounded framework for dynamic manipulation of deformable linear objects, featuring strong sim-to-real generalization.

</details>


### [119] [Variance-Reduced Model Predictive Path Integral via Quadratic Model Approximation](https://arxiv.org/abs/2602.03639)
*Fabian Schramm,Franki Nguimatsia Tiofack,Nicolas Perrin-Gilbert,Marc Toussaint,Justin Carpentier*

Main category: cs.RO

TL;DR: 提出一种混合方差缩减MPPI框架，通过整合先验模型到采样过程中，将目标函数分解为已知近似模型和残差项，降低方差并提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 基于采样的控制器（如MPPI）具有灵活性，但存在高方差和低样本效率的问题，特别是在样本获取昂贵或受限的场景中。

Method: 将目标函数分解为已知近似模型和残差项，采用二次近似推导出闭式模型引导先验，使样本集中在信息丰富区域。框架对几何信息源不可知，可使用精确导数、结构近似或梯度自由随机平滑构建二次模型。

Result: 在标准优化基准、非线性欠驱动倒立摆控制任务和具有非光滑动力学的接触丰富操作问题上验证，相比标准MPPI在低样本区域实现更快收敛和更优性能。

Conclusion: 该方法能使基于采样的控制策略在样本获取昂贵或受限的场景中更加实用，通过整合先验模型有效降低方差并提高样本效率。

Abstract: Sampling-based controllers, such as Model Predictive Path Integral (MPPI) methods, offer substantial flexibility but often suffer from high variance and low sample efficiency. To address these challenges, we introduce a hybrid variance-reduced MPPI framework that integrates a prior model into the sampling process. Our key insight is to decompose the objective function into a known approximate model and a residual term. Since the residual captures only the discrepancy between the model and the objective, it typically exhibits a smaller magnitude and lower variance than the original objective. Although this principle applies to general modeling choices, we demonstrate that adopting a quadratic approximation enables the derivation of a closed-form, model-guided prior that effectively concentrates samples in informative regions. Crucially, the framework is agnostic to the source of geometric information, allowing the quadratic model to be constructed from exact derivatives, structural approximations (e.g., Gauss- or Quasi-Newton), or gradient-free randomized smoothing. We validate the approach on standard optimization benchmarks, a nonlinear, underactuated cart-pole control task, and a contact-rich manipulation problem with non-smooth dynamics. Across these domains, we achieve faster convergence and superior performance in low-sample regimes compared to standard MPPI. These results suggest that the method can make sample-based control strategies more practical in scenarios where obtaining samples is expensive or limited.

</details>


### [120] [MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction](https://arxiv.org/abs/2602.03668)
*Jung Min Lee,Dohyeok Lee,Seokhun Ju,Taehyun Cho,Jin Woo Koo,Li Zhao,Sangwoo Hong,Jungwoo Lee*

Main category: cs.RO

TL;DR: MVP-LAM通过多视角视频学习离散潜在动作，提升VLA预训练效果


<details>
  <summary>Details</summary>
Motivation: 从多样的人类视频中学习潜在动作可以扩展机器人学习，但现有方法缺乏对真实动作的信息捕捉。需要学习能反映底层智能体动作的潜在动作，尽管没有真实标签。

Method: 提出多视角潜在动作模型（MVP-LAM），通过跨视角重建目标训练潜在动作：从一个视角推断的潜在动作必须能解释另一个视角的未来状态，减少对视角特定线索的依赖。

Result: 在Bridge V2数据集上，MVP-LAM产生更动作中心的潜在动作，与真实动作的互信息更高，动作预测效果更好，包括在分布外评估中。使用MVP-LAM潜在动作预训练VLA模型提升了SIMPLER和LIBERO-Long基准的下游操作性能。

Conclusion: MVP-LAM通过多视角学习能有效捕捉动作信息的潜在动作，显著提升VLA预训练效果和下游机器人操作任务性能。

Abstract: Learning \emph{latent actions} from diverse human videos enables scaling robot learning beyond embodiment-specific robot datasets, and these latent actions have recently been used as pseudo-action labels for vision-language-action (VLA) model pretraining. To make VLA pretraining effective, latent actions should contain information about the underlying agent's actions despite the absence of ground-truth labels. We propose \textbf{M}ulti-\textbf{V}iew\textbf{P}oint \textbf{L}atent \textbf{A}ction \textbf{M}odel (\textbf{MVP-LAM}), which learns discrete latent actions that are highly informative about ground-truth actions from time-synchronized multi-view videos. MVP-LAM trains latent actions with a \emph{cross-viewpoint reconstruction} objective, so that a latent action inferred from one view must explain the future in another view, reducing reliance on viewpoint-specific cues. On Bridge V2, MVP-LAM produces more action-centric latent actions, achieving higher mutual information with ground-truth actions and improved action prediction, including under out-of-distribution evaluation. Finally, pretraining VLAs with MVP-LAM latent actions improves downstream manipulation performance on the SIMPLER and LIBERO-Long benchmarks.

</details>


### [121] [A Scene Graph Backed Approach to Open Set Semantic Mapping](https://arxiv.org/abs/2602.03781)
*Martin Günther,Felix Igelbrink,Oscar Lima,Lennart Niecksch,Marian Renz,Martin Atzmueller*

Main category: cs.RO

TL;DR: 提出一种以3D语义场景图作为基础后端的地图架构，实现实时增量式场景图预测与更新，为高层推理提供稳定可验证的知识表示。


<details>
  <summary>Details</summary>
Motivation: 现有开放集语义地图和3D语义场景图方法通常将感知与表示解耦，将场景图作为后处理层，这限制了系统的一致性和可扩展性。需要一种能够在大规模真实环境中有效支持高层推理的表示方法。

Method: 提出以3D语义场景图作为整个地图过程的基础后端架构，利用增量式场景图预测技术实时推断和更新图结构，保持拓扑一致性和计算效率，支持平面和分层拓扑的显式空间表示。

Result: 该方法能够在探索环境时实时维护拓扑一致的地图，即使在大规模环境中长时间操作也能保持计算效率，为知识图谱、本体论和大语言模型等知识驱动框架提供稳定可验证的结构。

Conclusion: 通过将3D语义场景图作为基础表示，弥合了原始传感器数据与高层符号推理之间的鸿沟，使智能体能够以增强的可解释性、可信度和与人类概念对齐的方式操作。

Abstract: While Open Set Semantic Mapping and 3D Semantic Scene Graphs (3DSSGs) are established paradigms in robotic perception, deploying them effectively to support high-level reasoning in large-scale, real-world environments remains a significant challenge. Most existing approaches decouple perception from representation, treating the scene graph as a derivative layer generated post hoc. This limits both consistency and scalability. In contrast, we propose a mapping architecture where the 3DSSG serves as the foundational backend, acting as the primary knowledge representation for the entire mapping process.
  Our approach leverages prior work on incremental scene graph prediction to infer and update the graph structure in real-time as the environment is explored. This ensures that the map remains topologically consistent and computationally efficient, even during extended operations in large-scale settings. By maintaining an explicit, spatially grounded representation that supports both flat and hierarchical topologies, we bridge the gap between sub-symbolic raw sensor data and high-level symbolic reasoning. Consequently, this provides a stable, verifiable structure that knowledge-driven frameworks, ranging from knowledge graphs and ontologies to Large Language Models (LLMs), can directly exploit, enabling agents to operate with enhanced interpretability, trustworthiness, and alignment to human concepts.

</details>


### [122] [BridgeV2W: Bridging Video Generation Models to Embodied World Models via Embodiment Masks](https://arxiv.org/abs/2602.03793)
*Yixiang Chen,Peiyan Li,Jiabing Yang,Keji He,Xiangnan Wu,Yuan Xu,Kai Wang,Jing Liu,Nianfeng Liu,Yan Huang,Liang Wang*

Main category: cs.RO

TL;DR: BridgeV2W：一种将坐标空间动作转换为像素对齐的体现掩码，并通过ControlNet风格路径注入预训练视频生成模型的世界建模方法，解决了动作-视频不对齐、相机视角敏感和多体现架构不统一的问题。


<details>
  <summary>Details</summary>
Motivation: 现有体现世界模型面临三个关键挑战：坐标空间动作与像素空间视频之间的不对齐、对相机视角的敏感性，以及跨不同体现架构的非统一性。这些问题限制了世界模型在机器人领域的应用效果。

Method: BridgeV2W将坐标空间动作转换为从URDF和相机参数渲染的像素对齐体现掩码，通过ControlNet风格路径注入预训练视频生成模型。该方法还引入基于流的运动损失来减轻静态背景过拟合，专注于学习动态和任务相关区域。

Result: 在单臂（DROID）和双臂（AgiBot-G1）数据集上的实验表明，BridgeV2W在具有未见视角和场景的多样化挑战条件下，相比现有最先进方法提高了视频生成质量。同时展示了在下游实际任务（如策略评估和目标条件规划）中的潜力。

Conclusion: BridgeV2W通过将坐标动作转换为像素对齐掩码并注入视频生成模型，有效解决了体现世界模型中的关键对齐问题，实现了跨不同体现的统一架构，并在复杂场景下展现出优越的视频生成能力和实际应用潜力。

Abstract: Embodied world models have emerged as a promising paradigm in robotics, most of which leverage large-scale Internet videos or pretrained video generation models to enrich visual and motion priors. However, they still face key challenges: a misalignment between coordinate-space actions and pixel-space videos, sensitivity to camera viewpoint, and non-unified architectures across embodiments. To this end, we present BridgeV2W, which converts coordinate-space actions into pixel-aligned embodiment masks rendered from the URDF and camera parameters. These masks are then injected into a pretrained video generation model via a ControlNet-style pathway, which aligns the action control signals with predicted videos, adds view-specific conditioning to accommodate camera viewpoints, and yields a unified world model architecture across embodiments. To mitigate overfitting to static backgrounds, BridgeV2W further introduces a flow-based motion loss that focuses on learning dynamic and task-relevant regions. Experiments on single-arm (DROID) and dual-arm (AgiBot-G1) datasets, covering diverse and challenging conditions with unseen viewpoints and scenes, show that BridgeV2W improves video generation quality compared to prior state-of-the-art methods. We further demonstrate the potential of BridgeV2W on downstream real-world tasks, including policy evaluation and goal-conditioned planning. More results can be found on our project website at https://BridgeV2W.github.io .

</details>


### [123] [Conformal Reachability for Safe Control in Unknown Environments](https://arxiv.org/abs/2602.03799)
*Xinhang Ma,Junlin Wu,Yiannis Kantaros,Yevgeniy Vorobeychik*

Main category: cs.RO

TL;DR: 提出结合共形预测与可达性分析的未知动力学系统概率验证框架，用于学习具有可证明安全保证的控制策略


<details>
  <summary>Details</summary>
Motivation: 现有可证明安全控制方法大多假设系统动力学已知、确定或状态/动作空间有限，这严重限制了实际应用范围。需要为未知动力学系统开发概率验证框架。

Method: 1) 使用共形预测为未知动力学在每个时间步获取有效的置信区间；2) 通过可达性分析验证在共形不确定性边界内是否保持安全性；3) 开发算法训练控制策略，在优化名义奖励的同时最大化具有可靠概率安全保证的规划时域。

Result: 在七个安全控制场景（涵盖四个领域：倒立摆、车道跟随、无人机控制和安全导航）中评估，针对线性和非线性安全规范。实验表明，学习的策略实现了最强的可证明安全保证，同时保持较高的平均奖励。

Conclusion: 成功开发了结合共形预测与可达性分析的未知动力学系统概率验证框架，能够学习既安全又高效的控制策略，扩展了可证明安全控制的应用范围。

Abstract: Designing provably safe control is a core problem in trustworthy autonomy. However, most prior work in this regard assumes either that the system dynamics are known or deterministic, or that the state and action space are finite, significantly limiting application scope. We address this limitation by developing a probabilistic verification framework for unknown dynamical systems which combines conformal prediction with reachability analysis. In particular, we use conformal prediction to obtain valid uncertainty intervals for the unknown dynamics at each time step, with reachability then verifying whether safety is maintained within the conformal uncertainty bounds. Next, we develop an algorithmic approach for training control policies that optimize nominal reward while also maximizing the planning horizon with sound probabilistic safety guarantees. We evaluate the proposed approach in seven safe control settings spanning four domains -- cartpole, lane following, drone control, and safe navigation -- for both affine and nonlinear safety specifications. Our experiments show that the policies we learn achieve the strongest provable safety guarantees while still maintaining high average reward.

</details>
