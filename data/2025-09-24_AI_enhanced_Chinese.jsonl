{"id": "2509.18282", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18282", "abs": "https://arxiv.org/abs/2509.18282", "authors": ["Jesse Zhang", "Marius Memmel", "Kevin Kim", "Dieter Fox", "Jesse Thomason", "Fabio Ramos", "Erdem B\u0131y\u0131k", "Abhishek Gupta", "Anqi Li"], "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies", "comment": "11 pages", "summary": "Robotic manipulation policies often fail to generalize because they must\nsimultaneously learn where to attend, what actions to take, and how to execute\nthem. We argue that high-level reasoning about where and what can be offloaded\nto vision-language models (VLMs), leaving policies to specialize in how to act.\nWe present PEEK (Policy-agnostic Extraction of Essential Keypoints), which\nfine-tunes VLMs to predict a unified point-based intermediate representation:\n1. end-effector paths specifying what actions to take, and 2. task-relevant\nmasks indicating where to focus. These annotations are directly overlaid onto\nrobot observations, making the representation policy-agnostic and transferable\nacross architectures. To enable scalable training, we introduce an automatic\nannotation pipeline, generating labeled data across 20+ robot datasets spanning\n9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot\ngeneralization, including a 41.4x real-world improvement for a 3D policy\ntrained only in simulation, and 2-3.5x gains for both large VLAs and small\nmanipulation policies. By letting VLMs absorb semantic and visual complexity,\nPEEK equips manipulation policies with the minimal cues they need--where, what,\nand how. Website at https://peek-robot.github.io/.", "AI": {"tldr": "PEEK\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u4e2d\u95f4\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u6d4b\u672b\u7aef\u6267\u884c\u5668\u8def\u5f84\u548c\u4efb\u52a1\u76f8\u5173\u63a9\u7801\u6765\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7ecf\u5e38\u56e0\u4e3a\u9700\u8981\u540c\u65f6\u5b66\u4e60\u6ce8\u610f\u529b\u4f4d\u7f6e\u3001\u52a8\u4f5c\u9009\u62e9\u548c\u6267\u884c\u65b9\u5f0f\u800c\u6cdb\u5316\u5931\u8d25\u3002\u4f5c\u8005\u8ba4\u4e3a\u9ad8\u7ea7\u63a8\u7406\u53ef\u4ee5\u5378\u8f7d\u7ed9VLM\uff0c\u8ba9\u7b56\u7565\u4e13\u6ce8\u4e8e\u52a8\u4f5c\u6267\u884c\u3002", "method": "PEEK\u5fae\u8c03VLM\u6765\u9884\u6d4b\u7edf\u4e00\u7684\u57fa\u4e8e\u70b9\u7684\u4e2d\u95f4\u8868\u793a\uff1a1\uff09\u672b\u7aef\u6267\u884c\u5668\u8def\u5f84\uff08\u6307\u5b9a\u52a8\u4f5c\uff09\uff0c2\uff09\u4efb\u52a1\u76f8\u5173\u63a9\u7801\uff08\u6307\u5b9a\u6ce8\u610f\u529b\u4f4d\u7f6e\uff09\u3002\u8fd9\u4e9b\u6807\u6ce8\u76f4\u63a5\u53e0\u52a0\u5728\u673a\u5668\u4eba\u89c2\u5bdf\u4e0a\uff0c\u4f7f\u8868\u793a\u5177\u6709\u7b56\u7565\u65e0\u5173\u6027\u548c\u8de8\u67b6\u6784\u53ef\u8f6c\u79fb\u6027\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u8bc4\u4f30\u4e2d\uff0cPEEK\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ec\u4ec5\u6a21\u62df\u8bad\u7ec3\u76843D\u7b56\u7565\u5b9e\u73b041.4\u500d\u6539\u8fdb\uff0c\u5927\u578bVLA\u548c\u5c0f\u578b\u64cd\u4f5c\u7b56\u7565\u5206\u522b\u83b7\u5f972-3.5\u500d\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u8ba9VLM\u5438\u6536\u8bed\u4e49\u548c\u89c6\u89c9\u590d\u6742\u6027\uff0cPEEK\u4e3a\u64cd\u4f5c\u7b56\u7565\u63d0\u4f9b\u4e86\u6240\u9700\u7684\u6700\u5c0f\u63d0\u793a\u2014\u2014\u5728\u54ea\u91cc\u3001\u505a\u4ec0\u4e48\u548c\u600e\u4e48\u505a\u3002"}}
{"id": "2509.18311", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18311", "abs": "https://arxiv.org/abs/2509.18311", "authors": ["Benjamin A. Christie", "Sagar Parekh", "Dylan P. Losey"], "title": "Fine-Tuning Robot Policies While Maintaining User Privacy", "comment": null, "summary": "Recent works introduce general-purpose robot policies. These policies provide\na strong prior over how robots should behave -- e.g., how a robot arm should\nmanipulate food items. But in order for robots to match an individual person's\nneeds, users typically fine-tune these generalized policies -- e.g., showing\nthe robot arm how to make their own preferred dinners. Importantly, during the\nprocess of personalizing robots, end-users leak data about their preferences,\nhabits, and styles (e.g., the foods they prefer to eat). Other agents can\nsimply roll-out the fine-tuned policy and see these personally-trained\nbehaviors. This leads to a fundamental challenge: how can we develop robots\nthat personalize actions while keeping learning private from external agents?\nWe here explore this emerging topic in human-robot interaction and develop\nPRoP, a model-agnostic framework for personalized and private robot policies.\nOur core idea is to equip each user with a unique key; this key is then used to\nmathematically transform the weights of the robot's network. With the correct\nkey, the robot's policy switches to match that user's preferences -- but with\nincorrect keys, the robot reverts to its baseline behaviors. We show the\ngeneral applicability of our method across multiple model types in imitation\nlearning, reinforcement learning, and classification tasks. PRoP is practically\nadvantageous because it retains the architecture and behaviors of the original\npolicy, and experimentally outperforms existing encoder-based approaches. See\nvideos and code here: https://prop-icra26.github.io.", "AI": {"tldr": "PRoP\u662f\u4e00\u4e2a\u7528\u4e8e\u4e2a\u6027\u5316\u4e14\u79c1\u5bc6\u7684\u673a\u5668\u4eba\u7b56\u7565\u7684\u6a21\u578b\u65e0\u5173\u6846\u67b6\uff0c\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u7528\u6237\u5206\u914d\u552f\u4e00\u5bc6\u94a5\u6765\u6570\u5b66\u53d8\u6362\u673a\u5668\u4eba\u7f51\u7edc\u6743\u91cd\uff0c\u786e\u4fdd\u53ea\u6709\u6b63\u786e\u5bc6\u94a5\u624d\u80fd\u8bbf\u95ee\u4e2a\u6027\u5316\u884c\u4e3a\u3002", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u9700\u8981\u4e2a\u6027\u5316\u8c03\u6574\u4ee5\u5339\u914d\u7528\u6237\u504f\u597d\uff0c\u4f46\u4e2a\u6027\u5316\u8fc7\u7a0b\u4e2d\u4f1a\u6cc4\u9732\u7528\u6237\u6570\u636e\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4e2a\u6027\u5316\u53c8\u80fd\u4fdd\u62a4\u9690\u79c1\u7684\u673a\u5668\u4eba\u7b56\u7565\u3002", "method": "\u4e3a\u6bcf\u4e2a\u7528\u6237\u5206\u914d\u552f\u4e00\u5bc6\u94a5\uff0c\u7528\u8be5\u5bc6\u94a5\u6570\u5b66\u53d8\u6362\u673a\u5668\u4eba\u7f51\u7edc\u6743\u91cd\uff0c\u53ea\u6709\u6b63\u786e\u5bc6\u94a5\u624d\u80fd\u6fc0\u6d3b\u4e2a\u6027\u5316\u7b56\u7565\uff0c\u9519\u8bef\u5bc6\u94a5\u5219\u6062\u590d\u57fa\u7ebf\u884c\u4e3a\u3002", "result": "PRoP\u5728\u6a21\u4eff\u5b66\u4e60\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4fdd\u6301\u539f\u59cb\u7b56\u7565\u67b6\u6784\u548c\u884c\u4e3a\uff0c\u5b9e\u9a8c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u7f16\u7801\u5668\u7684\u65b9\u6cd5\u3002", "conclusion": "PRoP\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u4e2a\u6027\u5316\u673a\u5668\u4eba\u7b56\u7565\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4f18\u52bf\u3002"}}
{"id": "2509.18327", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18327", "abs": "https://arxiv.org/abs/2509.18327", "authors": ["Katherine H. Allen", "Chris Rogers", "Elaine S. Short"], "title": "Haptic Communication in Human-Human and Human-Robot Co-Manipulation", "comment": "9 pages, 18 figures, ROMAN 2025", "summary": "When a human dyad jointly manipulates an object, they must communicate about\ntheir intended motion plans. Some of that collaboration is achieved through the\nmotion of the manipulated object itself, which we call \"haptic communication.\"\nIn this work, we captured the motion of human-human dyads moving an object\ntogether with one participant leading a motion plan about which the follower is\nuninformed. We then captured the same human participants manipulating the same\nobject with a robot collaborator. By tracking the motion of the shared object\nusing a low-cost IMU, we can directly compare human-human shared manipulation\nto the motion of those same participants interacting with the robot.\nIntra-study and post-study questionnaires provided participant feedback on the\ncollaborations, indicating that the human-human collaborations are\nsignificantly more fluent, and analysis of the IMU data indicates that it\ncaptures objective differences in the motion profiles of the conditions. The\ndifferences in objective and subjective measures of accuracy and fluency\nbetween the human-human and human-robot trials motivate future research into\nimproving robot assistants for physical tasks by enabling them to send and\nreceive anthropomorphic haptic signals.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4eba-\u4eba\u548c\u4eba-\u673a\u5668\u4eba\u534f\u4f5c\u64cd\u7eb5\u7269\u4f53\u65f6\u7684\u8fd0\u52a8\u5dee\u5f02\uff0c\u53d1\u73b0\u4eba-\u4eba\u534f\u4f5c\u66f4\u52a0\u6d41\u7545\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u8fc7\u6539\u8fdb\u673a\u5668\u4eba\u7684\u89e6\u89c9\u4fe1\u53f7\u4f20\u9012\u80fd\u529b\u6765\u63d0\u5347\u673a\u5668\u4eba\u534f\u4f5c\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u5728\u534f\u4f5c\u64cd\u7eb5\u7269\u4f53\u65f6\u5982\u4f55\u901a\u8fc7\u89e6\u89c9\u4fe1\u53f7\u8fdb\u884c\u6c9f\u901a\uff0c\u5e76\u5c06\u8fd9\u79cd\u6c9f\u901a\u6a21\u5f0f\u5e94\u7528\u4e8e\u6539\u8fdb\u673a\u5668\u4eba\u534f\u4f5c\u80fd\u529b\u3002", "method": "\u901a\u8fc7IMU\u4f20\u611f\u5668\u8bb0\u5f55\u4eba-\u4eba\u534f\u4f5c\u548c\u4eba-\u673a\u5668\u4eba\u534f\u4f5c\u65f6\u7684\u7269\u4f53\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5e76\u7ed3\u5408\u95ee\u5377\u8c03\u67e5\u6536\u96c6\u4e3b\u89c2\u8bc4\u4ef7\u6570\u636e\u3002", "result": "\u4eba-\u4eba\u534f\u4f5c\u5728\u6d41\u7545\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4eba-\u673a\u5668\u4eba\u534f\u4f5c\uff0cIMU\u6570\u636e\u80fd\u591f\u5ba2\u89c2\u6355\u6349\u5230\u4e24\u79cd\u6761\u4ef6\u4e0b\u7684\u8fd0\u52a8\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u9700\u8981\u6539\u8fdb\u673a\u5668\u4eba\u53d1\u9001\u548c\u63a5\u6536\u7c7b\u4eba\u89e6\u89c9\u4fe1\u53f7\u7684\u80fd\u529b\uff0c\u4ee5\u63d0\u5347\u5176\u5728\u7269\u7406\u4efb\u52a1\u4e2d\u7684\u534f\u4f5c\u6027\u80fd\u3002"}}
{"id": "2509.18330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18330", "abs": "https://arxiv.org/abs/2509.18330", "authors": ["Marsette Vona"], "title": "The Landform Contextual Mesh: Automatically Fusing Surface and Orbital Terrain for Mars 2020", "comment": null, "summary": "The Landform contextual mesh fuses 2D and 3D data from up to thousands of\nMars 2020 rover images, along with orbital elevation and color maps from Mars\nReconnaissance Orbiter, into an interactive 3D terrain visualization.\nContextual meshes are built automatically for each rover location during\nmission ground data system processing, and are made available to mission\nscientists for tactical and strategic planning in the Advanced Science\nTargeting Tool for Robotic Operations (ASTTRO). A subset of them are also\ndeployed to the \"Explore with Perseverance\" public access website.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Landform contextual mesh\u6280\u672f\uff0c\u8be5\u6280\u672f\u878d\u5408\u4e86\u706b\u661f2020\u63a2\u6d4b\u8f66\u62cd\u6444\u7684\u6570\u5343\u5f202D\u548c3D\u56fe\u50cf\u6570\u636e\uff0c\u4ee5\u53ca\u706b\u661f\u52d8\u6d4b\u8f68\u9053\u98de\u884c\u5668\u63d0\u4f9b\u7684\u8f68\u9053\u9ad8\u7a0b\u548c\u5f69\u8272\u5730\u56fe\uff0c\u751f\u6210\u4ea4\u4e92\u5f0f3D\u5730\u5f62\u53ef\u89c6\u5316\u3002", "motivation": "\u4e3a\u706b\u661f\u4efb\u52a1\u79d1\u5b66\u5bb6\u63d0\u4f9b\u6218\u672f\u548c\u6218\u7565\u89c4\u5212\u7684\u4ea4\u4e92\u5f0f3D\u5730\u5f62\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u540c\u65f6\u5411\u516c\u4f17\u5f00\u653e\u90e8\u5206\u6570\u636e\u4ee5\u589e\u5f3a\u79d1\u5b66\u4f20\u64ad\u3002", "method": "\u81ea\u52a8\u4e3a\u6bcf\u4e2a\u63a2\u6d4b\u8f66\u4f4d\u7f6e\u6784\u5efa\u4e0a\u4e0b\u6587\u7f51\u683c\uff0c\u878d\u5408\u591a\u6e90\u6570\u636e\uff08\u63a2\u6d4b\u8f66\u56fe\u50cf\u3001\u8f68\u9053\u9ad8\u7a0b\u56fe\u3001\u5f69\u8272\u5730\u56fe\uff09\uff0c\u901a\u8fc7\u4efb\u52a1\u5730\u9762\u6570\u636e\u5904\u7406\u7cfb\u7edf\u751f\u6210\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51faLandform contextual mesh\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5df2\u96c6\u6210\u5230ASTTRO\u5de5\u5177\u4e2d\u4f9b\u79d1\u5b66\u5bb6\u4f7f\u7528\uff0c\u5e76\u5728\"Explore with Perseverance\"\u7f51\u7ad9\u4e0a\u5411\u516c\u4f17\u5f00\u653e\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3a\u706b\u661f\u63a2\u6d4b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u76843D\u5730\u5f62\u53ef\u89c6\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u79d1\u5b66\u51b3\u7b56\u548c\u516c\u4f17\u53c2\u4e0e\u3002"}}
{"id": "2509.18342", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18342", "abs": "https://arxiv.org/abs/2509.18342", "authors": ["Rajitha de Silva", "Jonathan Cox", "James R. Heselden", "Marija Popovic", "Cesar Cadena", "Riccardo Polvara"], "title": "Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation", "comment": "Sumbitted to ICRA 2026", "summary": "Accurate localisation is critical for mobile robots in structured outdoor\nenvironments, yet LiDAR-based methods often fail in vineyards due to repetitive\nrow geometry and perceptual aliasing. We propose a semantic particle filter\nthat incorporates stable object-level detections, specifically vine trunks and\nsupport poles into the likelihood estimation process. Detected landmarks are\nprojected into a birds eye view and fused with LiDAR scans to generate semantic\nobservations. A key innovation is the use of semantic walls, which connect\nadjacent landmarks into pseudo-structural constraints that mitigate row\naliasing. To maintain global consistency in headland regions where semantics\nare sparse, we introduce a noisy GPS prior that adaptively supports the filter.\nExperiments in a real vineyard demonstrate that our approach maintains\nlocalisation within the correct row, recovers from deviations where AMCL fails,\nand outperforms vision-based SLAM methods such as RTAB-Map.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u8461\u8404\u56ed\u73af\u5883\u7684\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u901a\u8fc7\u878d\u5408LiDAR\u626b\u63cf\u548c\u8bed\u4e49\u5730\u6807\uff08\u8461\u8404\u6811\u5e72\u548c\u652f\u6491\u6746\uff09\u6765\u89e3\u51b3\u91cd\u590d\u884c\u51e0\u4f55\u7ed3\u6784\u5bfc\u81f4\u7684\u5b9a\u4f4d\u95ee\u9898\u3002", "motivation": "\u5728\u7ed3\u6784\u5316\u6237\u5916\u73af\u5883\uff08\u5982\u8461\u8404\u56ed\uff09\u4e2d\uff0c\u7531\u4e8e\u91cd\u590d\u7684\u884c\u51e0\u4f55\u7ed3\u6784\u548c\u611f\u77e5\u6df7\u53e0\uff0c\u4f20\u7edf\u7684LiDAR\u5b9a\u4f4d\u65b9\u6cd5\u7ecf\u5e38\u5931\u8d25\uff0c\u9700\u8981\u66f4\u7a33\u5b9a\u7684\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u5c06\u68c0\u6d4b\u5230\u7684\u5730\u6807\u6295\u5f71\u5230\u9e1f\u77b0\u56fe\u5e76\u4e0eLiDAR\u626b\u63cf\u878d\u5408\u751f\u6210\u8bed\u4e49\u89c2\u6d4b\u3002\u521b\u65b0\u6027\u5730\u4f7f\u7528\u8bed\u4e49\u5899\u8fde\u63a5\u76f8\u90bb\u5730\u6807\u5f62\u6210\u4f2a\u7ed3\u6784\u7ea6\u675f\uff0c\u5e76\u5728\u8bed\u4e49\u7a00\u758f\u533a\u57df\u5f15\u5165\u81ea\u9002\u5e94GPS\u5148\u9a8c\u3002", "result": "\u5728\u771f\u5b9e\u8461\u8404\u56ed\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6b63\u786e\u7684\u884c\u5185\u4fdd\u6301\u5b9a\u4f4d\uff0c\u4eceAMCL\u5931\u8d25\u7684\u504f\u5dee\u4e2d\u6062\u590d\uff0c\u5e76\u4f18\u4e8eRTAB-Map\u7b49\u57fa\u4e8e\u89c6\u89c9\u7684SLAM\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8bed\u4e49\u7c92\u5b50\u6ee4\u6ce2\u5668\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u5730\u6807\u548c\u81ea\u9002\u5e94GPS\u5148\u9a8c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8461\u8404\u56ed\u73af\u5883\u4e2d\u7684\u5b9a\u4f4d\u6311\u6218\uff0c\u4e3a\u519c\u4e1a\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18384", "categories": ["cs.RO", "cs.FL"], "pdf": "https://arxiv.org/pdf/2509.18384", "abs": "https://arxiv.org/abs/2509.18384", "authors": ["Yunhao Yang", "Junyuan Hong", "Gabriel Jacob Perin", "Zhiwen Fan", "Li Yin", "Zhangyang Wang", "Ufuk Topcu"], "title": "AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback", "comment": null, "summary": "Large language models (LLMs) can translate natural language instructions into\nexecutable action plans for robotics, autonomous driving, and other domains.\nYet, deploying LLM-driven planning in the physical world demands strict\nadherence to safety and regulatory constraints, which current models often\nviolate due to hallucination or weak alignment. Traditional data-driven\nalignment methods, such as Direct Preference Optimization (DPO), require costly\nhuman labeling, while recent formal-feedback approaches still depend on\nresource-intensive fine-tuning. In this paper, we propose LAD-VF, a\nfine-tuning-free framework that leverages formal verification feedback for\nautomated prompt engineering. By introducing a formal-verification-informed\ntext loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts\nrather than model parameters. This yields three key benefits: (i) scalable\nadaptation without fine-tuning; (ii) compatibility with modular LLM\narchitectures; and (iii) interpretable refinement via auditable prompts.\nExperiments in robot navigation and manipulation tasks demonstrate that LAD-VF\nsubstantially enhances specification compliance, improving success rates from\n60% to over 90%. Our method thus presents a scalable and interpretable pathway\ntoward trustworthy, formally-verified LLM-driven control systems.", "AI": {"tldr": "LAD-VF\u662f\u4e00\u4e2a\u65e0\u9700\u5fae\u8c03\u7684\u6846\u67b6\uff0c\u5229\u7528\u5f62\u5f0f\u5316\u9a8c\u8bc1\u53cd\u9988\u8fdb\u884c\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\uff0c\u901a\u8fc7\u6587\u672c\u635f\u5931\u51fd\u6570\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u800c\u975e\u6a21\u578b\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347LLM\u9a71\u52a8\u89c4\u5212\u7cfb\u7edf\u7684\u89c4\u8303\u5408\u89c4\u6027\u3002", "motivation": "\u5f53\u524dLLM\u9a71\u52a8\u7684\u89c4\u5212\u7cfb\u7edf\u5728\u7269\u7406\u4e16\u754c\u90e8\u7f72\u65f6\u7ecf\u5e38\u8fdd\u53cd\u5b89\u5168\u7ea6\u675f\uff0c\u4f20\u7edf\u5bf9\u9f50\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6216\u8d44\u6e90\u5bc6\u96c6\u7684\u5fae\u8c03\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faLAD-VF\u6846\u67b6\uff0c\u96c6\u6210\u5f62\u5f0f\u5316\u9a8c\u8bc1\u53cd\u9988\u7684\u6587\u672c\u635f\u5931\u51fd\u6570\u4e0eLLM-AutoDiff\uff0c\u901a\u8fc7\u8fed\u4ee3\u63d0\u793a\u5de5\u7a0b\u800c\u975e\u6a21\u578b\u5fae\u8c03\u6765\u4f18\u5316\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\uff0cLAD-VF\u5c06\u6210\u529f\u7387\u4ece60%\u63d0\u5347\u81f390%\u4ee5\u4e0a\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u89c4\u8303\u5408\u89c4\u6027\u3002", "conclusion": "LAD-VF\u4e3a\u6784\u5efa\u53ef\u4fe1\u8d56\u3001\u53ef\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684LLM\u9a71\u52a8\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u8def\u5f84\u3002"}}
{"id": "2509.18407", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.18407", "abs": "https://arxiv.org/abs/2509.18407", "authors": ["Navya Tiwari", "Joseph Vazhaeparampil", "Victoria Preston"], "title": "Assistive Decision-Making for Right of Way Navigation at Uncontrolled Intersections", "comment": "6 pages, 5 figures. Accepted as a poster at Northeast Robotics\n  Colloquium (NERC 2025). Extended abstract", "summary": "Uncontrolled intersections account for a significant fraction of roadway\ncrashes due to ambiguous right-of-way rules, occlusions, and unpredictable\ndriver behavior. While autonomous vehicle research has explored\nuncertainty-aware decision making, few systems exist to retrofit human-operated\nvehicles with assistive navigation support. We present a driver-assist\nframework for right-of-way reasoning at uncontrolled intersections, formulated\nas a Partially Observable Markov Decision Process (POMDP). Using a custom\nsimulation testbed with stochastic traffic agents, pedestrians, occlusions, and\nadversarial scenarios, we evaluate four decision-making approaches: a\ndeterministic finite state machine (FSM), and three probabilistic planners:\nQMDP, POMCP, and DESPOT. Results show that probabilistic planners outperform\nthe rule-based baseline, achieving up to 97.5 percent collision-free navigation\nunder partial observability, with POMCP prioritizing safety and DESPOT\nbalancing efficiency and runtime feasibility. Our findings highlight the\nimportance of uncertainty-aware planning for driver assistance and motivate\nfuture integration of sensor fusion and environment perception modules for\nreal-time deployment in realistic traffic environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08POMDP\uff09\u7684\u9a7e\u9a76\u5458\u8f85\u52a9\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u63a7\u5236\u4ea4\u53c9\u53e3\u7684\u901a\u884c\u6743\u63a8\u7406\uff0c\u901a\u8fc7\u6982\u7387\u89c4\u5212\u5668\u663e\u8457\u63d0\u9ad8\u4e86\u78b0\u649e\u907f\u514d\u7387\u3002", "motivation": "\u65e0\u63a7\u5236\u4ea4\u53c9\u53e3\u7531\u4e8e\u901a\u884c\u6743\u89c4\u5219\u6a21\u7cca\u3001\u89c6\u7ebf\u906e\u6321\u548c\u9a7e\u9a76\u5458\u884c\u4e3a\u4e0d\u53ef\u9884\u6d4b\u7b49\u56e0\u7d20\uff0c\u662f\u9053\u8def\u4e8b\u6545\u7684\u9ad8\u53d1\u533a\u57df\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u4eba\u5de5\u9a7e\u9a76\u8f66\u8f86\u7684\u8f85\u52a9\u5bfc\u822a\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86\u9a7e\u9a76\u5458\u8f85\u52a9\u6846\u67b6\uff0c\u91c7\u7528POMDP\u8fdb\u884c\u5efa\u6a21\uff0c\u5728\u81ea\u5b9a\u4e49\u4eff\u771f\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u6bd4\u8f83\u4e86\u56db\u79cd\u51b3\u7b56\u65b9\u6cd5\uff1a\u786e\u5b9a\u6027\u6709\u9650\u72b6\u6001\u673a\uff08FSM\uff09\u548c\u4e09\u79cd\u6982\u7387\u89c4\u5212\u5668\uff08QMDP\u3001POMCP\u3001DESPOT\uff09\u3002", "result": "\u6982\u7387\u89c4\u5212\u5668\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fbe97.5%\u7684\u65e0\u78b0\u649e\u5bfc\u822a\u7387\uff0c\u5176\u4e2dPOMCP\u4f18\u5148\u8003\u8651\u5b89\u5168\u6027\uff0cDESPOT\u5728\u6548\u7387\u548c\u8fd0\u884c\u65f6\u95f4\u53ef\u884c\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u89c4\u5212\u5728\u9a7e\u9a76\u5458\u8f85\u52a9\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u5728\u771f\u5b9e\u4ea4\u901a\u73af\u5883\u4e2d\u96c6\u6210\u4f20\u611f\u5668\u878d\u5408\u548c\u73af\u5883\u611f\u77e5\u6a21\u5757\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u90e8\u7f72\u63d0\u4f9b\u4e86\u52a8\u529b\u3002"}}
{"id": "2509.18428", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18428", "abs": "https://arxiv.org/abs/2509.18428", "authors": ["Bahey Tharwat", "Yara Nasser", "Ali Abouzeid", "Ian Reid"], "title": "Latent Action Pretraining Through World Modeling", "comment": null, "summary": "Vision-Language-Action (VLA) models have gained popularity for learning\nrobotic manipulation tasks that follow language instructions. State-of-the-art\nVLAs, such as OpenVLA and $\\pi_{0}$, were trained on large-scale, manually\nlabeled action datasets collected through teleoperation. More recent\napproaches, including LAPA and villa-X, introduce latent action representations\nthat enable unsupervised pretraining on unlabeled datasets by modeling abstract\nvisual changes between frames. Although these methods have shown strong\nresults, their large model sizes make deployment in real-world settings\nchallenging. In this work, we propose LAWM, a model-agnostic framework to\npretrain imitation learning models in a self-supervised way, by learning latent\naction representations from unlabeled video data through world modeling. These\nvideos can be sourced from robot recordings or videos of humans performing\nactions with everyday objects. Our framework is designed to be effective for\ntransferring across tasks, environments, and embodiments. It outperforms models\ntrained with ground-truth robotics actions and similar pretraining methods on\nthe LIBERO benchmark and real-world setup, while being significantly more\nefficient and practical for real-world settings.", "AI": {"tldr": "LAWM\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e16\u754c\u5efa\u6a21\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\u6570\u636e\u4e2d\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u7528\u4e8e\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u6a21\u4eff\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u8de8\u4efb\u52a1\u3001\u73af\u5883\u548c\u5177\u8eab\u7684\u8fc1\u79fb\u3002", "motivation": "\u73b0\u6709\u7684VLA\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u4eba\u5de5\u6807\u6ce8\u7684\u52a8\u4f5c\u6570\u636e\u96c6\uff0c\u6a21\u578b\u4f53\u79ef\u5e9e\u5927\uff0c\u96be\u4ee5\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u3002\u9700\u8981\u66f4\u9ad8\u6548\u3001\u5b9e\u7528\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLAWM\u6846\u67b6\uff0c\u901a\u8fc7\u4e16\u754c\u5efa\u6a21\u4ece\u65e0\u6807\u7b7e\u89c6\u9891\uff08\u673a\u5668\u4eba\u8bb0\u5f55\u6216\u4eba\u7c7b\u65e5\u5e38\u52a8\u4f5c\u89c6\u9891\uff09\u4e2d\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\u8868\u793a\uff0c\u5b9e\u73b0\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3002", "result": "\u5728LIBERO\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e2d\uff0cLAWM\u4f18\u4e8e\u4f7f\u7528\u771f\u5b9e\u673a\u5668\u4eba\u52a8\u4f5c\u8bad\u7ec3\u7684\u6a21\u578b\u548c\u7c7b\u4f3c\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u66f4\u9ad8\u6548\u5b9e\u7528\u3002", "conclusion": "LAWM\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u8fc1\u79fb\u5230\u4e0d\u540c\u4efb\u52a1\u3001\u73af\u5883\u548c\u5177\u8eab\u8bbe\u7f6e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709VLA\u6a21\u578b\u90e8\u7f72\u56f0\u96be\u7684\u95ee\u9898\u3002"}}
{"id": "2509.18447", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18447", "abs": "https://arxiv.org/abs/2509.18447", "authors": ["Rishabh Madan", "Jiawei Lin", "Mahika Goel", "Angchen Xie", "Xiaoyu Liang", "Marcus Lee", "Justin Guo", "Pranav N. Thakkar", "Rohan Banerjee", "Jose Barreiros", "Kate Tsui", "Tom Silver", "Tapomayukh Bhattacharjee"], "title": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction", "comment": "Conference on Robot Learning (CoRL)", "summary": "Physical human-robot interaction (pHRI) requires robots to adapt to\nindividual contact preferences, such as where and how much force is applied.\nIdentifying preferences is difficult for a single contact; with whole-arm\ninteraction involving multiple simultaneous contacts between the robot and\nhuman, the challenge is greater because different body parts can impose\nincompatible force requirements. In caregiving tasks, where contact is frequent\nand varied, such conflicts are unavoidable. With multiple preferences across\nmultiple contacts, no single solution can satisfy all objectives--trade-offs\nare inherent, making prioritization essential. We present PrioriTouch, a\nframework for ranking and executing control objectives across multiple\ncontacts. PrioriTouch can prioritize from a general collection of controllers,\nmaking it applicable not only to caregiving scenarios such as bed bathing and\ndressing but also to broader multi-contact settings. Our method combines a\nnovel learning-to-rank approach with hierarchical operational space control,\nleveraging simulation-in-the-loop rollouts for data-efficient and safe\nexploration. We conduct a user study on physical assistance preferences, derive\npersonalized comfort thresholds, and incorporate them into PrioriTouch. We\nevaluate PrioriTouch through extensive simulation and real-world experiments,\ndemonstrating its ability to adapt to user contact preferences, maintain task\nperformance, and enhance safety and comfort. Website:\nhttps://emprise.cs.cornell.edu/prioritouch.", "AI": {"tldr": "PrioriTouch\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u4e2d\u5904\u7406\u591a\u63a5\u89e6\u70b9\u4f18\u5148\u7ea7\u63a7\u5236\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u548c\u6392\u5e8f\u63a7\u5236\u76ee\u6807\u6765\u9002\u5e94\u4e2a\u4f53\u63a5\u89e6\u504f\u597d\uff0c\u7279\u522b\u9002\u7528\u4e8e\u62a4\u7406\u573a\u666f\u3002", "motivation": "\u5728\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u4e2d\uff0c\u7279\u522b\u662f\u62a4\u7406\u4efb\u52a1\u4e2d\uff0c\u4e0d\u540c\u8eab\u4f53\u90e8\u4f4d\u53ef\u80fd\u6709\u76f8\u4e92\u51b2\u7a81\u7684\u529b\u9700\u6c42\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u591a\u63a5\u89e6\u70b9\u4f18\u5148\u7ea7\u7684\u65b9\u6cd5\u6765\u786e\u4fdd\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\u3002", "method": "\u7ed3\u5408\u5b66\u4e60\u6392\u5e8f\u65b9\u6cd5\u548c\u5206\u5c42\u64cd\u4f5c\u7a7a\u95f4\u63a7\u5236\uff0c\u5229\u7528\u4eff\u771f\u5faa\u73af\u6eda\u52a8\u8fdb\u884c\u6570\u636e\u9ad8\u6548\u7684\u5b89\u5168\u63a2\u7d22\uff0c\u4ece\u7528\u6237\u7814\u7a76\u4e2d\u83b7\u53d6\u4e2a\u6027\u5316\u8212\u9002\u9608\u503c\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\uff0cPrioriTouch\u80fd\u591f\u9002\u5e94\u7528\u6237\u63a5\u89e6\u504f\u597d\uff0c\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\u3002", "conclusion": "PrioriTouch\u6846\u67b6\u4e3a\u591a\u63a5\u89e6\u70b9\u7269\u7406\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u4f18\u5148\u7ea7\u63a7\u5236\u65b9\u6cd5\uff0c\u5728\u62a4\u7406\u7b49\u573a\u666f\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2509.18455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18455", "abs": "https://arxiv.org/abs/2509.18455", "authors": ["Yunshuang Li", "Yiyang Ling", "Gaurav S. Sukhatme", "Daniel Seita"], "title": "Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands", "comment": null, "summary": "Nonprehensile manipulation, such as pushing and pulling, enables robots to\nmove, align, or reposition objects that may be difficult to grasp due to their\ngeometry, size, or relationship to the robot or the environment. Much of the\nexisting work in nonprehensile manipulation relies on parallel-jaw grippers or\ntools such as rods and spatulas. In contrast, multi-fingered dexterous hands\noffer richer contact modes and versatility for handling diverse objects to\nprovide stable support over the objects, which compensates for the difficulty\nof modeling the dynamics of nonprehensile manipulation. Therefore, we propose\nGeometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile\nmanipulation with dexterous robotic hands. We study pushing and pulling by\nframing the problem as synthesizing and learning pre-contact dexterous hand\nposes that lead to effective manipulation. We generate diverse hand poses via\ncontact-guided sampling, filter them using physics simulation, and train a\ndiffusion model conditioned on object geometry to predict viable poses. At test\ntime, we sample hand poses and use standard motion planners to select and\nexecute pushing and pulling actions. We perform 840 real-world experiments with\nan Allegro Hand, comparing our method to baselines. The results indicate that\nGD2P offers a scalable route for training dexterous nonprehensile manipulation\npolicies. We further demonstrate GD2P on a LEAP Hand, highlighting its\napplicability to different hand morphologies. Our pre-trained models and\ndataset, including 1.3 million hand poses across 2.3k objects, will be\nopen-source to facilitate further research. Our project website is available\nat: geodex2p.github.io.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u51e0\u4f55\u611f\u77e5\u7684\u591a\u6307\u7075\u5de7\u624b\u63a8\u62c9\u64cd\u4f5c\uff08GD2P\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a5\u89e6\u5f15\u5bfc\u91c7\u6837\u751f\u6210\u591a\u6837\u5316\u624b\u90e8\u59ff\u6001\uff0c\u5229\u7528\u7269\u7406\u6a21\u62df\u7b5b\u9009\uff0c\u5e76\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u6765\u9884\u6d4b\u53ef\u884c\u7684\u64cd\u4f5c\u59ff\u6001\uff0c\u5b9e\u73b0\u4e86\u5bf9\u96be\u4ee5\u6293\u53d6\u7269\u4f53\u7684\u975e\u6293\u53d6\u5f0f\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u7684\u975e\u6293\u53d6\u5f0f\u64cd\u4f5c\u4e3b\u8981\u4f9d\u8d56\u5e73\u884c\u5939\u722a\u6216\u5de5\u5177\uff0c\u800c\u591a\u6307\u7075\u5de7\u624b\u80fd\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u63a5\u89e6\u6a21\u5f0f\u548c\u7a33\u5b9a\u6027\uff0c\u4f46\u975e\u6293\u53d6\u5f0f\u64cd\u4f5c\u7684\u52a8\u529b\u5b66\u5efa\u6a21\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u5229\u7528\u7075\u5de7\u624b\u4f18\u52bf\u7684\u65b9\u6cd5\u6765\u5904\u7406\u591a\u6837\u5316\u7684\u7269\u4f53\u3002", "method": "\u901a\u8fc7\u63a5\u89e6\u5f15\u5bfc\u91c7\u6837\u751f\u6210\u591a\u6837\u5316\u624b\u90e8\u59ff\u6001\uff0c\u4f7f\u7528\u7269\u7406\u6a21\u62df\u8fdb\u884c\u7b5b\u9009\uff0c\u8bad\u7ec3\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6839\u636e\u7269\u4f53\u51e0\u4f55\u9884\u6d4b\u53ef\u884c\u59ff\u6001\uff0c\u6700\u540e\u4f7f\u7528\u6807\u51c6\u8fd0\u52a8\u89c4\u5212\u5668\u9009\u62e9\u548c\u6267\u884c\u63a8\u62c9\u52a8\u4f5c\u3002", "result": "\u5728Allegro Hand\u4e0a\u8fdb\u884c\u4e86840\u6b21\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eGD2P\u4e3a\u8bad\u7ec3\u7075\u5de7\u975e\u6293\u53d6\u64cd\u4f5c\u7b56\u7565\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9014\u5f84\u3002\u5728LEAP Hand\u4e0a\u7684\u8fdb\u4e00\u6b65\u6f14\u793a\u4e5f\u9a8c\u8bc1\u4e86\u5176\u5bf9\u4e0d\u540c\u624b\u90e8\u5f62\u6001\u7684\u9002\u7528\u6027\u3002", "conclusion": "GD2P\u65b9\u6cd5\u6709\u6548\u5229\u7528\u4e86\u591a\u6307\u7075\u5de7\u624b\u7684\u4f18\u52bf\uff0c\u4e3a\u975e\u6293\u53d6\u5f0f\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u6570\u636e\u96c6\uff08\u5305\u542b130\u4e07\u624b\u90e8\u59ff\u6001\u548c2300\u4e2a\u7269\u4f53\uff09\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2509.18460", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18460", "abs": "https://arxiv.org/abs/2509.18460", "authors": ["Haeyoon Han", "Mahdi Taheri", "Soon-Jo Chung", "Fred Y. Hadaegh"], "title": "A Counterfactual Reasoning Framework for Fault Diagnosis in Robot Perception Systems", "comment": null, "summary": "Perception systems provide a rich understanding of the environment for\nautonomous systems, shaping decisions in all downstream modules. Hence,\naccurate detection and isolation of faults in perception systems is important.\nFaults in perception systems pose particular challenges: faults are often tied\nto the perceptual context of the environment, and errors in their multi-stage\npipelines can propagate across modules. To address this, we adopt a\ncounterfactual reasoning approach to propose a framework for fault detection\nand isolation (FDI) in perception systems. As opposed to relying on physical\nredundancy (i.e., having extra sensors), our approach utilizes analytical\nredundancy with counterfactual reasoning to construct perception reliability\ntests as causal outcomes influenced by system states and fault scenarios.\nCounterfactual reasoning generates reliability test results under hypothesized\nfaults to update the belief over fault hypotheses. We derive both passive and\nactive FDI methods. While the passive FDI can be achieved by belief updates,\nthe active FDI approach is defined as a causal bandit problem, where we utilize\nMonte Carlo Tree Search (MCTS) with upper confidence bound (UCB) to find\ncontrol inputs that maximize a detection and isolation metric, designated as\nEffective Information (EI). The mentioned metric quantifies the informativeness\nof control inputs for FDI. We demonstrate the approach in a robot exploration\nscenario, where a space robot performing vision-based navigation actively\nadjusts its attitude to increase EI and correctly isolate faults caused by\nsensor damage, dynamic scenes, and perceptual degradation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u63a8\u7406\u7684\u611f\u77e5\u7cfb\u7edf\u6545\u969c\u68c0\u6d4b\u4e0e\u9694\u79bb\u6846\u67b6\uff0c\u5229\u7528\u5206\u6790\u5197\u4f59\u800c\u975e\u7269\u7406\u5197\u4f59\uff0c\u901a\u8fc7\u88ab\u52a8\u548c\u4e3b\u52a8\u65b9\u6cd5\u5b9e\u73b0\u6545\u969c\u8bca\u65ad\u3002", "motivation": "\u611f\u77e5\u7cfb\u7edf\u5bf9\u81ea\u4e3b\u7cfb\u7edf\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u611f\u77e5\u6545\u969c\u5177\u6709\u73af\u5883\u4f9d\u8d56\u6027\u4e14\u5728\u591a\u9636\u6bb5\u7ba1\u9053\u4e2d\u4f20\u64ad\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u68c0\u6d4b\u548c\u9694\u79bb\u8fd9\u4e9b\u6545\u969c\u3002", "method": "\u91c7\u7528\u53cd\u4e8b\u5b9e\u63a8\u7406\u6784\u5efa\u611f\u77e5\u53ef\u9760\u6027\u6d4b\u8bd5\uff0c\u5c06\u4e3b\u52a8\u6545\u969c\u68c0\u6d4b\u4e0e\u9694\u79bb\u5efa\u6a21\u4e3a\u56e0\u679c\u591a\u81c2\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u4e0a\u7f6e\u4fe1\u754c\u7b97\u6cd5\u4f18\u5316\u63a7\u5236\u8f93\u5165\u4ee5\u6700\u5927\u5316\u6709\u6548\u4fe1\u606f\u3002", "result": "\u5728\u673a\u5668\u4eba\u63a2\u7d22\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u6210\u529f\u9694\u79bb\u7531\u4f20\u611f\u5668\u635f\u574f\u3001\u52a8\u6001\u573a\u666f\u548c\u611f\u77e5\u9000\u5316\u5f15\u8d77\u7684\u6545\u969c\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u73af\u5883\u4f9d\u8d56\u6027\u548c\u591a\u9636\u6bb5\u4f20\u64ad\u7684\u611f\u77e5\u6545\u969c\u573a\u666f\u3002"}}
{"id": "2509.18463", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18463", "abs": "https://arxiv.org/abs/2509.18463", "authors": ["Jannick van Buuren", "Roberto Giglio", "Loris Roveda", "Luka Peternel"], "title": "Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task", "comment": null, "summary": "This paper explores how deliberate mutations of reward function in\nreinforcement learning can produce diversified skill variations in robotic\nmanipulation tasks, examined with a liquid pouring use case. To this end, we\ndeveloped a new reward function mutation framework that is based on applying\nGaussian noise to the weights of the different terms in the reward function.\nInspired by the cost-benefit tradeoff model from human motor control, we\ndesigned the reward function with the following key terms: accuracy, time, and\neffort. The study was performed in a simulation environment created in NVIDIA\nIsaac Sim, and the setup included Franka Emika Panda robotic arm holding a\nglass with a liquid that needed to be poured into a container. The\nreinforcement learning algorithm was based on Proximal Policy Optimization. We\nsystematically explored how different configurations of mutated weights in the\nrewards function would affect the learned policy. The resulting policies\nexhibit a wide range of behaviours: from variations in execution of the\noriginally intended pouring task to novel skills useful for unexpected tasks,\nsuch as container rim cleaning, liquid mixing, and watering. This approach\noffers promising directions for robotic systems to perform diversified learning\nof specific tasks, while also potentially deriving meaningful skills for future\ntasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5956\u52b1\u51fd\u6570\u7a81\u53d8\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u9ad8\u65af\u566a\u58f0\u4f5c\u7528\u4e0b\u5bf9\u5956\u52b1\u51fd\u6570\u6743\u91cd\u8fdb\u884c\u53d8\u5f02\uff0c\u5728\u673a\u5668\u4eba\u6db2\u4f53\u503e\u5012\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u591a\u6837\u5316\u7684\u6280\u80fd\u5b66\u4e60\u3002", "motivation": "\u53d7\u4eba\u7c7b\u8fd0\u52a8\u63a7\u5236\u4e2d\u6210\u672c\u6548\u76ca\u6743\u8861\u6a21\u578b\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u6545\u610f\u7a81\u53d8\u5956\u52b1\u51fd\u6570\u6765\u4ea7\u751f\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u591a\u6837\u5316\u6280\u80fd\u53d8\u5f02\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4f18\u5316\u5355\u4e00\u76ee\u6807\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u9ad8\u65af\u566a\u58f0\u7684\u5956\u52b1\u51fd\u6570\u7a81\u53d8\u6846\u67b6\uff0c\u5956\u52b1\u51fd\u6570\u5305\u542b\u51c6\u786e\u6027\u3001\u65f6\u95f4\u548c\u52aa\u529b\u4e09\u4e2a\u5173\u952e\u9879\u3002\u5728NVIDIA Isaac Sim\u4eff\u771f\u73af\u5883\u4e2d\u4f7f\u7528Franka Emika Panda\u673a\u68b0\u81c2\u8fdb\u884c\u6db2\u4f53\u503e\u5012\u5b9e\u9a8c\uff0c\u91c7\u7528\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u4e0d\u540c\u6743\u91cd\u914d\u7f6e\u7684\u7a81\u53d8\u5956\u52b1\u51fd\u6570\u4ea7\u751f\u4e86\u5e7f\u6cdb\u7684\u884c\u4e3a\u7b56\u7565\uff1a\u4ece\u539f\u59cb\u503e\u5012\u4efb\u52a1\u6267\u884c\u7684\u53d8\u5316\u5230\u65b0\u9896\u6280\u80fd\uff0c\u5982\u5bb9\u5668\u8fb9\u7f18\u6e05\u6d01\u3001\u6db2\u4f53\u6df7\u5408\u548c\u6d47\u6c34\u7b49\u610f\u5916\u6709\u7528\u7684\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u6267\u884c\u7279\u5b9a\u4efb\u52a1\u7684\u591a\u6837\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u540c\u65f6\u53ef\u80fd\u4e3a\u672a\u6765\u4efb\u52a1\u884d\u751f\u51fa\u6709\u610f\u4e49\u7684\u6280\u80fd\u3002"}}
{"id": "2509.18466", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18466", "abs": "https://arxiv.org/abs/2509.18466", "authors": ["Junnosuke Kamohara", "Feiyang Wu", "Chinmayee Wamorkar", "Seth Hutchinson", "Ye Zhao"], "title": "RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain", "comment": null, "summary": "Model predictive control (MPC) has demonstrated effectiveness for humanoid\nbipedal locomotion; however, its applicability in challenging environments,\nsuch as rough and slippery terrain, is limited by the difficulty of modeling\nterrain interactions. In contrast, reinforcement learning (RL) has achieved\nnotable success in training robust locomotion policies over diverse terrain,\nyet it lacks guarantees of constraint satisfaction and often requires\nsubstantial reward shaping. Recent efforts in combining MPC and RL have shown\npromise of taking the best of both worlds, but they are primarily restricted to\nflat terrain or quadrupedal robots. In this work, we propose an RL-augmented\nMPC framework tailored for bipedal locomotion over rough and slippery terrain.\nOur method parametrizes three key components of\nsingle-rigid-body-dynamics-based MPC: system dynamics, swing leg controller,\nand gait frequency. We validate our approach through bipedal robot simulations\nin NVIDIA IsaacLab across various terrains, including stairs, stepping stones,\nand low-friction surfaces. Experimental results demonstrate that our\nRL-augmented MPC framework produces significantly more adaptive and robust\nbehaviors compared to baseline MPC and RL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u53cc\u8db3\u673a\u5668\u4eba\u5728\u7c97\u7cd9\u548c\u5149\u6ed1\u5730\u5f62\u4e0a\u884c\u8d70\u7684RL\u589e\u5f3aMPC\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u6446\u52a8\u817f\u63a7\u5236\u5668\u548c\u6b65\u6001\u9891\u7387\uff0c\u7ed3\u5408MPC\u548cRL\u7684\u4f18\u52bf\u3002", "motivation": "MPC\u5728\u53cc\u8db3\u884c\u8d70\u4e2d\u6709\u6548\u4f46\u96be\u4ee5\u5efa\u6a21\u590d\u6742\u5730\u5f62\u4ea4\u4e92\uff0cRL\u5728\u591a\u6837\u5316\u5730\u5f62\u4e0a\u8bad\u7ec3\u7a33\u5065\u7b56\u7565\u4f46\u7f3a\u4e4f\u7ea6\u675f\u4fdd\u8bc1\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u3002", "method": "\u53c2\u6570\u5316\u57fa\u4e8e\u5355\u521a\u4f53\u52a8\u529b\u5b66\u7684MPC\u7684\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u7cfb\u7edf\u52a8\u529b\u5b66\u3001\u6446\u52a8\u817f\u63a7\u5236\u5668\u548c\u6b65\u6001\u9891\u7387\uff0c\u5728NVIDIA IsaacLab\u4e2d\u8fdb\u884c\u53cc\u8db3\u673a\u5668\u4eba\u4eff\u771f\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cRL\u589e\u5f3a\u7684MPC\u6846\u67b6\u5728\u697c\u68af\u3001\u8e0f\u811a\u77f3\u548c\u4f4e\u6469\u64e6\u8868\u9762\u7b49\u591a\u6837\u5316\u5730\u5f62\u4e0a\uff0c\u6bd4\u57fa\u7ebfMPC\u548cRL\u4ea7\u751f\u66f4\u81ea\u9002\u5e94\u548c\u7a33\u5065\u7684\u884c\u4e3a\u3002", "conclusion": "RL\u589e\u5f3a\u7684MPC\u6846\u67b6\u6210\u529f\u7ed3\u5408\u4e86MPC\u7684\u7ea6\u675f\u4fdd\u8bc1\u548cRL\u7684\u5730\u5f62\u9002\u5e94\u6027\uff0c\u4e3a\u53cc\u8db3\u673a\u5668\u4eba\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u884c\u8d70\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18506", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18506", "abs": "https://arxiv.org/abs/2509.18506", "authors": ["Siyuan Yu", "Congkai Shen", "Yufei Xi", "James Dallas", "Michael Thompson", "John Subosits", "Hiroshi Yasuda", "Tulga Ersal"], "title": "Spatial Envelope MPC: High Performance Driving without a Reference", "comment": null, "summary": "This paper presents a novel envelope based model predictive control (MPC)\nframework designed to enable autonomous vehicles to handle high performance\ndriving across a wide range of scenarios without a predefined reference. In\nhigh performance autonomous driving, safe operation at the vehicle's dynamic\nlimits requires a real time planning and control framework capable of\naccounting for key vehicle dynamics and environmental constraints when\nfollowing a predefined reference trajectory is suboptimal or even infeasible.\nState of the art planning and control frameworks, however, are predominantly\nreference based, which limits their performance in such situations. To address\nthis gap, this work first introduces a computationally efficient vehicle\ndynamics model tailored for optimization based control and a continuously\ndifferentiable mathematical formulation that accurately captures the entire\ndrivable envelope. This novel model and formulation allow for the direct\nintegration of dynamic feasibility and safety constraints into a unified\nplanning and control framework, thereby removing the necessity for predefined\nreferences. The challenge of envelope planning, which refers to maximally\napproximating the safe drivable area, is tackled by combining reinforcement\nlearning with optimization techniques. The framework is validated through both\nsimulations and real world experiments, demonstrating its high performance\nacross a variety of tasks, including racing, emergency collision avoidance and\noff road navigation. These results highlight the framework's scalability and\nbroad applicability across a diverse set of scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5305\u7edc\u7ebf\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u4f7f\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u80fd\u591f\u5728\u6ca1\u6709\u9884\u5b9a\u4e49\u53c2\u8003\u8f68\u8ff9\u7684\u60c5\u51b5\u4e0b\u5904\u7406\u9ad8\u6027\u80fd\u9a7e\u9a76\u573a\u666f\u3002", "motivation": "\u5f53\u524d\u6700\u5148\u8fdb\u7684\u89c4\u5212\u548c\u63a7\u5236\u6846\u67b6\u4e3b\u8981\u57fa\u4e8e\u53c2\u8003\u8f68\u8ff9\uff0c\u8fd9\u5728\u8f66\u8f86\u9700\u8981\u8fbe\u5230\u52a8\u6001\u6781\u9650\u7684\u9ad8\u6027\u80fd\u9a7e\u9a76\u573a\u666f\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u56e0\u4e3a\u8ddf\u968f\u9884\u5b9a\u4e49\u53c2\u8003\u8f68\u8ff9\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u751a\u81f3\u4e0d\u53ef\u884c\u3002", "method": "\u9996\u5148\u5f00\u53d1\u4e86\u9488\u5bf9\u4f18\u5316\u63a7\u5236\u7684\u8ba1\u7b97\u9ad8\u6548\u8f66\u8f86\u52a8\u529b\u5b66\u6a21\u578b\u548c\u8fde\u7eed\u53ef\u5fae\u7684\u6570\u5b66\u516c\u5f0f\uff0c\u51c6\u786e\u6355\u6349\u6574\u4e2a\u53ef\u884c\u9a76\u5305\u7edc\u7ebf\u3002\u7136\u540e\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u4f18\u5316\u6280\u672f\u89e3\u51b3\u5305\u7edc\u89c4\u5212\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u6846\u67b6\u5728\u8d5b\u8f66\u3001\u7d27\u6025\u907f\u969c\u548c\u8d8a\u91ce\u5bfc\u822a\u7b49\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u53ef\u6269\u5c55\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4e3a\u9ad8\u6027\u80fd\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18576", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18576", "abs": "https://arxiv.org/abs/2509.18576", "authors": ["Zeyi Kang", "Liang He", "Yanxin Zhang", "Zuheng Ming", "Kaixing Zhao"], "title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA", "comment": null, "summary": "Multimodal semantic learning plays a critical role in embodied intelligence,\nespecially when robots perceive their surroundings, understand human\ninstructions, and make intelligent decisions. However, the field faces\ntechnical challenges such as effective fusion of heterogeneous data and\ncomputational efficiency in resource-constrained environments. To address these\nchallenges, this study proposes the lightweight LCMF cascaded attention\nframework, introducing a multi-level cross-modal parameter sharing mechanism\ninto the Mamba module. By integrating the advantages of Cross-Attention and\nSelective parameter-sharing State Space Models (SSMs), the framework achieves\nefficient fusion of heterogeneous modalities and semantic complementary\nalignment. Experimental results show that LCMF surpasses existing multimodal\nbaselines with an accuracy of 74.29% in VQA tasks and achieves competitive\nmid-tier performance within the distribution cluster of Large Language Model\nAgents (LLM Agents) in EQA video tasks. Its lightweight design achieves a\n4.35-fold reduction in FLOPs relative to the average of comparable baselines\nwhile using only 166.51M parameters (image-text) and 219M parameters\n(video-text), providing an efficient solution for Human-Robot Interaction (HRI)\napplications in resource-constrained scenarios with strong multimodal decision\ngeneralization capabilities.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7LCMF\u7ea7\u8054\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7ea7\u8de8\u6a21\u6001\u53c2\u6570\u5171\u4eab\u673a\u5236\u89e3\u51b3\u591a\u6a21\u6001\u8bed\u4e49\u5b66\u4e60\u4e2d\u7684\u5f02\u6784\u6570\u636e\u878d\u5408\u548c\u8ba1\u7b97\u6548\u7387\u95ee\u9898", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u8bed\u4e49\u5b66\u4e60\u5728\u5f02\u6784\u6570\u636e\u6709\u6548\u878d\u5408\u548c\u8d44\u6e90\u53d7\u9650\u73af\u5883\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u7684\u6280\u672f\u6311\u6218\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848", "method": "\u5728Mamba\u6a21\u5757\u4e2d\u5f15\u5165\u591a\u7ea7\u8de8\u6a21\u6001\u53c2\u6570\u5171\u4eab\u673a\u5236\uff0c\u7ed3\u5408\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u9009\u62e9\u6027\u53c2\u6570\u5171\u4eab\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u5f02\u6784\u6a21\u6001\u7684\u9ad8\u6548\u878d\u5408\u548c\u8bed\u4e49\u4e92\u8865\u5bf9\u9f50", "result": "\u5728VQA\u4efb\u52a1\u4e2d\u8fbe\u523074.29%\u51c6\u786e\u7387\uff0c\u5728EQA\u89c6\u9891\u4efb\u52a1\u4e2d\u8fbe\u5230LLM Agents\u5206\u5e03\u96c6\u7fa4\u7684\u4e2d\u7b49\u6027\u80fd\u6c34\u5e73\uff0cFLOPs\u76f8\u6bd4\u57fa\u7ebf\u5e73\u5747\u51cf\u5c114.35\u500d\uff0c\u53c2\u6570\u91cf\u4ec5\u4e3a166.51M\uff08\u56fe\u50cf-\u6587\u672c\uff09\u548c219M\uff08\u89c6\u9891-\u6587\u672c\uff09", "conclusion": "LCMF\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u4eba\u673a\u4ea4\u4e92\u5e94\u7528\u63d0\u4f9b\u4e86\u5177\u6709\u5f3a\u5927\u591a\u6a21\u6001\u51b3\u7b56\u6cdb\u5316\u80fd\u529b\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.18592", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18592", "abs": "https://arxiv.org/abs/2509.18592", "authors": ["Neel P. Bhatt", "Yunhao Yang", "Rohan Siva", "Pranay Samineni", "Daniel Milan", "Zhangyang Wang", "Ufuk Topcu"], "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation", "comment": "Codebase, datasets, and videos for VLN-Zero are available at:\n  https://vln-zero.github.io/", "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.", "AI": {"tldr": "VLN-Zero\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u7b26\u53f7\u573a\u666f\u56fe\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u795e\u7ecf\u7b26\u53f7\u5bfc\u822a\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5feb\u901f\u63a2\u7d22\u3001\u7b26\u53f7\u63a8\u7406\u548c\u7f13\u5b58\u6267\u884c\uff0c\u5728\u672a\u89c1\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7a77\u4e3e\u63a2\u7d22\u6216\u50f5\u5316\u7684\u5bfc\u822a\u7b56\u7565\uff0c\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u672a\u89c1\u73af\u5883\u4e2d\u5feb\u901f\u9002\u5e94\u4e14\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\u7684\u5bfc\u822a\u6846\u67b6\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u63a2\u7d22\u9636\u6bb5\u4f7f\u7528\u7ed3\u6784\u5316\u63d0\u793a\u5f15\u5bfcVLM\u641c\u7d22\u6784\u5efa\u7d27\u51d1\u573a\u666f\u56fe\uff1b\u90e8\u7f72\u9636\u6bb5\u4f7f\u7528\u795e\u7ecf\u7b26\u53f7\u89c4\u5212\u5668\u5728\u573a\u666f\u56fe\u4e0a\u63a8\u7406\u751f\u6210\u53ef\u6267\u884c\u8ba1\u5212\uff0c\u5e76\u901a\u8fc7\u7f13\u5b58\u6267\u884c\u6a21\u5757\u52a0\u901f\u9002\u5e94\u3002", "result": "VLN-Zero\u5728\u96f6\u6837\u672c\u6a21\u578b\u4e2d\u6210\u529f\u7387\u63d0\u9ad82\u500d\uff0c\u8d85\u8d8a\u5927\u591a\u6570\u5fae\u8c03\u57fa\u7ebf\uff0c\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u65f6\u95f4\u51cf\u534a\uff0cVLM\u8c03\u7528\u51cf\u5c1155%\u3002", "conclusion": "\u8be5\u6846\u67b6\u514b\u670d\u4e86\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u7684\u8ba1\u7b97\u4f4e\u6548\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u672a\u89c1\u73af\u5883\u4e2d\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u51b3\u7b56\u3002"}}
{"id": "2509.18597", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18597", "abs": "https://arxiv.org/abs/2509.18597", "authors": ["Yuan Meng", "Zhenguo Sun", "Max Fest", "Xukun Li", "Zhenshan Bing", "Alois Knoll"], "title": "Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills", "comment": "upload 9 main page - v1", "summary": "Large language models (LLMs)-based code generation for robotic manipulation\nhas recently shown promise by directly translating human instructions into\nexecutable code, but existing methods remain noisy, constrained by fixed\nprimitives and limited context windows, and struggle with long-horizon tasks.\nWhile closed-loop feedback has been explored, corrected knowledge is often\nstored in improper formats, restricting generalization and causing catastrophic\nforgetting, which highlights the need for learning reusable skills. Moreover,\napproaches that rely solely on LLM guidance frequently fail in extremely\nlong-horizon scenarios due to LLMs' limited reasoning capability in the robotic\ndomain, where such issues are often straightforward for humans to identify. To\naddress these challenges, we propose a human-in-the-loop framework that encodes\ncorrections into reusable skills, supported by external memory and\nRetrieval-Augmented Generation with a hint mechanism for dynamic reuse.\nExperiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world\nsettings, show that our framework achieves a 0.93 success rate (up to 27%\nhigher than baselines) and a 42% efficiency improvement in correction rounds.\nIt can robustly solve extremely long-horizon tasks such as \"build a house\",\nwhich requires planning over 20 primitives.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u534f\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4eba\u7c7b\u4fee\u6b63\u7f16\u7801\u4e3a\u53ef\u91cd\u7528\u6280\u80fd\u6765\u89e3\u51b3LLM\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u566a\u58f0\u3001\u56fa\u5b9a\u539f\u8bed\u9650\u5236\u548c\u957f\u65f6\u7a0b\u4efb\u52a1\u5904\u7406\u56f0\u96be\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u673a\u5668\u4eba\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u566a\u58f0\u5927\u3001\u53d7\u9650\u4e8e\u56fa\u5b9a\u539f\u8bed\u3001\u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u9650\u3001\u96be\u4ee5\u5904\u7406\u957f\u65f6\u7a0b\u4efb\u52a1\u7b49\u95ee\u9898\uff0c\u4e14\u95ed\u73af\u53cd\u9988\u4e2d\u7684\u4fee\u6b63\u77e5\u8bc6\u5b58\u50a8\u683c\u5f0f\u4e0d\u5f53\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u5e76\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\u3002", "method": "\u91c7\u7528\u4eba\u673a\u534f\u4f5c\u6846\u67b6\uff0c\u5c06\u4eba\u7c7b\u4fee\u6b63\u7f16\u7801\u4e3a\u53ef\u91cd\u7528\u6280\u80fd\uff0c\u7ed3\u5408\u5916\u90e8\u5b58\u50a8\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u914d\u5907\u63d0\u793a\u673a\u5236\u5b9e\u73b0\u52a8\u6001\u91cd\u7528\u3002", "result": "\u5728Ravens\u3001Franka Kitchen\u548cMetaWorld\u7b49\u73af\u5883\u4ee5\u53ca\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u8fbe\u52300.93\u7684\u6210\u529f\u7387\uff08\u6bd4\u57fa\u7ebf\u9ad827%\uff09\uff0c\u4fee\u6b63\u8f6e\u6b21\u6548\u7387\u63d0\u534742%\uff0c\u5e76\u80fd\u7a33\u5065\u89e3\u51b3\u9700\u8981\u89c4\u5212\u8d85\u8fc720\u4e2a\u539f\u8bed\u7684\u6781\u7aef\u957f\u65f6\u7a0b\u4efb\u52a1\u3002", "conclusion": "\u8be5\u4eba\u673a\u534f\u4f5c\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u673a\u5668\u4eba\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u548c\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2509.18608", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18608", "abs": "https://arxiv.org/abs/2509.18608", "authors": ["Ana Luiza Mineiro", "Francisco Affonso", "Marcelo Becker"], "title": "End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning", "comment": "Accepted to the 22nd International Conference on Advanced Robotics\n  (ICAR 2025). 7 pages", "summary": "Reliable navigation in under-canopy agricultural environments remains a\nchallenge due to GNSS unreliability, cluttered rows, and variable lighting. To\naddress these limitations, we present an end-to-end learning-based navigation\nsystem that maps raw 3D LiDAR data directly to control commands using a deep\nreinforcement learning policy trained entirely in simulation. Our method\nincludes a voxel-based downsampling strategy that reduces LiDAR input size by\n95.83%, enabling efficient policy learning without relying on labeled datasets\nor manually designed control interfaces. The policy was validated in\nsimulation, achieving a 100% success rate in straight-row plantations and\nshowing a gradual decline in performance as row curvature increased, tested\nacross varying sinusoidal frequencies and amplitudes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7aef\u5230\u7aef\u5b66\u4e60\u7684\u5bfc\u822a\u7cfb\u7edf\uff0c\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u5c06\u539f\u59cb3D LiDAR\u6570\u636e\u76f4\u63a5\u6620\u5c04\u5230\u63a7\u5236\u547d\u4ee4\uff0c\u89e3\u51b3\u519c\u4e1a\u51a0\u5c42\u4e0b\u73af\u5883\u4e2d\u7684\u53ef\u9760\u5bfc\u822a\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u519c\u4e1a\u51a0\u5c42\u73af\u5883\u4e0bGNSS\u4e0d\u53ef\u9760\u3001\u884c\u95f4\u6742\u4e71\u548c\u5149\u7167\u53d8\u5316\u7b49\u5bfc\u822a\u6311\u6218\uff0c\u907f\u514d\u5bf9\u6807\u8bb0\u6570\u636e\u96c6\u6216\u624b\u52a8\u8bbe\u8ba1\u63a7\u5236\u63a5\u53e3\u7684\u4f9d\u8d56\u3002", "method": "\u91c7\u7528\u4f53\u7d20\u964d\u91c7\u6837\u7b56\u7565\u5c06LiDAR\u8f93\u5165\u5c3a\u5bf8\u51cf\u5c1195.83%\uff0c\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u76f4\u63a5\u5904\u7406\u539f\u59cb3D LiDAR\u6570\u636e\u751f\u6210\u63a7\u5236\u547d\u4ee4\u3002", "result": "\u5728\u4eff\u771f\u9a8c\u8bc1\u4e2d\uff0c\u76f4\u7ebf\u884c\u79cd\u690d\u573a\u666f\u8fbe\u5230100%\u6210\u529f\u7387\uff0c\u4f46\u968f\u7740\u884c\u5f2f\u66f2\u5ea6\u589e\u52a0\u6027\u80fd\u9010\u6e10\u4e0b\u964d\uff0c\u6d4b\u8bd5\u4e86\u4e0d\u540c\u6b63\u5f26\u9891\u7387\u548c\u632f\u5e45\u4e0b\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u7684\u5b66\u4e60\u7b56\u7565\u80fd\u591f\u6709\u6548\u5904\u7406\u519c\u4e1a\u73af\u5883\u5bfc\u822a\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.18609", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18609", "abs": "https://arxiv.org/abs/2509.18609", "authors": ["Chengran Yuan", "Zijian Lu", "Zhanqi Zhang", "Yimin Zhao", "Zefan Huang", "Shuo Sun", "Jiawei Sun", "Jiahui Li", "Christina Dao Wen Lee", "Dongen Li", "Marcelo H. Ang Jr"], "title": "PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving", "comment": null, "summary": "End-to-end motion planning is promising for simplifying complex autonomous\ndriving pipelines. However, challenges such as scene understanding and\neffective prediction for decision-making continue to present substantial\nobstacles to its large-scale deployment. In this paper, we present PIE, a\npioneering framework that integrates advanced perception, reasoning, and\nintention modeling to dynamically capture interactions between the ego vehicle\nand surrounding agents. It incorporates a bidirectional Mamba fusion that\naddresses data compression losses in multimodal fusion of camera and LiDAR\ninputs, alongside a novel reasoning-enhanced decoder integrating Mamba and\nMixture-of-Experts to facilitate scene-compliant anchor selection and optimize\nadaptive trajectory inference. PIE adopts an action-motion interaction module\nto effectively utilize state predictions of surrounding agents to refine ego\nplanning. The proposed framework is thoroughly validated on the NAVSIM\nbenchmark. PIE, without using any ensemble and data augmentation techniques,\nachieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of\nprior state-of-the-art methods. Comprehensive quantitative and qualitative\nanalyses demonstrate that PIE is capable of reliably generating feasible and\nhigh-quality ego trajectories.", "AI": {"tldr": "PIE\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5148\u8fdb\u611f\u77e5\u3001\u63a8\u7406\u548c\u610f\u56fe\u5efa\u6a21\u52a8\u6001\u6355\u6349\u81ea\u8f66\u4e0e\u5468\u56f4\u667a\u80fd\u4f53\u4ea4\u4e92\uff0c\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "motivation": "\u7aef\u5230\u7aef\u8fd0\u52a8\u89c4\u5212\u867d\u7136\u7b80\u5316\u4e86\u81ea\u52a8\u9a7e\u9a76\u6d41\u7a0b\uff0c\u4f46\u573a\u666f\u7406\u89e3\u548c\u6709\u6548\u9884\u6d4b\u51b3\u7b56\u4ecd\u662f\u90e8\u7f72\u7684\u4e3b\u8981\u6311\u6218\u3002", "method": "\u91c7\u7528\u53cc\u5411Mamba\u878d\u5408\u89e3\u51b3\u591a\u6a21\u6001\u6570\u636e\u538b\u7f29\u635f\u5931\uff0c\u63a8\u7406\u589e\u5f3a\u89e3\u7801\u5668\u7ed3\u5408Mamba\u548cMixture-of-Experts\u4f18\u5316\u8f68\u8ff9\u63a8\u65ad\uff0c\u52a8\u4f5c-\u8fd0\u52a8\u4ea4\u4e92\u6a21\u5757\u5229\u7528\u5468\u56f4\u667a\u80fd\u4f53\u72b6\u6001\u9884\u6d4b\u4f18\u5316\u81ea\u8f66\u89c4\u5212\u3002", "result": "\u5728NAVSIM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e0d\u4f7f\u7528\u96c6\u6210\u548c\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u83b7\u5f9788.9 PDM\u5206\u6570\u548c85.6 EPDM\u5206\u6570\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "PIE\u80fd\u591f\u53ef\u9760\u751f\u6210\u53ef\u884c\u4e14\u9ad8\u8d28\u91cf\u7684\u81ea\u8f66\u8f68\u8ff9\uff0c\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2509.18610", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18610", "abs": "https://arxiv.org/abs/2509.18610", "authors": ["Maximilian Adang", "JunEn Low", "Ola Shorinwa", "Mac Schwager"], "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones", "comment": null, "summary": "Large vision-language models have driven remarkable progress in\nopen-vocabulary robot policies, e.g., generalist robot manipulation policies,\nthat enable robots to complete complex tasks specified in natural language.\nDespite these successes, open-vocabulary autonomous drone navigation remains an\nunsolved challenge due to the scarcity of large-scale demonstrations, real-time\ncontrol demands of drones for stabilization, and lack of reliable external pose\nestimation modules. In this work, we present SINGER for language-guided\nautonomous drone navigation in the open world using only onboard sensing and\ncompute. To train robust, open-vocabulary navigation policies, SINGER leverages\nthree central components: (i) a photorealistic language-embedded flight\nsimulator with minimal sim-to-real gap using Gaussian Splatting for efficient\ndata generation, (ii) an RRT-inspired multi-trajectory generation expert for\ncollision-free navigation demonstrations, and these are used to train (iii) a\nlightweight end-to-end visuomotor policy for real-time closed-loop control.\nThrough extensive hardware flight experiments, we demonstrate superior\nzero-shot sim-to-real transfer of our policy to unseen environments and unseen\nlanguage-conditioned goal objects. When trained on ~700k-1M observation action\npairs of language conditioned visuomotor data and deployed on hardware, SINGER\noutperforms a velocity-controlled semantic guidance baseline by reaching the\nquery 23.33% more on average, and maintains the query in the field of view\n16.67% more on average, with 10% fewer collisions.", "AI": {"tldr": "SINGER\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u6e90\u8bcd\u6c47\u65e0\u4eba\u673a\u5bfc\u822a\u7cfb\u7edf\uff0c\u4ec5\u4f7f\u7528\u673a\u8f7d\u4f20\u611f\u548c\u8ba1\u7b97\u5b9e\u73b0\u8bed\u8a00\u5f15\u5bfc\u7684\u81ea\u4e3b\u5bfc\u822a\uff0c\u901a\u8fc7\u6a21\u62df\u5668\u8bad\u7ec3\u548c\u8f7b\u91cf\u7ea7\u7b56\u7565\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u8bcd\u6c47\u81ea\u4e3b\u65e0\u4eba\u673a\u5bfc\u822a\u7684\u6311\u6218\uff0c\u5305\u62ec\u5927\u89c4\u6a21\u6f14\u793a\u6570\u636e\u7a00\u7f3a\u3001\u65e0\u4eba\u673a\u5b9e\u65f6\u63a7\u5236\u9700\u6c42\u4ee5\u53ca\u7f3a\u4e4f\u53ef\u9760\u7684\u5916\u90e8\u59ff\u6001\u4f30\u8ba1\u6a21\u5757\u3002", "method": "\u4f7f\u7528\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(i)\u57fa\u4e8e\u9ad8\u65af\u6cfc\u6e85\u7684\u5149\u771f\u5b9e\u8bed\u8a00\u5d4c\u5165\u98de\u884c\u6a21\u62df\u5668\uff0c(ii)RRT\u542f\u53d1\u7684\u591a\u8f68\u8ff9\u751f\u6210\u4e13\u5bb6\u7528\u4e8e\u65e0\u78b0\u649e\u5bfc\u822a\u6f14\u793a\uff0c(iii)\u8f7b\u91cf\u7ea7\u7aef\u5230\u7aef\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u8fdb\u884c\u5b9e\u65f6\u95ed\u73af\u63a7\u5236\u3002", "result": "\u5728\u786c\u4ef6\u98de\u884c\u5b9e\u9a8c\u4e2d\uff0cSINGER\u5728\u672a\u89c1\u73af\u5883\u548c\u672a\u89c1\u8bed\u8a00\u6761\u4ef6\u76ee\u6807\u7269\u4f53\u4e0a\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u96f6\u6837\u672c\u6a21\u62df\u5230\u771f\u5b9e\u8fc1\u79fb\u6027\u80fd\uff0c\u5e73\u5747\u5230\u8fbe\u67e5\u8be2\u76ee\u6807\u7387\u63d0\u9ad823.33%\uff0c\u89c6\u91ce\u4fdd\u6301\u7387\u63d0\u9ad816.67%\uff0c\u78b0\u649e\u51cf\u5c1110%\u3002", "conclusion": "SINGER\u6210\u529f\u5b9e\u73b0\u4e86\u4ec5\u4f7f\u7528\u673a\u8f7d\u4f20\u611f\u7684\u8bed\u8a00\u5f15\u5bfc\u81ea\u4e3b\u65e0\u4eba\u673a\u5bfc\u822a\uff0c\u4e3a\u5f00\u653e\u8bcd\u6c47\u65e0\u4eba\u673a\u5bfc\u822a\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18626", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18626", "abs": "https://arxiv.org/abs/2509.18626", "authors": ["Jay Patrikar", "Apoorva Sharma", "Sushant Veer", "Boyi Li", "Sebastian Scherer", "Marco Pavone"], "title": "The Case for Negative Data: From Crash Reports to Counterfactuals for Reasonable Driving", "comment": "8 pages, 5 figures", "summary": "Learning-based autonomous driving systems are trained mostly on incident-free\ndata, offering little guidance near safety-performance boundaries. Real crash\nreports contain precisely the contrastive evidence needed, but they are hard to\nuse: narratives are unstructured, third-person, and poorly grounded to sensor\nviews. We address these challenges by normalizing crash narratives to\nego-centric language and converting both logs and crashes into a unified\nscene-action representation suitable for retrieval. At decision time, our\nsystem adjudicates proposed actions by retrieving relevant precedents from this\nunified index; an agentic counterfactual extension proposes plausible\nalternatives, retrieves for each, and reasons across outcomes before deciding.\nOn a nuScenes benchmark, precedent retrieval substantially improves\ncalibration, with recall on contextually preferred actions rising from 24% to\n53%. The counterfactual variant preserves these gains while sharpening\ndecisions near risk.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u6545\u62a5\u544a\u68c0\u7d22\u7684\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u4e8b\u6545\u53d9\u8ff0\u8f6c\u6362\u4e3a\u81ea\u6211\u4e2d\u5fc3\u89c6\u89d2\u7684\u7edf\u4e00\u8868\u793a\uff0c\u5728\u51b3\u7b56\u65f6\u68c0\u7d22\u76f8\u5173\u5148\u4f8b\u6765\u6539\u8fdb\u5b89\u5168\u6027\u80fd\u8fb9\u754c\u9644\u8fd1\u7684\u51b3\u7b56\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e3b\u8981\u5728\u65e0\u4e8b\u6545\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u5bf9\u5b89\u5168\u6027\u80fd\u8fb9\u754c\u9644\u8fd1\u7684\u6307\u5bfc\u4e0d\u8db3\u3002\u771f\u5b9e\u4e8b\u6545\u62a5\u544a\u5305\u542b\u6240\u9700\u7684\u5bf9\u6bd4\u8bc1\u636e\uff0c\u4f46\u96be\u4ee5\u4f7f\u7528\uff0c\u56e0\u4e3a\u53d9\u8ff0\u662f\u975e\u7ed3\u6784\u5316\u3001\u7b2c\u4e09\u4eba\u79f0\u4e14\u4e0e\u4f20\u611f\u5668\u89c6\u56fe\u5173\u8054\u6027\u5dee\u3002", "method": "\u5c06\u4e8b\u6545\u53d9\u8ff0\u6807\u51c6\u5316\u4e3a\u81ea\u6211\u4e2d\u5fc3\u8bed\u8a00\uff0c\u5c06\u65e5\u5fd7\u548c\u4e8b\u6545\u8f6c\u6362\u4e3a\u7edf\u4e00\u7684\u573a\u666f-\u52a8\u4f5c\u8868\u793a\u7528\u4e8e\u68c0\u7d22\u3002\u5728\u51b3\u7b56\u65f6\uff0c\u7cfb\u7edf\u901a\u8fc7\u4ece\u7edf\u4e00\u7d22\u5f15\u4e2d\u68c0\u7d22\u76f8\u5173\u5148\u4f8b\u6765\u88c1\u51b3\u63d0\u8bae\u7684\u52a8\u4f5c\uff1b\u8fd8\u5305\u542b\u4e00\u4e2a\u53cd\u4e8b\u5b9e\u6269\u5c55\uff0c\u63d0\u51fa\u5408\u7406\u66ff\u4ee3\u65b9\u6848\u5e76\u4e3a\u6bcf\u4e2a\u65b9\u6848\u68c0\u7d22\u7ed3\u679c\uff0c\u5728\u51b3\u7b56\u524d\u8fdb\u884c\u8de8\u7ed3\u679c\u63a8\u7406\u3002", "result": "\u5728nuScenes\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5148\u4f8b\u68c0\u7d22\u663e\u8457\u6539\u5584\u4e86\u6821\u51c6\uff0c\u4e0a\u4e0b\u6587\u504f\u597d\u52a8\u4f5c\u7684\u53ec\u56de\u7387\u4ece24%\u63d0\u9ad8\u523053%\u3002\u53cd\u4e8b\u5b9e\u53d8\u4f53\u5728\u4fdd\u6301\u8fd9\u4e9b\u6536\u76ca\u7684\u540c\u65f6\uff0c\u5728\u98ce\u9669\u9644\u8fd1\u4f7f\u51b3\u7b56\u66f4\u52a0\u654f\u9510\u3002", "conclusion": "\u901a\u8fc7\u5c06\u4e8b\u6545\u62a5\u544a\u8f6c\u6362\u4e3a\u53ef\u68c0\u7d22\u7684\u7edf\u4e00\u8868\u793a\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u5229\u7528\u4e8b\u6545\u6570\u636e\u6539\u8fdb\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u5b89\u5168\u8fb9\u754c\u9644\u8fd1\u7684\u51b3\u7b56\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2509.18631", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18631", "abs": "https://arxiv.org/abs/2509.18631", "authors": ["Shuo Cheng", "Liqian Ma", "Zhenyang Chen", "Ajay Mandlekar", "Caelan Garrett", "Danfei Xu"], "title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training", "comment": null, "summary": "Behavior cloning has shown promise for robot manipulation, but real-world\ndemonstrations are costly to acquire at scale. While simulated data offers a\nscalable alternative, particularly with advances in automated demonstration\ngeneration, transferring policies to the real world is hampered by various\nsimulation and real domain gaps. In this work, we propose a unified\nsim-and-real co-training framework for learning generalizable manipulation\npolicies that primarily leverages simulation and only requires a few real-world\ndemonstrations. Central to our approach is learning a domain-invariant,\ntask-relevant feature space. Our key insight is that aligning the joint\ndistributions of observations and their corresponding actions across domains\nprovides a richer signal than aligning observations (marginals) alone. We\nachieve this by embedding an Optimal Transport (OT)-inspired loss within the\nco-training framework, and extend this to an Unbalanced OT framework to handle\nthe imbalance between abundant simulation data and limited real-world examples.\nWe validate our method on challenging manipulation tasks, showing it can\nleverage abundant simulation data to achieve up to a 30% improvement in the\nreal-world success rate and even generalize to scenarios seen only in\nsimulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u4eff\u771f\u4e0e\u771f\u5b9e\u4e16\u754c\u534f\u540c\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5c11\u91cf\u771f\u5b9e\u6f14\u793a\u548c\u5927\u91cf\u4eff\u771f\u6570\u636e\u5b66\u4e60\u6cdb\u5316\u6027\u5f3a\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u6838\u5fc3\u662f\u5b66\u4e60\u57df\u4e0d\u53d8\u7684\u4efb\u52a1\u76f8\u5173\u7279\u5f81\u7a7a\u95f4\u3002", "motivation": "\u884c\u4e3a\u514b\u9686\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u771f\u5b9e\u4e16\u754c\u6f14\u793a\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u89c4\u6a21\u5316\u83b7\u53d6\u3002\u4eff\u771f\u6570\u636e\u867d\u7136\u53ef\u6269\u5c55\uff0c\u4f46\u5b58\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u9886\u57df\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u635f\u5931\u5728\u534f\u540c\u8bad\u7ec3\u6846\u67b6\u4e2d\u5bf9\u9f50\u8de8\u57df\u7684\u89c2\u5bdf-\u52a8\u4f5c\u8054\u5408\u5206\u5e03\uff0c\u5e76\u6269\u5c55\u5230\u975e\u5e73\u8861OT\u6846\u67b6\u5904\u7406\u4eff\u771f\u4e0e\u771f\u5b9e\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "\u5728\u6311\u6218\u6027\u64cd\u4f5c\u4efb\u52a1\u4e0a\u9a8c\u8bc1\uff0c\u53ef\u63d0\u5347\u771f\u5b9e\u4e16\u754c\u6210\u529f\u7387\u9ad8\u8fbe30%\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u4ec5\u4eff\u771f\u4e2d\u89c1\u8fc7\u7684\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u5206\u5e03\u5bf9\u9f50\u6709\u6548\u7f29\u5c0f\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u5347\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.18636", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18636", "abs": "https://arxiv.org/abs/2509.18636", "authors": ["Yuan Zhou", "Jialiang Hou", "Guangtong Xu", "Fei Gao"], "title": "Number Adaptive Formation Flight Planning via Affine Deformable Guidance in Narrow Environments", "comment": null, "summary": "Formation maintenance with varying number of drones in narrow environments\nhinders the convergence of planning to the desired configurations. To address\nthis challenge, this paper proposes a formation planning method guided by\nDeformable Virtual Structures (DVS) with continuous spatiotemporal\ntransformation. Firstly, to satisfy swarm safety distance and preserve\nformation shape filling integrity for irregular formation geometries, we employ\nLloyd algorithm for uniform $\\underline{PA}$rtitioning and Hungarian algorithm\nfor $\\underline{AS}$signment (PAAS) in DVS. Subsequently, a spatiotemporal\ntrajectory involving DVS is planned using primitive-based path search and\nnonlinear trajectory optimization. The DVS trajectory achieves adaptive\ntransitions with respect to a varying number of drones while ensuring\nadaptability to narrow environments through affine transformation. Finally,\neach agent conducts distributed trajectory planning guided by desired\nspatiotemporal positions within the DVS, while incorporating collision\navoidance and dynamic feasibility requirements. Our method enables up to 15\\%\nof swarm numbers to join or leave in cluttered environments while rapidly\nrestoring the desired formation shape in simulation. Compared to cutting-edge\nformation planning method, we demonstrate rapid formation recovery capacity and\nenvironmental adaptability. Real-world experiments validate the effectiveness\nand resilience of our formation planning method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u53d8\u5f62\u865a\u62df\u7ed3\u6784\uff08DVS\uff09\u7684\u65e0\u4eba\u673a\u7f16\u961f\u89c4\u5212\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u72ed\u7a84\u73af\u5883\u4e2d\u5904\u7406\u65e0\u4eba\u673a\u6570\u91cf\u53d8\u5316\u7684\u60c5\u51b5\uff0c\u5b9e\u73b0\u5feb\u901f\u7f16\u961f\u6062\u590d\u548c\u73af\u5883\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u72ed\u7a84\u73af\u5883\u4e2d\u65e0\u4eba\u673a\u6570\u91cf\u53d8\u5316\u65f6\u7f16\u961f\u7ef4\u62a4\u56f0\u96be\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6536\u655b\u5230\u671f\u671b\u914d\u7f6e\u3002", "method": "\u91c7\u7528Lloyd\u7b97\u6cd5\u8fdb\u884c\u5747\u5300\u5206\u533a\u548c\u5308\u7259\u5229\u7b97\u6cd5\u8fdb\u884c\u5206\u914d\uff08PAAS\uff09\uff0c\u7ed3\u5408\u57fa\u4e8e\u57fa\u5143\u7684\u8def\u5f84\u641c\u7d22\u548c\u975e\u7ebf\u6027\u8f68\u8ff9\u4f18\u5316\u6765\u89c4\u5212DVS\u7684\u65f6\u7a7a\u8f68\u8ff9\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u8f6c\u6362\u548c\u4eff\u5c04\u53d8\u6362\u3002", "result": "\u5728\u6a21\u62df\u73af\u5883\u4e2d\u652f\u6301\u6700\u591a15%\u7684\u65e0\u4eba\u673a\u52a0\u5165\u6216\u79bb\u5f00\uff0c\u5e76\u80fd\u5feb\u901f\u6062\u590d\u671f\u671b\u7f16\u961f\u5f62\u72b6\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5c55\u73b0\u51fa\u66f4\u5feb\u7684\u7f16\u961f\u6062\u590d\u80fd\u529b\u548c\u73af\u5883\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u52a8\u6001\u65e0\u4eba\u673a\u7f16\u961f\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18644", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18644", "abs": "https://arxiv.org/abs/2509.18644", "authors": ["Juntu Zhao", "Wenbo Lu", "Di Zhang", "Yufeng Liu", "Yushen Liang", "Tianluo Zhang", "Yifeng Cao", "Junyuan Xie", "Yingdong Hu", "Shengjie Wang", "Junliang Guo", "Dequan Wang", "Yang Gao"], "title": "Do You Need Proprioceptive States in Visuomotor Policies?", "comment": "Project page: https://statefreepolicy.github.io", "summary": "Imitation-learning-based visuomotor policies have been widely used in robot\nmanipulation, where both visual observations and proprioceptive states are\ntypically adopted together for precise control. However, in this study, we find\nthat this common practice makes the policy overly reliant on the proprioceptive\nstate input, which causes overfitting to the training trajectories and results\nin poor spatial generalization. On the contrary, we propose the State-free\nPolicy, removing the proprioceptive state input and predicting actions only\nconditioned on visual observations. The State-free Policy is built in the\nrelative end-effector action space, and should ensure the full task-relevant\nvisual observations, here provided by dual wide-angle wrist cameras. Empirical\nresults demonstrate that the State-free policy achieves significantly stronger\nspatial generalization than the state-based policy: in real-world tasks such as\npick-and-place, challenging shirt-folding, and complex whole-body manipulation,\nspanning multiple robot embodiments, the average success rate improves from 0\\%\nto 85\\% in height generalization and from 6\\% to 64\\% in horizontal\ngeneralization. Furthermore, they also show advantages in data efficiency and\ncross-embodiment adaptation, enhancing their practicality for real-world\ndeployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u72b6\u6001\u7b56\u7565\uff08State-free Policy\uff09\uff0c\u901a\u8fc7\u79fb\u9664\u672c\u4f53\u611f\u77e5\u72b6\u6001\u8f93\u5165\uff0c\u4ec5\u57fa\u4e8e\u89c6\u89c9\u89c2\u5bdf\u9884\u6d4b\u52a8\u4f5c\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u4eff\u5b66\u4e60\u4e2d\u7b56\u7565\u8fc7\u5ea6\u4f9d\u8d56\u672c\u4f53\u611f\u77e5\u72b6\u6001\u5bfc\u81f4\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u901a\u5e38\u540c\u65f6\u4f7f\u7528\u89c6\u89c9\u89c2\u5bdf\u548c\u672c\u4f53\u611f\u77e5\u72b6\u6001\u8fdb\u884c\u7cbe\u786e\u63a7\u5236\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u505a\u6cd5\u4f1a\u4f7f\u7b56\u7565\u8fc7\u5ea6\u4f9d\u8d56\u672c\u4f53\u611f\u77e5\u72b6\u6001\u8f93\u5165\uff0c\u5bfc\u81f4\u5bf9\u8bad\u7ec3\u8f68\u8ff9\u7684\u8fc7\u62df\u5408\u548c\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51fa\u65e0\u72b6\u6001\u7b56\u7565\uff0c\u79fb\u9664\u672c\u4f53\u611f\u77e5\u72b6\u6001\u8f93\u5165\uff0c\u4ec5\u57fa\u4e8e\u89c6\u89c9\u89c2\u5bdf\u9884\u6d4b\u52a8\u4f5c\u3002\u7b56\u7565\u6784\u5efa\u5728\u76f8\u5bf9\u672b\u7aef\u6267\u884c\u5668\u52a8\u4f5c\u7a7a\u95f4\u4e2d\uff0c\u5e76\u901a\u8fc7\u53cc\u5e7f\u89d2\u8155\u90e8\u6444\u50cf\u5934\u63d0\u4f9b\u5b8c\u6574\u7684\u4efb\u52a1\u76f8\u5173\u89c6\u89c9\u89c2\u5bdf\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u65e0\u72b6\u6001\u7b56\u7565\u5728\u7a7a\u95f4\u6cdb\u5316\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u72b6\u6001\u7684\u7b56\u7565\uff1a\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\uff0c\u9ad8\u5ea6\u6cdb\u5316\u7684\u5e73\u5747\u6210\u529f\u7387\u4ece0%\u63d0\u5347\u523085%\uff0c\u6c34\u5e73\u6cdb\u5316\u4ece6%\u63d0\u5347\u523064%\u3002\u540c\u65f6\u8fd8\u5728\u6570\u636e\u6548\u7387\u548c\u8de8\u5177\u8eab\u9002\u5e94\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u52bf\u3002", "conclusion": "\u65e0\u72b6\u6001\u7b56\u7565\u901a\u8fc7\u51cf\u5c11\u5bf9\u672c\u4f53\u611f\u77e5\u72b6\u6001\u7684\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\uff0c\u589e\u5f3a\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u7684\u5b9e\u7528\u6027\u3002"}}
{"id": "2509.18648", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18648", "abs": "https://arxiv.org/abs/2509.18648", "authors": ["Yarden As", "Chengrui Qu", "Benjamin Unger", "Dongho Kang", "Max van der Hart", "Laixi Shi", "Stelian Coros", "Adam Wierman", "Andreas Krause"], "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer", "comment": null, "summary": "Safety remains a major concern for deploying reinforcement learning (RL) in\nreal-world applications. Simulators provide safe, scalable training\nenvironments, but the inevitable sim-to-real gap introduces additional safety\nconcerns, as policies must satisfy constraints in real-world conditions that\ndiffer from simulation. To address this challenge, robust safe RL techniques\noffer principled methods, but are often incompatible with standard scalable\ntraining pipelines. In contrast, domain randomization, a simple and popular\nsim-to-real technique, stands out as a promising alternative, although it often\nresults in unsafe behaviors in practice. We present SPiDR, short for\nSim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with\nprovable guarantees for safe sim-to-real transfer. SPiDR uses domain\nrandomization to incorporate the uncertainty about the sim-to-real gap into the\nsafety constraints, making it versatile and highly compatible with existing\ntraining pipelines. Through extensive experiments on sim-to-sim benchmarks and\ntwo distinct real-world robotic platforms, we demonstrate that SPiDR\neffectively ensures safety despite the sim-to-real gap while maintaining strong\nperformance.", "AI": {"tldr": "SPiDR\u662f\u4e00\u79cd\u57fa\u4e8e\u60b2\u89c2\u57df\u968f\u673a\u5316\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u6362\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165\u5b89\u5168\u7ea6\u675f\u6765\u4fdd\u8bc1\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u5f3a\u5316\u5b66\u4e60\u65f6\u5b89\u5168\u6027\u662f\u4e3b\u8981\u6311\u6218\uff0c\u6a21\u62df\u5668\u8bad\u7ec3\u5b58\u5728\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\uff0c\u4f20\u7edf\u9c81\u68d2\u5b89\u5168RL\u65b9\u6cd5\u96be\u4ee5\u4e0e\u53ef\u6269\u5c55\u8bad\u7ec3\u6d41\u7a0b\u517c\u5bb9\uff0c\u800c\u6d41\u884c\u7684\u57df\u968f\u673a\u5316\u65b9\u6cd5\u5728\u5b9e\u8df5\u4e2d\u5f80\u5f80\u5bfc\u81f4\u4e0d\u5b89\u5168\u884c\u4e3a\u3002", "method": "SPiDR\u4f7f\u7528\u57df\u968f\u673a\u5316\u6280\u672f\u5c06\u6a21\u62df\u5230\u73b0\u5b9e\u5dee\u8ddd\u7684\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165\u5b89\u5168\u7ea6\u675f\uff0c\u91c7\u7528\u60b2\u89c2\u7b56\u7565\u6765\u786e\u4fdd\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u73b0\u6709\u8bad\u7ec3\u6d41\u7a0b\u7684\u9ad8\u5ea6\u517c\u5bb9\u6027\u3002", "result": "\u901a\u8fc7\u5728\u6a21\u62df\u5230\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e24\u4e2a\u4e0d\u540c\u7684\u771f\u5b9e\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660eSPiDR\u80fd\u591f\u6709\u6548\u786e\u4fdd\u6a21\u62df\u5230\u73b0\u5b9e\u5dee\u8ddd\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "SPiDR\u63d0\u4f9b\u4e86\u4e00\u79cd\u5177\u6709\u53ef\u8bc1\u660e\u4fdd\u8bc1\u7684\u5b89\u5168\u6a21\u62df\u5230\u73b0\u5b9e\u8f6c\u6362\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u5b9e\u90e8\u7f72\u4e2d\u7684\u5b89\u5168\u95ee\u9898\u3002"}}
{"id": "2509.18666", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18666", "abs": "https://arxiv.org/abs/2509.18666", "authors": ["Kaizer Rahaman", "Simran Kumari", "Ashish R. Hota"], "title": "Distributionally Robust Safe Motion Planning with Contextual Information", "comment": null, "summary": "We present a distributionally robust approach for collision avoidance by\nincorporating contextual information. Specifically, we embed the conditional\ndistribution of future trajectory of the obstacle conditioned on the motion of\nthe ego agent in a reproducing kernel Hilbert space (RKHS) via the conditional\nkernel mean embedding operator. Then, we define an ambiguity set containing all\ndistributions whose embedding in the RKHS is within a certain distance from the\nempirical estimate of conditional mean embedding learnt from past data.\nConsequently, a distributionally robust collision avoidance constraint is\nformulated, and included in the receding horizon based motion planning\nformulation of the ego agent. Simulation results show that the proposed\napproach is more successful in avoiding collision compared to approaches that\ndo not include contextual information and/or distributional robustness in their\nformulation in several challenging scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u5206\u5e03\u9c81\u68d2\u78b0\u649e\u907f\u514d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6761\u4ef6\u6838\u5747\u503c\u5d4c\u5165\u5728RKHS\u4e2d\u5efa\u6a21\u969c\u788d\u7269\u672a\u6765\u8f68\u8ff9\u7684\u6761\u4ef6\u5206\u5e03\uff0c\u6784\u5efa\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u96c6\u5408\uff0c\u5e76\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u5f15\u5165\u5206\u5e03\u9c81\u68d2\u7ea6\u675f\u3002", "motivation": "\u4f20\u7edf\u78b0\u649e\u907f\u514d\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u4e0a\u4e0b\u6587\u4fe1\u606f\u6216\u7f3a\u4e4f\u5bf9\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u573a\u666f\u4e2d\u6027\u80fd\u53d7\u9650\u3002", "method": "\u4f7f\u7528\u6761\u4ef6\u6838\u5747\u503c\u5d4c\u5165\u5728RKHS\u4e2d\u8868\u793a\u969c\u788d\u7269\u8f68\u8ff9\u7684\u6761\u4ef6\u5206\u5e03\uff0c\u6784\u5efa\u57fa\u4e8e\u7ecf\u9a8c\u4f30\u8ba1\u7684\u5206\u5e03\u6a21\u7cca\u96c6\uff0c\u5e76\u5728\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u4e2d\u5f15\u5165\u5206\u5e03\u9c81\u68d2\u78b0\u649e\u907f\u514d\u7ea6\u675f\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6311\u6218\u6027\u573a\u666f\u4e2d\u6bd4\u4e0d\u8003\u8651\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u5206\u5e03\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u78b0\u649e\u907f\u514d\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5206\u5e03\u9c81\u68d2\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u5904\u7406\u5206\u5e03\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u78b0\u649e\u907f\u514d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.18671", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18671", "abs": "https://arxiv.org/abs/2509.18671", "authors": ["Kaixin Chai", "Hyunjun Lee", "Joseph J. Lim"], "title": "N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout", "comment": null, "summary": "In mobile manipulation, the manipulation policy has strong preferences for\ninitial poses where it is executed. However, the navigation module focuses\nsolely on reaching the task area, without considering which initial pose is\npreferable for downstream manipulation. To address this misalignment, we\nintroduce N2M, a transition module that guides the robot to a preferable\ninitial pose after reaching the task area, thereby substantially improving task\nsuccess rates. N2M features five key advantages: (1) reliance solely on\nego-centric observation without requiring global or historical information; (2)\nreal-time adaptation to environmental changes; (3) reliable prediction with\nhigh viewpoint robustness; (4) broad applicability across diverse tasks,\nmanipulation policies, and robot hardware; and (5) remarkable data efficiency\nand generalizability. We demonstrate the effectiveness of N2M through extensive\nsimulation and real-world experiments. In the PnPCounterToCab task, N2M\nimproves the averaged success rate from 3% with the reachability-based baseline\nto 54%. Furthermore, in the Toybox Handover task, N2M provides reliable\npredictions even in unseen environments with only 15 data samples, showing\nremarkable data efficiency and generalizability.", "AI": {"tldr": "N2M\u662f\u4e00\u4e2a\u8fc7\u6e21\u6a21\u5757\uff0c\u7528\u4e8e\u5728\u673a\u5668\u4eba\u5230\u8fbe\u4efb\u52a1\u533a\u57df\u540e\u5f15\u5bfc\u5176\u5230\u66f4\u9002\u5408\u64cd\u4f5c\u7684\u521d\u59cb\u59ff\u6001\uff0c\u663e\u8457\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u79fb\u52a8\u64cd\u4f5c\u4e2d\uff0c\u5bfc\u822a\u6a21\u5757\u53ea\u5173\u6ce8\u5230\u8fbe\u4efb\u52a1\u533a\u57df\uff0c\u800c\u4e0d\u8003\u8651\u4e0b\u6e38\u64cd\u4f5c\u7b56\u7565\u504f\u597d\u7684\u521d\u59cb\u59ff\u6001\uff0c\u5bfc\u81f4\u4e24\u8005\u4e0d\u5339\u914d\u3002", "method": "N2M\u4ec5\u4f9d\u8d56\u81ea\u6211\u4e2d\u5fc3\u89c2\u5bdf\uff0c\u65e0\u9700\u5168\u5c40\u6216\u5386\u53f2\u4fe1\u606f\uff0c\u80fd\u5b9e\u65f6\u9002\u5e94\u73af\u5883\u53d8\u5316\uff0c\u5177\u6709\u9ad8\u89c6\u89d2\u9c81\u68d2\u6027\u548c\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "result": "\u5728PnPCounterToCab\u4efb\u52a1\u4e2d\uff0cN2M\u5c06\u5e73\u5747\u6210\u529f\u7387\u4ece3%\u63d0\u5347\u523054%\uff1b\u5728Toybox Handover\u4efb\u52a1\u4e2d\uff0c\u4ec5\u752815\u4e2a\u6570\u636e\u6837\u672c\u5c31\u80fd\u5728\u672a\u89c1\u73af\u5883\u4e2d\u63d0\u4f9b\u53ef\u9760\u9884\u6d4b\u3002", "conclusion": "N2M\u901a\u8fc7\u89e3\u51b3\u5bfc\u822a\u4e0e\u64cd\u4f5c\u4e4b\u95f4\u7684\u59ff\u6001\u504f\u597d\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u79fb\u52a8\u64cd\u4f5c\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2509.18676", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.18676", "abs": "https://arxiv.org/abs/2509.18676", "authors": ["Sangjun Noh", "Dongwoo Nam", "Kangmin Kim", "Geonhyup Lee", "Yeonguk Yu", "Raeyoung Kang", "Kyoobin Lee"], "title": "3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space", "comment": "7 main scripts + 2 reference pages", "summary": "Learning robust visuomotor policies that generalize across diverse objects\nand interaction dynamics remains a central challenge in robotic manipulation.\nMost existing approaches rely on direct observation-to-action mappings or\ncompress perceptual inputs into global or object-centric features, which often\noverlook localized motion cues critical for precise and contact-rich\nmanipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework\nthat leverages scene-level 3D flow as a structured intermediate representation\nto capture fine-grained local motion cues. Our approach predicts the temporal\ntrajectories of sampled query points and conditions action generation on these\ninteraction-aware flows, implemented jointly within a unified diffusion\narchitecture. This design grounds manipulation in localized dynamics while\nenabling the policy to reason about broader scene-level consequences of\nactions. Extensive experiments on the MetaWorld benchmark show that 3D FDP\nachieves state-of-the-art performance across 50 tasks, particularly excelling\non medium and hard settings. Beyond simulation, we validate our method on eight\nreal-robot tasks, where it consistently outperforms prior baselines in\ncontact-rich and non-prehensile scenarios. These results highlight 3D flow as a\npowerful structural prior for learning generalizable visuomotor policies,\nsupporting the development of more robust and versatile robotic manipulation.\nRobot demonstrations, additional results, and code can be found at\nhttps://sites.google.com/view/3dfdp/home.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa3D Flow Diffusion Policy\uff083D FDP\uff09\uff0c\u4e00\u79cd\u5229\u7528\u573a\u666f\u7ea73D\u6d41\u4f5c\u4e3a\u7ed3\u6784\u5316\u4e2d\u95f4\u8868\u793a\u6765\u6355\u6349\u7cbe\u7ec6\u5c40\u90e8\u8fd0\u52a8\u7ebf\u7d22\u7684\u65b0\u6846\u67b6\uff0c\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u76f4\u63a5\u89c2\u5bdf\u5230\u52a8\u4f5c\u7684\u6620\u5c04\u6216\u5c06\u611f\u77e5\u8f93\u5165\u538b\u7f29\u4e3a\u5168\u5c40\u6216\u7269\u4f53\u4e2d\u5fc3\u7279\u5f81\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u7cbe\u786e\u548c\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u6240\u9700\u7684\u5173\u952e\u5c40\u90e8\u8fd0\u52a8\u7ebf\u7d22\u3002", "method": "\u9884\u6d4b\u91c7\u6837\u67e5\u8be2\u70b9\u7684\u65f6\u95f4\u8f68\u8ff9\uff0c\u5e76\u5728\u7edf\u4e00\u7684\u6269\u6563\u67b6\u6784\u4e2d\u57fa\u4e8e\u8fd9\u4e9b\u4ea4\u4e92\u611f\u77e5\u6d41\u6765\u751f\u6210\u52a8\u4f5c\uff0c\u5c06\u64cd\u4f5c\u5efa\u7acb\u5728\u5c40\u90e8\u52a8\u6001\u57fa\u7840\u4e0a\u540c\u65f6\u8003\u8651\u52a8\u4f5c\u7684\u5168\u5c40\u573a\u666f\u540e\u679c\u3002", "result": "\u5728MetaWorld\u57fa\u51c6\u6d4b\u8bd5\u768450\u4e2a\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u4e2d\u7b49\u548c\u56f0\u96be\u8bbe\u7f6e\u4e2d\u8868\u73b0\u4f18\u5f02\uff1b\u57288\u4e2a\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "3D\u6d41\u4f5c\u4e3a\u5b66\u4e60\u53ef\u6cdb\u5316\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u5f3a\u5927\u7ed3\u6784\u5316\u5148\u9a8c\uff0c\u652f\u6301\u5f00\u53d1\u66f4\u9c81\u68d2\u548c\u901a\u7528\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u3002"}}
{"id": "2509.18686", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18686", "abs": "https://arxiv.org/abs/2509.18686", "authors": ["Ziyi Xu", "Haohong Lin", "Shiqi Liu", "Ding Zhao"], "title": "Query-Centric Diffusion Policy for Generalizable Robotic Assembly", "comment": "8 pages, 7 figures", "summary": "The robotic assembly task poses a key challenge in building generalist robots\ndue to the intrinsic complexity of part interactions and the sensitivity to\nnoise perturbations in contact-rich settings. The assembly agent is typically\ndesigned in a hierarchical manner: high-level multi-part reasoning and\nlow-level precise control. However, implementing such a hierarchical policy is\nchallenging in practice due to the mismatch between high-level skill queries\nand low-level execution. To address this, we propose the Query-centric\nDiffusion Policy (QDP), a hierarchical framework that bridges high-level\nplanning and low-level control by utilizing queries comprising objects, contact\npoints, and skill information. QDP introduces a query-centric mechanism that\nidentifies task-relevant components and uses them to guide low-level policies,\nleveraging point cloud observations to improve the policy's robustness. We\nconduct comprehensive experiments on the FurnitureBench in both simulation and\nreal-world settings, demonstrating improved performance in skill precision and\nlong-horizon success rate. In the challenging insertion and screwing tasks, QDP\nimproves the skill-wise success rate by over 50% compared to baselines without\nstructured queries.", "AI": {"tldr": "\u63d0\u51faQuery-centric Diffusion Policy (QDP)\u5206\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u67e5\u8be2\u673a\u5236\u8fde\u63a5\u9ad8\u5c42\u89c4\u5212\u548c\u5e95\u5c42\u63a7\u5236\uff0c\u63d0\u5347\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u7684\u6027\u80fd", "motivation": "\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u56e0\u96f6\u4ef6\u4ea4\u4e92\u590d\u6742\u6027\u548c\u63a5\u89e6\u4e30\u5bcc\u73af\u5883\u4e2d\u7684\u566a\u58f0\u654f\u611f\u6027\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u5206\u5c42\u7b56\u7565\u5b58\u5728\u9ad8\u5c42\u6280\u80fd\u67e5\u8be2\u4e0e\u5e95\u5c42\u6267\u884c\u4e0d\u5339\u914d\u7684\u95ee\u9898", "method": "QDP\u6846\u67b6\u4f7f\u7528\u5305\u542b\u7269\u4f53\u3001\u63a5\u89e6\u70b9\u548c\u6280\u80fd\u4fe1\u606f\u7684\u67e5\u8be2\u6765\u8bc6\u522b\u4efb\u52a1\u76f8\u5173\u7ec4\u4ef6\u5e76\u6307\u5bfc\u5e95\u5c42\u7b56\u7565\uff0c\u5229\u7528\u70b9\u4e91\u89c2\u6d4b\u63d0\u9ad8\u7b56\u7565\u9c81\u68d2\u6027", "result": "\u5728FurnitureBench\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u4e2d\uff0cQDP\u5728\u6280\u80fd\u7cbe\u5ea6\u548c\u957f\u65f6\u7a0b\u6210\u529f\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u63d2\u5165\u548c\u62e7\u87ba\u4e1d\u4efb\u52a1\u4e2d\u6bd4\u65e0\u7ed3\u6784\u5316\u67e5\u8be2\u7684\u57fa\u7ebf\u65b9\u6cd5\u6210\u529f\u7387\u63d0\u9ad850%\u4ee5\u4e0a", "conclusion": "QDP\u901a\u8fc7\u67e5\u8be2\u4e2d\u5fc3\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5206\u5c42\u7b56\u7565\u4e2d\u7684\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u901a\u7528\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.18734", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18734", "abs": "https://arxiv.org/abs/2509.18734", "authors": ["Nishant Doshi", "Amey Sutvani", "Sanket Gujar"], "title": "Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation", "comment": null, "summary": "One of the challenges faced by Autonomous Aerial Vehicles is reliable\nnavigation through urban environments. Factors like reduction in precision of\nGlobal Positioning System (GPS), narrow spaces and dynamically moving obstacles\nmake the path planning of an aerial robot a complicated task. One of the skills\nrequired for the agent to effectively navigate through such an environment is\nto develop an ability to avoid collisions using information from onboard depth\nsensors. In this paper, we propose Reinforcement Learning of a virtual\nquadcopter robot agent equipped with a Depth Camera to navigate through a\nsimulated urban environment.", "AI": {"tldr": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u914d\u5907\u6df1\u5ea6\u76f8\u673a\u7684\u865a\u62df\u56db\u8f74\u98de\u884c\u5668\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u5bfc\u822a", "motivation": "\u81ea\u4e3b\u98de\u884c\u5668\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u9762\u4e34GPS\u7cbe\u5ea6\u4e0b\u964d\u3001\u72ed\u7a84\u7a7a\u95f4\u548c\u52a8\u6001\u969c\u788d\u7269\u7b49\u6311\u6218\uff0c\u9700\u8981\u53ef\u9760\u7684\u5bfc\u822a\u80fd\u529b", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8bad\u7ec3\u914d\u5907\u6df1\u5ea6\u76f8\u673a\u7684\u865a\u62df\u56db\u8f74\u98de\u884c\u5668\u5728\u6a21\u62df\u57ce\u5e02\u73af\u5883\u4e2d\u8fdb\u884c\u5bfc\u822a", "result": "\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5bfc\u822a\u65b9\u6cd5\uff0c\u4f46\u672a\u5728\u6458\u8981\u4e2d\u5c55\u793a\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u662f\u89e3\u51b3\u57ce\u5e02\u73af\u5883\u4e2d\u81ea\u4e3b\u98de\u884c\u5668\u5bfc\u822a\u6311\u6218\u7684\u6709\u6548\u65b9\u6cd5"}}
{"id": "2509.18757", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18757", "abs": "https://arxiv.org/abs/2509.18757", "authors": ["Omar Rayyan", "John Abanes", "Mahmoud Hafez", "Anthony Tzes", "Fares Abu-Dakka"], "title": "MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning", "comment": "For project website and videos, see https https://mv-umi.github.io", "summary": "Recent advances in imitation learning have shown great promise for developing\nrobust robot manipulation policies from demonstrations. However, this promise\nis contingent on the availability of diverse, high-quality datasets, which are\nnot only challenging and costly to collect but are often constrained to a\nspecific robot embodiment. Portable handheld grippers have recently emerged as\nintuitive and scalable alternatives to traditional robotic teleoperation\nmethods for data collection. However, their reliance solely on first-person\nview wrist-mounted cameras often creates limitations in capturing sufficient\nscene contexts. In this paper, we present MV-UMI (Multi-View Universal\nManipulation Interface), a framework that integrates a third-person perspective\nwith the egocentric camera to overcome this limitation. This integration\nmitigates domain shifts between human demonstration and robot deployment,\npreserving the cross-embodiment advantages of handheld data-collection devices.\nOur experimental results, including an ablation study, demonstrate that our\nMV-UMI framework improves performance in sub-tasks requiring broad scene\nunderstanding by approximately 47% across 3 tasks, confirming the effectiveness\nof our approach in expanding the range of feasible manipulation tasks that can\nbe learned using handheld gripper systems, without compromising the\ncross-embodiment advantages inherent to such systems.", "AI": {"tldr": "MV-UMI\u6846\u67b6\u901a\u8fc7\u6574\u5408\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u4e0e\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u76f8\u673a\uff0c\u89e3\u51b3\u4e86\u624b\u6301\u6293\u53d6\u5668\u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u573a\u666f\u4e0a\u4e0b\u6587\u6355\u83b7\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u624b\u6301\u6293\u53d6\u5668\u4f5c\u4e3a\u6570\u636e\u6536\u96c6\u5de5\u5177\u867d\u7136\u76f4\u89c2\u4e14\u53ef\u6269\u5c55\uff0c\u4f46\u4ec5\u4f9d\u8d56\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u624b\u8155\u76f8\u673a\u96be\u4ee5\u6355\u83b7\u8db3\u591f\u7684\u573a\u666f\u4e0a\u4e0b\u6587\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u5b66\u4e60\u8303\u56f4\u3002", "method": "\u63d0\u51faMV-UMI\u6846\u67b6\uff0c\u6574\u5408\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u4e0e\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u76f8\u673a\uff0c\u51cf\u8f7b\u4eba\u7c7b\u6f14\u793a\u4e0e\u673a\u5668\u4eba\u90e8\u7f72\u4e4b\u95f4\u7684\u9886\u57df\u504f\u79fb\uff0c\u540c\u65f6\u4fdd\u6301\u624b\u6301\u8bbe\u5907\u7684\u8de8\u5177\u8eab\u4f18\u52bf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cMV-UMI\u5728\u9700\u8981\u5e7f\u6cdb\u573a\u666f\u7406\u89e3\u7684\u5b50\u4efb\u52a1\u4e2d\u6027\u80fd\u63d0\u5347\u7ea647%\uff0c\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "MV-UMI\u6846\u67b6\u6269\u5c55\u4e86\u624b\u6301\u6293\u53d6\u5668\u7cfb\u7edf\u53ef\u5b66\u4e60\u7684\u64cd\u4f5c\u4efb\u52a1\u8303\u56f4\uff0c\u4e14\u4e0d\u635f\u5bb3\u5176\u56fa\u6709\u7684\u8de8\u5177\u8eab\u4f18\u52bf\u3002"}}
{"id": "2509.18778", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18778", "abs": "https://arxiv.org/abs/2509.18778", "authors": ["Shijia Ge", "Yinxin Zhang", "Shuzhao Xie", "Weixiang Zhang", "Mingcai Zhou", "Zhi Wang"], "title": "VGGT-DP: Generalizable Robot Control via Vision Foundation Models", "comment": "submitted to AAAI 2026", "summary": "Visual imitation learning frameworks allow robots to learn manipulation\nskills from expert demonstrations. While existing approaches mainly focus on\npolicy design, they often neglect the structure and capacity of visual\nencoders, limiting spatial understanding and generalization. Inspired by\nbiological vision systems, which rely on both visual and proprioceptive cues\nfor robust control, we propose VGGT-DP, a visuomotor policy framework that\nintegrates geometric priors from a pretrained 3D perception model with\nproprioceptive feedback. We adopt the Visual Geometry Grounded Transformer\n(VGGT) as the visual encoder and introduce a proprioception-guided visual\nlearning strategy to align perception with internal robot states, improving\nspatial grounding and closed-loop control. To reduce inference latency, we\ndesign a frame-wise token reuse mechanism that compacts multi-view tokens into\nan efficient spatial representation. We further apply random token pruning to\nenhance policy robustness and reduce overfitting. Experiments on challenging\nMetaWorld tasks show that VGGT-DP significantly outperforms strong baselines\nsuch as DP and DP3, particularly in precision-critical and long-horizon\nscenarios.", "AI": {"tldr": "VGGT-DP\u662f\u4e00\u4e2a\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u9884\u8bad\u7ec33D\u611f\u77e5\u6a21\u578b\u7684\u51e0\u4f55\u5148\u9a8c\u548c\u672c\u4f53\u611f\u89c9\u53cd\u9988\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u7684\u7a7a\u95f4\u7406\u89e3\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7b56\u7565\u8bbe\u8ba1\uff0c\u4f46\u5ffd\u89c6\u4e86\u89c6\u89c9\u7f16\u7801\u5668\u7684\u7ed3\u6784\u548c\u80fd\u529b\uff0c\u9650\u5236\u4e86\u7a7a\u95f4\u7406\u89e3\u548c\u6cdb\u5316\u3002\u53d7\u751f\u7269\u89c6\u89c9\u7cfb\u7edf\u542f\u53d1\uff0c\u9700\u8981\u7ed3\u5408\u89c6\u89c9\u548c\u672c\u4f53\u611f\u89c9\u7ebf\u7d22\u6765\u5b9e\u73b0\u9c81\u68d2\u63a7\u5236\u3002", "method": "\u91c7\u7528VGGT\u4f5c\u4e3a\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u5f15\u5165\u672c\u4f53\u611f\u89c9\u5f15\u5bfc\u7684\u89c6\u89c9\u5b66\u4e60\u7b56\u7565\uff0c\u8bbe\u8ba1\u5e27\u7ea7token\u91cd\u7528\u673a\u5236\u51cf\u5c11\u63a8\u7406\u5ef6\u8fdf\uff0c\u5e76\u5e94\u7528\u968f\u673atoken\u526a\u679d\u589e\u5f3a\u7b56\u7565\u9c81\u68d2\u6027\u3002", "result": "\u5728MetaWorld\u6311\u6218\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cVGGT-DP\u663e\u8457\u4f18\u4e8eDP\u548cDP3\u7b49\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u7cbe\u5ea6\u8981\u6c42\u9ad8\u548c\u957f\u65f6\u7a0b\u573a\u666f\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "VGGT-DP\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u5148\u9a8c\u548c\u672c\u4f53\u611f\u89c9\u53cd\u9988\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u7a7a\u95f4\u5b9a\u4f4d\u548c\u95ed\u73af\u63a7\u5236\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u6280\u80fd\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18786", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18786", "abs": "https://arxiv.org/abs/2509.18786", "authors": ["Johannes A. Gaus", "Loris Schneider", "Yitian Shi", "Jongseok Lee", "Rania Rayyes", "Rudolph Triebel"], "title": "Human-Interpretable Uncertainty Explanations for Point Cloud Registration", "comment": null, "summary": "In this paper, we address the point cloud registration problem, where\nwell-known methods like ICP fail under uncertainty arising from sensor noise,\npose-estimation errors, and partial overlap due to occlusion. We develop a\nnovel approach, Gaussian Process Concept Attribution (GP-CA), which not only\nquantifies registration uncertainty but also explains it by attributing\nuncertainty to well-known sources of errors in registration problems. Our\napproach leverages active learning to discover new uncertainty sources in the\nwild by querying informative instances. We validate GP-CA on three publicly\navailable datasets and in our real-world robot experiment. Extensive ablations\nsubstantiate our design choices. Our approach outperforms other\nstate-of-the-art methods in terms of runtime, high sample-efficiency with\nactive learning, and high accuracy. Our real-world experiment clearly\ndemonstrates its applicability. Our video also demonstrates that GP-CA enables\neffective failure-recovery behaviors, yielding more robust robotic perception.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGP-CA\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u70b9\u4e91\u914d\u51c6\u95ee\u9898\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e0e\u5f52\u56e0\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u53d1\u73b0\u65b0\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5982ICP\u5728\u4f20\u611f\u5668\u566a\u58f0\u3001\u59ff\u6001\u4f30\u8ba1\u8bef\u5dee\u548c\u90e8\u5206\u91cd\u53e0\u7b49\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u5bb9\u6613\u5931\u8d25\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u91cf\u5316\u5e76\u89e3\u91ca\u914d\u51c6\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u9ad8\u65af\u8fc7\u7a0b\u6982\u5ff5\u5f52\u56e0\uff08GP-CA\uff09\u65b9\u6cd5\uff0c\u5229\u7528\u4e3b\u52a8\u5b66\u4e60\u6765\u53d1\u73b0\u914d\u51c6\u95ee\u9898\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\uff0c\u5e76\u901a\u8fc7\u67e5\u8be2\u4fe1\u606f\u5b9e\u4f8b\u6765\u6539\u8fdb\u6a21\u578b\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86GP-CA\u7684\u6709\u6548\u6027\uff0c\u8be5\u65b9\u6cd5\u5728\u8fd0\u884c\u65f6\u95f4\u3001\u6837\u672c\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "GP-CA\u4e0d\u4ec5\u80fd\u591f\u6709\u6548\u91cf\u5316\u914d\u51c6\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd8\u80fd\u5b9e\u73b0\u6709\u6548\u7684\u6545\u969c\u6062\u590d\u884c\u4e3a\uff0c\u63d0\u9ad8\u673a\u5668\u4eba\u611f\u77e5\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2509.18793", "categories": ["cs.RO", "cs.MA", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.18793", "abs": "https://arxiv.org/abs/2509.18793", "authors": ["Lukas Zanger", "Bastian Lampe", "Lennart Reiher", "Lutz Eckstein"], "title": "Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations", "comment": "7 pages, 2 figures, 2 tables; Accepted to be published as part of the\n  2025 IEEE International Conference on Intelligent Transportation Systems\n  (ITSC 2025), Gold Coast, Australia, November 18-21, 2025", "summary": "Vehicles are becoming increasingly automated and interconnected, enabling the\nformation of cooperative intelligent transport systems (C-ITS) and the use of\noffboard services. As a result, cloud-native techniques, such as microservices\nand container orchestration, play an increasingly important role in their\noperation. However, orchestrating applications in a large-scale C-ITS poses\nunique challenges due to the dynamic nature of the environment and the need for\nefficient resource utilization. In this paper, we present a demand-driven\napplication management approach that leverages cloud-native techniques -\nspecifically Kubernetes - to address these challenges. Taking into account the\ndemands originating from different entities within the C-ITS, the approach\nenables the automation of processes, such as deployment, reconfiguration,\nupdate, upgrade, and scaling of microservices. Executing these processes on\ndemand can, for example, reduce computing resource consumption and network\ntraffic. A demand may include a request for provisioning an external supporting\nservice, such as a collective environment model. The approach handles changing\nand new demands by dynamically reconciling them through our proposed\napplication management framework built on Kubernetes and the Robot Operating\nSystem (ROS 2). We demonstrate the operation of our framework in the C-ITS use\ncase of collective environment perception and make the source code of the\nprototypical framework publicly available at\nhttps://github.com/ika-rwth-aachen/application_manager .", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eKubernetes\u7684\u9700\u6c42\u9a71\u52a8\u5e94\u7528\u7ba1\u7406\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21\u534f\u540c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u4e2d\u52a8\u6001\u73af\u5883\u4e0b\u7684\u5e94\u7528\u7f16\u6392\u6311\u6218\u3002", "motivation": "\u968f\u7740\u8f66\u8f86\u81ea\u52a8\u5316\u548c\u4e92\u8054\u7a0b\u5ea6\u7684\u63d0\u9ad8\uff0c\u534f\u540c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u9700\u8981\u9ad8\u6548\u7684\u4e91\u539f\u751f\u6280\u672f\u6765\u7ba1\u7406\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5e94\u7528\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u8d44\u6e90\u5229\u7528\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eKubernetes\u548cROS 2\u7684\u5e94\u7528\u7ba1\u7406\u6846\u67b6\uff0c\u91c7\u7528\u9700\u6c42\u9a71\u52a8\u7684\u65b9\u6cd5\u6765\u81ea\u52a8\u5316\u5fae\u670d\u52a1\u7684\u90e8\u7f72\u3001\u91cd\u914d\u7f6e\u3001\u66f4\u65b0\u548c\u6269\u5c55\u7b49\u6d41\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636eC-ITS\u4e2d\u4e0d\u540c\u5b9e\u4f53\u7684\u9700\u6c42\u52a8\u6001\u534f\u8c03\u5e94\u7528\u7ba1\u7406\uff0c\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u548c\u7f51\u7edc\u6d41\u91cf\uff0c\u5e76\u5728\u96c6\u4f53\u73af\u5883\u611f\u77e5\u7528\u4f8b\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u63d0\u51fa\u7684\u9700\u6c42\u9a71\u52a8\u5e94\u7528\u7ba1\u7406\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21C-ITS\u4e2d\u7684\u52a8\u6001\u5e94\u7528\u7f16\u6392\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u4e91\u539f\u751f\u5e94\u7528\u7ba1\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2509.18830", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18830", "abs": "https://arxiv.org/abs/2509.18830", "authors": ["Suzannah Wistreich", "Baiyu Shi", "Stephen Tian", "Samuel Clarke", "Michael Nath", "Chengyi Xu", "Zhenan Bao", "Jiajun Wu"], "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation", "comment": "Accepted to CoRL 2025", "summary": "Human skin provides a rich tactile sensing stream, localizing intentional and\nunintentional contact events over a large and contoured region. Replicating\nthese tactile sensing capabilities for dexterous robotic manipulation systems\nremains a longstanding challenge. In this work, we take a step towards this\ngoal by introducing DexSkin. DexSkin is a soft, conformable capacitive\nelectronic skin that enables sensitive, localized, and calibratable tactile\nsensing, and can be tailored to varying geometries. We demonstrate its efficacy\nfor learning downstream robotic manipulation by sensorizing a pair of parallel\njaw gripper fingers, providing tactile coverage across almost the entire finger\nsurfaces. We empirically evaluate DexSkin's capabilities in learning\nchallenging manipulation tasks that require sensing coverage across the entire\nsurface of the fingers, such as reorienting objects in hand and wrapping\nelastic bands around boxes, in a learning-from-demonstration framework. We then\nshow that, critically for data-driven approaches, DexSkin can be calibrated to\nenable model transfer across sensor instances, and demonstrate its\napplicability to online reinforcement learning on real robots. Our results\nhighlight DexSkin's suitability and practicality for learning real-world,\ncontact-rich manipulation. Please see our project webpage for videos and\nvisualizations: https://dex-skin.github.io/.", "AI": {"tldr": "DexSkin\u662f\u4e00\u79cd\u67d4\u8f6f\u3001\u53ef\u9002\u5e94\u7684\u7535\u5bb9\u5f0f\u7535\u5b50\u76ae\u80a4\uff0c\u80fd\u591f\u5b9e\u73b0\u7075\u654f\u3001\u5c40\u90e8\u5316\u548c\u53ef\u6821\u51c6\u7684\u89e6\u89c9\u4f20\u611f\uff0c\u53ef\u7528\u4e8e\u673a\u5668\u4eba\u7075\u5de7\u64cd\u4f5c\u4efb\u52a1\u7684\u5b66\u4e60\u3002", "motivation": "\u4eba\u7c7b\u76ae\u80a4\u63d0\u4f9b\u4e30\u5bcc\u7684\u89e6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u80fd\u591f\u5728\u5927\u9762\u79ef\u66f2\u9762\u533a\u57df\u5b9a\u4f4d\u6709\u610f\u548c\u65e0\u610f\u7684\u63a5\u89e6\u4e8b\u4ef6\u3002\u4e3a\u7075\u5de7\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u590d\u5236\u8fd9\u4e9b\u89e6\u89c9\u611f\u77e5\u80fd\u529b\u4ecd\u7136\u662f\u4e00\u4e2a\u957f\u671f\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86DexSkin\u7535\u5b50\u76ae\u80a4\u7cfb\u7edf\uff0c\u5c06\u5176\u5b89\u88c5\u5728\u5e73\u884c\u5939\u722a\u624b\u6307\u4e0a\uff0c\u63d0\u4f9b\u51e0\u4e4e\u6574\u4e2a\u624b\u6307\u8868\u9762\u7684\u89e6\u89c9\u8986\u76d6\u3002\u5728\u793a\u8303\u5b66\u4e60\u6846\u67b6\u4e0b\u8bc4\u4f30\u5176\u5728\u6311\u6218\u6027\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u4f20\u611f\u5668\u5b9e\u4f8b\u95f4\u7684\u6a21\u578b\u8fc1\u79fb\u80fd\u529b\u3002", "result": "DexSkin\u5728\u9700\u8981\u6574\u4e2a\u624b\u6307\u8868\u9762\u4f20\u611f\u8986\u76d6\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5982\u7269\u4f53\u5728\u624b\u4e2d\u91cd\u65b0\u5b9a\u5411\u548c\u5c06\u5f39\u6027\u5e26\u7f20\u7ed5\u5728\u76d2\u5b50\u4e0a\u3002\u540c\u65f6\u8bc1\u660e\u5176\u9002\u7528\u4e8e\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u3002", "conclusion": "DexSkin\u5c55\u793a\u4e86\u5176\u5728\u5b66\u4e60\u771f\u5b9e\u4e16\u754c\u3001\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u89e6\u89c9\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18865", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.18865", "abs": "https://arxiv.org/abs/2509.18865", "authors": ["Masato Kobayashi", "Thanpimon Buamanee"], "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation", "comment": null, "summary": "We propose Bilateral Control-Based Imitation Learning via Vision-Language\nFusion for Action Generation (Bi-VLA), a novel framework that extends bilateral\ncontrol-based imitation learning to handle more than one task within a single\nmodel. Conventional bilateral control methods exploit joint angle, velocity,\ntorque, and vision for precise manipulation but require task-specific models,\nlimiting their generality. Bi-VLA overcomes this limitation by utilizing robot\njoint angle, velocity, and torque data from leader-follower bilateral control\nwith visual features and natural language instructions through SigLIP and\nFiLM-based fusion. We validated Bi-VLA on two task types: one requiring\nsupplementary language cues and another distinguishable solely by vision.\nReal-robot experiments showed that Bi-VLA successfully interprets\nvision-language combinations and improves task success rates compared to\nconventional bilateral control-based imitation learning. Our Bi-VLA addresses\nthe single-task limitation of prior bilateral approaches and provides empirical\nevidence that combining vision and language significantly enhances versatility.\nExperimental results validate the effectiveness of Bi-VLA in real-world tasks.\nFor additional material, please visit the website:\nhttps://mertcookimg.github.io/bi-vla/", "AI": {"tldr": "Bi-VLA\u662f\u4e00\u4e2a\u57fa\u4e8e\u53cc\u8fb9\u63a7\u5236\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00\u878d\u5408\u5b9e\u73b0\u591a\u4efb\u52a1\u52a8\u4f5c\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u53cc\u8fb9\u63a7\u5236\u65b9\u6cd5\u53ea\u80fd\u5904\u7406\u5355\u4e00\u4efb\u52a1\u7684\u5c40\u9650\u6027", "motivation": "\u4f20\u7edf\u53cc\u8fb9\u63a7\u5236\u65b9\u6cd5\u867d\u7136\u5229\u7528\u5173\u8282\u89d2\u5ea6\u3001\u901f\u5ea6\u3001\u626d\u77e9\u548c\u89c6\u89c9\u4fe1\u606f\u8fdb\u884c\u7cbe\u786e\u64cd\u4f5c\uff0c\u4f46\u9700\u8981\u9488\u5bf9\u6bcf\u4e2a\u4efb\u52a1\u8bad\u7ec3\u7279\u5b9a\u6a21\u578b\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027", "method": "Bi-VLA\u7ed3\u5408\u4e86\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u53cc\u8fb9\u63a7\u5236\u7684\u673a\u5668\u4eba\u5173\u8282\u6570\u636e\uff08\u89d2\u5ea6\u3001\u901f\u5ea6\u3001\u626d\u77e9\uff09\u4e0e\u89c6\u89c9\u7279\u5f81\u548c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\uff0c\u901a\u8fc7SigLIP\u548cFiLM-based\u878d\u5408\u6280\u672f\u5b9e\u73b0\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cBi-VLA\u6210\u529f\u89e3\u8bfb\u89c6\u89c9-\u8bed\u8a00\u7ec4\u5408\uff0c\u76f8\u6bd4\u4f20\u7edf\u53cc\u8fb9\u63a7\u5236\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u8bed\u8a00\u8865\u5145\u63d0\u793a\u548c\u4ec5\u51ed\u89c6\u89c9\u53ef\u533a\u5206\u7684\u4e24\u7c7b\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u8272", "conclusion": "Bi-VLA\u89e3\u51b3\u4e86\u5148\u524d\u53cc\u8fb9\u63a7\u5236\u65b9\u6cd5\u7684\u5355\u4efb\u52a1\u9650\u5236\uff0c\u5b9e\u8bc1\u8868\u660e\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u80fd\u663e\u8457\u589e\u5f3a\u7cfb\u7edf\u7684\u901a\u7528\u6027\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027"}}
{"id": "2509.18937", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.18937", "abs": "https://arxiv.org/abs/2509.18937", "authors": ["Yanyuan Qiao", "Kieran Gilday", "Yutong Xie", "Josie Hughes"], "title": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands", "comment": null, "summary": "Designing robotic hand morphologies for diverse manipulation tasks requires\nbalancing dexterity, manufacturability, and task-specific functionality. While\nopen-source frameworks and parametric tools support reproducible design, they\nstill rely on expert heuristics and manual tuning. Automated methods using\noptimization are often compute-intensive, simulation-dependent, and rarely\ntarget dexterous hands. Large language models (LLMs), with their broad\nknowledge of human-object interactions and strong generative capabilities,\noffer a promising alternative for zero-shot design reasoning. In this paper, we\npresent Lang2Morph, a language-driven pipeline for robotic hand design. It uses\nLLMs to translate natural-language task descriptions into symbolic structures\nand OPH-compatible parameters, enabling 3D-printable task-specific\nmorphologies. The pipeline consists of: (i) Morphology Design, which maps tasks\ninto semantic tags, structural grammars, and OPH-compatible parameters; and\n(ii) Selection and Refinement, which evaluates design candidates based on\nsemantic alignment and size compatibility, and optionally applies LLM-guided\nrefinement when needed. We evaluate Lang2Morph across varied tasks, and results\nshow that our approach can generate diverse, task-relevant morphologies. To our\nknowledge, this is the first attempt to develop an LLM-based framework for\ntask-conditioned robotic hand design.", "AI": {"tldr": "Lang2Morph\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u4eba\u624b\u5f62\u6001\u8bbe\u8ba1\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u8f6c\u5316\u4e3a\u53ef3D\u6253\u5370\u7684\u7279\u5b9a\u4efb\u52a1\u5f62\u6001\u8bbe\u8ba1\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u624b\u8bbe\u8ba1\u4f9d\u8d56\u4e13\u5bb6\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u624b\u52a8\u8c03\u6574\uff0c\u81ea\u52a8\u5316\u4f18\u5316\u65b9\u6cd5\u8ba1\u7b97\u5bc6\u96c6\u4e14\u4f9d\u8d56\u4eff\u771f\u3002\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u5e7f\u6cdb\u7684\u4eba\u7c7b-\u7269\u4f53\u4ea4\u4e92\u77e5\u8bc6\u548c\u5f3a\u5927\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u96f6\u6837\u672c\u8bbe\u8ba1\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "Lang2Morph\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u9636\u6bb5\uff1a(i) \u5f62\u6001\u8bbe\u8ba1\uff1a\u5c06\u4efb\u52a1\u6620\u5c04\u4e3a\u8bed\u4e49\u6807\u7b7e\u3001\u7ed3\u6784\u8bed\u6cd5\u548cOPH\u517c\u5bb9\u53c2\u6570\uff1b(ii) \u9009\u62e9\u4e0e\u7cbe\u70bc\uff1a\u57fa\u4e8e\u8bed\u4e49\u5bf9\u9f50\u548c\u5c3a\u5bf8\u517c\u5bb9\u6027\u8bc4\u4f30\u8bbe\u8ba1\u5019\u9009\uff0c\u5e76\u5728\u9700\u8981\u65f6\u5e94\u7528LLM\u5f15\u5bfc\u7684\u7cbe\u70bc\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u591a\u6837\u5316\u4e14\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u5f62\u6001\u8bbe\u8ba1\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u4efb\u52a1\u6761\u4ef6\u5316\u673a\u5668\u4eba\u624b\u8bbe\u8ba1\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u8bed\u8a00\u9a71\u52a8\u8bbe\u8ba1\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2509.18953", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.18953", "abs": "https://arxiv.org/abs/2509.18953", "authors": ["Hanqing Liu", "Jiahuan Long", "Junqi Wu", "Jiacheng Hou", "Huili Tang", "Tingsong Jiang", "Weien Zhou", "Wen Yao"], "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations", "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as promising solutions for\nrobotic manipulation, yet their robustness to real-world physical variations\nremains critically underexplored. To bridge this gap, we propose Eva-VLA, the\nfirst unified framework that systematically evaluates the robustness of VLA\nmodels by transforming discrete physical variations into continuous\noptimization problems. However, comprehensively assessing VLA robustness\npresents two key challenges: (1) how to systematically characterize diverse\nphysical variations encountered in real-world deployments while maintaining\nevaluation reproducibility, and (2) how to discover worst-case scenarios\nwithout prohibitive real-world data collection costs efficiently. To address\nthe first challenge, we decompose real-world variations into three critical\ndomains: object 3D transformations that affect spatial reasoning, illumination\nvariations that challenge visual perception, and adversarial patches that\ndisrupt scene understanding. For the second challenge, we introduce a\ncontinuous black-box optimization framework that transforms discrete physical\nvariations into parameter optimization, enabling systematic exploration of\nworst-case scenarios. Extensive experiments on state-of-the-art OpenVLA models\nacross multiple benchmarks reveal alarming vulnerabilities: all variation types\ntrigger failure rates exceeding 60%, with object transformations causing up to\n97.8% failure in long-horizon tasks. Our findings expose critical gaps between\ncontrolled laboratory success and unpredictable deployment readiness, while the\nEva-VLA framework provides a practical pathway for hardening VLA-based robotic\nmanipulation models against real-world deployment challenges.", "AI": {"tldr": "Eva-VLA\u662f\u9996\u4e2a\u7cfb\u7edf\u8bc4\u4f30VLA\u6a21\u578b\u9c81\u68d2\u6027\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u79bb\u6563\u7269\u7406\u53d8\u5316\u8f6c\u5316\u4e3a\u8fde\u7eed\u4f18\u5316\u95ee\u9898\uff0c\u63ed\u793a\u4e86VLA\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u4e25\u91cd\u8106\u5f31\u6027\u3002", "motivation": "VLA\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5bf9\u771f\u5b9e\u4e16\u754c\u7269\u7406\u53d8\u5316\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\u6765\u5f25\u5408\u5b9e\u9a8c\u5ba4\u6210\u529f\u4e0e\u90e8\u7f72\u51c6\u5907\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u5c06\u7269\u7406\u53d8\u5316\u5206\u89e3\u4e3a\u4e09\u4e2a\u5173\u952e\u9886\u57df\uff1a\u7269\u4f533D\u53d8\u6362\u3001\u5149\u7167\u53d8\u5316\u548c\u5bf9\u6297\u6027\u8865\u4e01\uff1b\u5f15\u5165\u8fde\u7eed\u9ed1\u76d2\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u79bb\u6563\u7269\u7406\u53d8\u5316\u8f6c\u5316\u4e3a\u53c2\u6570\u4f18\u5316\u95ee\u9898\uff0c\u7cfb\u7edf\u63a2\u7d22\u6700\u574f\u60c5\u51b5\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6240\u6709\u53d8\u5316\u7c7b\u578b\u90fd\u5bfc\u81f4\u8d85\u8fc760%\u7684\u5931\u8d25\u7387\uff0c\u7269\u4f53\u53d8\u6362\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u4e2d\u5bfc\u81f4\u9ad8\u8fbe97.8%\u7684\u5931\u8d25\uff0c\u66b4\u9732\u4e86VLA\u6a21\u578b\u7684\u4e25\u91cd\u8106\u5f31\u6027\u3002", "conclusion": "Eva-VLA\u6846\u67b6\u63ed\u793a\u4e86VLA\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u5dee\u8ddd\uff0c\u540c\u65f6\u4e3a\u5f3a\u5316VLA\u57fa\u673a\u5668\u4eba\u64cd\u4f5c\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\u3002"}}
{"id": "2509.18954", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18954", "abs": "https://arxiv.org/abs/2509.18954", "authors": ["Minoo Dolatabadi", "Fardin Ayar", "Ehsan Javanmardi", "Manabu Tsukada", "Mahdi Javanmardi"], "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation", "comment": null, "summary": "LiDAR-based localization and SLAM often rely on iterative matching\nalgorithms, particularly the Iterative Closest Point (ICP) algorithm, to align\nsensor data with pre-existing maps or previous scans. However, ICP is prone to\nerrors in featureless environments and dynamic scenes, leading to inaccurate\npose estimation. Accurately predicting the uncertainty associated with ICP is\ncrucial for robust state estimation but remains challenging, as existing\napproaches often rely on handcrafted models or simplified assumptions.\nMoreover, a few deep learning-based methods for localizability estimation\neither depend on a pre-built map, which may not always be available, or provide\na binary classification of localizable versus non-localizable, which fails to\nproperly model uncertainty. In this work, we propose a data-driven framework\nthat leverages deep learning to estimate the registration error covariance of\nICP before matching, even in the absence of a reference map. By associating\neach LiDAR scan with a reliable 6-DoF error covariance estimate, our method\nenables seamless integration of ICP within Kalman filtering, enhancing\nlocalization accuracy and robustness. Extensive experiments on the KITTI\ndataset demonstrate the effectiveness of our approach, showing that it\naccurately predicts covariance and, when applied to localization using a\npre-built map or SLAM, reduces localization errors and improves robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728ICP\u5339\u914d\u524d\u4f30\u8ba1\u6ce8\u518c\u8bef\u5dee\u534f\u65b9\u5dee\uff0c\u65e0\u9700\u53c2\u8003\u5730\u56fe\u5373\u53ef\u63d0\u4f9b\u53ef\u9760\u76846\u81ea\u7531\u5ea6\u8bef\u5dee\u534f\u65b9\u5dee\u4f30\u8ba1\uff0c\u4ece\u800c\u63d0\u5347LiDAR\u5b9a\u4f4d\u548cSLAM\u7684\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edfICP\u7b97\u6cd5\u5728\u7279\u5f81\u7a00\u5c11\u73af\u5883\u548c\u52a8\u6001\u573a\u666f\u4e2d\u5bb9\u6613\u4ea7\u751f\u8bef\u5dee\uff0c\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u6a21\u578b\u6216\u7b80\u5316\u5047\u8bbe\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u9884\u5efa\u5730\u56fe\uff0c\u8981\u4e48\u53ea\u80fd\u8fdb\u884c\u4e8c\u5143\u5206\u7c7b\uff0c\u65e0\u6cd5\u51c6\u786e\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u5728ICP\u5339\u914d\u524d\u9884\u6d4b\u6ce8\u518c\u8bef\u5dee\u534f\u65b9\u5dee\uff0c\u5c06\u6bcf\u4e2aLiDAR\u626b\u63cf\u4e0e\u53ef\u9760\u76846\u81ea\u7531\u5ea6\u8bef\u5dee\u534f\u65b9\u5dee\u4f30\u8ba1\u5173\u8054\uff0c\u5b9e\u73b0\u4e0e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "result": "\u5728KITTI\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u51c6\u786e\u9884\u6d4b\u534f\u65b9\u5dee\uff0c\u5e94\u7528\u4e8e\u57fa\u4e8e\u9884\u5efa\u5730\u56fe\u7684\u5b9a\u4f4d\u6216SLAM\u65f6\uff0c\u80fd\u51cf\u5c11\u5b9a\u4f4d\u8bef\u5dee\u5e76\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3ICP\u7b97\u6cd5\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u95ee\u9898\uff0c\u4e3aLiDAR\u5b9a\u4f4d\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u65b9\u6cd5\u3002"}}
{"id": "2509.18979", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.18979", "abs": "https://arxiv.org/abs/2509.18979", "authors": ["Lorenzo Shaikewitz", "Tim Nguyen", "Luca Carlone"], "title": "Category-Level Object Shape and Pose Estimation in Less Than a Millisecond", "comment": null, "summary": "Object shape and pose estimation is a foundational robotics problem,\nsupporting tasks from manipulation to scene understanding and navigation. We\npresent a fast local solver for shape and pose estimation which requires only\ncategory-level object priors and admits an efficient certificate of global\noptimality. Given an RGB-D image of an object, we use a learned front-end to\ndetect sparse, category-level semantic keypoints on the target object. We\nrepresent the target object's unknown shape using a linear active shape model\nand pose a maximum a posteriori optimization problem to solve for position,\norientation, and shape simultaneously. Expressed in unit quaternions, this\nproblem admits first-order optimality conditions in the form of an eigenvalue\nproblem with eigenvector nonlinearities. Our primary contribution is to solve\nthis problem efficiently with self-consistent field iteration, which only\nrequires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector\npair at each iterate. Solving a linear system for the corresponding Lagrange\nmultipliers gives a simple global optimality certificate. One iteration of our\nsolver runs in about 100 microseconds, enabling fast outlier rejection. We test\nour method on synthetic data and a variety of real-world settings, including\ntwo public datasets and a drone tracking scenario. Code is released at\nhttps://github.com/MIT-SPARK/Fast-ShapeAndPose.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u5c40\u90e8\u6c42\u89e3\u5668\uff0c\u7528\u4e8e\u7269\u4f53\u5f62\u72b6\u548c\u59ff\u6001\u4f30\u8ba1\uff0c\u53ea\u9700\u8981\u7c7b\u522b\u7ea7\u7269\u4f53\u5148\u9a8c\uff0c\u5e76\u80fd\u63d0\u4f9b\u9ad8\u6548\u7684\u5168\u5c40\u6700\u4f18\u6027\u8bc1\u660e\u3002", "motivation": "\u7269\u4f53\u5f62\u72b6\u548c\u59ff\u6001\u4f30\u8ba1\u662f\u673a\u5668\u4eba\u6280\u672f\u7684\u57fa\u7840\u95ee\u9898\uff0c\u652f\u6301\u4ece\u64cd\u4f5c\u5230\u573a\u666f\u7406\u89e3\u548c\u5bfc\u822a\u7b49\u4efb\u52a1\u3002\u9700\u8981\u4e00\u79cd\u5feb\u901f\u4e14\u80fd\u4fdd\u8bc1\u5168\u5c40\u6700\u4f18\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5b66\u4e60\u7684\u524d\u7aef\u68c0\u6d4b\u7a00\u758f\u7684\u7c7b\u522b\u7ea7\u8bed\u4e49\u5173\u952e\u70b9\uff0c\u7528\u7ebf\u6027\u4e3b\u52a8\u5f62\u72b6\u6a21\u578b\u8868\u793a\u672a\u77e5\u5f62\u72b6\uff0c\u901a\u8fc7\u6700\u5927\u540e\u9a8c\u4f18\u5316\u95ee\u9898\u540c\u65f6\u6c42\u89e3\u4f4d\u7f6e\u3001\u65b9\u5411\u548c\u5f62\u72b6\u3002\u4f7f\u7528\u81ea\u6d3d\u573a\u8fed\u4ee3\u6cd5\u9ad8\u6548\u6c42\u89e3\u7279\u5f81\u503c\u95ee\u9898\u3002", "result": "\u6c42\u89e3\u5668\u6bcf\u6b21\u8fed\u4ee3\u7ea6\u9700100\u5fae\u79d2\uff0c\u80fd\u5feb\u901f\u5254\u9664\u5f02\u5e38\u503c\u3002\u5728\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u573a\u666f\uff08\u5305\u62ec\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u548c\u65e0\u4eba\u673a\u8ddf\u8e2a\u573a\u666f\uff09\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u53ef\u8bc1\u660e\u5168\u5c40\u6700\u4f18\u7684\u7269\u4f53\u5f62\u72b6\u548c\u59ff\u6001\u4f30\u8ba1\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2509.19012", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19012", "abs": "https://arxiv.org/abs/2509.19012", "authors": ["Dapeng Zhang", "Jin Sun", "Chenghui Hu", "Xiaoyan Wu", "Zhenlong Yuan", "Rui Zhou", "Fei Shen", "Qingguo Zhou"], "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey", "comment": null, "summary": "The emergence of Vision Language Action (VLA) models marks a paradigm shift\nfrom traditional policy-based control to generalized robotics, reframing Vision\nLanguage Models (VLMs) from passive sequence generators into active agents for\nmanipulation and decision-making in complex, dynamic environments. This survey\ndelves into advanced VLA methods, aiming to provide a clear taxonomy and a\nsystematic, comprehensive review of existing research. It presents a\ncomprehensive analysis of VLA applications across different scenarios and\nclassifies VLA approaches into several paradigms: autoregression-based,\ndiffusion-based, reinforcement-based, hybrid, and specialized methods; while\nexamining their motivations, core strategies, and implementations in detail. In\naddition, foundational datasets, benchmarks, and simulation platforms are\nintroduced. Building on the current VLA landscape, the review further proposes\nperspectives on key challenges and future directions to advance research in VLA\nmodels and generalizable robotics. By synthesizing insights from over three\nhundred recent studies, this survey maps the contours of this rapidly evolving\nfield and highlights the opportunities and challenges that will shape the\ndevelopment of scalable, general-purpose VLA methods.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u5bf9\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u7efc\u8ff0\u6027\u7814\u7a76\uff0c\u5206\u6790\u4e86VLA\u4ece\u4f20\u7edf\u7b56\u7565\u63a7\u5236\u5411\u901a\u7528\u673a\u5668\u4eba\u6280\u672f\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u7cfb\u7edf\u5206\u7c7b\u4e86\u73b0\u6709VLA\u65b9\u6cd5\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "VLA\u6a21\u578b\u7684\u51fa\u73b0\u6807\u5fd7\u7740\u673a\u5668\u4eba\u63a7\u5236\u4ece\u4f20\u7edf\u7b56\u7565\u65b9\u6cd5\u5411\u901a\u7528\u673a\u5668\u4eba\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u8fd9\u4e00\u5feb\u901f\u53d1\u5c55\u7684\u9886\u57df\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u6e05\u6670\u7684\u5206\u7c7b\u6846\u67b6\u548c\u53d1\u5c55\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u5206\u6790300\u591a\u9879\u6700\u65b0\u7814\u7a76\uff0c\u5c06VLA\u65b9\u6cd5\u5206\u4e3a\u81ea\u56de\u5f52\u3001\u6269\u6563\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u6df7\u5408\u548c\u4e13\u95e8\u65b9\u6cd5\u7b49\u8303\u5f0f\uff0c\u5e76\u8be6\u7ec6\u8003\u5bdf\u5176\u52a8\u673a\u3001\u6838\u5fc3\u7b56\u7565\u548c\u5b9e\u73b0\u65b9\u5f0f\u3002", "result": "\u5efa\u7acb\u4e86VLA\u65b9\u6cd5\u7684\u7cfb\u7edf\u5206\u7c7b\u4f53\u7cfb\uff0c\u4ecb\u7ecd\u4e86\u57fa\u7840\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u4eff\u771f\u5e73\u53f0\uff0c\u4e3a\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u53c2\u8003\u6846\u67b6\u3002", "conclusion": "VLA\u6a21\u578b\u5728\u901a\u7528\u673a\u5668\u4eba\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u53ef\u6269\u5c55\u6027\u548c\u901a\u7528\u6027\u7b49\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u6765\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2509.19023", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19023", "abs": "https://arxiv.org/abs/2509.19023", "authors": ["Shuai Liu", "Meng Cheng Lau"], "title": "Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion", "comment": "11 pages, 5 figures, 1 table, Computational Science Graduate Project", "summary": "We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a\ntwo-stage reinforcement learning framework for humanoid walking that requires\nno motion capture data or elaborate reward shaping. In the first stage, a\ncompact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via\nProximal Policy Optimization. This generates energy-efficient gait templates.\nIn the second stage, those dynamically consistent trajectories guide a\nfull-body policy trained with Soft Actor--Critic augmented by an adversarial\ndiscriminator, ensuring the student's five-dimensional gait feature\ndistribution matches the ROM's demonstrations. Experiments at 1\nmeter-per-second and 4 meter-per-second show that ROM-GRL produces stable,\nsymmetric gaits with substantially lower tracking error than a pure-reward\nbaseline. By distilling lightweight ROM guidance into high-dimensional\npolicies, ROM-GRL bridges the gap between reward-only and imitation-based\nlocomotion methods, enabling versatile, naturalistic humanoid behaviors without\nany human demonstrations.", "AI": {"tldr": "ROM-GRL\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u884c\u8d70\uff0c\u65e0\u9700\u8fd0\u52a8\u6355\u6349\u6570\u636e\u6216\u590d\u6742\u5956\u52b1\u8bbe\u8ba1\u3002\u7b2c\u4e00\u9636\u6bb5\u8bad\u7ec34\u81ea\u7531\u5ea6\u7b80\u5316\u6a21\u578b\u751f\u6210\u6b65\u6001\u6a21\u677f\uff0c\u7b2c\u4e8c\u9636\u6bb5\u7528\u8fd9\u4e9b\u8f68\u8ff9\u6307\u5bfc\u5168\u8eab\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8fd0\u52a8\u6355\u6349\u6570\u636e\u6216\u590d\u6742\u5956\u52b1\u8bbe\u8ba1\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7b80\u5316\u6a21\u578b\u6307\u5bfc\u6765\u6865\u63a5\u7eaf\u5956\u52b1\u65b9\u6cd5\u548c\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u7b2c\u4e00\u9636\u6bb5\uff1a\u4f7f\u7528PPO\u8bad\u7ec34\u81ea\u7531\u5ea6\u7b80\u5316\u6a21\u578b\u751f\u6210\u80fd\u91cf\u9ad8\u6548\u6b65\u6001\u6a21\u677f\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff1a\u4f7f\u7528SAC\u548c\u5bf9\u6297\u5224\u522b\u5668\u8bad\u7ec3\u5168\u8eab\u7b56\u7565\uff0c\u786e\u4fdd\u6b65\u6001\u7279\u5f81\u5206\u5e03\u4e0e\u7b80\u5316\u6a21\u578b\u5339\u914d\u3002", "result": "\u57281\u7c73/\u79d2\u548c4\u7c73/\u79d2\u901f\u5ea6\u4e0b\uff0cROM-GRL\u4ea7\u751f\u7a33\u5b9a\u3001\u5bf9\u79f0\u7684\u6b65\u6001\uff0c\u8ddf\u8e2a\u8bef\u5dee\u663e\u8457\u4f4e\u4e8e\u7eaf\u5956\u52b1\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "ROM-GRL\u901a\u8fc7\u5c06\u8f7b\u91cf\u7ea7\u7b80\u5316\u6a21\u578b\u6307\u5bfc\u878d\u5165\u9ad8\u7ef4\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u4eba\u7c7b\u6f14\u793a\u7684\u901a\u7528\u3001\u81ea\u7136\u7684\u4eba\u5f62\u673a\u5668\u4eba\u884c\u4e3a\u3002"}}
{"id": "2509.19037", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19037", "abs": "https://arxiv.org/abs/2509.19037", "authors": ["Qingzheng Cong", "Steven Oh", "Wen Fan", "Shan Luo", "Kaspar Althoefer", "Dandan Zhang"], "title": "TacEva: A Performance Evaluation Framework For Vision-Based Tactile Sensors", "comment": "14 pages, 8 figures. Equal contribution: Qingzheng Cong, Steven Oh,\n  Wen Fan. Corresponding author: Dandan Zhang (d.zhang17@imperial.ac.uk).\n  Additional resources at http://stevenoh2003.github.io/TacEva/", "summary": "Vision-Based Tactile Sensors (VBTSs) are widely used in robotic tasks because\nof the high spatial resolution they offer and their relatively low\nmanufacturing costs. However, variations in their sensing mechanisms,\nstructural dimension, and other parameters lead to significant performance\ndisparities between existing VBTSs. This makes it challenging to optimize them\nfor specific tasks, as both the initial choice and subsequent fine-tuning are\nhindered by the lack of standardized metrics. To address this issue, TacEva is\nintroduced as a comprehensive evaluation framework for the quantitative\nanalysis of VBTS performance. The framework defines a set of performance\nmetrics that capture key characteristics in typical application scenarios. For\neach metric, a structured experimental pipeline is designed to ensure\nconsistent and repeatable quantification. The framework is applied to multiple\nVBTSs with distinct sensing mechanisms, and the results demonstrate its ability\nto provide a thorough evaluation of each design and quantitative indicators for\neach performance dimension. This enables researchers to pre-select the most\nappropriate VBTS on a task by task basis, while also offering\nperformance-guided insights into the optimization of VBTS design. A list of\nexisting VBTS evaluation methods and additional evaluations can be found on our\nwebsite: https://stevenoh2003.github.io/TacEva/", "AI": {"tldr": "TacEva\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668(VBTS)\u7684\u5168\u9762\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5b9a\u4e49\u6807\u51c6\u5316\u6027\u80fd\u6307\u6807\u548c\u5b9e\u9a8c\u6d41\u7a0b\u6765\u89e3\u51b3VBTS\u6027\u80fd\u8bc4\u4f30\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u5728\u4f20\u611f\u673a\u5236\u3001\u7ed3\u6784\u5c3a\u5bf8\u7b49\u53c2\u6570\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u5bfc\u81f4\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u4f46\u7531\u4e8e\u7f3a\u4e4f\u6807\u51c6\u5316\u6307\u6807\uff0c\u96be\u4ee5\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8fdb\u884c\u4f18\u5316\u9009\u62e9\u548c\u5fae\u8c03\u3002", "method": "\u5f00\u53d1\u4e86TacEva\u6846\u67b6\uff0c\u5b9a\u4e49\u4e86\u4e00\u5957\u6027\u80fd\u6307\u6807\u6765\u6355\u6349\u5178\u578b\u5e94\u7528\u573a\u666f\u4e2d\u7684\u5173\u952e\u7279\u6027\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u6307\u6807\u8bbe\u8ba1\u4e86\u7ed3\u6784\u5316\u7684\u5b9e\u9a8c\u6d41\u7a0b\u4ee5\u786e\u4fdd\u4e00\u81f4\u6027\u548c\u53ef\u91cd\u590d\u6027\u91cf\u5316\u3002", "result": "\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u591a\u79cd\u5177\u6709\u4e0d\u540c\u4f20\u611f\u673a\u5236\u7684VBTS\uff0c\u7ed3\u679c\u8868\u660e\u80fd\u591f\u5bf9\u6bcf\u79cd\u8bbe\u8ba1\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u6027\u80fd\u7ef4\u5ea6\u63d0\u4f9b\u5b9a\u91cf\u6307\u6807\u3002", "conclusion": "TacEva\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u6309\u4efb\u52a1\u9884\u9009\u6700\u5408\u9002\u7684VBTS\uff0c\u540c\u65f6\u4e3aVBTS\u8bbe\u8ba1\u7684\u4f18\u5316\u63d0\u4f9b\u6027\u80fd\u6307\u5bfc\u7684\u89c1\u89e3\u3002"}}
{"id": "2509.19047", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19047", "abs": "https://arxiv.org/abs/2509.19047", "authors": ["Geonhyup Lee", "Yeongjin Lee", "Kangmin Kim", "Seongju Lee", "Sangjun Noh", "Seunghyeok Back", "Kyoobin Lee"], "title": "ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation", "comment": "9 pages, 9 figures", "summary": "Contact-rich manipulation tasks such as precision assembly require precise\ncontrol of interaction forces, yet existing imitation learning methods rely\nmainly on vision-only demonstrations. We propose ManipForce, a handheld system\ndesigned to capture high-frequency force-torque (F/T) and RGB data during\nnatural human demonstrations for contact-rich manipulation. Building on these\ndemonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT).\nFMT encodes asynchronous RGB and F/T signals using frequency- and\nmodality-aware embeddings and fuses them via bi-directional cross-attention\nwithin a transformer diffusion policy. Through extensive experiments on six\nreal-world contact-rich manipulation tasks - such as gear assembly, box\nflipping, and battery insertion - FMT trained on ManipForce demonstrations\nachieves robust performance with an average success rate of 83% across all\ntasks, substantially outperforming RGB-only baselines. Ablation and\nsampling-frequency analyses further confirm that incorporating high-frequency\nF/T data and cross-modal integration improves policy performance, especially in\ntasks demanding high precision and stable contact.", "AI": {"tldr": "\u63d0\u51faManipForce\u624b\u6301\u7cfb\u7edf\u6355\u83b7\u9ad8\u9891\u529b\u626d\u77e9\u548cRGB\u6570\u636e\uff0c\u5e76\u5f00\u53d1\u9891\u7387\u611f\u77e5\u591a\u6a21\u6001Transformer\uff08FMT\uff09\u7528\u4e8e\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\uff0c\u76f8\u6bd4\u4ec5RGB\u57fa\u7ebf\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u9700\u8981\u7cbe\u786e\u63a7\u5236\u4ea4\u4e92\u529b\uff0c\u4f46\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u89c6\u89c9\u6f14\u793a\uff0c\u7f3a\u4e4f\u529b\u626d\u77e9\u6570\u636e", "method": "\u4f7f\u7528ManipForce\u7cfb\u7edf\u91c7\u96c6RGB\u548c\u529b\u626d\u77e9\u6570\u636e\uff0c\u5f00\u53d1FMT\u6a21\u578b\u901a\u8fc7\u9891\u7387\u548c\u6a21\u6001\u611f\u77e5\u5d4c\u5165\u7f16\u7801\u5f02\u6b65\u4fe1\u53f7\uff0c\u5728Transformer\u6269\u6563\u7b56\u7565\u4e2d\u901a\u8fc7\u53cc\u5411\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u591a\u6a21\u6001\u6570\u636e", "result": "\u57286\u4e2a\u771f\u5b9e\u4e16\u754c\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5e73\u5747\u6210\u529f\u738783%\uff0c\u663e\u8457\u4f18\u4e8e\u4ec5RGB\u57fa\u7ebf\uff0c\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u9ad8\u9891\u529b\u626d\u77e9\u6570\u636e\u548c\u8de8\u6a21\u6001\u96c6\u6210\u63d0\u5347\u7b56\u7565\u6027\u80fd", "conclusion": "\u9ad8\u9891\u529b\u626d\u77e9\u6570\u636e\u548c\u591a\u6a21\u6001\u878d\u5408\u5bf9\u4e8e\u9ad8\u7cbe\u5ea6\u548c\u7a33\u5b9a\u63a5\u89e6\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0cManipForce\u548cFMT\u4e3a\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.19076", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19076", "abs": "https://arxiv.org/abs/2509.19076", "authors": ["Laura Connolly", "Aravind S. Kumar", "Kapi Ketan Mehta", "Lidia Al-Zogbi", "Peter Kazanzides", "Parvin Mousavi", "Gabor Fichtinger", "Axel Krieger", "Junichi Tokuda", "Russell H. Taylor", "Simon Leonard", "Anton Deguet"], "title": "SlicerROS2: A Research and Development Module for Image-Guided Robotic Interventions", "comment": null, "summary": "Image-guided robotic interventions involve the use of medical imaging in\ntandem with robotics. SlicerROS2 is a software module that combines 3D Slicer\nand robot operating system (ROS) in pursuit of a standard integration approach\nfor medical robotics research. The first release of SlicerROS2 demonstrated the\nfeasibility of using the C++ API from 3D Slicer and ROS to load and visualize\nrobots in real time. Since this initial release, we've rewritten and redesigned\nthe module to offer greater modularity, access to low-level features, access to\n3D Slicer's Python API, and better data transfer protocols. In this paper, we\nintroduce this new design as well as four applications that leverage the core\nfunctionalities of SlicerROS2 in realistic image-guided robotics scenarios.", "AI": {"tldr": "SlicerROS2\u662f\u4e00\u4e2a\u7ed3\u54083D Slicer\u548cROS\u7684\u8f6f\u4ef6\u6a21\u5757\uff0c\u7528\u4e8e\u533b\u5b66\u673a\u5668\u4eba\u7814\u7a76\u7684\u6807\u51c6\u96c6\u6210\u65b9\u6cd5\u3002\u65b0\u7248\u8bbe\u8ba1\u63d0\u4f9b\u66f4\u597d\u7684\u6a21\u5757\u5316\u3001\u4f4e\u7ea7\u529f\u80fd\u8bbf\u95ee\u548cPython API\u652f\u6301\u3002", "motivation": "\u4e3a\u533b\u5b66\u56fe\u50cf\u5f15\u5bfc\u673a\u5668\u4eba\u5e72\u9884\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u7684\u8f6f\u4ef6\u96c6\u6210\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b33D Slicer\u548cROS\u7cfb\u7edf\u4e4b\u95f4\u7684\u96c6\u6210\u95ee\u9898\u3002", "method": "\u91cd\u65b0\u8bbe\u8ba1\u548c\u91cd\u5199SlicerROS2\u6a21\u5757\uff0c\u589e\u5f3a\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u5bf9\u4f4e\u7ea7\u529f\u80fd\u7684\u8bbf\u95ee\uff0c\u652f\u63013D Slicer\u7684Python API\uff0c\u5e76\u6539\u8fdb\u6570\u636e\u4f20\u8f93\u534f\u8bae\u3002", "result": "\u5f00\u53d1\u4e86\u56db\u4e2a\u5e94\u7528\u6848\u4f8b\uff0c\u5728\u771f\u5b9e\u7684\u56fe\u50cf\u5f15\u5bfc\u673a\u5668\u4eba\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86SlicerROS2\u6838\u5fc3\u529f\u80fd\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65b0\u7248SlicerROS2\u4e3a\u533b\u5b66\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u548c\u7075\u6d3b\u7684\u96c6\u6210\u5e73\u53f0\uff0c\u652f\u6301\u66f4\u590d\u6742\u7684\u56fe\u50cf\u5f15\u5bfc\u673a\u5668\u4eba\u5e94\u7528\u5f00\u53d1\u3002"}}
{"id": "2509.19080", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.19080", "abs": "https://arxiv.org/abs/2509.19080", "authors": ["Zhennan Jiang", "Kai Liu", "Yuxin Qin", "Shuai Tian", "Yupeng Zheng", "Mingcai Zhou", "Chao Yu", "Haoran Li", "Dongbin Zhao"], "title": "World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation", "comment": null, "summary": "Robotic manipulation policies are commonly initialized through imitation\nlearning, but their performance is limited by the scarcity and narrow coverage\nof expert data. Reinforcement learning can refine polices to alleviate this\nlimitation, yet real-robot training is costly and unsafe, while training in\nsimulators suffers from the sim-to-real gap. Recent advances in generative\nmodels have demonstrated remarkable capabilities in real-world simulation, with\ndiffusion models in particular excelling at generation. This raises the\nquestion of how diffusion model-based world models can be combined to enhance\npre-trained policies in robotic manipulation. In this work, we propose\nWorld4RL, a framework that employs diffusion-based world models as\nhigh-fidelity simulators to refine pre-trained policies entirely in imagined\nenvironments for robotic manipulation. Unlike prior works that primarily employ\nworld models for planning, our framework enables direct end-to-end policy\noptimization. World4RL is designed around two principles: pre-training a\ndiffusion world model that captures diverse dynamics on multi-task datasets and\nrefining policies entirely within a frozen world model to avoid online\nreal-world interactions. We further design a two-hot action encoding scheme\ntailored for robotic manipulation and adopt diffusion backbones to improve\nmodeling fidelity. Extensive simulation and real-world experiments demonstrate\nthat World4RL provides high-fidelity environment modeling and enables\nconsistent policy refinement, yielding significantly higher success rates\ncompared to imitation learning and other baselines. More visualization results\nare available at https://world4rl.github.io/.", "AI": {"tldr": "World4RL\u662f\u4e00\u4e2a\u5229\u7528\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u60f3\u8c61\u73af\u5883\u4e2d\u4f18\u5316\u9884\u8bad\u7ec3\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\uff0c\u907f\u514d\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u7684\u6210\u672c\u548c\u98ce\u9669\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u901a\u5e38\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u521d\u59cb\u5316\uff0c\u4f46\u53d7\u9650\u4e8e\u4e13\u5bb6\u6570\u636e\u7684\u7a00\u7f3a\u6027\u548c\u8986\u76d6\u8303\u56f4\u3002\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u4f18\u5316\u7b56\u7565\uff0c\u4f46\u771f\u5b9e\u673a\u5668\u4eba\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u4e0d\u5b89\u5168\uff0c\u800c\u6a21\u62df\u5668\u8bad\u7ec3\u5b58\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51faWorld4RL\u6846\u67b6\uff0c\u9884\u8bad\u7ec3\u6269\u6563\u4e16\u754c\u6a21\u578b\u6355\u6349\u591a\u4efb\u52a1\u6570\u636e\u96c6\u4e2d\u7684\u591a\u6837\u5316\u52a8\u6001\uff0c\u5728\u51bb\u7ed3\u7684\u4e16\u754c\u6a21\u578b\u4e2d\u5b8c\u5168\u4f18\u5316\u7b56\u7565\u3002\u8bbe\u8ba1\u4e13\u95e8\u9488\u5bf9\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u53cc\u70ed\u52a8\u4f5c\u7f16\u7801\u65b9\u6848\uff0c\u91c7\u7528\u6269\u6563\u9aa8\u5e72\u7f51\u7edc\u63d0\u9ad8\u5efa\u6a21\u4fdd\u771f\u5ea6\u3002", "result": "\u5e7f\u6cdb\u7684\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0cWorld4RL\u63d0\u4f9b\u9ad8\u4fdd\u771f\u73af\u5883\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e00\u81f4\u7684\u7b56\u7565\u4f18\u5316\uff0c\u76f8\u6bd4\u6a21\u4eff\u5b66\u4e60\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u6210\u529f\u7387\u3002", "conclusion": "World4RL\u5c55\u793a\u4e86\u6269\u6563\u6a21\u578b\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5b89\u5168\u9ad8\u6548\u7684\u7b56\u7565\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2509.19102", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.19102", "abs": "https://arxiv.org/abs/2509.19102", "authors": ["Hongli Xu", "Lei Zhang", "Xiaoyue Hu", "Boyang Zhong", "Kaixin Bai", "Zolt\u00e1n-Csaba M\u00e1rton", "Zhenshan Bing", "Zhaopeng Chen", "Alois Christian Knoll", "Jianwei Zhang"], "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation", "comment": "project website: https://sites.google.com/view/funcanon, 11 pages", "summary": "General-purpose robotic skills from end-to-end demonstrations often leads to\ntask-specific policies that fail to generalize beyond the training\ndistribution. Therefore, we introduce FunCanon, a framework that converts\nlong-horizon manipulation tasks into sequences of action chunks, each defined\nby an actor, verb, and object. These chunks focus policy learning on the\nactions themselves, rather than isolated tasks, enabling compositionality and\nreuse. To make policies pose-aware and category-general, we perform functional\nobject canonicalization for functional alignment and automatic manipulation\ntrajectory transfer, mapping objects into shared functional frames using\naffordance cues from large vision language models. An object centric and action\ncentric diffusion policy FuncDiffuser trained on this aligned data naturally\nrespects object affordances and poses, simplifying learning and improving\ngeneralization ability. Experiments on simulated and real-world benchmarks\ndemonstrate category-level generalization, cross-task behavior reuse, and\nrobust sim2real deployment, showing that functional canonicalization provides a\nstrong inductive bias for scalable imitation learning in complex manipulation\ndomains. Details of the demo and supplemental material are available on our\nproject website https://sites.google.com/view/funcanon.", "AI": {"tldr": "FunCanon\u6846\u67b6\u5c06\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u5206\u89e3\u4e3a\u52a8\u4f5c\u5757\u5e8f\u5217\uff0c\u901a\u8fc7\u529f\u80fd\u5bf9\u8c61\u89c4\u8303\u5316\u5b9e\u73b0\u59ff\u6001\u611f\u77e5\u548c\u7c7b\u522b\u6cdb\u5316\uff0c\u4f7f\u7528FuncDiffuser\u6269\u6563\u7b56\u7565\u5728\u529f\u80fd\u5bf9\u9f50\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u7aef\u5230\u7aef\u6f14\u793a\u5b66\u4e60\u5bfc\u81f4\u7684\u4efb\u52a1\u7279\u5b9a\u7b56\u7565\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u529f\u80fd\u5206\u89e3\u548c\u89c4\u8303\u5316\u5b9e\u73b0\u7b56\u7565\u7684\u7ec4\u5408\u6027\u548c\u91cd\u7528\u6027\u3002", "method": "\u5c06\u957f\u65f6\u7a0b\u64cd\u4f5c\u4efb\u52a1\u5206\u89e3\u4e3a\u52a8\u4f5c\u5757\uff08\u6267\u884c\u8005\u3001\u52a8\u8bcd\u3001\u5bf9\u8c61\uff09\uff0c\u8fdb\u884c\u529f\u80fd\u5bf9\u8c61\u89c4\u8303\u5316\u5b9e\u73b0\u529f\u80fd\u5bf9\u9f50\u548c\u81ea\u52a8\u64cd\u4f5c\u8f68\u8ff9\u8fc1\u79fb\uff0c\u4f7f\u7528\u5bf9\u8c61\u4e2d\u5fc3\u548c\u52a8\u4f5c\u4e2d\u5fc3\u7684FuncDiffuser\u6269\u6563\u7b56\u7565\u8bad\u7ec3\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u7c7b\u522b\u7ea7\u6cdb\u5316\u3001\u8de8\u4efb\u52a1\u884c\u4e3a\u91cd\u7528\u548c\u7a33\u5065\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u90e8\u7f72\u80fd\u529b\u3002", "conclusion": "\u529f\u80fd\u89c4\u8303\u5316\u4e3a\u590d\u6742\u64cd\u4f5c\u9886\u57df\u7684\u53ef\u6269\u5c55\u6a21\u4eff\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5f52\u7eb3\u504f\u7f6e\u3002"}}
{"id": "2509.19105", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19105", "abs": "https://arxiv.org/abs/2509.19105", "authors": ["Sarvesh Prajapati", "Ananya Trivedi", "Nathaniel Hanson", "Bruce Maxwell", "Taskin Padir"], "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation", "comment": "8 pages, 10 figures, submitted to Robotic Computing & Communication", "summary": "Successful navigation in outdoor environments requires accurate prediction of\nthe physical interactions between the robot and the terrain. To this end,\nseveral methods rely on geometric or semantic labels to classify traversable\nsurfaces. However, such labels cannot distinguish visually similar surfaces\nthat differ in material properties. Spectral sensors enable inference of\nmaterial composition from surface reflectance measured across multiple\nwavelength bands. Although spectral sensing is gaining traction in robotics,\nwidespread deployment remains constrained by the need for custom hardware\nintegration, high sensor costs, and compute-intensive processing pipelines. In\nthis paper, we present RGB Image to Spectral Signature Neural Network (RS-Net),\na deep neural network designed to bridge the gap between the accessibility of\nRGB sensing and the rich material information provided by spectral data. RS-Net\npredicts spectral signatures from RGB patches, which we map to terrain labels\nand friction coefficients. The resulting terrain classifications are integrated\ninto a sampling-based motion planner for a wheeled robot operating in outdoor\nenvironments. Likewise, the friction estimates are incorporated into a\ncontact-force-based MPC for a quadruped robot navigating slippery surfaces.\nThus, we introduce a framework that learns the task-relevant physical property\nonce during training and thereafter relies solely on RGB sensing at test time.\nThe code is available at https://github.com/prajapatisarvesh/RS-Net.", "AI": {"tldr": "RS-Net\u662f\u4e00\u4e2a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u80fd\u591f\u4eceRGB\u56fe\u50cf\u9884\u6d4b\u5149\u8c31\u7279\u5f81\uff0c\u4ece\u800c\u83b7\u53d6\u5730\u5f62\u6750\u6599\u548c\u6469\u64e6\u7cfb\u6570\u4fe1\u606f\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u6237\u5916\u5bfc\u822a\u89c4\u5212\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u51e0\u4f55\u6216\u8bed\u4e49\u6807\u7b7e\u65e0\u6cd5\u533a\u5206\u89c6\u89c9\u76f8\u4f3c\u4f46\u6750\u6599\u5c5e\u6027\u4e0d\u540c\u7684\u8868\u9762\uff0c\u800c\u5149\u8c31\u4f20\u611f\u5668\u867d\u7136\u80fd\u63d0\u4f9b\u6750\u6599\u4fe1\u606f\u4f46\u5b58\u5728\u786c\u4ef6\u96c6\u6210\u590d\u6742\u3001\u6210\u672c\u9ad8\u3001\u8ba1\u7b97\u5bc6\u96c6\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faRS-Net\u795e\u7ecf\u7f51\u7edc\uff0c\u4eceRGB\u56fe\u50cf\u5757\u9884\u6d4b\u5149\u8c31\u7279\u5f81\uff0c\u7136\u540e\u5c06\u5149\u8c31\u7279\u5f81\u6620\u5c04\u5230\u5730\u5f62\u6807\u7b7e\u548c\u6469\u64e6\u7cfb\u6570\uff0c\u96c6\u6210\u5230\u8f6e\u5f0f\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u89c4\u5212\u548c\u56db\u8db3\u673a\u5668\u4eba\u7684MPC\u63a7\u5236\u4e2d\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u65f6\u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u7684\u7269\u7406\u5c5e\u6027\uff0c\u6d4b\u8bd5\u65f6\u4ec5\u9700RGB\u4f20\u611f\u5373\u53ef\u5b9e\u73b0\u51c6\u786e\u7684\u7269\u7406\u4ea4\u4e92\u9884\u6d4b\u3002", "conclusion": "RS-Net\u6210\u529f\u5f25\u5408\u4e86RGB\u4f20\u611f\u7684\u4fbf\u5229\u6027\u4e0e\u5149\u8c31\u6570\u636e\u4e30\u5bcc\u6750\u6599\u4fe1\u606f\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u673a\u5668\u4eba\u6237\u5916\u5bfc\u822a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19142", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19142", "abs": "https://arxiv.org/abs/2509.19142", "authors": ["Kangmin Kim", "Seunghyeok Back", "Geonhyup Lee", "Sangbeom Lee", "Sangjun Noh", "Kyoobin Lee"], "title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer", "comment": "8 pages, 5 figures", "summary": "Bimanual grasping is essential for robots to handle large and complex\nobjects. However, existing methods either focus solely on single-arm grasping\nor employ separate grasp generation and bimanual evaluation stages, leading to\ncoordination problems including collision risks and unbalanced force\ndistribution. To address these limitations, we propose BiGraspFormer, a unified\nend-to-end transformer framework that directly generates coordinated bimanual\ngrasps from object point clouds. Our key idea is the Single-Guided Bimanual\n(SGB) strategy, which first generates diverse single grasp candidates using a\ntransformer decoder, then leverages their learned features through specialized\nattention mechanisms to jointly predict bimanual poses and quality scores. This\nconditioning strategy reduces the complexity of the 12-DoF search space while\nensuring coordinated bimanual manipulation. Comprehensive simulation\nexperiments and real-world validation demonstrate that BiGraspFormer\nconsistently outperforms existing methods while maintaining efficient inference\nspeed (<0.05s), confirming the effectiveness of our framework. Code and\nsupplementary materials are available at https://sites.google.com/bigraspformer", "AI": {"tldr": "BiGraspFormer\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7aef\u5230\u7aeftransformer\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u7269\u4f53\u70b9\u4e91\u751f\u6210\u534f\u8c03\u7684\u53cc\u81c2\u6293\u53d6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u534f\u8c03\u95ee\u9898\u5982\u78b0\u649e\u98ce\u9669\u548c\u529b\u5206\u5e03\u4e0d\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u53cc\u81c2\u6293\u53d6\u65b9\u6cd5\u8981\u4e48\u53ea\u5173\u6ce8\u5355\u81c2\u6293\u53d6\uff0c\u8981\u4e48\u91c7\u7528\u5206\u79bb\u7684\u6293\u53d6\u751f\u6210\u548c\u53cc\u81c2\u8bc4\u4f30\u9636\u6bb5\uff0c\u5bfc\u81f4\u534f\u8c03\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u751f\u6210\u534f\u8c03\u53cc\u81c2\u6293\u53d6\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faSingle-Guided Bimanual\u7b56\u7565\uff0c\u5148\u7528transformer\u89e3\u7801\u5668\u751f\u6210\u591a\u6837\u7684\u5355\u6293\u53d6\u5019\u9009\uff0c\u7136\u540e\u901a\u8fc7\u4e13\u95e8\u7684\u6ce8\u610f\u529b\u673a\u5236\u5229\u7528\u5b66\u4e60\u5230\u7684\u7279\u5f81\u8054\u5408\u9884\u6d4b\u53cc\u81c2\u59ff\u6001\u548c\u8d28\u91cf\u5206\u6570\u3002", "result": "\u5728\u4eff\u771f\u5b9e\u9a8c\u548c\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\u4e2d\uff0cBiGraspFormer\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u901f\u5ea6\uff08<0.05\u79d2\uff09\u3002", "conclusion": "BiGraspFormer\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u53cc\u81c2\u6293\u53d6\u7684\u534f\u8c03\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.19168", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19168", "abs": "https://arxiv.org/abs/2509.19168", "authors": ["Mark Gonzales", "Ethan Oh", "Joseph Moore"], "title": "A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination", "comment": "8 Pages, 7 Figures", "summary": "In this paper, we present a receding-horizon, sampling-based planner capable\nof reasoning over multimodal policy distributions. By using the cross-entropy\nmethod to optimize a multimodal policy under a common cost function, our\napproach increases robustness against local minima and promotes effective\nexploration of the solution space. We show that our approach naturally extends\nto multi-robot collision-free planning, enables agents to share diverse\ncandidate policies to avoid deadlocks, and allows teams to minimize a global\nobjective without incurring the computational complexity of centralized\noptimization. Numerical simulations demonstrate that employing multiple modes\nsignificantly improves success rates in trap environments and in multi-robot\ncollision avoidance. Hardware experiments further validate the approach's\nreal-time feasibility and practical performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91c7\u6837\u7684\u91cd\u89c4\u5212\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u591a\u6a21\u6001\u7b56\u7565\u5206\u5e03\uff0c\u901a\u8fc7\u4ea4\u53c9\u71b5\u65b9\u6cd5\u4f18\u5316\u591a\u6a21\u6001\u7b56\u7565\uff0c\u63d0\u9ad8\u4e86\u5bf9\u5c40\u90e8\u6700\u5c0f\u503c\u7684\u9c81\u68d2\u6027\u548c\u89e3\u7a7a\u95f4\u7684\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u673a\u5668\u4eba\u89c4\u5212\u4e2d\u7684\u5c40\u90e8\u6700\u5c0f\u503c\u95ee\u9898\u548c\u6b7b\u9501\u60c5\u51b5\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8003\u8651\u591a\u79cd\u53ef\u80fd\u7b56\u7565\u7684\u89c4\u5212\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "method": "\u4f7f\u7528\u91cd\u89c4\u5212\u89c6\u91ce\u548c\u57fa\u4e8e\u91c7\u6837\u7684\u89c4\u5212\u5668\uff0c\u7ed3\u5408\u4ea4\u53c9\u71b5\u65b9\u6cd5\u4f18\u5316\u591a\u6a21\u6001\u7b56\u7565\u5206\u5e03\uff0c\u5728\u5171\u540c\u6210\u672c\u51fd\u6570\u4e0b\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\u591a\u6a21\u6001\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u9677\u9631\u73af\u5883\u548c\u591a\u673a\u5668\u4eba\u907f\u78b0\u4e2d\u7684\u6210\u529f\u7387\uff0c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5b9e\u65f6\u53ef\u884c\u6027\u548c\u5b9e\u9645\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u673a\u5668\u4eba\u534f\u4f5c\u89c4\u5212\u95ee\u9898\uff0c\u907f\u514d\u96c6\u4e2d\u5f0f\u4f18\u5316\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u63d0\u9ad8\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u63a2\u7d22\u80fd\u529b\u3002"}}
{"id": "2509.19169", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19169", "abs": "https://arxiv.org/abs/2509.19169", "authors": ["Tianyu Wu", "Xudong Han", "Haoran Sun", "Zishang Zhang", "Bangchao Huang", "Chaoyang Song", "Fang Wan"], "title": "MagiClaw: A Dual-Use, Vision-Based Soft Gripper for Bridging the Human Demonstration to Robotic Deployment Gap", "comment": "8 pages, 4 figures, accepted to Data@CoRL2025 Workshop", "summary": "The transfer of manipulation skills from human demonstration to robotic\nexecution is often hindered by a \"domain gap\" in sensing and morphology. This\npaper introduces MagiClaw, a versatile two-finger end-effector designed to\nbridge this gap. MagiClaw functions interchangeably as both a handheld tool for\nintuitive data collection and a robotic end-effector for policy deployment,\nensuring hardware consistency and reliability. Each finger incorporates a Soft\nPolyhedral Network (SPN) with an embedded camera, enabling vision-based\nestimation of 6-DoF forces and contact deformation. This proprioceptive data is\nfused with exteroceptive environmental sensing from an integrated iPhone, which\nprovides 6D pose, RGB video, and LiDAR-based depth maps. Through a custom iOS\napplication, MagiClaw streams synchronized, multi-modal data for real-time\nteleoperation, offline policy learning, and immersive control via mixed-reality\ninterfaces. We demonstrate how this unified system architecture lowers the\nbarrier to collecting high-fidelity, contact-rich datasets and accelerates the\ndevelopment of generalizable manipulation policies. Please refer to the iOS app\nat https://apps.apple.com/cn/app/magiclaw/id6661033548 for further details.", "AI": {"tldr": "MagiClaw\u662f\u4e00\u4e2a\u591a\u529f\u80fd\u53cc\u6307\u672b\u7aef\u6267\u884c\u5668\uff0c\u901a\u8fc7\u8f6f\u591a\u9762\u4f53\u7f51\u7edc\u548c\u5d4c\u5165\u5f0f\u6444\u50cf\u5934\u5b9e\u73b0\u89c6\u89c9\u529b\u4f30\u8ba1\uff0c\u7ed3\u5408iPhone\u63d0\u4f9b\u591a\u6a21\u6001\u6570\u636e\uff0c\u89e3\u51b3\u4eba\u673a\u64cd\u4f5c\u6280\u80fd\u8f6c\u79fb\u4e2d\u7684\u9886\u57df\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4eba\u7c7b\u6f14\u793a\u5230\u673a\u5668\u4eba\u6267\u884c\u4e4b\u95f4\u7684\u4f20\u611f\u548c\u5f62\u6001\u5b66\"\u9886\u57df\u5dee\u8ddd\"\u95ee\u9898\uff0c\u964d\u4f4e\u6536\u96c6\u9ad8\u4fdd\u771f\u63a5\u89e6\u4e30\u5bcc\u6570\u636e\u96c6\u7684\u96be\u5ea6\u3002", "method": "\u8bbe\u8ba1MagiClaw\u53cc\u6307\u672b\u7aef\u6267\u884c\u5668\uff0c\u96c6\u6210\u4e86\u8f6f\u591a\u9762\u4f53\u7f51\u7edc\u548c\u5d4c\u5165\u5f0f\u6444\u50cf\u5934\u8fdb\u884c6\u81ea\u7531\u5ea6\u529b\u4f30\u8ba1\uff0c\u7ed3\u5408iPhone\u63d0\u4f9b6D\u59ff\u6001\u3001RGB\u89c6\u9891\u548cLiDAR\u6df1\u5ea6\u56fe\u7b49\u591a\u6a21\u6001\u6570\u636e\u3002", "result": "\u5f00\u53d1\u4e86\u7edf\u4e00\u7684\u7cfb\u7edf\u67b6\u6784\uff0c\u80fd\u591f\u5b9e\u73b0\u5b9e\u65f6\u9065\u64cd\u4f5c\u3001\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u548c\u6df7\u5408\u73b0\u5b9e\u754c\u9762\u6c89\u6d78\u5f0f\u63a7\u5236\u3002", "conclusion": "MagiClaw\u7cfb\u7edf\u964d\u4f4e\u4e86\u6536\u96c6\u9ad8\u8d28\u91cf\u63a5\u89e6\u6570\u636e\u96c6\u7684\u95e8\u69db\uff0c\u52a0\u901f\u4e86\u901a\u7528\u5316\u64cd\u4f5c\u7b56\u7565\u7684\u5f00\u53d1\u3002"}}
{"id": "2509.19246", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.19246", "abs": "https://arxiv.org/abs/2509.19246", "authors": ["Sinan O\u011fuz", "Emanuele Garone", "Marco Dorigo", "Mary Katherine Heinrich"], "title": "Proactive-reactive detection and mitigation of intermittent faults in robot swarms", "comment": null, "summary": "Intermittent faults are transient errors that sporadically appear and\ndisappear. Although intermittent faults pose substantial challenges to\nreliability and coordination, existing studies of fault tolerance in robot\nswarms focus instead on permanent faults. One reason for this is that\nintermittent faults are prohibitively difficult to detect in the fully\nself-organized ad-hoc networks typical of robot swarms, as their network\ntopologies are transient and often unpredictable. However, in the recently\nintroduced self-organizing nervous systems (SoNS) approach, robot swarms are\nable to self-organize persistent network structures for the first time, easing\nthe problem of detecting intermittent faults. To address intermittent faults in\nrobot swarms that have persistent networks, we propose a novel\nproactive-reactive strategy to detection and mitigation, based on\nself-organized backup layers and distributed consensus in a multiplex network.\nProactively, the robots self-organize dynamic backup paths before faults occur,\nadapting to changes in the primary network topology and the robots' relative\npositions. Reactively, robots use one-shot likelihood ratio tests to compare\ninformation received along different paths in the multiplex network, enabling\nearly fault detection. Upon detection, communication is temporarily rerouted in\na self-organized way, until the detected fault resolves. We validate the\napproach in representative scenarios of faulty positional data occurring during\nformation control, demonstrating that intermittent faults are prevented from\ndisrupting convergence to desired formations, with high fault detection\naccuracy and low rates of false positives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u673a\u5668\u4eba\u7fa4\u4f53\u4e2d\u95f4\u6b47\u6027\u6545\u969c\u7684\u4e3b\u52a8-\u53cd\u5e94\u5f0f\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7b56\u7565\uff0c\u5229\u7528\u81ea\u7ec4\u7ec7\u5907\u4efd\u5c42\u548c\u591a\u8def\u7f51\u7edc\u4e2d\u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u6765\u89e3\u51b3\u95f4\u6b47\u6027\u6545\u969c\u95ee\u9898\u3002", "motivation": "\u95f4\u6b47\u6027\u6545\u969c\u662f\u5468\u671f\u6027\u51fa\u73b0\u548c\u6d88\u5931\u7684\u77ac\u65f6\u9519\u8bef\uff0c\u5bf9\u53ef\u9760\u6027\u548c\u534f\u8c03\u6027\u6784\u6210\u91cd\u5927\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6c38\u4e45\u6027\u6545\u969c\uff0c\u800c\u95f4\u6b47\u6027\u6545\u969c\u5728\u673a\u5668\u4eba\u7fa4\u4f53\u81ea\u7ec4\u7ec7\u7f51\u7edc\u4e2d\u96be\u4ee5\u68c0\u6d4b\uff0c\u56e0\u4e3a\u7f51\u7edc\u62d3\u6251\u662f\u77ac\u6001\u4e14\u4e0d\u53ef\u9884\u6d4b\u7684\u3002", "method": "\u91c7\u7528\u4e3b\u52a8-\u53cd\u5e94\u5f0f\u7b56\u7565\uff1a\u4e3b\u52a8\u65b9\u9762\uff0c\u673a\u5668\u4eba\u5728\u6545\u969c\u53d1\u751f\u524d\u81ea\u7ec4\u7ec7\u52a8\u6001\u5907\u4efd\u8def\u5f84\uff1b\u53cd\u5e94\u65b9\u9762\uff0c\u4f7f\u7528\u4e00\u6b21\u6027\u4f3c\u7136\u6bd4\u68c0\u9a8c\u6bd4\u8f83\u591a\u8def\u7f51\u7edc\u4e2d\u4e0d\u540c\u8def\u5f84\u7684\u4fe1\u606f\uff0c\u5b9e\u73b0\u65e9\u671f\u6545\u969c\u68c0\u6d4b\u3002\u68c0\u6d4b\u5230\u6545\u969c\u540e\uff0c\u901a\u4fe1\u4ee5\u81ea\u7ec4\u7ec7\u65b9\u5f0f\u4e34\u65f6\u91cd\u8def\u7531\u3002", "result": "\u5728\u5f62\u6210\u63a7\u5236\u4e2d\u6545\u969c\u4f4d\u7f6e\u6570\u636e\u7684\u4ee3\u8868\u6027\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u8bc1\u660e\u95f4\u6b47\u6027\u6545\u969c\u4e0d\u4f1a\u5e72\u6270\u671f\u671b\u5f62\u6210\u7684\u6536\u655b\uff0c\u5177\u6709\u9ad8\u6545\u969c\u68c0\u6d4b\u7cbe\u5ea6\u548c\u4f4e\u8bef\u62a5\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u7fa4\u4f53\u4e2d\u95f4\u6b47\u6027\u6545\u969c\u7684\u68c0\u6d4b\u4e0e\u7f13\u89e3\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u7ec4\u7ec7\u5907\u4efd\u5c42\u548c\u5206\u5e03\u5f0f\u5171\u8bc6\u5b9e\u73b0\u4e86\u9ad8\u53ef\u9760\u6027\u7684\u6545\u969c\u5904\u7406\u3002"}}
{"id": "2509.19261", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.19261", "abs": "https://arxiv.org/abs/2509.19261", "authors": ["Kuanqi Cai", "Chunfeng Wang", "Zeqi Li", "Haowen Yao", "Weinan Chen", "Luis Figueredo", "Aude Billard", "Arash Ajoudani"], "title": "Imitation-Guided Bimanual Planning for Stable Manipulation under Changing External Forces", "comment": null, "summary": "Robotic manipulation in dynamic environments often requires seamless\ntransitions between different grasp types to maintain stability and efficiency.\nHowever, achieving smooth and adaptive grasp transitions remains a challenge,\nparticularly when dealing with external forces and complex motion constraints.\nExisting grasp transition strategies often fail to account for varying external\nforces and do not optimize motion performance effectively. In this work, we\npropose an Imitation-Guided Bimanual Planning Framework that integrates\nefficient grasp transition strategies and motion performance optimization to\nenhance stability and dexterity in robotic manipulation. Our approach\nintroduces Strategies for Sampling Stable Intersections in Grasp Manifolds for\nseamless transitions between uni-manual and bi-manual grasps, reducing\ncomputational costs and regrasping inefficiencies. Additionally, a Hierarchical\nDual-Stage Motion Architecture combines an Imitation Learning-based Global Path\nGenerator with a Quadratic Programming-driven Local Planner to ensure real-time\nmotion feasibility, obstacle avoidance, and superior manipulability. The\nproposed method is evaluated through a series of force-intensive tasks,\ndemonstrating significant improvements in grasp transition efficiency and\nmotion performance. A video demonstrating our simulation results can be viewed\nat\n\\href{https://youtu.be/3DhbUsv4eDo}{\\textcolor{blue}{https://youtu.be/3DhbUsv4eDo}}.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u4eff\u5f15\u5bfc\u7684\u53cc\u624b\u673a\u5668\u4eba\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u7a33\u5b9a\u7684\u6293\u53d6\u6d41\u5f62\u91c7\u6837\u7b56\u7565\u548c\u5206\u5c42\u8fd0\u52a8\u67b6\u6784\uff0c\u5b9e\u73b0\u65e0\u7f1d\u6293\u53d6\u8f6c\u6362\u548c\u4f18\u5316\u7684\u8fd0\u52a8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u64cd\u4f5c\u65f6\u4e0d\u540c\u6293\u53d6\u7c7b\u578b\u4e4b\u95f4\u5e73\u6ed1\u8f6c\u6362\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u5916\u90e8\u529b\u548c\u590d\u6742\u8fd0\u52a8\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u6709\u6548\u5904\u7406\u53d8\u5316\u7684\u5916\u90e8\u529b\u5e76\u4f18\u5316\u8fd0\u52a8\u6027\u80fd\u3002", "method": "\u91c7\u7528\u6a21\u4eff\u5f15\u5bfc\u7684\u53cc\u624b\u673a\u5668\u4eba\u89c4\u5212\u6846\u67b6\uff0c\u5305\u62ec\u7a33\u5b9a\u7684\u6293\u53d6\u6d41\u5f62\u91c7\u6837\u7b56\u7565\u5b9e\u73b0\u5355\u53cc\u624b\u6293\u53d6\u65e0\u7f1d\u8f6c\u6362\uff0c\u4ee5\u53ca\u5206\u5c42\u53cc\u9636\u6bb5\u8fd0\u52a8\u67b6\u6784\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u7684\u5168\u5c40\u8def\u5f84\u751f\u6210\u5668\u548c\u4e8c\u6b21\u89c4\u5212\u9a71\u52a8\u7684\u5c40\u90e8\u89c4\u5212\u5668\u3002", "result": "\u5728\u529b\u5bc6\u96c6\u578b\u4efb\u52a1\u8bc4\u4f30\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6293\u53d6\u8f6c\u6362\u6548\u7387\u548c\u8fd0\u52a8\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u589e\u5f3a\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7a33\u5b9a\u6027\u548c\u7075\u5de7\u6027\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u590d\u6742\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.19292", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19292", "abs": "https://arxiv.org/abs/2509.19292", "authors": ["Yang Jin", "Jun Lv", "Han Xue", "Wendi Chen", "Chuan Wen", "Cewu Lu"], "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration", "comment": null, "summary": "Intelligent agents progress by continually refining their capabilities\nthrough actively exploring environments. Yet robot policies often lack\nsufficient exploration capability due to action mode collapse. Existing methods\nthat encourage exploration typically rely on random perturbations, which are\nunsafe and induce unstable, erratic behaviors, thereby limiting their\neffectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a\nframework that enhances policy exploration and improvement in robotic\nmanipulation. SOE learns a compact latent representation of task-relevant\nfactors and constrains exploration to the manifold of valid actions, ensuring\nsafety, diversity, and effectiveness. It can be seamlessly integrated with\narbitrary policy models as a plug-in module, augmenting exploration without\ndegrading the base policy performance. Moreover, the structured latent space\nenables human-guided exploration, further improving efficiency and\ncontrollability. Extensive experiments in both simulation and real-world tasks\ndemonstrate that SOE consistently outperforms prior methods, achieving higher\ntask success rates, smoother and safer exploration, and superior sample\nefficiency. These results establish on-manifold exploration as a principled\napproach to sample-efficient policy self-improvement. Project website:\nhttps://ericjin2002.github.io/SOE", "AI": {"tldr": "SOE\u662f\u4e00\u4e2a\u7528\u4e8e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u7b56\u7565\u81ea\u6539\u8fdb\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6709\u6548\u52a8\u4f5c\u6d41\u5f62\u4e0a\u8fdb\u884c\u7ea6\u675f\u6027\u63a2\u7d22\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u968f\u673a\u6270\u52a8\u65b9\u6cd5\u7684\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u7b56\u7565\u63a2\u7d22\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u968f\u673a\u6270\u52a8\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u4e0d\u5b89\u5168\u548c\u4e0d\u7a33\u5b9a\u7684\u884c\u4e3a\uff0c\u9650\u5236\u4e86\u63a2\u7d22\u6548\u679c\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u5b89\u5168\u6027\u53c8\u80fd\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u7684\u65b9\u6cd5\u3002", "method": "SOE\u5b66\u4e60\u4efb\u52a1\u76f8\u5173\u56e0\u7d20\u7684\u7d27\u51d1\u6f5c\u5728\u8868\u793a\uff0c\u5c06\u63a2\u7d22\u7ea6\u675f\u5728\u6709\u6548\u52a8\u4f5c\u6d41\u5f62\u4e0a\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u63d2\u4ef6\u6a21\u5757\u4e0e\u4efb\u610f\u7b56\u7565\u6a21\u578b\u96c6\u6210\uff0c\u540c\u65f6\u652f\u6301\u4eba\u5de5\u5f15\u5bfc\u63a2\u7d22\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSOE\u5728\u4efb\u52a1\u6210\u529f\u7387\u3001\u63a2\u7d22\u5e73\u6ed1\u6027\u548c\u5b89\u5168\u6027\u3001\u6837\u672c\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u5728\u6d41\u5f62\u4e0a\u7684\u63a2\u7d22\u4e3a\u6837\u672c\u9ad8\u6548\u7b56\u7565\u81ea\u6539\u8fdb\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u65b9\u6cd5\uff0cSOE\u6846\u67b6\u5177\u6709\u5b89\u5168\u3001\u591a\u6837\u4e14\u6709\u6548\u7684\u7279\u70b9\u3002"}}
{"id": "2509.19301", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.19301", "abs": "https://arxiv.org/abs/2509.19301", "authors": ["Lars Ankile", "Zhenyu Jiang", "Rocky Duan", "Guanya Shi", "Pieter Abbeel", "Anusha Nagabandi"], "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies", "comment": null, "summary": "Recent advances in behavior cloning (BC) have enabled impressive visuomotor\ncontrol policies. However, these approaches are limited by the quality of human\ndemonstrations, the manual effort required for data collection, and the\ndiminishing returns from increasing offline data. In comparison, reinforcement\nlearning (RL) trains an agent through autonomous interaction with the\nenvironment and has shown remarkable success in various domains. Still,\ntraining RL policies directly on real-world robots remains challenging due to\nsample inefficiency, safety concerns, and the difficulty of learning from\nsparse rewards for long-horizon tasks, especially for high-degree-of-freedom\n(DoF) systems. We present a recipe that combines the benefits of BC and RL\nthrough a residual learning framework. Our approach leverages BC policies as\nblack-box bases and learns lightweight per-step residual corrections via\nsample-efficient off-policy RL. We demonstrate that our method requires only\nsparse binary reward signals and can effectively improve manipulation policies\non high-degree-of-freedom (DoF) systems in both simulation and the real world.\nIn particular, we demonstrate, to the best of our knowledge, the first\nsuccessful real-world RL training on a humanoid robot with dexterous hands. Our\nresults demonstrate state-of-the-art performance in various vision-based tasks,\npointing towards a practical pathway for deploying RL in the real world.\nProject website: https://residual-offpolicy-rl.github.io", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u884c\u4e3a\u514b\u9686\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6b8b\u5dee\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528BC\u7b56\u7565\u4f5c\u4e3a\u57fa\u7840\uff0c\u901a\u8fc7\u6837\u672c\u9ad8\u6548\u7684\u79bb\u7b56\u7565RL\u5b66\u4e60\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u4fee\u6b63\uff0c\u5b9e\u73b0\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u6210\u529fRL\u8bad\u7ec3\u3002", "motivation": "\u884c\u4e3a\u514b\u9686\u53d7\u9650\u4e8e\u4eba\u7c7b\u6f14\u793a\u8d28\u91cf\u548c\u6570\u636e\u6536\u96c6\u6210\u672c\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8bad\u7ec3\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u3001\u5b89\u5168\u95ee\u9898\u548c\u7a00\u758f\u5956\u52b1\u7b49\u6311\u6218\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u6765\u5f00\u53d1\u5b9e\u7528\u7684\u771f\u5b9e\u4e16\u754cRL\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u6b8b\u5dee\u5b66\u4e60\u6846\u67b6\uff0c\u5c06BC\u7b56\u7565\u4f5c\u4e3a\u9ed1\u76d2\u57fa\u7840\u7b56\u7565\uff0c\u901a\u8fc7\u6837\u672c\u9ad8\u6548\u7684\u79bb\u7b56\u7565RL\u5b66\u4e60\u6bcf\u6b65\u7684\u8f7b\u91cf\u7ea7\u6b8b\u5dee\u4fee\u6b63\uff0c\u4ec5\u9700\u7a00\u758f\u4e8c\u5143\u5956\u52b1\u4fe1\u53f7\u3002", "result": "\u5728\u9ad8\u81ea\u7531\u5ea6\u7cfb\u7edf\u4e0a\u6709\u6548\u6539\u8fdb\u4e86\u64cd\u4f5c\u7b56\u7565\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u90fd\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u7279\u522b\u662f\u5728\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u9996\u4e2a\u6210\u529f\u7684\u771f\u5b9e\u4e16\u754cRL\u8bad\u7ec3\uff0c\u5728\u5404\u79cd\u89c6\u89c9\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5728\u771f\u5b9e\u4e16\u754c\u90e8\u7f72RL\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\uff0c\u7ed3\u5408\u4e86BC\u548cRL\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u771f\u5b9e\u673a\u5668\u4ebaRL\u8bad\u7ec3\u7684\u5173\u952e\u6311\u6218\u3002"}}
