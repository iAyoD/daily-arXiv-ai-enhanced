{"id": "2510.26837", "categories": ["cs.RO", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2510.26837", "abs": "https://arxiv.org/abs/2510.26837", "authors": ["Conor K. Trygstad", "Nestor O. Perez-Arancibia"], "title": "Force Characterization of Insect-Scale Aquatic Propulsion Based on Fluid-Structure Interaction", "comment": "To be presented at ICAR 2025 in San Juan, Argentina", "summary": "We present force characterizations of two newly developed insect-scale\npropulsors--one single-tailed and one double-tailed--for microrobotic swimmers\nthat leverage fluid-structure interaction (FSI) to generate thrust. The designs\nof these two devices were inspired by anguilliform swimming and are driven by\nsoft tails excited by high-work-density (HWD) actuators powered by shape-memory\nalloy (SMA) wires. While these propulsors have been demonstrated to be suitable\nfor microrobotic aquatic locomotion and controllable with simple architectures\nfor trajectory tracking in the two-dimensional (2D) space, the characteristics\nand magnitudes of the associated forces have not been studied systematically.\nIn the research presented here, we adopted a theoretical framework based on the\nnotion of reactive forces and obtained experimental data for characterization\nusing a custom-built micro-N-resolution force sensor. We measured maximum and\ncycle-averaged force values with multi-test means of respectively 0.45 mN and\n2.97 micro-N, for the tested single-tail propulsor. For the dual-tail\npropulsor, we measured maximum and cycle-averaged force values with multi-test\nmeans of 0.61 mN and 22.6 micro-N, respectively. These results represent the\nfirst measurements of the instantaneous thrust generated by insect-scale\npropulsors of this type and provide insights into FSI for efficient\nmicrorobotic propulsion.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u4e24\u79cd\u65b0\u578b\u6606\u866b\u5c3a\u5ea6\u63a8\u8fdb\u5668\u8fdb\u884c\u4e86\u529b\u7279\u6027\u8868\u5f81\uff0c\u5305\u62ec\u5355\u5c3e\u548c\u53cc\u5c3e\u8bbe\u8ba1\uff0c\u6d4b\u91cf\u4e86\u5176\u6700\u5927\u63a8\u529b\u548c\u5468\u671f\u5e73\u5747\u63a8\u529b\u503c\u3002", "motivation": "\u867d\u7136\u8fd9\u4e9b\u63a8\u8fdb\u5668\u5df2\u88ab\u8bc1\u660e\u9002\u7528\u4e8e\u5fae\u578b\u673a\u5668\u4eba\u6c34\u4e0b\u8fd0\u52a8\uff0c\u4f46\u5176\u76f8\u5173\u529b\u7684\u7279\u6027\u548c\u5927\u5c0f\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u7814\u7a76\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u53cd\u4f5c\u7528\u529b\u6982\u5ff5\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u4f7f\u7528\u5b9a\u5236\u5fae\u725b\u7ea7\u529b\u4f20\u611f\u5668\u83b7\u53d6\u5b9e\u9a8c\u6570\u636e\u8fdb\u884c\u8868\u5f81\u3002", "result": "\u5355\u5c3e\u63a8\u8fdb\u5668\u7684\u6700\u5927\u63a8\u529b\u548c\u5468\u671f\u5e73\u5747\u63a8\u529b\u5206\u522b\u4e3a0.45 mN\u548c2.97 \u03bcN\uff1b\u53cc\u5c3e\u63a8\u8fdb\u5668\u5206\u522b\u4e3a0.61 mN\u548c22.6 \u03bcN\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4ee3\u8868\u4e86\u6b64\u7c7b\u6606\u866b\u5c3a\u5ea6\u63a8\u8fdb\u5668\u77ac\u65f6\u63a8\u529b\u7684\u9996\u6b21\u6d4b\u91cf\uff0c\u4e3a\u9ad8\u6548\u5fae\u578b\u673a\u5668\u4eba\u63a8\u8fdb\u7684\u6d41\u4f53-\u7ed3\u6784\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2510.26855", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26855", "abs": "https://arxiv.org/abs/2510.26855", "authors": ["Reihaneh Mirjalili"], "title": "Leveraging Foundation Models for Enhancing Robot Perception and Action", "comment": "Doctoral thesis", "summary": "This thesis investigates how foundation models can be systematically\nleveraged to enhance robotic capabilities, enabling more effective\nlocalization, interaction, and manipulation in unstructured environments. The\nwork is structured around four core lines of inquiry, each addressing a\nfundamental challenge in robotics while collectively contributing to a cohesive\nframework for semantics-aware robotic intelligence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u7cfb\u7edf\u6027\u5730\u5229\u7528\u57fa\u7840\u6a21\u578b\u589e\u5f3a\u673a\u5668\u4eba\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u5b9a\u4f4d\u3001\u4ea4\u4e92\u548c\u64cd\u4f5c", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u57fa\u672c\u6311\u6218\uff0c\u6784\u5efa\u8bed\u4e49\u611f\u77e5\u7684\u673a\u5668\u4eba\u667a\u80fd\u7edf\u4e00\u6846\u67b6", "method": "\u56f4\u7ed5\u56db\u4e2a\u6838\u5fc3\u7814\u7a76\u8def\u7ebf\u5c55\u5f00\uff0c\u6bcf\u4e2a\u8def\u7ebf\u9488\u5bf9\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u95ee\u9898", "result": "\u63d0\u51fa\u4e86\u589e\u5f3a\u673a\u5668\u4eba\u5b9a\u4f4d\u3001\u4ea4\u4e92\u548c\u64cd\u4f5c\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u65b9\u6cd5", "conclusion": "\u57fa\u7840\u6a21\u578b\u80fd\u591f\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u8bed\u4e49\u611f\u77e5\u548c\u667a\u80fd\u8868\u73b0"}}
{"id": "2510.26900", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2510.26900", "abs": "https://arxiv.org/abs/2510.26900", "authors": ["Jahir Argote-Gerald", "Genki Miyauchi", "Julian Rau", "Paul Trodden", "Roderich Gross"], "title": "Design for One, Deploy for Many: Navigating Tree Mazes with Multiple Agents", "comment": "7 pages, 7 figures, to be published in MRS 2025", "summary": "Maze-like environments, such as cave and pipe networks, pose unique\nchallenges for multiple robots to coordinate, including communication\nconstraints and congestion. To address these challenges, we propose a\ndistributed multi-agent maze traversal algorithm for environments that can be\nrepresented by acyclic graphs. It uses a leader-switching mechanism where one\nagent, assuming a head role, employs any single-agent maze solver while the\nother agents each choose an agent to follow. The head role gets transferred to\nneighboring agents where necessary, ensuring it follows the same path as a\nsingle agent would. The multi-agent maze traversal algorithm is evaluated in\nsimulations with groups of up to 300 agents, various maze sizes, and multiple\nsingle-agent maze solvers. It is compared against strategies that are na\\\"ive,\nor assume either global communication or full knowledge of the environment. The\nalgorithm outperforms the na\\\"ive strategy in terms of makespan and\nsum-of-fuel. It is superior to the global-communication strategy in terms of\nmakespan but is inferior to it in terms of sum-of-fuel. The findings suggest it\nis asymptotically equivalent to the full-knowledge strategy with respect to\neither metric. Moreover, real-world experiments with up to 20 Pi-puck robots\nconfirm the feasibility of the approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u591a\u667a\u80fd\u4f53\u8ff7\u5bab\u904d\u5386\u7b97\u6cd5\uff0c\u4f7f\u7528\u9886\u5bfc\u8005\u5207\u6362\u673a\u5236\uff0c\u5728\u65e0\u73af\u56fe\u8868\u793a\u7684\u8ff7\u5bab\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u534f\u8c03\u3002", "motivation": "\u89e3\u51b3\u6d1e\u7a74\u548c\u7ba1\u9053\u7f51\u7edc\u7b49\u8ff7\u5bab\u73af\u5883\u4e2d\u591a\u673a\u5668\u4eba\u534f\u8c03\u7684\u6311\u6218\uff0c\u5305\u62ec\u901a\u4fe1\u9650\u5236\u548c\u62e5\u5835\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9886\u5bfc\u8005\u5207\u6362\u673a\u5236\uff0c\u4e00\u4e2a\u667a\u80fd\u4f53\u62c5\u4efb\u5934\u89d2\u8272\u4f7f\u7528\u5355\u667a\u80fd\u4f53\u8ff7\u5bab\u6c42\u89e3\u5668\uff0c\u5176\u4ed6\u667a\u80fd\u4f53\u8ddf\u968f\u9009\u62e9\u7684\u667a\u80fd\u4f53\uff0c\u5728\u5fc5\u8981\u65f6\u5c06\u5934\u89d2\u8272\u8f6c\u79fb\u7ed9\u76f8\u90bb\u667a\u80fd\u4f53\u3002", "result": "\u5728\u6700\u591a300\u4e2a\u667a\u80fd\u4f53\u7684\u6a21\u62df\u4e2d\uff0c\u7b97\u6cd5\u5728makespan\u65b9\u9762\u4f18\u4e8ena\u00efve\u548c\u5168\u5c40\u901a\u4fe1\u7b56\u7565\uff0c\u5728sum-of-fuel\u65b9\u9762\u4f18\u4e8ena\u00efve\u4f46\u52a3\u4e8e\u5168\u5c40\u901a\u4fe1\u7b56\u7565\uff0c\u4e0e\u5168\u77e5\u8bc6\u7b56\u7565\u6e10\u8fd1\u7b49\u4ef7\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u8ff7\u5bab\u73af\u5883\u4e2d\u6709\u6548\u534f\u8c03\u591a\u667a\u80fd\u4f53\uff0c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u3002"}}
{"id": "2510.26909", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.26909", "abs": "https://arxiv.org/abs/2510.26909", "authors": ["Tim Windecker", "Manthan Patel", "Moritz Reuss", "Richard Schwarzkopf", "Cesar Cadena", "Rudolf Lioutikov", "Marco Hutter", "Jonas Frey"], "title": "NaviTrace: Evaluating Embodied Navigation of Vision-Language Models", "comment": "9 pages, 6 figures, under review at IEEE conference", "summary": "Vision-language models demonstrate unprecedented performance and\ngeneralization across a wide range of tasks and scenarios. Integrating these\nfoundation models into robotic navigation systems opens pathways toward\nbuilding general-purpose robots. Yet, evaluating these models' navigation\ncapabilities remains constrained by costly real-world trials, overly simplified\nsimulations, and limited benchmarks. We introduce NaviTrace, a high-quality\nVisual Question Answering benchmark where a model receives an instruction and\nembodiment type (human, legged robot, wheeled robot, bicycle) and must output a\n2D navigation trace in image space. Across 1000 scenarios and more than 3000\nexpert traces, we systematically evaluate eight state-of-the-art VLMs using a\nnewly introduced semantic-aware trace score. This metric combines Dynamic Time\nWarping distance, goal endpoint error, and embodiment-conditioned penalties\nderived from per-pixel semantics and correlates with human preferences. Our\nevaluation reveals consistent gap to human performance caused by poor spatial\ngrounding and goal localization. NaviTrace establishes a scalable and\nreproducible benchmark for real-world robotic navigation. The benchmark and\nleaderboard can be found at\nhttps://leggedrobotics.github.io/navitrace_webpage/.", "AI": {"tldr": "\u63d0\u51fa\u4e86NaviTrace\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u7684\u80fd\u529b\uff0c\u5305\u542b1000\u4e2a\u573a\u666f\u548c3000\u591a\u4e2a\u4e13\u5bb6\u8f68\u8ff9\uff0c\u901a\u8fc7\u8bed\u4e49\u611f\u77e5\u8f68\u8ff9\u8bc4\u5206\u7cfb\u7edf\u8bc4\u4f30\u4e868\u4e2a\u6700\u5148\u8fdb\u7684VLM\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bfc\u822a\u80fd\u529b\u7684\u65b9\u6cd5\u53d7\u5230\u771f\u5b9e\u4e16\u754c\u8bd5\u9a8c\u6210\u672c\u9ad8\u3001\u6a21\u62df\u8fc7\u4e8e\u7b80\u5316\u3001\u57fa\u51c6\u6d4b\u8bd5\u6709\u9650\u7684\u7ea6\u675f\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f15\u5165NaviTrace\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6a21\u578b\u63a5\u6536\u6307\u4ee4\u548c\u4f53\u73b0\u7c7b\u578b\uff08\u4eba\u7c7b\u3001\u817f\u5f0f\u673a\u5668\u4eba\u3001\u8f6e\u5f0f\u673a\u5668\u4eba\u3001\u81ea\u884c\u8f66\uff09\uff0c\u8f93\u51fa\u56fe\u50cf\u7a7a\u95f4\u4e2d\u76842D\u5bfc\u822a\u8f68\u8ff9\uff0c\u4f7f\u7528\u7ed3\u5408\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u8ddd\u79bb\u3001\u76ee\u6807\u7aef\u70b9\u8bef\u5dee\u548c\u57fa\u4e8e\u6bcf\u50cf\u7d20\u8bed\u4e49\u7684\u4f53\u73b0\u6761\u4ef6\u60e9\u7f5a\u7684\u8bed\u4e49\u611f\u77e5\u8f68\u8ff9\u8bc4\u5206\u3002", "result": "\u8bc4\u4f30\u63ed\u793a\u4e86\u4e0e\u4eba\u7c7b\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u4e00\u81f4\u5dee\u8ddd\uff0c\u4e3b\u8981\u7531\u4e8e\u7a7a\u95f4\u57fa\u7840\u548c\u76ee\u6807\u5b9a\u4f4d\u80fd\u529b\u4e0d\u8db3\uff0c\u8be5\u8bc4\u5206\u4e0e\u4eba\u7c7b\u504f\u597d\u76f8\u5173\u3002", "conclusion": "NaviTrace\u4e3a\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5bfc\u822a\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2510.26915", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.26915", "abs": "https://arxiv.org/abs/2510.26915", "authors": ["Zachary Ravichandran", "Fernando Cladera", "Ankit Prabhu", "Jason Hughes", "Varun Murali", "Camillo Taylor", "George J. Pappas", "Vijay Kumar"], "title": "Heterogeneous Robot Collaboration in Unstructured Environments with Grounded Generative Intelligence", "comment": null, "summary": "Heterogeneous robot teams operating in realistic settings often must\naccomplish complex missions requiring collaboration and adaptation to\ninformation acquired online. Because robot teams frequently operate in\nunstructured environments -- uncertain, open-world settings without prior maps\n-- subtasks must be grounded in robot capabilities and the physical world.\nWhile heterogeneous teams have typically been designed for fixed\nspecifications, generative intelligence opens the possibility of teams that can\naccomplish a wide range of missions described in natural language. However,\ncurrent large language model (LLM)-enabled teaming methods typically assume\nwell-structured and known environments, limiting deployment in unstructured\nenvironments. We present SPINE-HT, a framework that addresses these limitations\nby grounding the reasoning abilities of LLMs in the context of a heterogeneous\nrobot team through a three-stage process. Given language specifications\ndescribing mission goals and team capabilities, an LLM generates grounded\nsubtasks which are validated for feasibility. Subtasks are then assigned to\nrobots based on capabilities such as traversability or perception and refined\ngiven feedback collected during online operation. In simulation experiments\nwith closed-loop perception and control, our framework achieves nearly twice\nthe success rate compared to prior LLM-enabled heterogeneous teaming\napproaches. In real-world experiments with a Clearpath Jackal, a Clearpath\nHusky, a Boston Dynamics Spot, and a high-altitude UAV, our method achieves an\n87\\% success rate in missions requiring reasoning about robot capabilities and\nrefining subtasks with online feedback. More information is provided at\nhttps://zacravichandran.github.io/SPINE-HT.", "AI": {"tldr": "SPINE-HT\u662f\u4e00\u4e2a\u7528\u4e8e\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8fc7\u7a0b\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4e0e\u673a\u5668\u4eba\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u89c4\u5212\u3002", "motivation": "\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u4efb\u52a1\u65f6\uff0c\u9700\u8981\u9002\u5e94\u5728\u7ebf\u83b7\u53d6\u7684\u4fe1\u606f\u5e76\u8fdb\u884c\u534f\u4f5c\u3002\u73b0\u6709\u57fa\u4e8eLLM\u7684\u56e2\u961f\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u73af\u5883\u7ed3\u6784\u826f\u597d\u4e14\u5df2\u77e5\uff0c\u9650\u5236\u4e86\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u8fc7\u7a0b\uff1a1) LLM\u6839\u636e\u4efb\u52a1\u76ee\u6807\u548c\u56e2\u961f\u80fd\u529b\u751f\u6210\u63a5\u5730\u7684\u5b50\u4efb\u52a1\u5e76\u8fdb\u884c\u53ef\u884c\u6027\u9a8c\u8bc1\uff1b2) \u57fa\u4e8e\u673a\u5668\u4eba\u80fd\u529b\uff08\u5982\u79fb\u52a8\u6027\u6216\u611f\u77e5\u80fd\u529b\uff09\u5206\u914d\u5b50\u4efb\u52a1\uff1b3) \u6839\u636e\u5728\u7ebf\u64cd\u4f5c\u671f\u95f4\u6536\u96c6\u7684\u53cd\u9988\u7cbe\u70bc\u5b50\u4efb\u52a1\u3002", "result": "\u5728\u5177\u6709\u95ed\u73af\u611f\u77e5\u548c\u63a7\u5236\u7684\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u6846\u67b6\u7684\u6210\u529f\u7387\u6bd4\u5148\u524d\u57fa\u4e8eLLM\u7684\u5f02\u6784\u56e2\u961f\u65b9\u6cd5\u63d0\u9ad8\u4e86\u8fd1\u4e00\u500d\u3002\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u4f7f\u7528\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u5b9e\u73b0\u4e8687%\u7684\u6210\u529f\u7387\u3002", "conclusion": "SPINE-HT\u6846\u67b6\u901a\u8fc7\u5c06LLM\u7684\u63a8\u7406\u80fd\u529b\u4e0e\u673a\u5668\u4eba\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f02\u6784\u673a\u5668\u4eba\u56e2\u961f\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2510.26935", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.FL"], "pdf": "https://arxiv.org/pdf/2510.26935", "abs": "https://arxiv.org/abs/2510.26935", "authors": ["Yunhao Yang", "Neel P. Bhatt", "Pranay Samineni", "Rohan Siva", "Zhanyang Wang", "Ufuk Topcu"], "title": "RepV: Safety-Separable Latent Spaces for Scalable Neurosymbolic Plan Verification", "comment": "Code and data are available at: https://repv-project.github.io/", "summary": "As AI systems migrate to safety-critical domains, verifying that their\nactions comply with well-defined rules remains a challenge. Formal methods\nprovide provable guarantees but demand hand-crafted temporal-logic\nspecifications, offering limited expressiveness and accessibility. Deep\nlearning approaches enable evaluation of plans against natural-language\nconstraints, yet their opaque decision process invites misclassifications with\npotentially severe consequences. We introduce RepV, a neurosymbolic verifier\nthat unifies both views by learning a latent space where safe and unsafe plans\nare linearly separable. Starting from a modest seed set of plans labeled by an\noff-the-shelf model checker, RepV trains a lightweight projector that embeds\neach plan, together with a language model-generated rationale, into a\nlow-dimensional space; a frozen linear boundary then verifies compliance for\nunseen natural-language rules in a single forward pass.\n  Beyond binary classification, RepV provides a probabilistic guarantee on the\nlikelihood of correct verification based on its position in the latent space.\nThis guarantee enables a guarantee-driven refinement of the planner, improving\nrule compliance without human annotations. Empirical evaluations show that RepV\nimproves compliance prediction accuracy by up to 15% compared to baseline\nmethods while adding fewer than 0.2M parameters. Furthermore, our refinement\nframework outperforms ordinary fine-tuning baselines across various planning\ndomains. These results show that safety-separable latent spaces offer a\nscalable, plug-and-play primitive for reliable neurosymbolic plan verification.\nCode and data are available at: https://repv-project.github.io/.", "AI": {"tldr": "RepV\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u9a8c\u8bc1\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u5b89\u5168\u4e0e\u4e0d\u5b89\u5168\u8ba1\u5212\u7684\u7ebf\u6027\u53ef\u5206\u6f5c\u5728\u7a7a\u95f4\uff0c\u5b9e\u73b0\u5bf9\u81ea\u7136\u8bed\u8a00\u89c4\u5219\u7684\u5408\u89c4\u6027\u9a8c\u8bc1\uff0c\u5e76\u63d0\u4f9b\u6982\u7387\u4fdd\u8bc1\u3002", "motivation": "\u5728AI\u7cfb\u7edf\u5e94\u7528\u4e8e\u5b89\u5168\u5173\u952e\u9886\u57df\u65f6\uff0c\u9700\u8981\u9a8c\u8bc1\u5176\u884c\u4e3a\u662f\u5426\u7b26\u5408\u660e\u786e\u5b9a\u4e49\u7684\u89c4\u5219\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5f62\u5f0f\u5316\u65b9\u6cd5\u9700\u8981\u624b\u5de5\u5236\u4f5c\u65f6\u5e8f\u903b\u8f91\u89c4\u8303\uff0c\u8868\u8fbe\u80fd\u529b\u6709\u9650\uff1b\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u80fd\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u7ea6\u675f\uff0c\u4f46\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u8bef\u5224\u3002", "method": "RepV\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u4ece\u5c11\u91cf\u6a21\u578b\u68c0\u67e5\u5668\u6807\u8bb0\u7684\u8ba1\u5212\u5f00\u59cb\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u6295\u5f71\u5668\u5c06\u6bcf\u4e2a\u8ba1\u5212\u4e0e\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u539f\u7406\u5d4c\u5165\u4f4e\u7ef4\u7a7a\u95f4\uff0c\u4f7f\u7528\u51bb\u7ed3\u7ebf\u6027\u8fb9\u754c\u5bf9\u672a\u89c1\u81ea\u7136\u8bed\u8a00\u89c4\u5219\u8fdb\u884c\u4e00\u6b21\u6027\u524d\u5411\u4f20\u64ad\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0cRepV\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5c06\u5408\u89c4\u6027\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u9ad8\u4e8615%\uff0c\u540c\u65f6\u4ec5\u589e\u52a0\u4e0d\u52300.2M\u53c2\u6570\u3002\u5176\u7cbe\u70bc\u6846\u67b6\u5728\u5404\u79cd\u89c4\u5212\u9886\u57df\u5747\u4f18\u4e8e\u666e\u901a\u5fae\u8c03\u57fa\u7ebf\u3002", "conclusion": "\u5b89\u5168\u53ef\u5206\u79bb\u7684\u6f5c\u5728\u7a7a\u95f4\u4e3a\u53ef\u9760\u7684\u795e\u7ecf\u7b26\u53f7\u8ba1\u5212\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u5373\u63d2\u5373\u7528\u7684\u539f\u8bed\u3002"}}
{"id": "2510.27010", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27010", "abs": "https://arxiv.org/abs/2510.27010", "authors": ["William E. Heap", "Yimeng Qin", "Kai Hammond", "Anish Bayya", "Haonon Kong", "Allison M. Okamura"], "title": "A Hermetic, Transparent Soft Growing Vine Robot System for Pipe Inspection", "comment": "8 pages, 7 figures", "summary": "Rehabilitation of aging pipes requires accurate condition assessment and\nmapping far into the pipe interiors. Soft growing vine robot systems are\nparticularly promising for navigating confined, sinuous paths such as in pipes,\nbut are currently limited by complex subsystems and a lack of validation in\nreal-world industrial settings. In this paper, we introduce the concept and\nimplementation of a hermetic and transparent vine robot system for visual\ncondition assessment and mapping within non-branching pipes. This design\nencloses all mechanical and electrical components within the vine robot's soft,\nairtight, and transparent body, protecting them from environmental interference\nwhile enabling visual sensing. Because this approach requires an enclosed\nmechanism for transporting sensors, we developed, modeled, and tested a\npassively adapting enclosed tip mount. Finally, we validated the hermetic and\ntransparent vine robot system concept through a real-world condition assessment\nand mapping task in a wastewater pipe. This work advances the use of\nsoft-growing vine robots in pipe inspection by developing and demonstrating a\nrobust, streamlined, field-validated system suitable for continued development\nand deployment.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5bc6\u5c01\u900f\u660e\u7684\u85e4\u8513\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u7ba1\u9053\u5185\u90e8\u89c6\u89c9\u72b6\u6001\u8bc4\u4f30\u548c\u6d4b\u7ed8\uff0c\u901a\u8fc7\u5c06\u673a\u68b0\u548c\u7535\u6c14\u7ec4\u4ef6\u5c01\u88c5\u5728\u673a\u5668\u4eba\u8f6f\u4f53\u5185\u5b9e\u73b0\u73af\u5883\u9632\u62a4\u3002", "motivation": "\u73b0\u6709\u8f6f\u4f53\u85e4\u8513\u673a\u5668\u4eba\u5728\u7ba1\u9053\u68c0\u67e5\u4e2d\u9762\u4e34\u5b50\u7cfb\u7edf\u590d\u6742\u548c\u7f3a\u4e4f\u771f\u5b9e\u5de5\u4e1a\u73af\u5883\u9a8c\u8bc1\u7684\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7a33\u5065\u3001\u7b80\u5316\u7684\u7cfb\u7edf\u3002", "method": "\u8bbe\u8ba1\u4e86\u5bc6\u5c01\u900f\u660e\u7684\u85e4\u8513\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u5f00\u53d1\u4e86\u88ab\u52a8\u9002\u5e94\u7684\u5c01\u95ed\u5f0f\u5c16\u7aef\u5b89\u88c5\u5ea7\u6765\u8fd0\u8f93\u4f20\u611f\u5668\uff0c\u5e76\u5728\u5e9f\u6c34\u7ba1\u9053\u4e2d\u8fdb\u884c\u5b9e\u5730\u9a8c\u8bc1\u3002", "result": "\u6210\u529f\u9a8c\u8bc1\u4e86\u5bc6\u5c01\u900f\u660e\u85e4\u8513\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u771f\u5b9e\u5e9f\u6c34\u7ba1\u9053\u4e2d\u8fdb\u884c\u72b6\u6001\u8bc4\u4f30\u548c\u6d4b\u7ed8\u7684\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u8f6f\u4f53\u85e4\u8513\u673a\u5668\u4eba\u5728\u7ba1\u9053\u68c0\u67e5\u4e2d\u7684\u5e94\u7528\uff0c\u5f00\u53d1\u5e76\u5c55\u793a\u4e86\u4e00\u4e2a\u9002\u5408\u6301\u7eed\u5f00\u53d1\u548c\u90e8\u7f72\u7684\u7a33\u5065\u3001\u7b80\u5316\u7684\u73b0\u573a\u9a8c\u8bc1\u7cfb\u7edf\u3002"}}
{"id": "2510.27033", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2510.27033", "abs": "https://arxiv.org/abs/2510.27033", "authors": ["Simindokht Jahangard", "Mehrzad Mohammadi", "Abhinav Dhall", "Hamid Rezatofighi"], "title": "A Multi-Modal Neuro-Symbolic Approach for Spatial Reasoning-Based Visual Grounding in Robotics", "comment": null, "summary": "Visual reasoning, particularly spatial reasoning, is a challenging cognitive\ntask that requires understanding object relationships and their interactions\nwithin complex environments, especially in robotics domain. Existing\nvision_language models (VLMs) excel at perception tasks but struggle with\nfine-grained spatial reasoning due to their implicit, correlation-driven\nreasoning and reliance solely on images. We propose a novel neuro_symbolic\nframework that integrates both panoramic-image and 3D point cloud information,\ncombining neural perception with symbolic reasoning to explicitly model spatial\nand logical relationships. Our framework consists of a perception module for\ndetecting entities and extracting attributes, and a reasoning module that\nconstructs a structured scene graph to support precise, interpretable queries.\nEvaluated on the JRDB-Reasoning dataset, our approach demonstrates superior\nperformance and reliability in crowded, human_built environments while\nmaintaining a lightweight design suitable for robotics and embodied AI\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5168\u666f\u56fe\u50cf\u548c3D\u70b9\u4e91\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7a7a\u95f4\u548c\u903b\u8f91\u5173\u7cfb\u6765\u6539\u8fdb\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u673a\u5668\u4eba\u9886\u57df\u9700\u8981\u7406\u89e3\u590d\u6742\u73af\u5883\u4e2d\u7269\u4f53\u5173\u7cfb\u7684\u573a\u666f\u3002", "method": "\u6784\u5efa\u5305\u542b\u611f\u77e5\u6a21\u5757\u548c\u63a8\u7406\u6a21\u5757\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u611f\u77e5\u6a21\u5757\u68c0\u6d4b\u5b9e\u4f53\u5e76\u63d0\u53d6\u5c5e\u6027\uff0c\u63a8\u7406\u6a21\u5757\u6784\u5efa\u7ed3\u6784\u5316\u573a\u666f\u56fe\u6765\u652f\u6301\u7cbe\u786e\u3001\u53ef\u89e3\u91ca\u7684\u67e5\u8be2\u3002", "result": "\u5728JRDB-Reasoning\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u62e5\u6324\u7684\u4eba\u9020\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u8bbe\u8ba1\u7684\u540c\u65f6\uff0c\u4e3a\u673a\u5668\u4eba\u548c\u5177\u8eabAI\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7a7a\u95f4\u63a8\u7406\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.27048", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27048", "abs": "https://arxiv.org/abs/2510.27048", "authors": ["Eric T. Chang", "Peter Ballentine", "Zhanpeng He", "Do-Gon Kim", "Kai Jiang", "Hua-Hsuan Liang", "Joaquin Palacios", "William Wang", "Pedro Piacenza", "Ioannis Kymissis", "Matei Ciocarlie"], "title": "SpikeATac: A Multimodal Tactile Finger with Taxelized Dynamic Sensing for Dexterous Manipulation", "comment": "9 pages, 8 figures, under review", "summary": "In this work, we introduce SpikeATac, a multimodal tactile finger combining a\ntaxelized and highly sensitive dynamic response (PVDF) with a static\ntransduction method (capacitive) for multimodal touch sensing. Named for its\n`spiky' response, SpikeATac's 16-taxel PVDF film sampled at 4 kHz provides\nfast, sensitive dynamic signals to the very onset and breaking of contact. We\ncharacterize the sensitivity of the different modalities, and show that\nSpikeATac provides the ability to stop quickly and delicately when grasping\nfragile, deformable objects. Beyond parallel grasping, we show that SpikeATac\ncan be used in a learning-based framework to achieve new capabilities on a\ndexterous multifingered robot hand. We use a learning recipe that combines\nreinforcement learning from human feedback with tactile-based rewards to\nfine-tune the behavior of a policy to modulate force. Our hardware platform and\nlearning pipeline together enable a difficult dexterous and contact-rich task\nthat has not previously been achieved: in-hand manipulation of fragile objects.\nVideos are available at\n\\href{https://roamlab.github.io/spikeatac/}{roamlab.github.io/spikeatac}.", "AI": {"tldr": "SpikeATac\u662f\u4e00\u79cd\u7ed3\u5408PVDF\u52a8\u6001\u54cd\u5e94\u548c\u7535\u5bb9\u5f0f\u9759\u6001\u4f20\u611f\u7684\u591a\u6a21\u6001\u89e6\u89c9\u624b\u6307\uff0c\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u7075\u654f\u7684\u63a5\u89e6\u68c0\u6d4b\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u6846\u67b6\u5728\u7075\u5de7\u673a\u68b0\u624b\u4e0a\u5b8c\u6210\u5bf9\u6613\u788e\u7269\u4f53\u7684\u7cbe\u7ec6\u64cd\u4f5c\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u611f\u77e5\u52a8\u6001\u548c\u9759\u6001\u89e6\u89c9\u4fe1\u606f\u7684\u591a\u6a21\u6001\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u6613\u788e\u7269\u4f53\u7684\u7cbe\u7ec6\u6293\u53d6\u548c\u7075\u5de7\u64cd\u4f5c\u3002", "method": "\u7ed3\u540816\u4e2a\u7a0e\u5143\u7684PVDF\u8584\u819c\uff084kHz\u91c7\u6837\uff09\u548c\u7535\u5bb9\u5f0f\u4f20\u611f\uff0c\u4f7f\u7528\u57fa\u4e8e\u5b66\u4e60\u7684\u6846\u67b6\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\u548c\u89e6\u89c9\u5956\u52b1\u6765\u5fae\u8c03\u7b56\u7565\u3002", "result": "SpikeATac\u80fd\u591f\u5feb\u901f\u505c\u6b62\u5e76\u7cbe\u7ec6\u6293\u53d6\u6613\u788e\u53ef\u53d8\u5f62\u7269\u4f53\uff0c\u5b9e\u73b0\u4e86\u5148\u524d\u672a\u8fbe\u6210\u7684\u7075\u5de7\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\uff1a\u6613\u788e\u7269\u4f53\u7684\u624b\u5185\u64cd\u4f5c\u3002", "conclusion": "SpikeATac\u786c\u4ef6\u5e73\u53f0\u548c\u5b66\u4e60\u7ba1\u9053\u5171\u540c\u5b9e\u73b0\u4e86\u5bf9\u6613\u788e\u7269\u4f53\u7684\u7cbe\u7ec6\u7075\u5de7\u64cd\u4f5c\uff0c\u4e3a\u673a\u5668\u4eba\u89e6\u89c9\u4f20\u611f\u548c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u80fd\u529b\u3002"}}
{"id": "2510.27114", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27114", "abs": "https://arxiv.org/abs/2510.27114", "authors": ["Dohyeok Lee", "Jung Min Lee", "Munkyung Kim", "Seokhun Ju", "Jin Woo Koo", "Kyungjae Lee", "Dohyeong Kim", "TaeHyun Cho", "Jungwoo Lee"], "title": "Learning Generalizable Visuomotor Policy through Dynamics-Alignment", "comment": "9 pages, 6 figures", "summary": "Behavior cloning methods for robot learning suffer from poor generalization\ndue to limited data support beyond expert demonstrations. Recent approaches\nleveraging video prediction models have shown promising results by learning\nrich spatiotemporal representations from large-scale datasets. However, these\nmodels learn action-agnostic dynamics that cannot distinguish between different\ncontrol inputs, limiting their utility for precise manipulation tasks and\nrequiring large pretraining datasets. We propose a Dynamics-Aligned Flow\nMatching Policy (DAP) that integrates dynamics prediction into policy learning.\nOur method introduces a novel architecture where policy and dynamics models\nprovide mutual corrective feedback during action generation, enabling\nself-correction and improved generalization. Empirical validation demonstrates\ngeneralization performance superior to baseline methods on real-world robotic\nmanipulation tasks, showing particular robustness in OOD scenarios including\nvisual distractions and lighting variations.", "AI": {"tldr": "\u63d0\u51faDAP\u65b9\u6cd5\uff0c\u5c06\u52a8\u529b\u5b66\u9884\u6d4b\u6574\u5408\u5230\u7b56\u7565\u5b66\u4e60\u4e2d\uff0c\u901a\u8fc7\u7b56\u7565\u548c\u52a8\u529b\u5b66\u6a21\u578b\u7684\u76f8\u4e92\u6821\u6b63\u53cd\u9988\u5b9e\u73b0\u52a8\u4f5c\u751f\u6210\u7684\u81ea\u6821\u6b63\u548c\u6cdb\u5316\u80fd\u529b\u63d0\u5347\u3002", "motivation": "\u884c\u4e3a\u514b\u9686\u65b9\u6cd5\u7531\u4e8e\u6570\u636e\u652f\u6301\u6709\u9650\u800c\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u73b0\u6709\u89c6\u9891\u9884\u6d4b\u6a21\u578b\u5b66\u4e60\u52a8\u4f5c\u65e0\u5173\u7684\u52a8\u529b\u5b66\uff0c\u65e0\u6cd5\u533a\u5206\u4e0d\u540c\u63a7\u5236\u8f93\u5165\uff0c\u9650\u5236\u4e86\u5728\u7cbe\u786e\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "\u8bbe\u8ba1\u65b0\u9896\u67b6\u6784\uff0c\u7b56\u7565\u548c\u52a8\u529b\u5b66\u6a21\u578b\u5728\u52a8\u4f5c\u751f\u6210\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u76f8\u4e92\u6821\u6b63\u53cd\u9988\uff0c\u5b9e\u73b0\u81ea\u6821\u6b63\u80fd\u529b\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u5728\u5305\u62ec\u89c6\u89c9\u5e72\u6270\u548c\u5149\u7167\u53d8\u5316\u7684OOD\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u7279\u522b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "DAP\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u52a8\u529b\u5b66\u9884\u6d4b\u4e0e\u7b56\u7565\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2510.27151", "categories": ["cs.RO", "/"], "pdf": "https://arxiv.org/pdf/2510.27151", "abs": "https://arxiv.org/abs/2510.27151", "authors": ["Xueliang Cheng", "Kanzhong Yao", "Andrew West", "Ognjen Marjanovic", "Barry Lennox", "Keir Groves"], "title": "Confined Space Underwater Positioning Using Collaborative Robots", "comment": "31 pages including appendix, 24 figures", "summary": "Positioning of underwater robots in confined and cluttered spaces remains a\nkey challenge for field operations. Existing systems are mostly designed for\nlarge, open-water environments and struggle in industrial settings due to poor\ncoverage, reliance on external infrastructure, and the need for feature-rich\nsurroundings. Multipath effects from continuous sound reflections further\ndegrade signal quality, reducing accuracy and reliability. Accurate and easily\ndeployable positioning is essential for repeatable autonomous missions;\nhowever, this requirement has created a technological bottleneck limiting\nunderwater robotic deployment. This paper presents the Collaborative Aquatic\nPositioning (CAP) system, which integrates collaborative robotics and sensor\nfusion to overcome these limitations. Inspired by the \"mother-ship\" concept,\nthe surface vehicle acts as a mobile leader to assist in positioning a\nsubmerged robot, enabling localization even in GPS-denied and highly\nconstrained environments. The system is validated in a large test tank through\nrepeatable autonomous missions using CAP's position estimates for real-time\ntrajectory control. Experimental results demonstrate a mean Euclidean distance\n(MED) error of 70 mm, achieved in real time without requiring fixed\ninfrastructure, extensive calibration, or environmental features. CAP leverages\nadvances in mobile robot sensing and leader-follower control to deliver a step\nchange in accurate, practical, and infrastructure-free underwater localization.", "AI": {"tldr": "\u63d0\u51fa\u534f\u4f5c\u6c34\u4e0b\u5b9a\u4f4d\u7cfb\u7edf(CAP)\uff0c\u901a\u8fc7\u6c34\u9762\u673a\u5668\u4eba\u4f5c\u4e3a\u79fb\u52a8\u9886\u5bfc\u8005\u534f\u52a9\u6c34\u4e0b\u673a\u5668\u4eba\u5b9a\u4f4d\uff0c\u5728\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b070mm\u7cbe\u5ea6\u7684\u5b9e\u65f6\u5b9a\u4f4d\uff0c\u65e0\u9700\u56fa\u5b9a\u57fa\u7840\u8bbe\u65bd\u3002", "motivation": "\u89e3\u51b3\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u53d7\u9650\u548c\u6742\u4e71\u7a7a\u95f4\u4e2d\u7684\u5b9a\u4f4d\u6311\u6218\uff0c\u73b0\u6709\u7cfb\u7edf\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u56e0\u8986\u76d6\u8303\u56f4\u5dee\u3001\u4f9d\u8d56\u5916\u90e8\u57fa\u7840\u8bbe\u65bd\u548c\u9700\u8981\u7279\u5f81\u4e30\u5bcc\u73af\u5883\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u534f\u4f5c\u673a\u5668\u4eba\u548c\u4f20\u611f\u5668\u878d\u5408\u65b9\u6cd5\uff0c\u6c34\u9762\u8f66\u8f86\u4f5c\u4e3a\u79fb\u52a8\u9886\u5bfc\u8005\u534f\u52a9\u6c34\u4e0b\u673a\u5668\u4eba\u5b9a\u4f4d\uff0c\u7ed3\u5408\u79fb\u52a8\u673a\u5668\u4eba\u611f\u77e5\u548c\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u63a7\u5236\u6280\u672f\u3002", "result": "\u5728\u5927\u578b\u6d4b\u8bd5\u6c60\u4e2d\u9a8c\u8bc1\uff0c\u5b9e\u73b070mm\u7684\u5e73\u5747\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u8bef\u5dee\uff0c\u5b9e\u65f6\u8f68\u8ff9\u63a7\u5236\uff0c\u65e0\u9700\u56fa\u5b9a\u57fa\u7840\u8bbe\u65bd\u3001\u5e7f\u6cdb\u6821\u51c6\u6216\u73af\u5883\u7279\u5f81\u3002", "conclusion": "CAP\u7cfb\u7edf\u901a\u8fc7\u79fb\u52a8\u673a\u5668\u4eba\u611f\u77e5\u548c\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u63a7\u5236\u7684\u8fdb\u6b65\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u3001\u5b9e\u7528\u4e14\u65e0\u9700\u57fa\u7840\u8bbe\u65bd\u7684\u6c34\u4e0b\u5b9a\u4f4d\u6280\u672f\u7a81\u7834\u3002"}}
{"id": "2510.27178", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27178", "abs": "https://arxiv.org/abs/2510.27178", "authors": ["Xuan-Thuan Nguyen", "Khac Nam Nguyen", "Ngoc Duy Tran", "Thi Thoa Mac", "Anh Nguyen", "Hoang Hiep Ly", "Tung D. Ta"], "title": "MobiDock: Design and Control of A Modular Self Reconfigurable Bimanual Mobile Manipulator via Robotic Docking", "comment": "ICRA2026 submited", "summary": "Multi-robot systems, particularly mobile manipulators, face challenges in\ncontrol coordination and dynamic stability when working together. To address\nthis issue, this study proposes MobiDock, a modular self-reconfigurable mobile\nmanipulator system that allows two independent robots to physically connect and\nform a unified mobile bimanual platform. This process helps transform a complex\nmulti-robot control problem into the management of a simpler, single system.\nThe system utilizes an autonomous docking strategy based on computer vision\nwith AprilTag markers and a new threaded screw-lock mechanism. Experimental\nresults show that the docked configuration demonstrates better performance in\ndynamic stability and operational efficiency compared to two independently\ncooperating robots. Specifically, the unified system has lower Root Mean Square\n(RMS) Acceleration and Jerk values, higher angular precision, and completes\ntasks significantly faster. These findings confirm that physical\nreconfiguration is a powerful design principle that simplifies cooperative\ncontrol, improving stability and performance for complex tasks in real-world\nenvironments.", "AI": {"tldr": "MobiDock\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u81ea\u91cd\u6784\u79fb\u52a8\u673a\u68b0\u81c2\u7cfb\u7edf\uff0c\u5141\u8bb8\u4e24\u4e2a\u72ec\u7acb\u673a\u5668\u4eba\u7269\u7406\u8fde\u63a5\u5f62\u6210\u7edf\u4e00\u7684\u79fb\u52a8\u53cc\u81c2\u5e73\u53f0\uff0c\u5c06\u590d\u6742\u591a\u673a\u5668\u4eba\u63a7\u5236\u95ee\u9898\u7b80\u5316\u4e3a\u5355\u4e00\u7cfb\u7edf\u7ba1\u7406\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u7cfb\u7edf\uff08\u7279\u522b\u662f\u79fb\u52a8\u673a\u68b0\u81c2\uff09\u5728\u534f\u4f5c\u4e2d\u7684\u63a7\u5236\u534f\u8c03\u548c\u52a8\u6001\u7a33\u5b9a\u6027\u6311\u6218\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8ba1\u7b97\u673a\u89c6\u89c9\u548cAprilTag\u6807\u8bb0\u7684\u81ea\u4e3b\u5bf9\u63a5\u7b56\u7565\uff0c\u4ee5\u53ca\u65b0\u578b\u87ba\u7eb9\u87ba\u65cb\u9501\u5b9a\u673a\u5236\u3002", "result": "\u5bf9\u63a5\u914d\u7f6e\u5728\u52a8\u6001\u7a33\u5b9a\u6027\u548c\u64cd\u4f5c\u6548\u7387\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff1aRMS\u52a0\u901f\u5ea6\u548c\u6025\u52a8\u5ea6\u503c\u66f4\u4f4e\uff0c\u89d2\u5ea6\u7cbe\u5ea6\u66f4\u9ad8\uff0c\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u663e\u8457\u7f29\u77ed\u3002", "conclusion": "\u7269\u7406\u91cd\u6784\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u80fd\u591f\u7b80\u5316\u534f\u4f5c\u63a7\u5236\uff0c\u63d0\u9ad8\u73b0\u5b9e\u73af\u5883\u4e2d\u590d\u6742\u4efb\u52a1\u7684\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2510.27184", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27184", "abs": "https://arxiv.org/abs/2510.27184", "authors": ["Hoang Hiep Ly", "Cong-Nhat Nguyen", "Doan-Quang Tran", "Quoc-Khanh Dang", "Ngoc Duy Tran", "Thi Thoa Mac", "Anh Nguyen", "Xuan-Thuan Nguyen", "Tung D. Ta"], "title": "Hybrid Gripper Finger Enabling In-Grasp Friction Modulation Using Inflatable Silicone Pockets", "comment": "Submitted to ICRA 2026", "summary": "Grasping objects with diverse mechanical properties, such as heavy, slippery,\nor fragile items, remains a significant challenge in robotics. Conventional\ngrippers often rely on applying high normal forces, which can cause damage to\nobjects. To address this limitation, we present a hybrid gripper finger that\ncombines a rigid structural shell with a soft, inflatable silicone pocket. The\ngripper finger can actively modulate its surface friction by controlling the\ninternal air pressure of the silicone pocket. Results from fundamental\nexperiments indicate that increasing the internal pressure results in a\nproportional increase in the effective coefficient of friction. This enables\nthe gripper to stably lift heavy and slippery objects without increasing the\ngripping force and to handle fragile or deformable objects, such as eggs,\nfruits, and paper cups, with minimal damage by increasing friction rather than\napplying excessive force. The experimental results demonstrate that the hybrid\ngripper finger with adaptable friction provides a robust and safer alternative\nto relying solely on high normal forces, thereby enhancing the gripper\nflexibility in handling delicate, fragile, and diverse objects.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u521a\u6027\u5916\u58f3\u548c\u53ef\u5145\u6c14\u7845\u80f6\u888b\u7684\u6df7\u5408\u5939\u722a\u624b\u6307\uff0c\u901a\u8fc7\u63a7\u5236\u5185\u90e8\u6c14\u538b\u4e3b\u52a8\u8c03\u8282\u8868\u9762\u6469\u64e6\u529b\uff0c\u5b9e\u73b0\u5b89\u5168\u6293\u53d6\u4e0d\u540c\u673a\u68b0\u7279\u6027\u7684\u7269\u4f53\u3002", "motivation": "\u4f20\u7edf\u5939\u722a\u4f9d\u8d56\u9ad8\u6cd5\u5411\u529b\u6293\u53d6\u7269\u4f53\uff0c\u5bb9\u6613\u635f\u574f\u91cd\u7269\u3001\u6613\u6ed1\u7269\u4f53\u548c\u6613\u788e\u7269\u54c1\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5b89\u5168\u7684\u6293\u53d6\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u6df7\u5408\u5939\u722a\u624b\u6307\uff0c\u5305\u542b\u521a\u6027\u7ed3\u6784\u5916\u58f3\u548c\u67d4\u8f6f\u53ef\u5145\u6c14\u7684\u7845\u80f6\u888b\uff0c\u901a\u8fc7\u8c03\u8282\u5185\u90e8\u6c14\u538b\u6765\u4e3b\u52a8\u63a7\u5236\u8868\u9762\u6469\u64e6\u7cfb\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u589e\u52a0\u5185\u90e8\u538b\u529b\u53ef\u7ebf\u6027\u63d0\u9ad8\u6709\u6548\u6469\u64e6\u7cfb\u6570\uff0c\u80fd\u591f\u7a33\u5b9a\u6293\u53d6\u91cd\u6ed1\u7269\u4f53\u800c\u4e0d\u589e\u52a0\u5939\u6301\u529b\uff0c\u5e76\u80fd\u5b89\u5168\u5904\u7406\u6613\u788e\u7269\u54c1\u5982\u9e21\u86cb\u3001\u6c34\u679c\u548c\u7eb8\u676f\u3002", "conclusion": "\u5177\u6709\u53ef\u8c03\u6469\u64e6\u529b\u7684\u6df7\u5408\u5939\u722a\u624b\u6307\u4e3a\u4f9d\u8d56\u9ad8\u6cd5\u5411\u529b\u7684\u4f20\u7edf\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u5b89\u5168\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u589e\u5f3a\u4e86\u5939\u722a\u5904\u7406\u5404\u7c7b\u7269\u4f53\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2510.27191", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27191", "abs": "https://arxiv.org/abs/2510.27191", "authors": ["Marcus Hoerger", "Muhammad Sudrajat", "Hanna Kurniawati"], "title": "Vectorized Online POMDP Planning", "comment": "8 pages, 3 figures. Submitted to ICRA 2026", "summary": "Planning under partial observability is an essential capability of autonomous\nrobots. The Partially Observable Markov Decision Process (POMDP) provides a\npowerful framework for planning under partial observability problems, capturing\nthe stochastic effects of actions and the limited information available through\nnoisy observations. POMDP solving could benefit tremendously from massive\nparallelization of today's hardware, but parallelizing POMDP solvers has been\nchallenging. They rely on interleaving numerical optimization over actions with\nthe estimation of their values, which creates dependencies and synchronization\nbottlenecks between parallel processes that can quickly offset the benefits of\nparallelization. In this paper, we propose Vectorized Online POMDP Planner\n(VOPP), a novel parallel online solver that leverages a recent POMDP\nformulation that analytically solves part of the optimization component,\nleaving only the estimation of expectations for numerical computation. VOPP\nrepresents all data structures related to planning as a collection of tensors\nand implements all planning steps as fully vectorized computations over this\nrepresentation. The result is a massively parallel solver with no dependencies\nand synchronization bottlenecks between parallel computations. Experimental\nresults indicate that VOPP is at least 20X more efficient in computing\nnear-optimal solutions compared to an existing state-of-the-art parallel online\nsolver.", "AI": {"tldr": "VOPP\u662f\u4e00\u79cd\u65b0\u578b\u5e76\u884c\u5728\u7ebfPOMDP\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u5411\u91cf\u5316\u8ba1\u7b97\u6d88\u9664\u5e76\u884c\u5316\u74f6\u9888\uff0c\u6bd4\u73b0\u6709\u6700\u4f18\u5e76\u884c\u6c42\u89e3\u5668\u6548\u7387\u63d0\u5347\u81f3\u5c1120\u500d", "motivation": "POMDP\u6846\u67b6\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u89c4\u5212\u5f88\u5f3a\u5927\uff0c\u4f46\u73b0\u6709\u5e76\u884c\u5316\u65b9\u6cd5\u5b58\u5728\u4f9d\u8d56\u6027\u548c\u540c\u6b65\u74f6\u9888\uff0c\u62b5\u6d88\u4e86\u5e76\u884c\u5316\u7684\u4f18\u52bf", "method": "\u91c7\u7528\u5411\u91cf\u5316\u8868\u793a\u6240\u6709\u89c4\u5212\u6570\u636e\u7ed3\u6784\u4e3a\u5f20\u91cf\u96c6\u5408\uff0c\u5c06\u6240\u6709\u89c4\u5212\u6b65\u9aa4\u5b9e\u73b0\u4e3a\u5b8c\u5168\u5411\u91cf\u5316\u8ba1\u7b97\uff0c\u6d88\u9664\u5e76\u884c\u8ba1\u7b97\u95f4\u7684\u4f9d\u8d56\u548c\u540c\u6b65\u74f6\u9888", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eVOPP\u5728\u8ba1\u7b97\u63a5\u8fd1\u6700\u4f18\u89e3\u65b9\u9762\u6bd4\u73b0\u6709\u6700\u4f18\u5e76\u884c\u5728\u7ebf\u6c42\u89e3\u5668\u6548\u7387\u81f3\u5c11\u63d0\u534720\u500d", "conclusion": "VOPP\u901a\u8fc7\u5411\u91cf\u5316\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86POMDP\u5e76\u884c\u5316\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5e76\u884c\u6c42\u89e3"}}
{"id": "2510.27327", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27327", "abs": "https://arxiv.org/abs/2510.27327", "authors": ["Robert Pommeranz", "Kevin Tebbe", "Ralf Heynicke", "Gerd Scholl"], "title": "A Modular and Scalable System Architecture for Heterogeneous UAV Swarms Using ROS 2 and PX4-Autopilot", "comment": null, "summary": "In this paper a modular and scalable architecture for heterogeneous\nswarm-based Counter Unmanned Aerial Systems (C-UASs) built on PX4-Autopilot and\nRobot Operating System 2 (ROS 2) framework is presented. The proposed\narchitecture emphasizes seamless integration of hardware components by\nintroducing independent ROS 2 nodes for each component of a Unmanned Aerial\nVehicle (UAV). Communication between swarm participants is abstracted in\nsoftware, allowing the use of various technologies without architectural\nchanges. Key functionalities are supported, e.g. leader following and formation\nflight to maneuver the swarm. The system also allows computer vision algorithms\nto be integrated for the detection and tracking of UAVs. Additionally, a ground\nstation control is integrated for the coordination of swarm operations.\nSwarm-based Unmanned Aerial System (UAS) architecture is verified within a\nGazebo simulation environment but also in real-world demonstrations.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8ePX4-Autopilot\u548cROS 2\u7684\u6a21\u5757\u5316\u5f02\u6784\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u67b6\u6784\uff0c\u652f\u6301\u7fa4\u7ec4\u8ddf\u968f\u3001\u7f16\u961f\u98de\u884c\u548c\u8ba1\u7b97\u673a\u89c6\u89c9\u96c6\u6210\uff0c\u5df2\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u3002", "motivation": "\u6784\u5efa\u53ef\u6269\u5c55\u7684\u5f02\u6784\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u67b6\u6784\uff0c\u5b9e\u73b0\u786c\u4ef6\u7ec4\u4ef6\u7684\u65e0\u7f1d\u96c6\u6210\u548c\u591a\u79cd\u901a\u4fe1\u6280\u672f\u7684\u7075\u6d3b\u4f7f\u7528\u3002", "method": "\u91c7\u7528PX4-Autopilot\u548cROS 2\u6846\u67b6\uff0c\u4e3a\u65e0\u4eba\u673a\u5404\u7ec4\u4ef6\u521b\u5efa\u72ec\u7acbROS 2\u8282\u70b9\uff0c\u62bd\u8c61\u5316\u7fa4\u7ec4\u95f4\u901a\u4fe1\uff0c\u652f\u6301\u591a\u79cd\u5173\u952e\u6280\u672f\u529f\u80fd\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u6a21\u5757\u5316\u67b6\u6784\uff0c\u652f\u6301\u7fa4\u7ec4\u64cd\u4f5c\u534f\u8c03\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u96c6\u6210\uff0c\u5e76\u5728Gazebo\u4eff\u771f\u548c\u5b9e\u9645\u6f14\u793a\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u529f\u80fd\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u5f02\u6784\u53cd\u65e0\u4eba\u673a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u786c\u4ef6\u96c6\u6210\u80fd\u529b\u3002"}}
{"id": "2510.27333", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27333", "abs": "https://arxiv.org/abs/2510.27333", "authors": ["Hao Cheng", "Yanbo Jiang", "Qingyuan Shi", "Qingwen Meng", "Keyu Chen", "Wenhao Yu", "Jianqiang Wang", "Sifa Zheng"], "title": "Modified-Emergency Index (MEI): A Criticality Metric for Autonomous Driving in Lateral Conflict", "comment": null, "summary": "Effective, reliable, and efficient evaluation of autonomous driving safety is\nessential to demonstrate its trustworthiness. Criticality metrics provide an\nobjective means of assessing safety. However, as existing metrics primarily\ntarget longitudinal conflicts, accurately quantifying the risks of lateral\nconflicts - prevalent in urban settings - remains challenging. This paper\nproposes the Modified-Emergency Index (MEI), a metric designed to quantify\nevasive effort in lateral conflicts. Compared to the original Emergency Index\n(EI), MEI refines the estimation of the time available for evasive maneuvers,\nenabling more precise risk quantification. We validate MEI on a public lateral\nconflict dataset based on Argoverse-2, from which we extract over 1,500\nhigh-quality AV conflict cases, including more than 500 critical events. MEI is\nthen compared with the well-established ACT and the widely used PET metrics.\nResults show that MEI consistently outperforms them in accurately quantifying\ncriticality and capturing risk evolution. Overall, these findings highlight MEI\nas a promising metric for evaluating urban conflicts and enhancing the safety\nassessment framework for autonomous driving. The open-source implementation is\navailable at https://github.com/AutoChengh/MEI.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u7d27\u6025\u6307\u6570\uff08MEI\uff09\u6765\u91cf\u5316\u6a2a\u5411\u51b2\u7a81\u4e2d\u7684\u89c4\u907f\u52aa\u529b\uff0c\u76f8\u6bd4\u73b0\u6709\u6307\u6807\u80fd\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u57ce\u5e02\u73af\u5883\u4e2d\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u5173\u952e\u6027\u6307\u6807\u4e3b\u8981\u9488\u5bf9\u7eb5\u5411\u51b2\u7a81\uff0c\u96be\u4ee5\u51c6\u786e\u91cf\u5316\u57ce\u5e02\u73af\u5883\u4e2d\u666e\u904d\u7684\u6a2a\u5411\u51b2\u7a81\u98ce\u9669\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u6539\u8fdb\u539f\u59cb\u7d27\u6025\u6307\u6570\uff08EI\uff09\uff0c\u4f18\u5316\u89c4\u907f\u64cd\u4f5c\u53ef\u7528\u65f6\u95f4\u7684\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5728\u57fa\u4e8eArgoverse-2\u7684\u516c\u5f00\u6a2a\u5411\u51b2\u7a81\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u63d0\u53d61500\u591a\u4e2a\u9ad8\u8d28\u91cf\u81ea\u52a8\u9a7e\u9a76\u51b2\u7a81\u6848\u4f8b\u3002", "result": "MEI\u5728\u51c6\u786e\u91cf\u5316\u5173\u952e\u6027\u548c\u6355\u6349\u98ce\u9669\u6f14\u53d8\u65b9\u9762\u6301\u7eed\u4f18\u4e8eACT\u548cPET\u6307\u6807\uff0c\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u8bc4\u4f30\u6a2a\u5411\u51b2\u7a81\u98ce\u9669\u3002", "conclusion": "MEI\u662f\u8bc4\u4f30\u57ce\u5e02\u51b2\u7a81\u548c\u589e\u5f3a\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\u7684\u6709\u524d\u666f\u6307\u6807\uff0c\u5f00\u6e90\u5b9e\u73b0\u5df2\u63d0\u4f9b\u3002"}}
{"id": "2510.27420", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2510.27420", "abs": "https://arxiv.org/abs/2510.27420", "authors": ["Roman Freiberg", "Alexander Qualmann", "Ngo Anh Vien", "Gerhard Neumann"], "title": "Towards a Multi-Embodied Grasping Agent", "comment": "9 pages, 3 figures", "summary": "Multi-embodiment grasping focuses on developing approaches that exhibit\ngeneralist behavior across diverse gripper designs. Existing methods often\nlearn the kinematic structure of the robot implicitly and face challenges due\nto the difficulty of sourcing the required large-scale data. In this work, we\npresent a data-efficient, flow-based, equivariant grasp synthesis architecture\nthat can handle different gripper types with variable degrees of freedom and\nsuccessfully exploit the underlying kinematic model, deducing all necessary\ninformation solely from the gripper and scene geometry. Unlike previous\nequivariant grasping methods, we translated all modules from the ground up to\nJAX and provide a model with batching capabilities over scenes, grippers, and\ngrasps, resulting in smoother learning, improved performance and faster\ninference time. Our dataset encompasses grippers ranging from humanoid hands to\nparallel yaw grippers and includes 25,000 scenes and 20 million grasps.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u9ad8\u6548\u3001\u57fa\u4e8e\u6d41\u7684\u7b49\u53d8\u6293\u53d6\u5408\u6210\u67b6\u6784\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u540c\u81ea\u7531\u5ea6\u7c7b\u578b\u7684\u5939\u722a\uff0c\u4ec5\u4ece\u5939\u722a\u548c\u573a\u666f\u51e0\u4f55\u4e2d\u63a8\u65ad\u5fc5\u8981\u4fe1\u606f\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u591a\u4f53\u6293\u53d6\u65b9\u6cd5\u901a\u5e38\u9690\u5f0f\u5b66\u4e60\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u7ed3\u6784\uff0c\u9762\u4e34\u5927\u89c4\u6a21\u6570\u636e\u83b7\u53d6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8eJAX\u91cd\u65b0\u6784\u5efa\u6240\u6709\u6a21\u5757\u7684\u7b49\u53d8\u6293\u53d6\u65b9\u6cd5\uff0c\u5177\u6709\u573a\u666f\u3001\u5939\u722a\u548c\u6293\u53d6\u6279\u5904\u7406\u80fd\u529b\uff0c\u5229\u7528\u5939\u722a\u548c\u573a\u666f\u51e0\u4f55\u4fe1\u606f\u63a8\u65ad\u8fd0\u52a8\u5b66\u6a21\u578b\u3002", "result": "\u6784\u5efa\u4e86\u5305\u542b25,000\u4e2a\u573a\u666f\u548c2000\u4e07\u6b21\u6293\u53d6\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4ece\u4eba\u5f62\u624b\u5230\u5e73\u884c\u5939\u722a\u7684\u5404\u79cd\u5939\u722a\u7c7b\u578b\uff0c\u5b9e\u73b0\u4e86\u66f4\u5e73\u6ed1\u7684\u5b66\u4e60\u3001\u6539\u8fdb\u7684\u6027\u80fd\u548c\u66f4\u5feb\u7684\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u3001\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u7b49\u53d8\u6293\u53d6\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u79cd\u5939\u722a\u8bbe\u8ba1\u3002"}}
{"id": "2510.27428", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27428", "abs": "https://arxiv.org/abs/2510.27428", "authors": ["Hehui Zheng", "Bhavya Sukhija", "Chenhao Li", "Klemens Iten", "Andreas Krause", "Robert K. Katzschmann"], "title": "Learning Soft Robotic Dynamics with Active Exploration", "comment": null, "summary": "Soft robots offer unmatched adaptability and safety in unstructured\nenvironments, yet their compliant, high-dimensional, and nonlinear dynamics\nmake modeling for control notoriously difficult. Existing data-driven\napproaches often fail to generalize, constrained by narrowly focused task\ndemonstrations or inefficient random exploration. We introduce SoftAE, an\nuncertainty-aware active exploration framework that autonomously learns\ntask-agnostic and generalizable dynamics models of soft robotic systems. SoftAE\nemploys probabilistic ensemble models to estimate epistemic uncertainty and\nactively guides exploration toward underrepresented regions of the state-action\nspace, achieving efficient coverage of diverse behaviors without task-specific\nsupervision. We evaluate SoftAE on three simulated soft robotic platforms -- a\ncontinuum arm, an articulated fish in fluid, and a musculoskeletal leg with\nhybrid actuation -- and on a pneumatically actuated continuum soft arm in the\nreal world. Compared with random exploration and task-specific model-based\nreinforcement learning, SoftAE produces more accurate dynamics models, enables\nsuperior zero-shot control on unseen tasks, and maintains robustness under\nsensing noise, actuation delays, and nonlinear material effects. These results\ndemonstrate that uncertainty-driven active exploration can yield scalable,\nreusable dynamics models across diverse soft robotic morphologies, representing\na step toward more autonomous, adaptable, and data-efficient control in\ncompliant robots.", "AI": {"tldr": "SoftAE\u662f\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u4e3b\u52a8\u63a2\u7d22\u6846\u67b6\uff0c\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u4efb\u52a1\u65e0\u5173\u548c\u53ef\u6cdb\u5316\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u901a\u8fc7\u6982\u7387\u96c6\u6210\u6a21\u578b\u4f30\u8ba1\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4e3b\u52a8\u5f15\u5bfc\u63a2\u7d22\u5230\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u533a\u57df\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5177\u6709\u65e0\u4e0e\u4f26\u6bd4\u7684\u9002\u5e94\u6027\u548c\u5b89\u5168\u6027\uff0c\u4f46\u5176\u67d4\u987a\u3001\u9ad8\u7ef4\u548c\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7279\u6027\u4f7f\u5f97\u63a7\u5236\u5efa\u6a21\u975e\u5e38\u56f0\u96be\u3002\u73b0\u6709\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u6cdb\u5316\uff0c\u53d7\u5230\u4efb\u52a1\u6f14\u793a\u8303\u56f4\u72ed\u7a84\u6216\u968f\u673a\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\u7684\u9650\u5236\u3002", "method": "\u91c7\u7528\u6982\u7387\u96c6\u6210\u6a21\u578b\u4f30\u8ba1\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3b\u52a8\u5f15\u5bfc\u63a2\u7d22\u5230\u72b6\u6001-\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u533a\u57df\uff0c\u5b9e\u73b0\u591a\u6837\u5316\u884c\u4e3a\u7684\u9ad8\u6548\u8986\u76d6\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u7684\u76d1\u7763\u3002\u5728\u4e09\u4e2a\u6a21\u62df\u8f6f\u4f53\u673a\u5668\u4eba\u5e73\u53f0\u548c\u4e00\u4e2a\u771f\u5b9e\u6c14\u52a8\u8fde\u7eed\u8f6f\u81c2\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u4e0e\u968f\u673a\u63a2\u7d22\u548c\u4efb\u52a1\u7279\u5b9a\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u76f8\u6bd4\uff0cSoftAE\u4ea7\u751f\u4e86\u66f4\u51c6\u786e\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u96f6\u6837\u672c\u63a7\u5236\u6027\u80fd\uff0c\u5e76\u5728\u4f20\u611f\u566a\u58f0\u3001\u9a71\u52a8\u5ef6\u8fdf\u548c\u975e\u7ebf\u6027\u6750\u6599\u6548\u5e94\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u4e3b\u52a8\u63a2\u7d22\u53ef\u4ee5\u5728\u591a\u6837\u5316\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u5f62\u6001\u4e2d\u4ea7\u751f\u53ef\u6269\u5c55\u3001\u53ef\u91cd\u7528\u7684\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u671d\u7740\u66f4\u81ea\u4e3b\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u548c\u6570\u636e\u6548\u7387\u66f4\u9ad8\u7684\u67d4\u987a\u673a\u5668\u4eba\u63a7\u5236\u8fc8\u51fa\u4e86\u4e00\u6b65\u3002"}}
{"id": "2510.27436", "categories": ["cs.RO", "cs.HC", "I.2.9; I.3.6"], "pdf": "https://arxiv.org/pdf/2510.27436", "abs": "https://arxiv.org/abs/2510.27436", "authors": ["Tomoko Yonezawa", "Hirotake Yamazoe", "Atsuo Fujino", "Daigo Suhara", "Takaya Tamamoto", "Yuto Nishiguchi"], "title": "Preliminary Prototyping of Avoidance Behaviors Triggered by a User's Physical Approach to a Robot", "comment": "Workshop on Socially Aware and Cooperative Intelligent Systems in HAI\n  2025", "summary": "Human-robot interaction frequently involves physical proximity or contact. In\nhuman-human settings, people flexibly accept, reject, or tolerate such\napproaches depending on the relationship and context. We explore the design of\na robot's rejective internal state and corresponding avoidance behaviors, such\nas withdrawing or pushing away, when a person approaches. We model the\naccumulation and decay of discomfort as a function of interpersonal distance,\nand implement tolerance (endurance) and limit-exceeding avoidance driven by the\nDominance axis of the PAD affect model. The behaviors and their intensities are\nrealized on an arm robot. Results illustrate a coherent pipeline from internal\nstate parameters to graded endurance motions and, once a limit is crossed, to\navoidance actions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u673a\u5668\u4eba\u62d2\u7edd\u5185\u90e8\u72b6\u6001\u548c\u76f8\u5e94\u56de\u907f\u884c\u4e3a\uff0c\u5f53\u4eba\u63a5\u8fd1\u65f6\u673a\u5668\u4eba\u4f1a\u64a4\u9000\u6216\u63a8\u5f00\u3002\u6a21\u578b\u57fa\u4e8e\u4eba\u9645\u8ddd\u79bb\u7684\u4e0d\u9002\u7d2f\u79ef\u548c\u8870\u51cf\uff0c\u5b9e\u73b0\u5bb9\u5fcd\u548c\u6781\u9650\u8d85\u51fa\u7684\u56de\u907f\u884c\u4e3a\u3002", "motivation": "\u5728\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\uff0c\u7269\u7406\u63a5\u8fd1\u6216\u63a5\u89e6\u5f88\u5e38\u89c1\u3002\u4eba\u7c7b\u4f1a\u6839\u636e\u5173\u7cfb\u548c\u60c5\u5883\u7075\u6d3b\u63a5\u53d7\u3001\u62d2\u7edd\u6216\u5bb9\u5fcd\u8fd9\u79cd\u63a5\u8fd1\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u673a\u5668\u4eba\u80fd\u591f\u8868\u8fbe\u62d2\u7edd\u7684\u5185\u90e8\u72b6\u6001\u548c\u56de\u907f\u884c\u4e3a\u3002", "method": "\u57fa\u4e8e\u4eba\u9645\u8ddd\u79bb\u51fd\u6570\u5efa\u6a21\u4e0d\u9002\u611f\u7684\u7d2f\u79ef\u548c\u8870\u51cf\uff0c\u4f7f\u7528PAD\u60c5\u611f\u6a21\u578b\u7684\u652f\u914d\u8f74\u5b9e\u73b0\u5bb9\u5fcd\uff08\u5fcd\u8010\uff09\u548c\u6781\u9650\u8d85\u51fa\u7684\u56de\u907f\u884c\u4e3a\u3002\u5728\u624b\u81c2\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e0d\u540c\u5f3a\u5ea6\u7684\u884c\u4e3a\u548c\u52a8\u4f5c\u3002", "result": "\u5c55\u793a\u4e86\u4ece\u5185\u90e8\u72b6\u6001\u53c2\u6570\u5230\u5206\u7ea7\u5fcd\u8010\u52a8\u4f5c\uff0c\u518d\u5230\u6781\u9650\u88ab\u7a81\u7834\u65f6\u7684\u56de\u907f\u52a8\u4f5c\u7684\u8fde\u8d2f\u6d41\u7a0b\u3002", "conclusion": "\u6210\u529f\u5b9e\u73b0\u4e86\u673a\u5668\u4eba\u57fa\u4e8e\u5185\u90e8\u72b6\u6001\u7684\u62d2\u7edd\u548c\u56de\u907f\u884c\u4e3a\uff0c\u4e3a\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u4e92\u4e2d\u7684\u7269\u7406\u63a5\u8fd1\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u7684\u54cd\u5e94\u673a\u5236\u3002"}}
{"id": "2510.27545", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.27545", "abs": "https://arxiv.org/abs/2510.27545", "authors": ["Travis Davies", "Yiqi Huang", "Alexi Gladstone", "Yunxin Liu", "Xiang Chen", "Heng Ji", "Huxian Liu", "Luhui Hu"], "title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities", "comment": "9 pages, 6 figures, 4 tables", "summary": "Implicit policies parameterized by generative models, such as Diffusion\nPolicy, have become the standard for policy learning and Vision-Language-Action\n(VLA) models in robotics. However, these approaches often suffer from high\ncomputational cost, exposure bias, and unstable inference dynamics, which lead\nto divergence under distribution shifts. Energy-Based Models (EBMs) address\nthese issues by learning energy landscapes end-to-end and modeling equilibrium\ndynamics, offering improved robustness and reduced exposure bias. Yet, policies\nparameterized by EBMs have historically struggled to scale effectively. Recent\nwork on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs\nto high-dimensional spaces, but their potential for solving core challenges in\nphysically embodied models remains underexplored. We introduce a new\nenergy-based architecture, EBT-Policy, that solves core issues in robotic and\nreal-world settings. Across simulated and real-world tasks, EBT-Policy\nconsistently outperforms diffusion-based policies, while requiring less\ntraining and inference computation. Remarkably, on some tasks it converges\nwithin just two inference steps, a 50x reduction compared to Diffusion Policy's\n100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior\nmodels, such as zero-shot recovery from failed action sequences using only\nbehavior cloning and without explicit retry training. By leveraging its scalar\nenergy for uncertainty-aware inference and dynamic compute allocation,\nEBT-Policy offers a promising path toward robust, generalizable robot behavior\nunder distribution shifts.", "AI": {"tldr": "\u63d0\u51fa\u4e86EBT-Policy\uff0c\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u7684\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u3001\u9c81\u68d2\u6027\u548c\u63a8\u7406\u7a33\u5b9a\u6027\u65b9\u9762\u4f18\u4e8e\u6269\u6563\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u7684\u9690\u5f0f\u7b56\u7565\uff08\u5982\u6269\u6563\u7b56\u7565\uff09\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u3001\u66b4\u9732\u504f\u5dee\u548c\u4e0d\u7a33\u5b9a\u63a8\u7406\u52a8\u6001\u7b49\u95ee\u9898\uff0c\u800c\u57fa\u4e8e\u80fd\u91cf\u7684\u6a21\u578b\uff08EBMs\uff09\u867d\u7136\u80fd\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4f46\u96be\u4ee5\u6709\u6548\u6269\u5c55\u3002", "method": "\u5f00\u53d1\u4e86EBT-Policy\u67b6\u6784\uff0c\u5229\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u53d8\u6362\u5668\uff08EBTs\uff09\u7684\u53ef\u6269\u5c55\u6027\uff0c\u5b66\u4e60\u7aef\u5230\u7aef\u7684\u80fd\u91cf\u666f\u89c2\u5e76\u5efa\u6a21\u5e73\u8861\u52a8\u6001\uff0c\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u63a8\u7406\u548c\u52a8\u6001\u8ba1\u7b97\u5206\u914d\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\uff0cEBT-Policy\u59cb\u7ec8\u4f18\u4e8e\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u8ba1\u7b97\u9700\u6c42\u66f4\u5c11\uff0c\u67d0\u4e9b\u4efb\u52a1\u4ec5\u97002\u6b65\u63a8\u7406\uff08\u76f8\u6bd4\u6269\u6563\u7b56\u7565\u7684100\u6b65\u51cf\u5c1150\u500d\uff09\uff0c\u5e76\u5c55\u73b0\u51fa\u96f6\u6837\u672c\u6062\u590d\u7b49\u65b0\u5174\u80fd\u529b\u3002", "conclusion": "EBT-Policy\u4e3a\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u5b9e\u73b0\u9c81\u68d2\u3001\u53ef\u6cdb\u5316\u7684\u673a\u5668\u4eba\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u8def\u5f84\u3002"}}
{"id": "2510.27558", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.27558", "abs": "https://arxiv.org/abs/2510.27558", "authors": ["Sushil Samuel Dinesh", "Shinkyu Park"], "title": "Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs", "comment": null, "summary": "This paper presents a framework that leverages pre-trained foundation models\nfor robotic manipulation without domain-specific training. The framework\nintegrates off-the-shelf models, combining multimodal perception from\nfoundation models with a general-purpose reasoning model capable of robust task\nsequencing. Scene graphs, dynamically maintained within the framework, provide\nspatial awareness and enable consistent reasoning about the environment. The\nframework is evaluated through a series of tabletop robotic manipulation\nexperiments, and the results highlight its potential for building robotic\nmanipulation systems directly on top of off-the-shelf foundation models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5229\u7528\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6846\u67b6\uff0c\u65e0\u9700\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\uff0c\u901a\u8fc7\u96c6\u6210\u73b0\u6210\u6a21\u578b\u5b9e\u73b0\u591a\u6a21\u6001\u611f\u77e5\u548c\u9c81\u68d2\u4efb\u52a1\u5e8f\u5217\u63a8\u7406\u3002", "motivation": "\u65e8\u5728\u5229\u7528\u73b0\u6210\u7684\u57fa\u7840\u6a21\u578b\u6784\u5efa\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\uff0c\u907f\u514d\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\uff0c\u964d\u4f4e\u5f00\u53d1\u6210\u672c\u3002", "method": "\u96c6\u6210\u73b0\u6210\u6a21\u578b\uff0c\u7ed3\u5408\u57fa\u7840\u6a21\u578b\u7684\u591a\u6a21\u6001\u611f\u77e5\u80fd\u529b\u548c\u901a\u7528\u63a8\u7406\u6a21\u578b\u7684\u4efb\u52a1\u5e8f\u5217\u80fd\u529b\uff0c\u4f7f\u7528\u52a8\u6001\u7ef4\u62a4\u7684\u573a\u666f\u56fe\u63d0\u4f9b\u7a7a\u95f4\u611f\u77e5\u3002", "result": "\u901a\u8fc7\u684c\u9762\u673a\u5668\u4eba\u64cd\u4f5c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u76f4\u63a5\u57fa\u4e8e\u73b0\u6210\u57fa\u7840\u6a21\u578b\u6784\u5efa\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u8bc1\u660e\u4e86\u76f4\u63a5\u5229\u7528\u73b0\u6210\u57fa\u7840\u6a21\u578b\u6784\u5efa\u673a\u5668\u4eba\u64cd\u4f5c\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2510.27666", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2510.27666", "abs": "https://arxiv.org/abs/2510.27666", "authors": ["Dong Heon Han", "Xiaohao Xu", "Yuxi Chen", "Yusheng Zhou", "Xinqi Zhang", "Jiaqi Wang", "Daniel Bruder", "Xiaonan Huang"], "title": "Whole-Body Proprioceptive Morphing: A Modular Soft Gripper for Robust Cross-Scale Grasping", "comment": null, "summary": "Biological systems, such as the octopus, exhibit masterful cross-scale\nmanipulation by adaptively reconfiguring their entire form, a capability that\nremains elusive in robotics. Conventional soft grippers, while compliant, are\nmostly constrained by a fixed global morphology, and prior shape-morphing\nefforts have been largely confined to localized deformations, failing to\nreplicate this biological dexterity. Inspired by this natural exemplar, we\nintroduce the paradigm of collaborative, whole-body proprioceptive morphing,\nrealized in a modular soft gripper architecture. Our design is a distributed\nnetwork of modular self-sensing pneumatic actuators that enables the gripper to\nintelligently reconfigure its entire topology, achieving multiple morphing\nstates that are controllable to form diverse polygonal shapes. By integrating\nrich proprioceptive feedback from embedded sensors, our system can seamlessly\ntransition from a precise pinch to a large envelope grasp. We experimentally\ndemonstrate that this approach expands the grasping envelope and enhances\ngeneralization across diverse object geometries (standard and irregular) and\nscales (up to 10$\\times$), while also unlocking novel manipulation modalities\nsuch as multi-object and internal hook grasping. This work presents a low-cost,\neasy-to-fabricate, and scalable framework that fuses distributed actuation with\nintegrated sensing, offering a new pathway toward achieving biological levels\nof dexterity in robotic manipulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u8f6f\u4f53\u6293\u624b\u7684\u534f\u4f5c\u5168\u8eab\u672c\u4f53\u611f\u77e5\u5f62\u6001\u53d8\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u81ea\u611f\u77e5\u6c14\u52a8\u6267\u884c\u5668\u7f51\u7edc\u5b9e\u73b0\u667a\u80fd\u62d3\u6251\u91cd\u6784\uff0c\u80fd\u591f\u5f62\u6210\u591a\u79cd\u591a\u8fb9\u5f62\u5f62\u72b6\u5e76\u9002\u5e94\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\u548c\u5c3a\u5ea6\u7684\u7269\u4f53\u3002", "motivation": "\u53d7\u7ae0\u9c7c\u7b49\u751f\u7269\u7cfb\u7edf\u80fd\u591f\u81ea\u9002\u5e94\u91cd\u6784\u6574\u4e2a\u5f62\u6001\u7684\u542f\u53d1\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u8f6f\u4f53\u6293\u624b\u53d7\u9650\u4e8e\u56fa\u5b9a\u5168\u5c40\u5f62\u6001\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u7c7b\u4f3c\u751f\u7269\u7075\u5de7\u6027\u7684\u8de8\u5c3a\u5ea6\u64cd\u4f5c\u80fd\u529b\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u8f6f\u4f53\u6293\u624b\u67b6\u6784\uff0c\u7531\u5206\u5e03\u5f0f\u81ea\u611f\u77e5\u6c14\u52a8\u6267\u884c\u5668\u7f51\u7edc\u7ec4\u6210\uff0c\u901a\u8fc7\u4e30\u5bcc\u7684\u672c\u4f53\u611f\u77e5\u53cd\u9988\u5b9e\u73b0\u4ece\u7cbe\u786e\u634f\u53d6\u5230\u5927\u578b\u5305\u7edc\u6293\u53d6\u7684\u65e0\u7f1d\u8f6c\u6362\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6269\u5c55\u4e86\u6293\u53d6\u5305\u7edc\uff0c\u589e\u5f3a\u4e86\u4e0d\u540c\u51e0\u4f55\u5f62\u72b6\uff08\u6807\u51c6\u548c\u5f02\u5f62\uff09\u548c\u5c3a\u5ea6\uff08\u9ad8\u8fbe10\u500d\uff09\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u89e3\u9501\u4e86\u591a\u7269\u4f53\u548c\u5185\u90e8\u94a9\u6293\u7b49\u65b0\u578b\u64cd\u4f5c\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u6613\u5236\u9020\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u878d\u5408\u5206\u5e03\u5f0f\u9a71\u52a8\u4e0e\u96c6\u6210\u4f20\u611f\uff0c\u4e3a\u5b9e\u73b0\u673a\u5668\u4eba\u64cd\u4f5c\u8fbe\u5230\u751f\u7269\u7075\u5de7\u6027\u6c34\u5e73\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
